{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AIsquare/RAG/blob/main/RAG_APP_Using_LLamaindex_and_Mistral.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SroVkUFhAEXu"
      },
      "source": [
        "# For creating RAG application\n",
        "#This code can be divided into two main parts: Setting up and interacting with Weaviate and LlamaIndex and Creating and querying the LLM setup with HuggingFace models. Here's a brief explanation of each part:\n",
        "\n",
        "1. DATA\n",
        "2. Embedding Model\n",
        "3. DATA BASE\n",
        "4. PROMPT\n",
        "5. LLM\n",
        "\n",
        "Connect all the things"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsOdTqZm6NHK"
      },
      "source": [
        "1. huggingface hub using hf token(inferencing)\n",
        "2. load the model in local memory from the hugging face(finetune)\n",
        "\n",
        "here lots of memory is requierd(7b,10b,50b,100b)\n",
        "\n",
        "bitandbytes\n",
        "\n",
        "\n",
        "3. GGML(GPT-Generated Model Language), GGUF(GPT-Generated Unified Format)\n",
        "\n",
        "4. OLAMAM,LLAMA-CPP,LM-STUDIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfh2N71kA5xc",
        "outputId": "866b6ed8-efcc-4cac-e58b-8fb86a229fbc",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-llms-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub<0.24.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.23.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.10.37)\n",
            "Requirement already satisfied: text-generation<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers[torch]<5.0.0,>=4.37.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-huggingface) (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (4.11.0)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.1)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.30.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (9.4.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.14.1)\n",
            "Requirement already satisfied: pydantic<3,>2 in /usr/local/lib/python3.10/dist-packages (from text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (12.4.127)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.4.3)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (0.30.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]<5.0.0,>=4.37.0->llama-index-llms-huggingface) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (4.0.3)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.11)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.4.2)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>2->text-generation<0.8.0,>=0.7.0->llama-index-llms-huggingface) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<0.24.0,>=0.23.0->llama-index-llms-huggingface) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (6.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (3.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-llms-huggingface) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (0.16.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-llms-huggingface) (1.1.1)\n",
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.28)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.5)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.28 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.37)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.9)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.19)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.22)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: openai>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.30.1)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (1.6.1)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (2.31.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.28->llama-index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.28->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.11)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama-index) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.28->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.14.0->llama-index-agent-openai<0.3.0,>=0.1.4->llama-index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (6.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.28->llama-index) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.28->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.28->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.28->llama-index) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.28->llama-index) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index\n",
        "#!pip install llama-index==0.10.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1gJd8s_B16e",
        "outputId": "098230a4-55cb-40f3-8019-a946d62fec5e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: weaviate-client in /usr/local/lib/python3.10/dist-packages (4.5.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.30.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.31.0)\n",
            "Requirement already satisfied: httpx==0.27.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.27.0)\n",
            "Requirement already satisfied: validators==0.22.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (0.22.0)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (2.7.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.63.0)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.63.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client) (1.63.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.27.0->weaviate-client) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx==0.27.0->weaviate-client) (0.14.0)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client) (42.0.7)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client) (5.26.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client) (67.7.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.30.0->weaviate-client) (2.0.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx==0.27.0->weaviate-client) (1.2.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install weaviate-client"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Weaviate Setup:"
      ],
      "metadata": {
        "id": "VpxrGUnjwEQk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szntdLjo4X4a"
      },
      "outputs": [],
      "source": [
        "WEAVIATE_CLUSTER=\"https://aamir-iqbal-wr2uvybi.weaviate.network\"\n",
        "WEAVIATE_API_KEY=\"CLbmSREHHVRDdFbsdfaXV9FVYxzFi8fIWyFO6gb\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpQbtRJC6Xdo"
      },
      "outputs": [],
      "source": [
        "# WEAVIATE_CLUSTER=\"https://myliveclass-fwid7n3l.weaviate.network\"\n",
        "# WEAVIATE_API_KEY=\"FJCNGX11EfNzDgnhLeDuhgJzQRLyXGAbtJFF\"\n",
        "import weaviate\n",
        "client=weaviate.Client(url=WEAVIATE_CLUSTER,auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1oSzxFB8BOe"
      },
      "outputs": [],
      "source": [
        "client=weaviate.Client(url=WEAVIATE_CLUSTER,auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Data:\n",
        "## Add your knowledge based document here"
      ],
      "metadata": {
        "id": "2QLKIafrwIuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRipP6pv9AFv"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70xyvG9l9iVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a69ca7c-29a6-4b07-aece-dcb753ecc74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘Data’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL_xITuo9pqu"
      },
      "outputs": [],
      "source": [
        "loader=SimpleDirectoryReader('/content/Data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19PileNa-A4f"
      },
      "outputs": [],
      "source": [
        "documents=loader.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJwGY3mM-E0-",
        "outputId": "ba6753b5-593c-490e-e91b-ddc35ed26039"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(id_='35c48751-b987-44ea-97e1-1d9ef77358ca', embedding=None, metadata={'page_label': 'C1', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Regression \\nModeling \\nStrategiesFrank E. Harrell, Jr.\\nWith Applications to Linear Models, \\nLogistic and Ordinal Regression, \\nand Survival Analysis\\nSecond EditionSpringer Series in Statistics', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70ee3d39-85a0-4c99-b94c-a331bc2c54eb', embedding=None, metadata={'page_label': 'i', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Springer Series in Statistics\\nAdvisors:\\nP. Bickel, P. Diggle, S.E. Feinberg, U. Gather,\\nI. Olkin, S. Zeger\\nMore information about this series at http://www.springer.com/series/692', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b676792-9b1e-41ce-bea3-2fa4004470c6', embedding=None, metadata={'page_label': 'ii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='846c8095-5d7d-48e2-97fb-b2d8f146483e', embedding=None, metadata={'page_label': 'iii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Frank E. Harrell, Jr.\\nRegression Modeling\\nStrategies\\nWith Applications to Linear Models,\\nLogistic and Ordinal Regression,\\nand Survival Analysis\\nSecond Edition\\n123', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b32e004-83a4-40a8-b716-cf3ee6cb9cc4', embedding=None, metadata={'page_label': 'iv', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Frank E. Harrell, Jr.\\nDepartment of Biostatistics\\nSchool of Medicine\\nVanderbilt University\\nNashville, TN, USA\\nISSN 0172-7397 ISSN 2197-568X (electronic)\\nSpringer Series in Statistics\\nISBN 978-3-319-19424-0 ISBN 978-3-319-19425-7 (eBook)\\nDOI 10.1007/978-3-319-19425-7\\nLibrary of Congress Control Number: 2015942921\\nSpringer Cham Heidelberg New York Dordrecht London\\n©Springer Science+Business Media New York 2001\\n©Springer International Publishing Switzerland 2015\\nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of\\nthe material is concerned, speciﬁcally the rights of tr anslation, reprinting, reuse of illustrations, recitation,\\nbroadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information\\nstorage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology\\nnow known or hereafter developed.\\nThe use of general descriptive names, registere d names, trademarks, service marks, etc. in this publication\\ndoes not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant\\nprotective laws and regulations and the refore free for ge neral use.\\nThe publisher, the authors and the editors are safe to assume that the advice and information in this book\\nare believed to be true and accurate at the date of publication. Neither the publisher nor the authors or\\nthe editors give a warranty, express or implied, with respect to the material contained herein or for any\\nerrors or omissions that may have been made.\\nPrinted on acid-free paper\\nSpringer International Publishing AG Switzerland is part of Springer Science+Business Media ( www.\\nspringer.com )', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b3befea-9131-49d5-a1ac-6c42be96d84a', embedding=None, metadata={'page_label': 'v', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='To the memories of Frank E. Harrell, Sr.,\\nRichard Jackson, L. Richard Smith, John\\nBurdeshaw, and Todd Nick, and with\\nappreciation to Liana and Charlotte\\nHarrell, two high school math teachers:\\nCarolyn Wailes (n´ ee Gaston) and Floyd\\nChristian, two college professors: David\\nHurst (who advised me to choose the ﬁeld\\nof biostatistics) and Doug Stocks, and my\\ngraduate advisor P. K. Sen.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f9e873c1-009b-436b-8f45-6c65798d5296', embedding=None, metadata={'page_label': 'vi', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dce3729d-0c8b-4bbd-acb8-a3056f599a14', embedding=None, metadata={'page_label': 'vii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface\\nThere are many books that are excellent sources of knowledge about\\nindividual statistical tools (survival models, general linear models, etc.), but\\nthe art of data analysis is about choosing and using multiple tools. In the\\nwords of Chatﬁeld [ 100, p. 420]“ ...studentstypicallyknowthetechnicalde-\\ntails of regressionfor example, but not necessarily when and how to apply it.\\nThis argues the need for a better balance in the literature and in statistical\\nteaching between techniques and problem solving strategies .” Whether ana-\\nlyzing risk factors, adjusting for biases in observationalstudies, or developing\\npredictive models, there are common problems that few regression texts ad-\\ndress. For example, there are missing data in the majority of datasets one is\\nlikely to encounter (other than those used in textbooks!) but most regression\\ntexts do not include methods for dealing with such data eﬀectively, and most\\ntexts on missing data do not cover regression modeling.\\nThis book links standard regression modeling approaches with\\n•methods for relaxing linearity assumptions that still allow one to easily\\nobtain predictions and conﬁdence limits for future observations, and to do\\nformal hypothesis tests,\\n•non-additive modeling approaches not requiring the assumption that\\ninteractions are always linear ×linear,\\n•methods for imputing missing data and for penalizing variances for incom-\\nplete data,\\n•methods for handling large numbers of predictors without resorting to\\nproblematic stepwise variable selection techniques,\\n•data reduction methods (unsupervised learning methods, some of which\\nare based on multivariate psychometric techniques too seldom used in\\nstatistics)thathelpwiththeproblemof“toomanyvariablestoanalyzeand\\nnot enough observations”as well as making the model more interpretable\\nwhen there are predictor variables containing overlapping information,\\n•methods for quantifying predictive accuracy of a ﬁtted model,\\nvii', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='812149ac-b2b2-4c6d-9e21-cebf0f0c09f1', embedding=None, metadata={'page_label': 'viii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='viii Preface\\n•powerfulmodelvalidationtechniquesbasedonthebootstrapthatallowthe\\nanalyst to estimate predictive accuracy nearly unbiasedly without holding\\nback data from the model development process, and\\n•graphical methods for understanding complex models.\\nOn the last point, this text has special emphasis on what could be called\\n“presentation graphics for ﬁtted models” to help make regression analyses\\nmore palatable to non-statisticians. For example, nomograms have long been\\nused to make equations portable, but they are not drawn routinely because\\ndoing so is very labor-intensive. An Rfunction called nomogram in the package\\ndescribed below draws nomograms from a regression ﬁt, and these diagrams\\ncan be used to communicate modeling results as well as to obtain predicted\\nvalues manually even in the presence of complex variable transformations.\\nMost ofthe methods in this text apply to all regressionmodels, but special\\nemphasis is given to some of the most popular ones: multiple regressionusing\\nleast squares and its generalized least squares extension for serial (repeated\\nmeasurement) data, the binary logistic model, models for ordinal responses,\\nparametric survival regression models, and the Cox semiparametric survival\\nmodel. There is also a chapter on nonparametrictransform-both-sidesregres-\\nsion. Emphasis is given to detailed case studies for these methods as well as\\nfor data reduction, imputation, model simpliﬁcation, and other tasks. Ex-\\ncept for the case study on survival of Titanic passengers, all examples are\\nfrom biomedical research. However, the methods presented here have broad\\napplication to other areas including economics, epidemiology, sociology, psy-\\nchology, engineering, and predicting consumer behavior and other business\\noutcomes.\\nThis text is intended for Masters or PhD level graduate students who\\nhave had a general introductory probability and statistics course and who\\nare well versed in ordinary multiple regressionand intermediate algebra. The\\nbook is also intended to serve as a reference for data analysts and statistical\\nmethodologists. Readers without a strong background in applied statistics\\nmay wish to ﬁrst study one of the many introductory applied statistics and\\nregression texts that are available. The author’s course notes Biostatistics\\nfor Biomedical Research on the text’s web site covers basic regression and\\nmany other topics. The paper by Nick and Hardin [476]also provides a good\\nintroduction to multivariable modeling and interpretation. There are many\\nexcellent intermediate level texts on regression analysis. One of them is by\\nFox, which also has a companion software-based text [200,201]. For readers\\ninterested in medical or epidemiologic research, Steyerberg’s excellent text\\nClinical Prediction Models [586]isanidealcompanionfor Regression Modeling\\nStrategies . Steyerberg’s book provides further explanations, examples, and\\nsimulationsofmanyofthe methods presentedhere.Andno textonregression\\nmodeling should fail to mention the seminal work of John Nelder [450].\\nThe overall philosophy of this book is summarized by the following state-\\nments.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3911a954-42d7-48ad-b7fc-3dca882e996d', embedding=None, metadata={'page_label': 'ix', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface ix\\n•Satisfaction of model assumptions improves precision and increases statis-\\ntical power.\\n•It is moreproductive to make a model ﬁt step by step (e.g., transformation\\nestimation) than to postulate a simple model and ﬁnd out what went\\nwrong.\\n•Graphical methods should be married to formal inference.\\n•Overﬁtting occurs frequently, so data reduction and model validation are\\nimportant.\\n•In most researchprojects,the cost of data collectionfar outweighs the cost\\nof data analysis, so it is important to use the most eﬃcient and accurate\\nmodeling techniques, to avoid categorizing continuous variables, and to\\nnot remove data from the estimation sample just to be able to validate the\\nmodel.\\n•The bootstrap is a breakthrough for statistical modeling, and the analyst\\nshould use it for many steps of the modeling strategy, including deriva-\\ntion of distribution-free conﬁdence intervals and estimation of optimism\\nin model ﬁt that takes into account variations caused by the modeling\\nstrategy.\\n•Imputation of missing data is better than discarding incomplete observa-\\ntions.\\n•Variance often dominates bias, so bi ased methods such as penalized max-\\nimum likelihood estimation yield models that have a greater chance of\\naccurately predicting future observations.\\n•Software without multiple facilities for assessing and ﬁxing model ﬁt may\\nonly seem to be user-friendly.\\n•Carefully ﬁtting an improper model is better than badly ﬁtting (and over-\\nﬁtting) a well-chosen one.\\n•Methods thatworkforalltypesofregressionmodelsarethe mostvaluable.\\n•Using the data to guide the data analysis is almost as dangerous as not\\ndoing so.\\n•There are beneﬁts to modeling by deciding how many degrees of freedom\\n(i.e.,number ofregressionparameters)canbe“spent,”deciding wherethey\\nshould be spent, and then spending them.\\nOn the last point, the author believes that signiﬁcance tests and P-values\\nare problematic, especially when making modeling decisions. Judging by the\\nincreasedemphasisonconﬁdenceintervalsinscientiﬁcjournalsthereisreason\\nto believe that hypothesis testing is gradually being de-emphasized. Yet the\\nreader will notice that this text contains many P-values. How does that make\\nsense when, for example, the text recommends against simplifying a model\\nwhen a test of linearity is not signiﬁcant? First, some readers may wish to\\nemphasize hypothesis testing in general, and some hypotheses have special\\ninterest, such as in pharmacology where one may be interested in whether\\nthe eﬀect of a drug is linear in log dose. Second, many of the more interesting\\nhypothesis tests in the text are tests of complexity (nonlinearity, interaction)\\nof the overall model. Null hypotheses of linearity of eﬀects in particular are', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a12e6d9a-24f9-4df9-babb-c8503c268517', embedding=None, metadata={'page_label': 'x', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='x Preface\\nfrequently rejected, providing formal evidence that the analyst’s investment\\nof time to use more than simple statistical models was warranted.\\nThe rapid development of Bayesianmodeling methods and rise in their use\\nis exciting. Full Bayesian modeling greatly reduces the need for the approxi-\\nmations made for conﬁdence intervals and distributions of test statistics, and\\nBayesian methods formalize the still rather ad hoc frequentist approach to\\npenalized maximum likelihood estimation by using skeptical prior distribu-\\ntions to obtain well-deﬁned posterior distributions that automatically deal\\nwith shrinkage. The Bayesianapproachalso provides a formal mechanism for\\nincorporating information external to the data. Although Bayesian methods\\narebeyondthescopeofthistext, the textisBayesianinspiritbyemphasizing\\nthe careful use of subject matter expertise while building statistical models.\\nThe text emphasizes predictive modeling, but as discussed in Chapter 1,\\ndeveloping good predictions goes hand in hand with accurate estimation of\\neﬀects and with hypothesis testing (when appropriate). Besides emphasis\\non multivariable modeling, the text includes a Chapter 17introducing sur-\\nvival analysis and methods for analyzing various types of single and multiple\\ne v e n t s .T h i sb o o kd o e sn o tp r o v i d ee x a m p l e so fa n a l y s e so fo n ec o m m o n\\ntype of response variable, namely, cost and related measures of resource con-\\nsumption. However, least squares modeling presented in Chapter 15.1,t h e\\nrobust rank-based methods presented in Chapters 13,15,a n d20,a n dt h e\\ntransform-both-sides regression models discussed in Chapter 16are very ap-\\nplicable and robust for modeling economic outcomes. See [167]and[260]for\\nexample analyses of such dependent variables using, respectively, the Cox\\nmodel and nonparametric additive regression. The central Web site for this\\nbook (seethe Appendix) hasmuchmorematerialonthe use ofthe Coxmodel\\nfor analyzing costs.\\nThis text does not address some important study design issues that if not\\nrespected can doom a predictive modeling or estimation project to failure.\\nSee Laupacis, Sekar, and Stiell [378]for a list of some of these issues.\\nHeavy use is made of the S language used by R.Ris the focus because\\nit is an elegant object-oriented system in which it is easy to implement new\\nstatistical ideas. Many Rusers around the world have done so, and their work\\nhas beneﬁted many of the procedures described here. Ralso has a uniform\\nsyntaxforspecifyingstatisticalmodels(with respecttocategoricalpredictors,\\ninteractions, etc.), no matter which type of model is being ﬁtted [96].\\nThe free, open-source statistical software system Rhas been adopted by\\nanalysts and research statisticians worldwide. Its capabilities are growing\\nexponentially because of the involvement of an ever-growing community of\\nstatisticians who are adding new tools to the base Rsystem through con-\\ntributed packages. All of the functions used in this text are available in R.\\nSee the book’s Web site for updated information about software availability.\\nReaders who don’t use Ror any other statistical software environment will\\nstill ﬁnd the statistical methods and case studies in this text useful, and it is\\nhoped that the code that is presented will make the statistical methods more', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='864e9069-fe79-4212-a323-90c928c3e58a', embedding=None, metadata={'page_label': 'xi', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xi\\nconcrete. At the very least, the code demonstrates that all of the methods\\npresented in the text are feasible.\\nThis text does not teach analysts how to use R. For that, the reader may\\nwishtoseereadingrecommendationson www.r-project.org aswellasVenables\\nand Ripley [635](which is also an excellent companion to this text) and the\\nmany other excellent texts on R. See the Appendix for more information.\\nIn addition to powerful features that are built into R,t h i st e x tu s e sa\\npackage of freely available Rfunctions called rmswritten by the author. rms\\ntracks modeling details related to the expanded Xor design matrix. It is a\\nseries of over 200 functions for model ﬁtting, testing, estimation, validation,\\ngraphics, prediction, and typesetting by storing enhanced model design at-\\ntributes in the ﬁt. rmsincludes functions for least squares and penalized least\\nsquares multiple regression modeling in addition to functions for binary and\\nordinal regression, generalized least squares for analyzing serial data, quan-\\ntile regression, and survival analysis that are emphasized in this text. Other\\nfreely available miscellaneous Rfunctions used in the text are found in the\\nHmiscpackage also written by the author. Functions in Hmiscinclude facilities\\nfor data reduction, imputation, power and sample size calculation, advanced\\ntable making, recoding variables, importing and inspecting data, and general\\ngraphics. Consult the Appendix for information on obtaining Hmiscandrms.\\nThe author and his colleagues have written SAS macros for ﬁtting re-\\nstricted cubic splines and for other basic operations. See the Appendix for\\nmore information. It is unfair not to mention some excellent capabilities of\\nother statistical packages such as Stata (which has also been extended to\\nprovide regression splines and other modeling tools), but the extendability\\nand graphics of Rmakes it especially attractive for all aspects of the compre-\\nhensive modeling strategy presented in this book.\\nPortions of Chapters 4and20were published as reference [269].S o m eo f\\nChapter 13was published as reference [272].\\nThe author may be contacted by electronic mail at f.harrell@\\nvanderbilt.edu and would appreciate being informed of unclear points, er-\\nrors, and omissions in this book. Suggestions for improvementsand for future\\ntopics are also welcome. As described in the Web site, instructors may con-\\ntact the author to obtain copies of quizzes and extra assignments (both with\\nanswers)related to much of the materialin the earlierchapters,and to obtain\\nfull solutions (with graphical output) to the majority of assignments in the\\ntext.\\nMajor changes since the ﬁrst edition include the following:\\n1. Creation of a now mature Rpackage, rms, that replaces and greatly ex-\\ntends the Designlibrary used in the ﬁrst edition\\n2. Conversion of all of the book’s code to R\\n3. Conversion of the book source into knitr[677]reproducible documents\\n4. All code from the text is executable and is on the web site\\n5. Use of color graphics and use of the ggplot2graphics package [667]\\n6. Scanned images were re-drawn', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3f7bc45-2cec-477a-8176-969038b79cc3', embedding=None, metadata={'page_label': 'xii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xii Preface\\n7. New text about problems with dichotomization of continuous variables\\nand with classiﬁcation (as opposed to prediction)\\n8. Expanded material on multiple imputation and predictive mean match-\\ning and emphasis on multiple imputation (using the Hmisc aregImpute\\nfunction) instead of single imputation\\n9. Addition of redundancy analysis\\n10. Added a new section in Chapter 5on bootstrap conﬁdence intervals for\\nrankings of predictors\\n11. Replacement of the U.S. presidential election data with analyses of a new\\ndiabetes dataset from NHANES using ordinal and quantile regression\\n12. More emphasis on semiparametric ordinal regression models for contin-\\nuousY, as direct competitors of ordinary multiple regression, with a\\ndetailed case study\\n13. A new chapter on generalized least squares for analysis of serial response\\ndata\\n14. Thecasestudyinimputationanddatareductionwascompletelyreworked\\nand now focuses only on data reduction, with the addition of sparse prin-\\ncipal components\\n15. More information about indexes of predictive accuracy\\n16. Augmentation of the chapter on maximum likelihood to include more\\nﬂexible ways of testing contrasts as well as new methods for obtaining\\nsimultaneous conﬁdence intervals\\n17. Binary logistic regression case study 1 was completely re-worked, now\\nprovidingexamplesofmodel selectionand model approximationaccuracy\\n18. Single imputation was dropped from binary logistic case study 2\\n19. The case study in transform-both-sides regression modeling has been re-\\nworked using simulated data where true transformations are known, and\\na new example of the smearing estimator was added\\n20. Addition of 225 references, most of them published 2001–2014\\n21. New guidance on minimum sample sizes needed by some of the models\\n22. De-emphasis of bootstrap bumping [610]for obtaining simultaneous con-\\nﬁdence regions, in favor of a general multiplicity approach [307].\\nAcknowledgments\\nA good deal of the writing of the ﬁrst edition of this book was done durin g\\nmy 17 years on the faculty of Duke University. I wish to thank my close col-\\nleague Kerry Lee for providing many valuable ideas, fruitful collaborations,\\nand well-organized lecture notes from which I have greatly beneﬁted over the\\npastyears.TerryTherneauofMayoClinichasgivenmemanyofhiswonderful\\nideas for many years, and has written state-of-the-art Rsoftware for survival\\nanalysis that forms the core of survival analysis software in my rmspackage.\\nMichael Symons of the Department of Biostatisticsof the University of North', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0d487aeb-f1b8-491d-9a91-dfcfaa09a5c5', embedding=None, metadata={'page_label': 'xiii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Preface xiii\\nCarolinaat ChapelHill and TimothyMorganofthe DivisionofPublicHealth\\nSciences at Wake Forest University School of Medicine also provided course\\nmaterials, some of which motivated portions of this text. My former clini-\\ncal colleagues in the Cardiology Division at Duke University, Robert Caliﬀ,\\nPhillip Harris, Mark Hlatky, Dan Mark, David Pryor, and Robert Rosati,\\nfor many years provided valuable motivation, feedback, and ideas through\\nour interaction on clinical problems. Besides Kerry Lee, statistical colleagues\\nL. Richard Smith, Lawrence Muhlbaier, and Elizabeth DeLong clariﬁed my\\nthinking and gave me new ideas on numerous occasions. Charlotte Nelson\\nand Carlos Alzola frequently helped me debug S routines when they thought\\nthey were just analyzing data.\\nFormer students Bercedis Peterson, James Herndon, Robert McMahon,\\nandYuan-LiShenhaveprovidedmanyinsightsintologisticandsurvivalmod-\\neling. Associations with Doug Wagner and William Knaus of the University\\nof Virginia, Ken Oﬀord of Mayo Clinic, David Naftel of the University of Al-\\nabama in Birmingham, Phil Miller of Washington University, and Phil Good-\\nman ofthe UniversityofNevadaRenohaveprovidedmanyvaluableideasand\\nmotivations for this work, as have Michael Schemper of Vienna University,\\nJanez Stare of Ljubljana University, Slovenia, Ewout Steyerberg of Erasmus\\nUniversity, Rotterdam, Karel Moons of Utrecht University, and Drew Levy of\\nGenentech. Richard Goldstein, along with several anonymous reviewers, pro-\\nvided many helpful criticisms of a p revious version of this manuscript that\\nresulted in signiﬁcant improvements, and critical reading by Bob Edson (VA\\nCooperative Studies Program, Palo Alto) resulted in many error corrections.\\nThanks to Brian Ripley of the University of Oxford for providing many help-\\nful software tools and statistical insights that greatly aided in the produ ction\\nof this book, and to Bill Venables of CSIRO Australia for wisdom, both sta-\\ntistical and otherwise. This work would also not have been possible without\\nthe S environment developed by Rick Becker, John Chambers, Allan Wilks,\\nand the Rlanguage developed by Ross Ihaka and Robert Gentleman.\\nWork for the second edition was done in the excellent academic environ-\\nment of Vanderbilt University, where biostatistical and biomedical colleagues\\nand graduate students provided new insights and stimulating discussions.\\nThanks to Nick Cox, Durham University, UK, who provided from his careful\\nreading of the ﬁrst edition a very large number of improvements and correc-\\ntions that were incorporated into the second. Four anonymous reviewers of\\nthe second edition also made numerous suggestions that improved the text.\\nNashville, TN, USA Frank E. Harrell, Jr.\\nJuly 2015', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='357a62c0-5af2-4e75-8d62-a7a5f81de55e', embedding=None, metadata={'page_label': 'xiv', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3e9e80f-1360-4e5b-84e0-beaedaf9a367', embedding=None, metadata={'page_label': 'xv', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents\\nTypographical Conventions ...................................xxv\\n1 Introduction .............................................. 1\\n1.1 Hypothesis Testing, Estimation, and Prediction ........... 1\\n1.2 Examples of Uses of Predictive Multivariable Modeling ..... 3\\n1.3 Prediction vs. Classiﬁcation ............................. 4\\n1.4 Planning for Modeling ................................. 6\\n1.4.1 Emphasizing Continuous Variables ............... 8\\n1.5 Choice of the Model ................................... 8\\n1.6 Further Reading ....................................... 11\\n2 General Aspects of Fitting Regression Models ............ 13\\n2.1 Notation for Multivariable Regression Models ............. 13\\n2.2 Model Formulations ................................... 14\\n2.3 Interpreting Model Parameters .......................... 15\\n2.3.1 Nominal Predictors ............................. 16\\n2.3.2 Interactions .................................... 16\\n2.3.3 Example: Inference for a Simple Model ............ 17\\n2.4 Relaxing Linearity Assumption for Continuous Predictors .. 18\\n2.4.1 Avoiding Categorization ......................... 18\\n2.4.2 Simple Nonlinear Terms ......................... 21\\n2.4.3 Splines for Estimating Shape of Regression\\nFunction and Determining Predictor\\nTransformations ................................ 22\\n2.4.4 Cubic Spline Functions .......................... 23\\n2.4.5 Restricted Cubic Splines ........................ 24\\n2.4.6 Choosing Number and Position of Knots .......... 26\\n2.4.7 Nonparametric Regression ....................... 28\\n2.4.8 Advantages of Regression Splines over\\nOther Methods ................................. 30\\nxv', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2861a512-964e-410b-89a0-9baa3ede3be1', embedding=None, metadata={'page_label': 'xvi', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xvi Contents\\n2.5 Recursive Partitioning: Tree-Based Models ................ 30\\n2.6 Multiple Degree of Freedom Tests of Association .......... 31\\n2.7 Assessment of Model Fit ............................... 33\\n2.7.1 Regression Assumptions ......................... 33\\n2.7.2 Modeling and Testing Complex Interactions ....... 36\\n2.7.3 Fitting Ordinal Predictors ....................... 38\\n2.7.4 Distributional Assumptions ...................... 39\\n2.8 Further Reading ....................................... 40\\n2.9 Problems ............................................. 42\\n3 Missing Data ............................................. 45\\n3.1 Types of Missing Data ................................. 45\\n3.2 Prelude to Modeling ................................... 46\\n3.3 Missing Values for Diﬀerent Types of Response Variables ... 47\\n3.4 Problems with Simple Alternatives to Imputation ......... 47\\n3.5 Strategies for Developing an Imputation Model ............ 49\\n3.6 Single Conditional Mean Imputation ..................... 52\\n3.7 Predictive Mean Matching .............................. 52\\n3.8 Multiple Imputation ................................... 53\\n3.8.1 The aregImpute and Other Chained Equations\\nApproaches .................................... 55\\n3.9 Diagnostics ........................................... 56\\n3.10 Summary and Rough Guidelines ......................... 56\\n3.11 Further Reading ....................................... 58\\n3.12 Problems ............................................. 59\\n4 Multivariable Modeling Strategies ........................ 63\\n4.1 Prespeciﬁcation of Predictor Complexity Without\\nLater Simpliﬁcation .................................... 64\\n4.2 Checking Assumptions of Multiple Predictors\\nSimultaneously ........................................ 67\\n4.3 Variable Selection ..................................... 67\\n4.4 Sample Size, Overﬁtting, and Limits on Number\\nof Predictors .......................................... 72\\n4.5 Shrinkage ............................................ 75\\n4.6 Collinearity ........................................... 78\\n4.7 Data Reduction ....................................... 79\\n4.7.1 Redundancy Analysis ........................... 80\\n4.7.2 Variable Clustering ............................. 81\\n4.7.3 Transformation and Scaling Variables Without\\nUsingY....................................... 81\\n4.7.4 Simultaneous Transformation and Imputation ...... 83\\n4.7.5 Simple Scoring of Variable Clusters ............... 85\\n4.7.6 Simplifying Cluster Scores ....................... 87\\n4.7.7 How Much Data Reduction Is Necessary? ......... 87', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ae708c3-a370-482d-8923-94cdbac98fe0', embedding=None, metadata={'page_label': 'xvii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xvii\\n4.8 Other Approaches to Predictive Modeling ................ 89\\n4.9 Overly Inﬂuential Observations .......................... 90\\n4.10 Comparing Two Models ................................ 92\\n4.11 Improving the Practice of Multivariable Prediction ........ 94\\n4.12 Summary: Possible Modeling Strategies .................. 94\\n4.12.1 Developing Predictive Models .................... 95\\n4.12.2 Developing Models for Eﬀect Estimation .......... 98\\n4.12.3 Developing Models for Hypothesis Testing ......... 99\\n4.13 Further Reading .......................................100\\n4.14 Problems .............................................102\\n5 Describing, Resampling, Validating, and Simplifying\\nthe Model ................................................103\\n5.1 Describing the Fitted Model ............................103\\n5.1.1 Interpreting Eﬀects .............................103\\n5.1.2 Indexes of Model Performance ...................104\\n5.2 The Bootstrap ........................................106\\n5.3 Model Validation ......................................109\\n5.3.1 Introduction ...................................109\\n5.3.2 Which Quantities Should Be Used in Validation? ... 110\\n5.3.3 Data-Splitting .................................111\\n5.3.4 Improvements on Data-Splitting: Resampling ......112\\n5.3.5 Validation Using the Bootstrap ..................114\\n5.4 Bootstrapping Ranks of Predictors .......................117\\n5.5 Simplifying the Final Model by Approximating It ..........118\\n5.5.1 Diﬃculties Using Full Models ....................118\\n5.5.2 Approximating the Full Model ...................119\\n5.6 Further Reading .......................................121\\n5.7 Problem ..............................................124\\n6RSoftware ................................................127\\n6.1 The RModeling Language ..............................128\\n6.2 User-Contributed Functions .............................129\\n6.3 The rmsPackage ......................................130\\n6.4 Other Functions .......................................141\\n6.5 Further Reading .......................................142\\n7 Modeling Longitudinal Responses using Generalized\\nLeast Squares .............................................143\\n7.1 Notation and Data Setup ...............................143\\n7.2 Model Speciﬁcation for Eﬀects on E(Y)..................144\\n7.3 Modeling Within-Subject Dependence ....................144\\n7.4 Parameter Estimation Procedure ........................147\\n7.5 Common Correlation Structures .........................147\\n7.6 Checking Model Fit ....................................148', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4b5623f-3aa0-44d4-a1fa-9001cdd190fc', embedding=None, metadata={'page_label': 'xviii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xviii Contents\\n7.7 Sample Size Considerations .............................148\\n7.8RSoftware ............................................149\\n7.9 Case Study ...........................................149\\n7.9.1 Graphical Exploration of Data ...................150\\n7.9.2 Using Generalized Least Squares .................151\\n7.10 Further Reading .......................................158\\n8 Case Study in Data Reduction ............................161\\n8.1 Data .................................................161\\n8.2 How Many Parameters Can Be Estimated? ...............164\\n8.3 Redundancy Analysis ..................................164\\n8.4 Variable Clustering ....................................166\\n8.5 Transformation and Single Imputation Using transcan ..... 167\\n8.6 Data Reduction Using Principal Components .............170\\n8.6.1 Sparse Principal Components ....................175\\n8.7 Transformation Using Nonparametric Smoothers ..........176\\n8.8 Further Reading .......................................177\\n8.9 Problems .............................................178\\n9 Overview of Maximum Likelihood Estimation ............181\\n9.1 General Notions—Simple Cases .........................181\\n9.2 Hypothesis Tests ......................................185\\n9.2.1 Likelihood Ratio Test ...........................185\\n9.2.2 Wald Test .....................................186\\n9.2.3 Score Test .....................................186\\n9.2.4 Normal Distribution—One Sample ...............187\\n9.3 General Case .........................................188\\n9.3.1 Global Test Statistics ...........................189\\n9.3.2 Testing a Subset of the Parameters ...............190\\n9.3.3 Tests Based on Contrasts ........................192\\n9.3.4 Which Test Statistics to Use When ...............193\\n9.3.5 Example: Binomial—Comparing Two\\nProportions ....................................194\\n9.4 Iterative ML Estimation ................................195\\n9.5 Robust Estimation of the Covariance Matrix ..............196\\n9.6 Wald, Score, and Likelihood-Based Conﬁdence Intervals ....198\\n9.6.1 Simultaneous Wald Conﬁdence Regions ...........199\\n9.7 Bootstrap Conﬁdence Regions ...........................199\\n9.8 Further Use of the Log Likelihood .......................203\\n9.8.1 Rating Two Models, Penalizing for Complexity ..... 203\\n9.8.2 Testing Whether One Model Is Better\\nthan Another ..................................204\\n9.8.3 Unitless Index of Predictive Ability ...............205\\n9.8.4 Unitless Index of Adequacy of a Subset\\nof Predictors ...................................207\\n9.9 Weighted Maximum Likelihood Estimation ...............208\\n9.10 Penalized Maximum Likelihood Estimation ...............209', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca1a5fc1-48fb-4a10-a077-373911488254', embedding=None, metadata={'page_label': 'xix', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xix\\n9.11 Further Reading .......................................213\\n9.12 Problems .............................................216\\n10 Binary Logistic Regression ................................219\\n10.1 Model ................................................219\\n10.1.1 Model Assumptions and Interpretation\\nof Parameters ..................................221\\n10.1.2 Odds Ratio, Risk Ratio, and Risk Diﬀerence .......224\\n10.1.3 Detailed Example ..............................225\\n10.1.4 Design Formulations ............................230\\n10.2 Estimation ...........................................231\\n10.2.1 Maximum Likelihood Estimates ..................231\\n10.2.2 Estimation of Odds Ratios and Probabilities .......232\\n10.2.3 Minimum Sample Size Requirement ..............233\\n10.3 Test Statistics .........................................234\\n10.4 Residuals .............................................235\\n10.5 Assessment of Model Fit ...............................236\\n10.6 Collinearity ...........................................255\\n10.7 Overly Inﬂuential Observations ..........................255\\n10.8 Quantifying Predictive Ability ..........................256\\n10.9 Validating the Fitted Model ............................259\\n10.10 Describing the Fitted Model ............................264\\n10.11RFunctions ...........................................269\\n10.12 Further Reading .......................................271\\n10.13 Problems .............................................273\\n11 Binary Logistic Regression Case Study 1 .................275\\n11.1 Overview .............................................275\\n11.2 Background ...........................................275\\n11.3 Data Transformations and Single Imputation .............276\\n11.4 Regression on Original Variables, Principal Components\\nand Pretransformations ................................277\\n11.5 Description of Fitted Model .............................278\\n11.6 Backwards Step-Down .................................280\\n11.7 Model Approximation ..................................287\\n12 Logistic Model Case Study 2: Survival of Titanic\\nPassengers ................................................291\\n12.1 Descriptive Statistics ...................................291\\n12.2 Exploring Trends with Nonparametric Regression ..........294\\n12.3 Binary Logistic Model With Casewise Deletion\\nof Missing Values ......................................296\\n12.4 Examining Missing Data Patterns .......................302\\n12.5 Multiple Imputation ...................................304\\n12.6 Summarizing the Fitted Model ..........................307', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70bb72a6-4346-4971-acf6-4e206e2860e6', embedding=None, metadata={'page_label': 'xx', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xx Contents\\n13 Ordinal Logistic Regression ...............................311\\n13.1 Background ...........................................311\\n13.2 Ordinality Assumption .................................312\\n13.3 Proportional Odds Model ...............................313\\n13.3.1 Model ........................................313\\n13.3.2 Assumptions and Interpretation of Parameters .....313\\n13.3.3 Estimation ....................................314\\n13.3.4 Residuals ......................................314\\n13.3.5 Assessment of Model Fit ........................315\\n13.3.6 Quantifying Predictive Ability ...................318\\n13.3.7 Describing the Fitted Model .....................318\\n13.3.8 Validating the Fitted Model .....................318\\n13.3.9 RFunctions ....................................319\\n13.4 Continuation Ratio Model ..............................319\\n13.4.1 Model ........................................319\\n13.4.2 Assumptions and Interpretation of Parameters .....320\\n13.4.3 Estimation ....................................320\\n13.4.4 Residuals ......................................321\\n13.4.5 Assessment of Model Fit ........................321\\n13.4.6 Extended CR Model ............................321\\n13.4.7 Role of Penalization in Extended CR Model .......322\\n13.4.8 Validating the Fitted Model .....................322\\n13.4.9 RFunctions ....................................323\\n13.5 Further Reading .......................................324\\n13.6 Problems .............................................324\\n14 Case Study in Ordinal Regression, Data Reduction,\\nand Penalization ..........................................327\\n14.1 Response Variable .....................................328\\n14.2 Variable Clustering ....................................329\\n14.3 Developing Cluster Summary Scores .....................330\\n14.4 Assessing Ordinality of Yfor each X,a n dU n a d j u s t e d\\nChecking of PO and CR Assumptions ....................333\\n14.5 A Tentative Full Proportional Odds Model ...............333\\n14.6 Residual Plots ........................................336\\n14.7 Graphical Assessment of Fit of CR Model ................338\\n14.8 Extended Continuation Ratio Model .....................340\\n14.9 Penalized Estimation ..................................342\\n14.10 Using Approximations to Simplify the Model .............348\\n14.11 Validating the Model ..................................353\\n14.12 Summary .............................................355\\n14.13 Further Reading .......................................356\\n14.14 Problems .............................................357', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b26ddfea-d379-4e16-8fb4-a60e3d991238', embedding=None, metadata={'page_label': 'xxi', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xxi\\n15 Regression Models for Continuous Yand Case Study\\nin Ordinal Regression .....................................359\\n15.1 The Linear Model .....................................359\\n15.2 Quantile Regression ....................................360\\n15.3 Ordinal Regression Models for Continuous Y..............361\\n15.3.1 Minimum Sample Size Requirement ..............363\\n15.4 Comparison of Assumptions of Various Models ............364\\n15.5 Dataset and Descriptive Statistics .......................365\\n15.5.1 Checking Assumptions of OLS and Other Models ... 368\\n15.6 Ordinal Regression Applied to HbA 1c....................370\\n15.6.1 Checking Fit for Various Models Using Age ........370\\n15.6.2 Examination of BMI ............................374\\n15.6.3 Consideration of All Body Size Measurements ...... 375\\n16 Transform-Both-Sides Regression .........................389\\n16.1 Background ...........................................389\\n16.2 Generalized Additive Models ............................390\\n16.3 Nonparametric Estimation of Y-Transformation ...........390\\n16.4 Obtaining Estimates on the Original Scale ................391\\n16.5RFunctions ...........................................392\\n16.6 Case Study ...........................................393\\n17 Introduction to Survival Analysis .........................399\\n17.1 Background ...........................................399\\n17.2 Censoring, Delayed Entry, and Truncation ................401\\n17.3 Notation, Survival, and Hazard Functions ................402\\n17.4 Homogeneous Failure Time Distributions .................407\\n17.5 Nonparametric Estimation of SandΛ...................409\\n17.5.1 Kaplan–Meier Estimator ........................409\\n17.5.2 Altschuler–Nelson Estimator .....................413\\n17.6 Analysis of Multiple Endpoints ..........................413\\n17.6.1 Competing Risks ...............................414\\n17.6.2 Competing Dependent Risks .....................414\\n17.6.3 State Transitions and Multiple Types of Nonfatal\\nEvents........................................416\\n17.6.4 Joint Analysis of Tim e and Severity of an Event .... 417\\n17.6.5 Analysis of Multiple Events ......................417\\n17.7RFunctions ...........................................418\\n17.8 Further Reading .......................................420\\n17.9 Problems .............................................421\\n18 Parametric Survival Models ..............................423\\n18.1 Homogeneous Models (No Predictors) ....................423\\n18.1.1 Speciﬁc Models ................................423\\n18.1.2 Estimation ....................................424\\n18.1.3 Assessment of Model Fit ........................426', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0b2c56e-d309-4813-aca7-5ee8775b1a05', embedding=None, metadata={'page_label': 'xxii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='xxii Contents\\n18.2 Parametric Proportional Hazards Models .................427\\n18.2.1 Model ........................................427\\n18.2.2 Model Assumptions and Interpretation\\nof Parameters ..................................428\\n18.2.3 Hazard Ratio, Risk Ratio, and Risk Diﬀerence .....430\\n18.2.4 Speciﬁc Models ................................431\\n18.2.5 Estimation ....................................432\\n18.2.6 Assessment of Model Fit ........................434\\n18.3 Accelerated Failure Time Models ........................436\\n18.3.1 Model ........................................436\\n18.3.2 Model Assumptions and Interpretation\\nof Parameters ..................................436\\n18.3.3 Speciﬁc Models ................................437\\n18.3.4 Estimation ....................................438\\n18.3.5 Residuals ......................................440\\n18.3.6 Assessment of Model Fit ........................440\\n18.3.7 Validating the Fitted Model .....................446\\n18.4 Buckley–James Regression Model ........................447\\n18.5 Design Formulations ...................................447\\n18.6 Test Statistics .........................................447\\n18.7 Quantifying Predictive Ability ..........................447\\n18.8 Time-Dependent Covariates .............................447\\n18.9RFunctions ...........................................448\\n18.10 Further Reading .......................................450\\n18.11 Problems .............................................451\\n19 Case Study in Parametric Survival Modeling and Model\\nApproximation ...........................................453\\n19.1 Descriptive Statistics ...................................453\\n19.2 Checking Adequacy of Log-Normal Accelerated Failure\\nTime Model ..........................................458\\n19.3 Summarizing the Fitted Model ..........................466\\n19.4 Internal Validation of the Fitted Model Using\\nthe Bootstrap .........................................466\\n19.5 Approximating the Full Model ..........................469\\n19.6 Problems .............................................473\\n20 Cox Proportional Hazards Regression Model .............475\\n20.1 Model ................................................475\\n20.1.1 Preliminaries ..................................475\\n20.1.2 Model Deﬁnition ...............................476\\n20.1.3 Estimation of β................................476\\n20.1.4 Model Assumptions and Interpretation\\nof Parameters ..................................478\\n20.1.5 Example ......................................478', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9175552a-0078-4627-8a4a-c1c6246e4f56', embedding=None, metadata={'page_label': 'xxiii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Contents xxiii\\n20.1.6 Design Formulations ............................480\\n20.1.7 Extending the Model by Stratiﬁcation ............481\\n20.2 Estimation of Survival Probability and Secondary\\nParameters ...........................................483\\n20.3 Sample Size Considerations .............................486\\n20.4 Test Statistics .........................................486\\n20.5 Residuals .............................................487\\n20.6 Assessment of Model Fit ...............................487\\n20.6.1 Regression Assumptions .........................487\\n20.6.2 Proportional Hazards Assumption ................494\\n2 0 . 7 W h a tt oD oW h e nP HF a i l s ............................501\\n20.8 Collinearity ...........................................503\\n20.9 Overly Inﬂuential Observations ..........................504\\n20.10 Quantifying Predictive Ability ..........................504\\n20.11 Validating the Fitted Model ............................506\\n20.11.1 Validation of Model Calibration ..................506\\n20.11.2 Validation of Discrimination and Other Statistical\\nIndexes .......................................507\\n20.12 Describing the Fitted Model ............................509\\n20.13RFunctions ...........................................513\\n20.14 Further Reading .......................................517\\n21 Case Study in Cox Regression ............................521\\n21.1 Choosing the Number of Parameters and Fitting\\nthe Model ............................................521\\n21.2 Checking Proportional Hazards .........................525\\n21.3 Testing Interactions ....................................527\\n21.4 Describing Predictor Eﬀects ............................527\\n21.5 Validating the Model ..................................529\\n21.6 Presenting the Model ..................................530\\n21.7 Problems .............................................531\\nA Datasets, RPackages, and Internet Resources .............535\\nReferences ....................................................539\\nIndex.........................................................571', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ad488775-fa9e-434a-8e6a-3e981294826c', embedding=None, metadata={'page_label': 'xxiv', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d994872-ceb0-49e3-87a9-3d8f1f1e0a5c', embedding=None, metadata={'page_label': 'xxv', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Typographical Conventions\\nBoxed numbers in the margins such as 1correspond to numbers at the end\\nof chapters in sections named “Further Reading.” Bracketed numbers and\\nnumeric superscripts in the text refer to the bibliography, while alphabetic\\nsuperscripts indicate footnotes.\\nRlanguage commands and names of Rfunctions and packages are set in\\ntypewriter font , as are most variable names.\\nRcode blocks are set oﬀ with a shadowbox, and Routput that is not directly\\nusing L ATEX appears in a box that is framed on three sides.\\nIn the S language upon which Ris based, x←yis read“ xgets the value of\\ny.”The assignment operator ←, used in the text for aesthetic reasons (as are\\n≤and≥), is entered by the user as <-. Comments begin with #, subscripts\\nuse brackets ([ ]), and the missing value is denoted by NA(not available).\\nIn ordinary text and mathematical exp ressions, [logical vari able] and [logical\\nexpression] imply a value of 1 if the logi cal variable or expression is true, and\\n0o t h e r w i s e .\\nxxv', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc3d8978-b087-458a-9e5c-34e2b23aa732', embedding=None, metadata={'page_label': '1', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 1\\nIntroduction\\n1.1 Hypothesis Testing, Estimation, and Prediction\\nStatistics comprises among other areas study design, hypothesis testing,\\nestimation, and prediction. This text aims at the last area, by presenting\\nmethods that enable an analyst to develop models that will make accurate\\npredictions of responses for futureobservations. Prediction could be consid-\\neredasupersetofhypothesistestingandestimation,sothemethodspresented\\nhere will also assist the analyst in those areas. It is worth pausing to explain\\nhow this is so.\\nIn traditional hypothesis testing one often chooses a null hypothesis de-\\nﬁned as the absence of some eﬀect. For example, in testing whether a vari-\\nable such as cholesterol is a risk factor for sudden death, one might test the\\nnull hypothesis that an increase in cholesterol does not increase the risk of\\ndeath.Hypothesistestingcaneasilybedonewithinthecontextofastatistical\\nmodel, but a model is not required. When one only wishes to assess whether\\nan eﬀect is zero, P-values may be computed using permutation or rank (non-\\nparametric) tests while making only minimal assumptions. But there are still\\nreasonsfor preferringa model-basedapproac hovertechniques that only yield\\nP-values.\\n1. Permutation and rank tests do not easily give rise to estimates of magni-\\ntudesof eﬀects.\\n2. These tests cannot be readily extended to incorporate complexities such\\nas cluster sampling or repeated measurements within subjects.\\n3. Oncethe analystis familiarwith a model, that modelmay be usedto carry\\nout many diﬀerent statistical tests; there is no need to learn speciﬁc for-\\nmulas to handle the specia l cases. The two-sample t-test is a special case\\nof the ordinary multiple regression model having as its sole Xvariable\\na dummy variable indicating group membership. The Wilcoxon-Mann-\\nWhitney test is a special case of the proportional odds ordinal logistic\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b279e18b-24a0-4713-ae5e-d0f3a1ea9277', embedding=None, metadata={'page_label': '2', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2 1 Introduction\\nmodel.664The analysis of variance (multiple group) test and the Kruskal–\\nWallis test can easily be obtained from these two regression models by\\nusing more than one dummy predictor variable.\\nEven without complexities such as repeated measurements, problems can\\narise when many hypotheses are to be tested. Testing too many hypotheses\\nis relatedto ﬁtting too many predictorsin a regressionmodel. One commonly\\nhears the statement that“the dataset was too small to allow modeling, so we\\njust did hypothesis tests.”It is unlikely that the resulting inferences would be\\nreliable. If the sample size is insuﬃcient for modeling it is often insuﬃcient\\nfor tests or estimation. This is especially true when one desires to publish\\nan estimate of the eﬀect corresponding to the hypothesis yielding the small-\\nestP-value. Ordinary point estimates are known to be badly biased when\\nthe quantity to be estimated was determined by “data dredging.” This can\\nbe remedied by the same kind of shrinkage used in multivariable modeling\\n(Section 9.10).\\nStatisticalestimationisusuallymodel-based.Forexample,onemightusea\\nsurvival regression model to estimate t he relative eﬀect of increasing choles-\\nterol from 200 to 250 mg/dl on the hazard of death. Variables other than\\ncholesterol may also be in the regression model, to allow estimation of the\\neﬀect of increasing cholesterol, holding other risk factors constant. But ac-\\ncurate estimation of the cholesterol eﬀect will depend on how cholesterol as\\nwell as each of the adjustment variables is assumed to relate to the hazard\\nof death. If linear relationships are incorrectly assumed, estimates will be\\ninaccurate. Accurate estimation also depends on avoiding overﬁtting the ad-\\njustment variables.If the datasetcontains 200subjects,30of whomdied, and\\nif one adjusted for 15“confounding”variables, the estimates would be“over-\\nadjusted”for the eﬀects of the 15 variables, as some of their apparent eﬀects\\nwould actually result from spurious associations with the response variable\\n(time until death). The overadjustment would reduce the cholesterol eﬀect.\\nThe resulting unreliability of estimates equals the degree to which the overall\\nmodel fails to validate on an independent sample.\\nIt is often useful to think of eﬀect estimates as diﬀerences between two\\npredicted values from a model. This way, one can account for nonlinearities\\nand interactions. For example, if cholesterol is represented nonlinearly in a\\nlogistic regression model, predicted values on the“linear combination of X’s\\nscale”arepredictedlogoddsofanevent.Theincreaseinlogoddsfromraising\\ncholesterol from 200 to 250 mg/dl is the diﬀerence in predicted values, where\\ncholesterol is set to 250 and then to 200, and all other variables are held\\nconstant. The point estimate of the 250:200 mg/dl odds ratio is the anti-log\\nof this diﬀerence. If cholesterol is represented nonlinearly in the model, it\\ndoes not matter how many terms in the model involve cholesterol as long as\\nthe overall predicted values are obtained.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e05f276a-1c6f-484a-b3c9-1467a4ef8526', embedding=None, metadata={'page_label': '3', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.2 Examples of Uses of Predictive Multivariable Modeling 3\\nThus when one develops a reasonable mult ivariable predictive model, hy-\\npothesis testing and estimation of eﬀects are byproducts of the ﬁtted model.\\nSo predictivemodelingis often desirableevenwhen predictionis notthe main\\ngoal.\\n1.2 Examples of Uses of Predictive Multivariable\\nModeling\\nThere is an endless variety of uses for multivariable models. Predictive mod-\\nels have long been used in business to forecast ﬁnancial performance and\\nto model consumer purchasing and loan pay-back behavior. In ecology, re-\\ngression models are used to predict the probability that a ﬁsh species will\\ndisappear from a lake. Survival models have been used to predict product\\nlife (e.g., time to burn-out of an mechanical part, time until saturation of a\\ndisposable diaper). Models are commonly used in discrimination litigation in\\nan attempt to determine whether race or sex is used as the basis for hiring\\nor promotion, after taking other personnel characteristics into account.\\nMultivariable models are used extensively in medicine, epidemiology, bio-\\nstatistics, health services research, pharmaceutical research, and related\\nﬁelds. The author has worked primarily in these ﬁelds, so most of the ex-\\namples in this text come from those areas. In medicine, two of the major\\nareas of application are diagnosis and prognosis. There models are used to\\npredict the probability that a certain type of patient will be shown to have a\\nspeciﬁc disease, or to predict the time course of an already diagnosed disease.\\nIn observational studies in which one desires to compare patient outcomes\\nbetween two or more treatments, multivariable modeling is very important\\nbecause of the biases caused by nonrandom treatment assignment. Here the\\nsimultaneouseﬀects ofseveraluncontrolledvariablesmustbe controlled(held\\nconstant mathematically if using a regression model) so that the eﬀect of the\\nfactor of interest can be more purely estimated. A newer technique for more\\naggressively adjusting for nonrandom treatment assignment, the propensity\\nscore,116,530providesyet another opportunityfor multivariablemodeling (see\\nSection10.1.4). The propensity score is merely the predicted value from a\\nmultivariable model where the response variable is the exposure or the treat-\\nment actually used. The estimated propensity score is then used in a second\\nstep as an adjustment variable in the model for the response of interest.\\nIt is not widely recognized that multivariable modeling is extremely valu-\\nable even in well-designed randomized experiments. Such studies are often\\ndesigned to make relativecomparisons of two or more treatments, using odds\\nratios, hazard ratios, and other measure s of relative eﬀects. But to be able\\nto estimate absolute eﬀects one must develop a multivariable model of the\\nresponse variable. This model can predict, for example, the probability that a\\npatient on treatmentA with characteristics Xwill surviveﬁveyears,orit can', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0697d6e1-5b85-4293-82a0-a42c90965992', embedding=None, metadata={'page_label': '4', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4 1 Introduction\\npredict the life expectancy for this patient. By making the same prediction\\nfor a patient on treatment B with the same characteristics, one can estimate\\nthe absolute diﬀerence in probabilities or life expectancies. This approach\\nrecognizes that low-risk patients must have less absolute beneﬁt of treatment\\n(lower change in outcome probability) than high-risk patients,351af a c tt h a t\\nhas been ignored in many clinical trials. Another reason for multivariable\\nmodeling in randomized clinical trials is that when the basic response model\\nis nonlinear (e.g., logistic, Cox, parametric survival models), the unadjusted\\nestimate ofthe treatment eﬀect is not correctif there is moderateheterogene-\\nity of subjects, even with perfect balance of baseline characteristics across\\nthe treatment groups.a9,24,198,588So even when investigators are interested\\nin simple comparisons of two groups’ responses, multivariable modeling can\\nbe advantageous and sometimes mandatory.\\nCost-eﬀectiveness analysis is becoming increasingly used in health care re-\\nsearch, and the “eﬀectiveness” (denominator of the cost-eﬀectiveness ratio)\\nis always a measure of absolute eﬀectiveness. As absolute eﬀectiveness varies\\ndramatically with the risk proﬁles of subjects, it must be estimated for indi-\\nvidual subjects using a multivariable model90,344.\\n1.3 Prediction vs. Classiﬁcation\\nFor problems rangingfrom bioinformatic sto marketing,many analystsdesire\\nto develop“classiﬁers”instead of developing predictive models. Consider an\\noptimum case for classiﬁer developme nt, in which the response variable is\\nbinary, the two levels represent a sharp dichotomy with no gray zone (e.g.,\\ncomplete success vs. total failure with no possibility of a partial success), the\\nuser of the classiﬁer is forced to make one of the two choices, the cost of\\nmisclassiﬁcation is the same for every future observation, and the ratio of the\\ncost of a false positive to that of a false negative equals the (often hidden)\\nratio implied by the analyst’s classiﬁcation rule. Even if all of those condi-\\ntions are met, classiﬁcation is still inferior to probability modeling for driving\\nthe development of a predictive instrument o r for estimation o r hypothesis\\ntesting. It is far better to use the full information in the data to develop a\\nprobability model, then develop classiﬁcation rules on the basis of estimated\\nprobabilities. At the least, this forces the analyst to use a proper accuracy\\nscore219in ﬁnding or weighting data features.\\nWhenthedependentvariableisordinalorcontinuous,classiﬁcationthrough\\nforced up-front dichotomization in an attempt to simplify the problem results\\nin arbitrarinessand major information loss even when the optimum cut point\\naFor example, unadjusted odds ratios from 2 ×2 tables are diﬀerent from adjusted\\nodds ratios when there is variation in subjects’ risk factors within each treatment\\ngroup, even when the distribution of the risk factors is identical between the two\\ngroups.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b441b5f2-4a1b-408f-97c4-6fd861a3eca8', embedding=None, metadata={'page_label': '5', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.3 Prediction vs. Classiﬁcation 5\\n(the median) is used. Dichtomizing the outcome at a diﬀerent point may re-\\nquire a many-fold increase in sample size to make up for the lost informa-\\ntion187. In the area of medical diagnosis, it is often the case that the disease\\nis really on a continuum, and predicting the severity of disease (rather than\\njust its presence or absence) will greatly increase power and precision, not to\\nmention making the result less arbitrary.\\nIt is important to note that two-groupclassiﬁcationrepresents an artiﬁcial\\nforced choice. It is not often the case that the user of the classiﬁer needs to\\nbe limited to two possible actions. The best option for many subjects may\\nbe to refuse to make a decision or to obtain more data (e.g., order another\\nmedical diagnostic test). A gray zone can be helpful, and predictions include\\ngray zones automatically.\\nUnlike prediction (e.g., of absolute risk), classiﬁcation implicitly uses util-\\nity functions (also called loss or cost f unctions, e.g., cost of a false positive\\nclassiﬁcation). Implicit utility functions are highly problematic. First, it is\\nwell known that the utility function depends on variables that are not pre-\\ndictive of outcome and are not collected (e.g., subjects’ preferences) that\\nare available only at the decision point. Second, the approach assumes every\\nsubject has the same utility functionb. Third, the analyst presumptuously\\nassumes that the subject’s utility coincides with his own.\\nFormaldecisionanalysisusessubject-speciﬁcutilitiesandoptimumpredic-\\ntions based on all available data62,74,183,210,219,642c. It follows that receiver\\nbSimple examples to the contrary are the less weight given to a false negative diagno-\\nsis of cancer in the elderly and the aversion of some subjects to surgery or chemother-\\napy.\\ncTo make an optimal decision you need to know all relevant data about an individual\\n(used to estimate the probability of an outcome), and the utility (cost, loss function)\\nof making each decision. Sensitivity and speciﬁcity do not provide this informati on.\\nFor example, if one estimated that the probability of a disease given age, sex , and\\nsymptoms is 0.1 and the“cost”of a false positive equaled the“cost”of a false negative,\\none would act as if the person does not have the disease. Given other utilities, one\\nwouldmakediﬀerentdecisions.Iftheutilitiesareunknown,onegivesthebestestimate\\nof the probability of the outcome to the decision maker and let her incorporate he r\\nown unspoken utilities in making an optimum decision for her.\\nBesidesthefactthatcutoﬀs thatare notindividualizeddonotapplytoindividuals,\\nonly to groups, individual decision making does not utilize sensitivity and speciﬁcity.\\nFor an individualwe can compute Prob( Y=1|X=x); we don’tcare about Prob( Y=\\n1|X>c), and an individual having X=xwould be quite puzzled if she were given\\nProb(X>c|future unknown Y) when she already knows X=xsoXis no longer a\\nrandom variable.\\nEven when group decision making is needed, sensitivity and speciﬁcity can be\\nbypassed. For mass marketing, for example, one can rank order individuals by the\\nestimated probability of buying the product, to create a lift curve. This is then used\\nto target the kmost likely buyers where kis chosen to meet total program cost\\nconstraints.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='37693b81-7df8-4123-97b6-1d17fb34a54d', embedding=None, metadata={'page_label': '6', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6 1 Introduction\\noperating characteristic curve (ROCd) analysis is misleading except for the\\nspecial case of mass one-time group decision making with unknown utilities\\n(e.g., launching a ﬂu vaccination program). 1\\nAn analyst’s goal should be the development of the most accurate and\\nreliable predictive model or the best model on which to base estimation or\\nhypothesis testing. In the vast majority of cases, classiﬁcation is the task of\\nthe user of the predictive model, at the point in which utilities (costs) and\\npreferences are known.\\n1.4 Planning for Modeling\\nWhen undertaking the development of a model to predict a response, one\\nof the ﬁrst questions the researcher must ask is“will this model actually be\\nused?” Many models are never used, for several reasons522including: (1) it\\nwas not deemed relevant to make predictions in the setting envisioned by\\nthe authors; (2) potential users of the model did not trust the relationships,\\nweights, or variables used to make the predictions; and (3) the variables\\nnecessary to make the predictions were not routinely available.\\nOnce the researcher convinces herself that a predictive model is worth\\ndeveloping, there are many study design issues to be addressed.18,378Models\\nare often developed using a“convenience sample,”that is, a dataset that was\\nnot collected with such predictions in mind. The resulting models are often\\nfraught with diﬃculties such as the following.\\n1. The most important predictor or response variables may not have been\\ncollected, tempting the researchers to make do with variables that do not\\ncapture the real underlying processes.\\n2. The subjects appearingin the datasetare ill-deﬁned, or they are not repre-\\nsentative of the population for which inferences are to be drawn; similarly,\\nthe data collection sites may not represent the kind of variation in the\\npopulation of sites.\\n3. Key variables are missing in large numbers of subjects.\\n4. Data are not missing at random; for example, data may not have been\\ncollected on subjects who dropped out of a study early, or on patients who\\nwere too sick to be interviewed.\\n5. Operational deﬁnitions of some of the key variables were never made.\\n6. Observer variability studies may not have been done, so that the relia-\\nbility of measurements is unknown, or there are other kinds of important\\nmeasurement errors.\\nA predictive model will be more accurate, as well as useful, when data col-\\nlection is planned prospectively. That way one can design data collection\\ndThe ROC curve is a plot of sensitivity vs. one minus speciﬁcity as one varies a\\ncutoﬀ on a continuous predictor used to make a decision.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0d9c3153-19d0-4b9e-aad9-e3f32ee83015', embedding=None, metadata={'page_label': '7', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.4 Planning for Modeling 7\\ninstruments containing the necessary variables, and all terms can be given\\nstandard deﬁnitions (for both descriptive and response variables) for use at\\nall data collection sites. Also, steps can be taken to minimize the amount of\\nmissing data.\\nIn the context of describing and modeling health outcomes, Iezzoni317has\\nan excellent discussion of the dimension s of risk that should be captured by\\nvariables included in the model. She lists these general areas that should be\\nquantiﬁed by predictor variables:\\n1. age,\\n2. sex,\\n3. acute clinical stability,\\n4. principal diagnosis,\\n5. severity of principal diagnosis,\\n6. extent and severity of comorbidities,\\n7. physical functional status,\\n8. psychological, cognitive, and psychosocial functioning,\\n9. cultural, ethnic, and socioeconomic attributes and behaviors,\\n10. health status and quality of life, and\\n11. patient attitudes and preferences for outcomes.\\nSome baseline covariates to be sure to capture in general include\\n1. a baseline measurement of the response variable,\\n2. the subject’s most recent status,\\n3. the subject’s trajectory as of time zero or past levels of a key variable,\\n4. variables explaining much of the variation in the response, and\\n5. more subtle predictors whose distributions strongly diﬀer between the\\nlevels of a key variable of interest in an observational study.\\nMany things can go wrong in statistical modeling, including the following.\\n1. The process generating the data is not stable.\\n2. The model is misspeciﬁed with regard to nonlinearities or interactions, or\\nthere are predictors missing.\\n3. The model is misspeciﬁed in terms of the transformation of the response\\nvariable or the model’s distributional assumptions.\\n4. Themodelcontainsdiscontinuities(e.g.,bycategorizingcontinuouspredic-\\ntors or ﬁtting regression shapes with sudden changes) that can be gamed\\nby users.\\n5. Correlations among subjects are not speciﬁed, or the correlation structure\\nis misspeciﬁed, resulting in ineﬃcient parameter estimates and overconﬁ-\\ndent inference.\\n6. The model is overﬁtted, resulting in predictions that are too extreme or\\npositive associations that are false.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='297b62d6-12a7-4e8f-a97f-ef7c617d283e', embedding=None, metadata={'page_label': '8', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8 1 Introduction\\n7. The user of the model relies on predictions obtained by extrapolating to\\ncombinations of predictor values well outside the range of the dataset used\\nto develop the model.\\n8. Accurate and discriminating predictions can lead to behavior changes that\\nmake future predictions inaccurate.\\n1.4.1 Emphasizing Continuous Variables\\nWhen designing the data collection it is important to emphasize the use of\\ncontinuousvariablesovercategoricalones.Somecategoricalvariablesaresub-\\njective and hard to standardize, and on the average they do not contain the\\nsame amount of statistical information as continuous variables. Above all, it\\nis unwise to categorizenaturally continuous variables during data collection,e\\nas the original values can then not be recovered, and if another researcher\\nfeels that the (arbitrary) cutoﬀ values were incorrect, other cutoﬀs cannot\\nbe substituted. Many researchers make the mistake of assuming that catego-\\nrizing a continuous variable will result in less measurement error. This is a\\nfalse assumption, for if a subject is placed in the wrong interval this will be\\nas much as a 100% error. Thus the magnitude of the error multiplied by the\\nprobability of an error is no better with categorization. 2\\n1.5 Choice of the Model\\nThe actualmethod bywhichan underlyingstatisticalmodelshould bechosen\\nby the analyst is not well developed. A. P. Dawid is quoted in Lehmann397\\nas saying the following.\\nWhere do probability models come from? To judge by the resounding silence\\nover this question on the part of most statisticians, it seems highly embarrass-\\ning. In general, the theoretician is happy to accept that his abstract probability\\ntriple (Ω,A,P) was found under a gooseberry bush, while the applied statisti-\\ncian’s model“just growed”. 3\\nIn biostatistics, epidemiology, econom ics, psychology, sociology, and many\\nother ﬁelds it is seldom the case that subject matter knowledge exists that\\nwould allow the analyst to pre-specify a model (e.g., Weibull or log-normal\\nsurvival model), a transformation for the response variable, and a structure\\neAn exception may be sensitive variables such as income level. Subjects may be more\\nwilling to check a box corresponding to a wide interval containing their income. It\\nis unlikely that a reduction in the probability that a subject will inﬂate her income\\nwill oﬀset the loss of precision due to categorization of income, but there will be a\\ndecrease in the number of refusals. This reduction in missing data can more than\\noﬀset the lack of precision.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0523850d-d724-4ec6-a867-b571a1a10633', embedding=None, metadata={'page_label': '9', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.5 Choice of the Model 9\\nfor how predictors appear in the model (e.g., transformations, addition of\\nnonlinear terms, interaction terms). Indeed, some authors question whether\\nthe notion of a true model even exists in many cases.100We are for bet-\\nter or worse forced to develop models empirically in the majority of cases.\\nFortunately, careful and objective validation of the accuracy of model pre-\\ndictions against observable responses can lend credence to a model, if a good\\nvalidation is not merely the result of overﬁtting (see Section 5.3).\\nThere are a few general guidelines that can help in choosingthe basic form\\nof the statistical model.\\n1. The model must use the data eﬃciently. If, for example, one were inter-\\nested in predicting the probability that a patient with a speciﬁc set of\\ncharacteristics would live ﬁve years from diagnosis, an ineﬃcient model\\nwould be a binary logistic model. A more eﬃcient method, and one that\\nwould also allow for losses to follow-up before ﬁve years, would be a semi-\\nparametric (rank based) or parametric survival model. Such a model uses\\nindividual times of events in estimating coeﬃcients, but it can easily be\\nused to estimate the probability of surviving ﬁve years. As another exam-\\nple, if one were interested in predicting patients’ quality of life on a scale\\nof excellent, very good, good, fair, and poor, a polytomous (multinomial)\\ncategoricalresponse model would not be eﬃcient as it would not make use\\nof the ordering of responses.\\n2. Choose a model that ﬁts overall structures likely to be present in the\\ndata. In modeling survival time in chronic disease one might feel that the\\nimportance of most of the risk factors is constant over time. In that case,\\na proportional hazards model such as the Cox or Weibull model would\\nbe a good initial choice. If on the other hand one were studying acutely\\nill patients whose risk factors wane in importance as the patients survive\\nlonger, a model such as the log-normal or log-logistic regression model\\nwould be more appropriate.\\n3. Choose a model that is robust to problems in the data that are diﬃcult to\\ncheck. For example, the Cox proportionalhazards model and ordinal logis-\\ntic models are not aﬀected by monotonic transformations of the response\\nvariable.\\n4. Choose a model whose mathematical form is appropriate for the response\\nbeing modeled. This often has to do with minimizing the need for in-\\nteraction terms that are included only to address a basic lack of ﬁt. For\\nexample, many researchers have used ordinary linear regression models\\nfor binary responses, because of their simplicity. But such models allow\\npredicted probabilities to be outside the interval [0 ,1], and strange in-\\nteractions among the predictor variables are needed to make predictions\\nremain in the legal range.\\n5. Choose a model that is readily extendible. The Cox model, by its use of\\nstratiﬁcation, easily allows a few of the predictors, especially if they are\\ncategorical, to violate the assumption of equal regression coeﬃcients over', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c5cc8b66-bc75-4703-ae38-fae78b822f84', embedding=None, metadata={'page_label': '10', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10 1 Introduction\\ntime (proportional hazards assumption). The continuation ratio ordinal\\nlogisticmodelcanalsobegeneralizedeasilytoallowforvaryingcoeﬃcients\\nof some of the predictors as one proceeds across categoriesof the response.\\nR. A. Fisher as quoted in Lehmann397had these suggestions about model\\nbuilding:“(a) We must conﬁne ourselves to those forms which we know how\\nto handle,”and (b)“More or less elaborate forms will be suitable according\\nto the volume of the data.”Ameen [ 100, p. 453] stated that a good model is\\n“(a) satisfactory in performance relative to the stated objective, (b) logically\\nsound, (c) representative, (d) questionable and subject to on-line interroga-\\ntion, (e) able to accommodate external or expert information and (f) able to\\nconvey information.”\\nIt is very typical to use the data to make decisions about the form of\\nthe model as well as about how predictors are represented in the model.\\nThen, once a model is developed, the entire modeling process is routinely\\nforgotten,and statisticalquantities suchasstandarderrors,conﬁdencelimits,\\nP-values, and R2are computed as if the resulting model were entirely pre-\\nspeciﬁed. However, Faraway,186Draper,163Chatﬁeld,100Buckland et al.80\\nand others have written about the severe problems that result from treating\\nan empirically derived model as if it were pre-speciﬁed and as if it were the\\ncorrect model. As Chatﬁeld states [ 100, p. 426]:“It is indeed strange that we\\noften admit model uncertainty by searching for a best model but then ignore\\nthis uncertainty by making inferences and predictions as if certain that the\\nbest ﬁtting model is actually true.”\\nStepwise variable selection is one of the most widely used and abused of\\nall data analysis techniques. Much is said about this technique later (see Sec-\\ntion4.3), but there are many other elements of model development that will\\nneed to be accounted for when making statistical inferences, and unfortu-\\nnately it is diﬃcult to derive quantities such as conﬁdence limits that are\\nproperly adjusted for uncertainties such as the data-based choice between a\\nWeibull and a log-normal regression model. 4\\nYe678developed a general method for estimating the“generalized degrees\\nof freedom”(GDF) for any“data mining”or model selection procedure based\\non least squares. The GDF is an extremely useful index of the amount of\\n“data dredging” or overﬁtting that has been done in a modeling process.\\nIt is also useful for estimating the residual variance with less bias. In one\\nexample, Ye developed a regressiontree usingrecursivepartitioninginvolving\\n10 candidate predictor variables on 100 observations. The resulting tree had\\n19 nodes and GDF of 76. The usual way of estimating the residual variance\\ninvolves dividing the pooled within-node sum of squares by 100 −19, but Ye\\nshowed that dividing by 100 −76 instead yielded a much less biased (and\\nmuch higher) estimate of σ2. In another example, Ye considered stepwise\\nvariable selection using 20 candidate predictors and 22 observations. When\\nthere is no true association between any of the predictors and the response,\\nYe found that GDF = 14.1 for a strategy that selected the best ﬁve-variable\\nmodel. 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98d9f0ac-7221-4a57-a952-0cd33b0532cb', embedding=None, metadata={'page_label': '11', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='1.6 Further Reading 11\\nGiven that the choice of the model has been made (e.g., a log-normal\\nmodel), penalized maximum likelihood estimation has major advantages in\\nthe battle between making the model ﬁt adequately and avoiding overﬁtting\\n(Sections 9.10and13.4.7).Penalizationlessensthe needformodel selection.\\n1.6 Further Reading\\n1Briggs and Zaretzki74eloquently state the problem with ROC curves and the\\nareas under them (AUC):\\nStatistics such as the AUC are not especially relevant to someone who\\nmust make a decision about a particular xc. ...ROC curves lack or ob-\\nscure several quantities that are necessary for evaluating the operational\\neﬀectiveness of diagnostic tests. ...ROC curves were ﬁrst used to check\\nhow radio receivers (like radar receivers) operated over a range of fre-\\nquencies. ...This is not how must ROC curves are used now, particularly\\nin medicine. The receiver of a diagnostic measurement ...wants to make\\na decision based on some xc, and is not especially interested in how well\\nhe would have done had he used some diﬀerent cutoﬀ.\\nIn the discussion to their paper, David Hand states\\nWhen integrating to yield the overall AUC measure, it is necessary to\\ndecide what weight to give each value in the integration. The AUC im-\\nplicitly does this using a weighting derived empirically from the data.\\nThis is nonsensical. The relative importance of misclassifying a case as\\na noncase, compared to the reverse, cannot come from the data itself. It\\nmust come externally, from considerations of the severity one attaches to\\nthe diﬀerent kinds of misclassiﬁcations.\\nAUC, only because it equals the concordance probability in the binary Ycase,\\nis still often useful as a predictive discrimination measure.\\n2More severe problems caused by dichotomizing continuous variables are dis-\\nc u s s e di n[ 13,17,45,82,185,294,379,521,597].\\n3See the excellent editorial by Mallows434for more about model choice. See\\nBreiman and discussants67for an interesting debate about the use of data\\nmodels vs. algorithms. This material also covers interpretability vs. predictive\\naccuracy and several other topics.\\n4See [15,80,100,163,186,415] for information about accounting for model selec-\\ntion in making ﬁnal inferences. Faraway186demonstrated that the bootstrap\\nhas good potential in related although somewhat simpler settings, and Buck-\\nland et al.80developed a promising bootstrap weighting method for accounting\\nfor model uncertainty.\\n5Tibshirani and Knight611developed another approach to estimating the gener-\\nalized degrees of freedom. Luo et al.430developed a way to add noise of known\\nvariance to the response variable to tune the stopping rule used for variable\\nselection. Zou et al.689showed that the lasso, an approach that simultaneously\\nselects variables and shrinks coeﬃcients, has a nice property. Since it uses pe-\\nnalization (shrinkage), an unbiased estimate of its eﬀective number of degrees\\nof freedom is the number of nonzero regression coeﬃcients in the ﬁnal model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='14fc1567-0f3a-40ac-9425-4c3f20cc043b', embedding=None, metadata={'page_label': '13', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 2\\nGeneral Aspects of Fitting\\nRegression Models\\n2.1 Notation for Multivariable Regression Models\\nThe ordinary multiple linear regression model is frequently used and has\\nparameters that are easily interpreted. In this chapter we study a general\\nclass of regression models, those stated in terms of a weighted sum of a set\\nof independent or predictor variables. It is shown that after linearizing the\\nmodel with respect to the predictor variables, the parameters in such re-\\ngression models are also readily interpreted. Also, all the designs used in\\nordinary linear regression can be used in this general setting. These designs\\ninclude analysis of variance (ANOVA) s etups, interaction eﬀects, and nonlin-\\near eﬀects. Besidesdescribingand interpretinggeneralregressionmodels, this\\nchapter also describes, in general terms, how the three types of assumptions\\nof regression models can be examined.\\nFirst we introduce notation for regression models. Let Ydenote the re-\\nsponse (dependent) variable, and let X=X1,X2,...,X pdenote a list or\\nvector of predictor variables (also called covariables or independent, descrip-\\ntor, or concomitant variables). These predictor variables are assumed to be\\nconstants for a given individual or subject from the population of interest.\\nLetβ=β0,β1,...,β pdenote the list of regression coeﬃcients (parameters).\\nβ0is anoptionalinterceptparameter,and β1,...,β pareweightsorregression\\ncoeﬃcients corresponding to X1,...,X p. We use matrix or vector notation\\nto describe a weighted sum of the Xs:\\nXβ=β0+β1X1+...+βpXp, (2.1)\\nwhere there is an implied X0=1 .\\nA regression model is stated in terms of a connection between the predic-\\ntorsXand the response Y.L e tC(Y|X) denote a propertyof the distribution\\nofYgivenX(as a function of X). For example, C(Y|X)c o u l db e E(Y|X),\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 213', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9f7cdac1-fc52-45cd-979c-c0b97ff3171b', embedding=None, metadata={'page_label': '14', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14 2 General Aspects of Fitting Regression Models\\nthe expected value or average of YgivenX,o rC(Y|X) could be the proba-\\nbility that Y=1g i v e n X(whereY= 0 or 1).\\n2.2 Model Formulations\\nWe deﬁne a regression function as a function that describes interesting prop-\\nertiesofYthatmayvaryacrossindividualsinthepopulation. Xdescribesthe\\nlist of factors determining these properties. Stated mathematically, a general\\nregression model is given by\\nC(Y|X)=g(X). (2.2)\\nWe restrict our attention to models that, after a certain transformation, are\\nlinearintheunknownparameters,thatis,modelsthatinvolve Xonlythrough\\na weighted sum of all the Xs. Thegeneral linear regression model is given by\\nC(Y|X)=g(Xβ). (2.3)\\nFor example, the ordinary linear regression model is\\nC(Y|X)=E(Y|X)=Xβ, (2.4)\\nand given X,Yhas a normal distribution with mean Xβand constant vari-\\nanceσ2. The binary logistic regression model129,647is\\nC(Y|X)=P r o b {Y=1|X}=(1+exp( −Xβ))−1, (2.5)\\nwhereYcan take on the values 0 and 1. In general the model, when\\nstated in terms of the property C(Y|X), may not be linear in Xβ;t h a t\\nisC(Y|X)=g(Xβ), where g(u) is nonlinear in u. For example, a regression\\nmodel could be E(Y|X)=(Xβ).5. The model may be made linear in the\\nunknown parameters by a transformation in the property C(Y|X):\\nh(C(Y|X)) =Xβ, (2.6)\\nwhereh(u)=g−1(u), the inverse function of g. As an example consider the\\nbinary logistic regression model given by\\nC(Y|X)=P r o b {Y=1|X}=(1+exp( −Xβ))−1. (2.7)\\nIfh(u) = logit( u)=l o g (u/(1−u)), the transformed model becomes\\nh(Prob(Y=1|X)) = log(exp( Xβ)) =Xβ. (2.8)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b744295f-d44e-4a71-9b23-43a800b9df4f', embedding=None, metadata={'page_label': '15', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Interpreting Model Parameters 15\\nThe transformation h(C(Y|X)) is sometimes called a link function .L e t\\nh(C(Y|X)) be denoted by C′(Y|X). The generallinear regressionmodel then\\nbecomes\\nC′(Y|X)=Xβ. (2.9)\\nIn other words, the model states that some property C′ofY,g i v e nX,i s\\na weighted sum of the Xs(Xβ). In the ordinary linear regression model,\\nC′(Y|X)=E(Y|X). In the logistic regression case, C′(Y|X) is the logit of\\nthe probability that Y= 1, logProb {Y=1}/[1−Prob{Y=1}]. This is the\\nlog of the odds that Y=1v e r s u s Y=0 .\\nIt is important to note that the general linear regression model has two\\nmajorcomponents: C′(Y|X)andXβ.Theﬁrstparthastodowithaproperty\\nor transformation of Y. The second, Xβ,i st h elinear regression orlinear\\npredictor part. The method of least squares can sometimes be used to ﬁt\\nthe model if C′(Y|X)=E(Y|X). Other cases must be handled using other\\nmethods such as maximum likelihood estimation or nonlinear least squares.\\n2.3 Interpreting Model Parameters\\nIn the originalmodel, C(Y|X) speciﬁes the wayin which Xaﬀects a property\\nofY. Except in the ordinarylinear regressionmodel, it is diﬃcult to interpret\\nthe individual parameters if the model is stated in terms of C(Y|X). In the\\nmodelC′(Y|X)=Xβ=β0+β1X1+...+βpXp, the regression parameter\\nβjis interpreted as the change in the property C′ofYper unit change in\\nthe descriptor variable Xj, all other descriptors remaining constanta:\\nβj=C′(Y|X1,X2,...,X j+1,...,X p)−C′(Y|X1,X2,...,X j,...,X p).\\n(2.10)\\nIn the ordinary linear regression model, for example, βjis the change in\\nexpected value of Yper unit change in Xj. In the logistic regression model\\nβjis the change in log odds that Y= 1 per unit change in Xj.W h e na\\nnon-interacting Xjis a dichotomous variable or a continuous one that is\\nlinearly related to C′,Xjis represented by a single term in the model and\\nits contribution is described fully by βj.\\nIn all that follows, we drop the′fromC′and assume that C(Y|X)i st h e\\nproperty of Ythat is linearly related to the weighted sum of the Xs.\\naNote that it is not necessary to “hold constant” all other variables to be able to\\ninterpret the eﬀect of one predictor. It is suﬃcient to hold constant the weighted sum\\nof all the variables other than Xj. And in many cases it is not physically possible to\\nhold other variables constant while varying one, e.g., when a model contains Xand\\nX2(David Hoaglin, personal communication).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='66053eb3-e3c8-4b8c-83ed-2e7705373a0b', embedding=None, metadata={'page_label': '16', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16 2 General Aspects of Fitting Regression Models\\n2.3.1 Nominal Predictors\\nSuppose that we wish to model the eﬀect of two or more treatments and be\\nable to test for diﬀerences between the treatments in some property of Y.\\nA nominal or polytomous factor such as treatment group having klevels, in\\nwhichthereisnodeﬁniteorderingofcategories,isfullydescribedbyaseriesof\\nk−1 binary indicatorvariables(sometimes called dummy variables ). Suppose\\nthat there are four treatments, J,K,L,a n dM, and the treatment factor is\\ndenoted by T. The model can be written as\\nC(Y|T=J)=β0\\nC(Y|T=K)=β0+β1 (2.11)\\nC(Y|T=L)=β0+β2\\nC(Y|T=M)=β0+β3.\\nThe four treatments are thus completely speciﬁed by three regressionparam-\\neters and one interceptthat we areusing to denote treatment J, the reference\\ntreatment. This model can be written in the previous notation as\\nC(Y|T)=Xβ=β0+β1X1+β2X2+β3X3, (2.12)\\nwhere\\nX1=1i fT=K,0o t h e r w i s e\\nX2=1i fT=L,0 otherwise (2.13)\\nX3=1i fT=M,0o t h e r w i s e .\\nFor treatment J(T=J), all three Xs are zero and C(Y|T=J)=β0.\\nThe test for any diﬀerences in the property C(Y) between treatments is\\nH0:β1=β2=β3=0 .\\nThis model is an analysis of variance ork-sample -type model. If there are\\nother descriptor covariables in the model, it becomes an analysis of covari-\\nance-type model.\\n2.3.2 Interactions\\nSuppose that a model has descriptor variables X1andX2and that the eﬀect\\nof the two Xs cannot be separated; that is the eﬀect of X1onYdepends on\\nthe level of X2and vice versa. One simple way to describe this interaction is\\nto add the constructed variable X3=X1X2to the model:\\nC(Y|X)=β0+β1X1+β2X2+β3X1X2. (2.14)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='258fca10-5589-4dd0-bc90-f32c061ee177', embedding=None, metadata={'page_label': '17', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.3 Interpreting Model Parameters 17\\nItisnowdiﬃculttointerpret β1andβ2inisolation.However,wemayquantify\\nthe eﬀect of a one-unit increase in X1ifX2is held constant as\\nTable 2.1 Parameters in a simple model with interaction\\nParameter Meaning\\nβ0 C(Y|age=0,sex=m)\\nβ1 C(Y|age=x+1,sex=m)−C(Y|age=x,sex=m)\\nβ2 C(Y|age=0,sex=f)−C(Y|age=0,sex=m)\\nβ3 C(Y|age=x+1,sex=f)−C(Y|age=x,sex=f)−\\n[C(Y|age=x+1,sex=m)−C(Y|age=x,sex=m)]\\nC(Y|X1+1,X2)−C(Y|X1,X2)\\n=β0+β1(X1+1)+β2X2\\n+β3(X1+1)X2 (2.15)\\n−[β0+β1X1+β2X2+β3X1X2]\\n=β1+β3X2.\\nLikewise, the eﬀect of a one-unit increase in X2onCifX1is held constant is\\nβ2+β3X1. Interactionscanbe much morecomplex than canbe modeled with\\na product of two terms. If X1is binary, the interaction may take the form\\nof a diﬀerence in shape (and/or distribution) of X2versusC(Y) depending\\non whether X1=0o rX1= 1 (e.g., logarithm vs. square root). When both\\nvariables are continuous, the possibilities are much greater (this case is dis-\\ncussed later). Interactions among more than two variables can be exceedingly\\ncomplex.\\n2.3.3 Example: Inference for a Simple Model\\nSuppose we postulated the model\\nC(Y|age,sex)=β0+β1age+β2[sex=f]+β3age[sex=f],\\nwhere [sex=f] is a 0–1 indicator variable for sex = female; the reference cell\\nis sex = male corresponding to a zero value of the indicator variable. This is\\na model that assumes\\n1. age is linearly related to C(Y)f o rm a l e s ,\\n2. age is linearly related to C(Y) for females, and\\n3. whatever distribution, variance, and independence assumptions are appro-\\npriate for the model being considered.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5bbbb9c9-426f-4a4d-9b83-ded8bff3706e', embedding=None, metadata={'page_label': '18', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18 2 General Aspects of Fitting Regression Models\\nWe are thus assuming that the interaction between age and sex is simple;\\nthat is it only alters the slope of the age eﬀect. The parameters in the model\\nhave interpretations shown in Table 2.1.β3is the diﬀerence in slopes (female\\n–m a l e ) .\\nThere are many useful hypotheses that can be tested for this model. First\\nlet’s consider two hypotheses that are seldom appropriate although they are\\nroutinely tested.\\n1.H0:β1= 0: This tests whether age is associated with Yfor males.\\n2.H0:β2= 0: This tests whether sex is associated with Yfor zero-yearolds.\\nNow consider more useful hypotheses.For each hypothesis we should write\\nwhat is being tested, translate this to tests in terms of parameters, write the\\nalternative hypothesis, and describe what the test has maximum power to\\ndetect. The latter component of a hypothesis test needs to be emphasized, as\\nalmost every statistical test is focused on one speciﬁc pattern to detect. For\\nexample, a test of association against an a lternative hypothesis that a slope\\nis nonzero will have maximum power when the true association is linear.\\nIf the true regression model is exponential in X, a linear regression test\\nwill have some power to detect“non-ﬂatness”but it will not be as powerful\\nas the test from a well-speciﬁed exponential regression eﬀect. If the true\\neﬀect is U-shaped, a test of association based on a linear model will have\\nalmost no power to detect association. If one tests for association against\\na quadratic (parabolic) alternative, the test will have some power to detect\\na logarithmic shape but it will have very little power to detect a cyclical\\ntrend having multiple “humps.” In a quadratic regression model, a test of\\nlinearityagainstaquadraticalternativehypothesiswillhavereasonablepower\\nto detect a quadratic nonlinear eﬀect but very limited power to detect a\\nmultiphase cyclical trend. Therefore in the tests in Table 2.2keep in mind\\nthat power is maximal when linearity of the age relationship holds for both\\nsexes. In fact it may be useful to write alternative hypothesesas, for example,\\n“Ha: age is associated with C(Y), powered to detect a linearrelationship.”\\nNote that if there is an interaction eﬀect, we know that there is both an\\nage and a sex eﬀect. However, there can also be age or sex eﬀects when the\\nlines are parallel. That’s why the tests of total association have 2 d.f.\\n2.4 Relaxing Linearity Assumption for Continuous\\nPredictors\\n2.4.1 Avoiding Categorization\\nRelationships among variables are seldom linear, except in special cases\\nsuch as when one variable is compared with itself measured at a diﬀerent\\ntime. It is a common belief among practitioners who do not study bias and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d334b76d-ecb4-4865-8f89-3ac43f941617', embedding=None, metadata={'page_label': '19', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Relaxing Linearity Assumption for Continuous Predictors 19\\neﬃciency in depth that the presence of non-linearity should be dealt with by\\nchopping continuous variables into intervals. Nothing could be more disas-\\ntrous.13,14,17,45,82,185,187,215,294,300,379,446,465,521,533,559,597,646\\nTable 2.2 Most Useful Tests for Linear Age×SexModel\\nNull or Alternative Hypothesis Mathematical\\nStatement\\nEﬀect of age is independent of sex or H0:β3=0\\nEﬀect of sex is independent of age or\\nAge and sex are additive\\nAge eﬀects are parallel\\nAge interacts with sex Ha:β3̸=0\\nAge modiﬁes eﬀect of sex\\nSex modiﬁes eﬀect of age\\nSex and age are non-additive (synergistic)\\nAge is not associated with Y H0:β1=β3=0\\nAge is associated with Y Ha:β1̸=0o rβ3̸=0\\nAge is associated with Yfor either\\nFemales or males\\nSex is not associated with Y H0:β2=β3=0\\nSex is associated with Y Ha:β2̸=0o rβ3̸=0\\nSex is associated with Yfor some\\nValue of age\\nNeither age nor sex is associated with YH0:β1=β2=β3=0\\nEither age or sex is associated with YHa:β1̸=0o rβ2̸=0o rβ3̸=0\\nProblems caused by dichotomization include the following.\\n1. Estimated values will have reduced precision, and associated tests will have re-\\nduced power.\\n2. Categorization assumes that the relationship between the predictor and the re-\\nsponse is ﬂat within intervals; this assumption is far less reasonable than a lin-\\nearity assumption in most cases.\\n3. To make a continuous predictor be more accurately modeled when categorization\\nis used, multiple intervals are required. The needed indicator variables will spend\\nmore degrees of freedom than will ﬁtting a smooth relationship, hence power a nd\\nprecision will suﬀer. And because of sample size limitations in the very low and\\nvery high range of the variable, the outer intervals (e.g., outer quintiles) will be\\nwide, resulting in signiﬁcant heterogeneity of subjects within those intervals, and\\nresidual confounding.\\n4. Categorization assumesthatthereisadiscontinuityinresponseasint ervalbound-\\naries are crossed. Other than the eﬀect of time (e.g., an instant stock price d rop\\nafter bad news), there are very few examples in which such discontinuities have\\nbeen shown to exist.\\n5. Categorization only seems to yield interpretable estimates such as odds ratios.\\nFor example, suppose one computes the odds ratio for stroke for persons with\\na systolic blood pressure >160 mmHg compared with persons with a blood', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='20a8d266-dff3-4920-b58c-4251a048b73b', embedding=None, metadata={'page_label': '20', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20 2 General Aspects of Fitting Regression Models\\npressure ≤160 mmHg. The interpretation of the resulting odds ratio will depend\\non the exact distribution of blood pressures in the sample (the proportion of\\nsubjects >170,>180, etc.). On the other hand, if blood pressure is modeled as\\na continuous variable (e.g., using a regression spline, quadratic, or linear eﬀect)\\none can estimate the ratio of odds for exactsettings of the predictor, e.g., the\\nodds ratio for 200 mmHg compared with 120 mmHg.\\n6. Categorization does not condition on full information. When, for example, the\\nrisk of stroke is being assessed for a new subject with a known blood pressure\\n(say 162 mmHg), the subject does not report to her physician“my blood pressure\\nexceeds 160”but rather reports 162 mmHg. The risk for this subject will be much\\nlower than that of a subject with a blood pressure of 200 mmHg.\\n7. If cutpoints are determined in a way that is not blinded to the response var i-\\nable, calculation of P-values and conﬁdence intervals requires special simulation\\ntechniques; ordinary inferential methods are completely invalid. For example, if\\ncutpoints are chosen by trial and error in a way that utilizes the response, even\\ninformally, ordinary P-values will be too small and conﬁdence intervals will not\\nhave the claimed coverage probabilities. The correct Monte-Carlo simulations\\nmust take into account both multiplicities and uncertainty in the choice of cut-\\npoints. For example, if a cutpoint is chosen that minimizes the P-value and the\\nresulting P-value is 0.05, the true type I error can easily be above 0.5300.\\n8. Likewise, categorization that is not blinded to the response variable results in\\nbiased eﬀect estimates17,559.\\n9. “Optimal” cutpoints do not replicate over studies. Hollander et al.300state that\\n“...the optimal cutpoint approach has disadvantages. One of these is that in al-\\nmost every study where this method is applied, another cutpoint will emerge.\\nThis makes comparisons across studies extremely diﬃcult or even impossible.\\nAltman et al. point out this problem for studies of the prognostic relevance of the\\nS-phase fraction in breast cancer published in the literature. They identiﬁed 19\\ndiﬀerent cutpoints used in the literature; some of them were solely used because\\nthey emerged as the ‘optimal’ cutpoint in a speciﬁc data set. In a meta-analysis on\\nthe relationship between cathepsin-D content and disease-free survival in node-\\nnegative breast cancer patients, 12 studies were in included with 12 diﬀerent\\ncutpoints ...Interestingly, neither cathepsin-D nor the S-phase fraction are r ec-\\nommended to be used as prognostic markers in breast cancer in the recent update\\nof the American Society of Clinical Oncology.” Giannoni et al.215demonstrated\\nthat many claimed“optimal cutpoints”are just the observed median values in the\\nsample, which happens to optimize statistical power for detecting a separation in\\noutcomes and have nothing to do with true outcome thresholds. Disagreements\\nin cutpoints (which are bound to happen whenever one searches for things that\\ndo not exist) cause severe interpretation problems. One study may provide an\\nodds ratio for comparing body mass index (BMI) >30 with BMI ≤30, another\\nfor comparing BMI >28 with BMI ≤28. Neither of these odds ratios has a good\\ndeﬁnition and the two estimates are not comparable.\\n10. Cutpoints are arbitrary and manipulatable;cutpoints can befound that can result\\nin both positive and negative associations646.\\n11. If a confounder is adjusted for by categorization, there will be residual confound-\\ningthat can beexplainedaway byinclusionof thecontinuousform ofthe predictor\\nin the model in addition to the categories.\\nWhen cutpoints are chosen using Y, categorizationrepresents one of those\\nfew times in statistics where both type I and type II errors are elevated.\\nA scientiﬁc quantity is a quantity which can be deﬁned outside of the\\nspeciﬁcs of the current experiment. The kind of high:low estimates that re-\\nsult from categorizinga continuous variable are not scientiﬁc quantities; their\\ninterpretation depends on the entire sample distribution of continuous mea-\\nsurements within the chosen intervals.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='aac0d3ac-dde7-485a-a578-876b78b94826', embedding=None, metadata={'page_label': '21', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Relaxing Linearity Assumption for Continuous Predictors 21\\nTo summarize problems with categorization it is useful to examine its\\neﬀective assumptions. Suppose one assumes there is a single cutpoint cfor\\npredictor X. Assumptions implicit in seeking or using this cutpoint include\\n(1) the relationship between Xand the response Yis discontinuous at X=c\\nand only X=c;( 2 )cis correctly found as the cutpoint; (3) Xvs.Yis\\nﬂat to the left of c;( 4 )Xvs.Yis ﬂat to the right of c; (5) the “optimal”\\ncutpoint does not depend on the values of other predictors. Failure to have\\nthese assumptions satisﬁed will result in great error in estimating c(because\\nit doesn’t exist), low predictive accuracy, serious lack of model ﬁt, residual\\nconfounding, and overestimation of eﬀects of remaining variables.\\nA better approach that maximizes power and that only assumes a smooth\\nrelationship is to use regression splines for predictors that are not known\\nto predict linearly. Use of ﬂexible parametric approaches such as this allows\\nstandard inference techniques ( P-values, conﬁdence limits) to be used, as\\nwill be described below. Before introducing splines, we consider the simplest\\napproach to allowing for nonlinearity.\\n2.4.2 Simple Nonlinear Terms\\nIf a continuous predictor is represented, say, as X1in the model, the model\\nis assumed to be linear in X1. Often, however, the property of Yof interest\\ndoes not behave linearly in all the predictors. The simplest way to describe\\na nonlinear eﬀect of X1is to include a term for X2=X2\\n1in the model:\\nC(Y|X1)=β0+β1X1+β2X2\\n1. (2.16)\\nIf the model is truly linear in X1,β2will be zero. This model formulation\\nallows one to test H0: model is linear in X1againstHa: model is quadratic\\n(parabolic) in X1by testing H0:β2=0 .\\nNonlinear eﬀects will frequently not be of a parabolic nature. If a trans-\\nformation of the predictor is known to induce linearity, that transformation\\n(e.g., log( X)) may be substituted for the predictor. However, often the trans-\\nformation is not known. Higher powers of X1may be included in the model\\nto approximate many types of relationships, but polynomials have some un-\\ndesirable properties (e.g., undesirable peaks and valleys, and the ﬁt in one\\nregion of Xcan be greatly aﬀected by data in other regions433) and will not\\nadequately ﬁt many functional forms.156For example, polynomials do not\\nadequately ﬁt logarithmic func tions or“threshold”eﬀects.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a69d8458-5d75-46d7-b7f2-42019d17fcfa', embedding=None, metadata={'page_label': '22', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='22 2 General Aspects of Fitting Regression Models\\n2.4.3 Splines for Estimating Shape of Regression\\nFunction and Determining Predictor\\nTransformations\\nAd r a f t s m a n ’ s splineis a ﬂexible stripofmetal orrubber used to drawcurves.\\nSpline functions are piecewise polynomials used in curve ﬁtting. That is, they\\nare polynomials within intervals of Xthat are connected across diﬀerent\\nintervals of X. Splines have been used, principally in the physical sciences,\\nto approximate a wide variety of functions. The simplest spline function is a\\nlinear spline function, a piecewise linear function. Suppose that the xaxis is\\ndivided into intervals with endpoints at a,b,a n dc, calledknots. The linear\\nspline function is given by\\nf(X)=β0+β1X+β2(X−a)++β3(X−b)++β4(X−c)+,(2.17)\\nwhere\\n(u)+=u, u >0,\\n0,u≤0. (2.18)\\nThe number of knots can vary depending on the amount of available data for\\nﬁtting the function. The linear spline function can be rewritten as\\nf(X)= β0+β1X, X ≤a\\n=β0+β1X+β2(X−a)a<X≤b(2.19)\\n=β0+β1X+β2(X−a)+β3(X−b)b<X≤c\\n=β0+β1X+β2(X−a)\\n+β3(X−b)+β4(X−c)c<X .\\nA linear spline is depicted in Figure 2.1.\\nThegenerallinearregressionmodelcanbewrittenassumingonlypiecewise\\nlinearity in Xby incorporating constructed variables X2,X3,a n dX4:\\nC(Y|X)=f(X)=Xβ, (2.20)\\nwhereXβ=β0+β1X1+β2X2+β3X3+β4X4,a n d\\nX1=XX2=(X−a)+\\nX3=(X−b)+X4=(X−c)+. (2.21)\\nBy modeling a slope incrementfor Xin an interval( a,b]i nt e rm sof( X−a)+,\\nthe function is constrained to join (“meet”) at the knots. Overall linearity in\\nXcan be tested by testing H0:β2=β3=β4=0 .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='14896edf-dcf0-484c-8638-0bc24b670ca6', embedding=None, metadata={'page_label': '23', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Relaxing Linearity Assumption for Continuous Predictors 23\\nXf(X)\\n0123456\\nFig. 2.1 A linear spline function with knots at a=1,b=3,c=5 .\\n2.4.4 Cubic Spline Functions\\nAlthough the linear spline is simple and can approximate many common\\nrelationships, it is not smooth and will not ﬁt highly curved functions well.\\nThese problems can be overcome by using piecewise polynomials of order\\nhigherthanlinear.Cubic polynomialshavebeenfoundtohaveniceproperties\\nwith good ability to ﬁt sharply curving shapes. Cubic splines can be made to\\nbesmoothatthejoinpoints(knots)byforcingtheﬁrstandsecondderivatives\\nof the function to agree at the knots. Such a smooth cubic spline function\\nwith three knots ( a,b,c)i sg i v e nb y\\nf(X)=β0+β1X+β2X2+β3X3\\n+β4(X−a)3\\n++β5(X−b)3\\n++β6(X−c)3\\n+ (2.22)\\n=Xβ\\nwith the following constructed variables:\\nX1=XX2=X2\\nX3=X3X4=(X−a)3\\n+ (2.23)\\nX5=(X−b)3\\n+X6=(X−c)3\\n+.\\nIf the cubic spline function has kknots, the function will require estimat-\\ningk+ 3 regression coeﬃcients besides the intercept. See Section 2.4.6for\\ninformation on choosing the number and location of knots. 1\\nThere are more numerically stable ways to form a design matrix for cubic\\nspline functions that are based on B-splines instead of the truncated power\\nbasis152,575used here. However,B-splines are more complex and do not allow\\nfor extrapolation beyond the outer knots, and the truncated power basis\\nseldom presents estimation problems (see Section 4.6) when modern methods\\nsuch as the Q–R decomposition are used for matrix inversion. 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='004c3785-c94a-473b-ad5b-e793b86ef2fc', embedding=None, metadata={'page_label': '24', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='24 2 General Aspects of Fitting Regression Models\\n2.4.5 Restricted Cubic Splines\\nStone and Koo595have found that cubic spline functions do have a drawback\\ninthattheycanbepoorlybehavedinthetails,thatisbeforetheﬁrstknotand\\nafter the last knot. They cite advantages of constraining the function to be\\nlinear in the tails. Their restricted cubic spline function (also called natural\\nsplines) has the additional advantage that only k−1 parameters must be 3\\nestimated (besides the intercept) as opposed to k+ 3 parameters with the\\nunrestrictedcubicspline.Therestrictedsplinefunctionwith kknotst1,...,tk\\nis given by156\\nf(X)=β0+β1X1+β2X2+...+βk−1Xk−1, (2.24)\\nwhereX1=Xand forj=1,...,k−2,\\nXj+1=(X−tj)3\\n+−(X−tk−1)3\\n+(tk−tj)/(tk−tk−1)\\n+(X−tk)3\\n+(tk−1−tj)/(tk−tk−1). (2.25)\\nIt can be shownthat Xjislinearin XforX≥tk. Fornumericalbehaviorand\\nto put all basis functions for Xon the same scale, RHmiscandrmspackage\\nfunctions by default divide the terms in Eq. 2.25by\\nτ=(tk−t1)2. (2.26)\\nFigure2.2displays the τ-scaled spline component variables Xjforj=\\n2,3,4a n dk= 5 and one set of knots. The l eft graph magniﬁes the lower\\nportion of the curves.\\nrequire( Hmisc)\\nx←rcspline.eval(seq(0,1,.01),\\nknots=seq(.05,.95,length=5), inclx=T)\\nxm←x\\nxm[xm > .0106] ←NA\\nmatplot(x[,1], xm, type=\"l\",ylim=c(0,.01),\\nxlab=expression(X), ylab= \\'\\', lty=1)\\nmatplot(x[,1], x, type=\"l\",\\nxlab=expression(X), ylab= \\'\\', lty=1)\\nFigure2.3displays some typical shapes of restricted cubic spline functions\\nwithk=3,4,5, and 6. These functions were generated using random β.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='722a767e-c5fa-4cf5-8c97-b9c094203e22', embedding=None, metadata={'page_label': '25', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Relaxing Linearity Assumption for Continuous Predictors 25\\n0.0 0.4 0.80.0000.0020.0040.0060.0080.010\\nX0.0 0.4 0.80.00.20.40.60.81.0\\nX\\nFig. 2.2 Restricted cubic spline component variables for k= 5 and knots at X=\\n.05,.275,.5,.725, and .95. Nonlinear basis functions are scaled by τ. The left panel\\nis ay–magniﬁcation of the right panel. Fitted functions such as those in Figure 2.3\\nwill be linear combinations of these basis functions as long as knots are at the same\\nlocations used here.\\nx←seq(0, 1, length=300)\\nfor(nk in 3:6) {\\nset.seed(nk)\\nknots ←seq(.05, .95, length=nk)\\nxx←rcspline.eval(x, knots=knots, inclx=T)\\nfor(i in 1 : (nk - 1))\\nxx[,i] ←(xx[,i] - min(xx[,i])) /\\n(max(xx[,i]) - min(xx[,i]))\\nfor(i in 1 : 20) {\\nbeta ←2*runif(nk-1) - 1\\nxbeta ←xx %*% beta + 2 * runif(1) - 1\\nxbeta ←(xbeta - min(xbeta)) /\\n(max(xbeta) - min(xbeta))\\nif(i == 1) {\\nplot(x, xbeta, type=\"l\", lty=1,\\nxlab=expression(X), ylab= \\'\\', bty=\"l\")\\ntitle(sub=paste(nk,\"knots\"), adj=0, cex=.75)\\nfor(j in 1 : nk)\\narrows(knots[j], .04, knots[j], -.03,\\nangle=20, length=.07, lwd=1.5)\\n}\\nelse lines(x, xbeta, col=i)\\n}\\n}', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1dde8516-693c-4e98-9d10-2d6b51c3cd38', embedding=None, metadata={'page_label': '26', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='26 2 General Aspects of Fitting Regression Models\\nOnceβ0,...,β k−1areestimated,therestrictedcubicsplinecanberestated\\nin the form\\nf(X)=β0+β1X+β2(X−t1)3\\n++β3(X−t2)3\\n+\\n+...+βk+1(X−tk)3\\n+ (2.27)\\nby dividing β2,...,β k−1byτ(Eq.2.26) and computing\\nβk=[β2(t1−tk)+β3(t2−tk)+...+βk−1(tk−2−tk)]/(tk−tk−1) (2.28)\\nβk+1=[β2(t1−tk−1)+β3(t2−tk−1)+...+βk−1(tk−2−tk−1)]/(tk−1−tk).\\nA test of linearity in Xcan be obtained by testing\\nH0:β2=β3=...=βk−1=0. (2.29)\\nThe truncated power basis for restricted cubic splines does allow for 4\\nrational (i.e., linear) extrapolation beyond the outer knots. However, when\\nthe outer knots are in the tails of the data, extrapolation can still be danger-\\nous.\\nWhen nonlinear terms in Equation 2.25are normalized, for example, by\\ndividing them by the square of the diﬀerence in the outer knots to make all\\nterms have units of X, the ordinary truncated power basis has no numerical\\ndiﬃculties when modern matrix algebra software is used.\\n2.4.6 Choosing Number and Position of Knots\\nWe have assumed that the locations of the knots are speciﬁed in advance;\\nthat is, the knot locations are not treated as free parameters to be estimated.\\nIf knots were free parameters, the ﬁtted function would have more ﬂexibility\\nbut at the cost of instability of estimates, statistical inference problems, and\\ninability to use standard regression modeling software for estimating regres-\\nsion parameters.\\nHow then does the analyst pre-assign knot locations? If the regression\\nrelationship were described by prior experience, pre-speciﬁcation of knot lo-\\ncations would be easy. For example, if a function were known to change\\ncurvature at X=a, a knot could be placed at a. However, in most situations\\nthere is no way to pre-specify knots. Fortunately, Stone593has found that\\nthe location of knots in a restricted cubic spline model is not very crucial in\\nmost situations; the ﬁt depends much more on the choice of k,t h en u m b e ro f\\nknots. Placing knots at ﬁxed quantiles (percentiles) of a predictor’s marginal 5\\ndistribution is a good approach in most datasets. This ensures that enough\\npoints are available in each interval, and also guards against letting outliers\\noverly inﬂuence knot placement. Recommended equally spaced quantiles are\\nshown in Table 2.3.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3326cb3-19fa-40ff-9cb6-61a939de7450', embedding=None, metadata={'page_label': '27', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Relaxing Linearity Assumption for Continuous Predictors 27\\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nX\\n3 knots0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nX\\n4 knots\\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nX\\n5 knots0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nX\\n6 knots\\nFig. 2.3 Some typical restricted cubic spline functions for k=3,4,5,6. They–axis\\nisXβ. Arrows indicate knots. These curves were derived by randomly choosing values\\nofβsubject to standard deviations of ﬁtted functions being normalized.\\nTable 2.3 Default quantiles for knots\\nk Quantiles\\n3 .10 .5 .90\\n4 .05 .35 .65 .95\\n5.05 .275 .5 .725 .95\\n6.05 .23 .41 .59 .77 .95\\n7.025 .1833 .3417 .5 .6583 .8167 .975', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e56a599-4762-4c81-ba5b-b438a04cb21c', embedding=None, metadata={'page_label': '28', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='28 2 General Aspects of Fitting Regression Models\\nThe principal reason for using less extreme default quantiles for k=3a n d\\nmore extreme ones for k= 7 is that one usually uses k= 3 for small sample\\nsizes and k= 7 for large samples. When the sample size is less than 100, the\\nouter quantiles should be replaced by the ﬁfth smallest and ﬁfth largest data\\npoints, respectively.595What about the choiceof k? The ﬂexibility of possible\\nﬁts must be tempered by the sample size available to estimate the unknown\\nparameters. Stone593has found that more than 5 knots are seldom required\\nin a restricted cubic spline model. The principal decision then is between\\nk=3,4, or 5. For many datasets, k= 4 oﬀers an adequate ﬁt of the model\\nand is a good compromise between ﬂexibility and loss of precision caused\\nby overﬁtting a small sample. When the sample size is large (e.g., n≥100\\nwith a continuous uncensored response variable), k= 5 is a good choice.\\nSmall samples ( <30, say) may require the use of k= 3. Akaike’s information\\ncriterion (AIC, Section 9.8.1) can be used for a data-based choice of k.T h e\\nvalue of kmaximizing the model likelihood ratio χ2−2kwould be the best\\n“for the money”using AIC.\\nThe analyst may wish to devote more knots to variables that are thought\\nto be more important, and risk lack of ﬁt for less important variables. In this\\nwaythetotalnumberofestimatedparameterscanbecontrolled(Section 4.1).\\n2.4.7 Nonparametric Regression\\nOne of the most important results of an analysis is the estimation of the\\ntendency (trend) of how Xrelates to Y. This trend is useful in its own right\\nand it may be suﬃcient for obtaining predicted values in some situations, but\\ntrend estimates can also be used to guide formal regressionmodeling (by sug-\\ngesting predictor variable transformations) and to check model assumptions.\\nNonparametric smoothers are excellent tools for determining the shape\\nof the relationship between a predictor and the response. The standard non-\\nparametricsmoothersworkwhen onei sinterestedinassessingonecontinuous\\npredictor at a time and when the property of the response that shouldbe lin-\\nearly related to the predictor is a standard measure of central tendency. For\\nexample, when C(Y)i sE(Y)o rP r [Y= 1], standard smoothers are useful,\\nbut when C(Y) is a measure of variability or a rate (instantaneous risk), or\\nwhenYis only incompletely measured for some subjects (e.g., Yis censored\\nfor some subjects), simple smoothers will not work.\\nThe oldest and simplest nonparametric smoother is the moving average.\\nSuppose that the data consist of the points X=1,2,3,5,and 8, with the\\ncorresponding Yvalues2.1,3.8,5.7,11.1,and17.2.Tosmooththerelationship\\nwe could estimate E(Y|X=2 )b y( 2 .1+3.8+5.7)/3a n dE(Y|X=(2+3+\\n5)/3) by (3.8+5.7+11.1)/3. Note that overlap is ﬁne; that is one point may\\nbe contained in two sets that are averaged.You can immediately see that the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da174b7b-8dee-4e2c-8862-cc1745c018b0', embedding=None, metadata={'page_label': '29', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.4 Relaxing Linearity Assumption for Continuous Predictors 29\\nsimple moving average has a problem in estimating E(Y) at the outer values\\nofX. The estimates are quite sensitive to the choice of the number of points\\n(or interval width) to use in“binning”the data.\\nA moving least squares linear regression smoother is far superior to a\\nmoving ﬂat line smoother (moving average). Cleveland’s111moving linear\\nregression smoother loesshas become the most popular smoother. To obtain\\nthe smoothed value of YatX=x,w et a k ea l lt h ed a t ah a v i n g Xvalues\\nwithin a suitable interval about x. Then a linear regression is ﬁtted to all\\nof these points, and the predicted value from this regression at X=xis\\ntaken as the estimate of E(Y|X=x). Actually, loessuses weighted least\\nsquares estimates, which is why it is called a locally weighted least squares\\nmethod. The weights are chosen so that points near X=xare given the\\nmost weightbin the calculation of the slope and intercept. Surprisingly, a\\ngood default choice for the interval about xis an interval containing 2 /3o f\\nthe data points! The weighting function is devised so that points near the\\nextremes of this interval receive almost no weight in the calculation of the\\nslope and intercept.\\nBecause loessuses a moving straight line rather than a moving ﬂat one,\\nit provides much better behavior at the extremes of the Xs. For example,\\none can ﬁt a straight line to the ﬁrst three data points and then obtain the\\npredicted value at the lowest X, which takes into account that this Xis not\\nthe middle of the three Xs.\\nloessobtains smoothed values for E(Y) at each observed value of X.\\nEstimates for other Xs are obtained by linear interpolation.\\nTheloessalgorithm has another component. After making an initial es-\\ntimate of the trend line, loesscan look for outliers oﬀ this trend. It can\\nthen delete or down-weight those apparent outliers to obtain a more robust\\ntrend estimate. Now, diﬀerent points will appear to be outliers with respect\\nto this second trend estimate. The new set of outliers is taken into account\\nand another trend line is derived. By default, the process stops after these\\nthree iterations. loessworks exceptionally well for binary Yas long as the\\niterations that look for outliers are not done, that is only one iteration is\\nperformed.\\nFor a single X,Friedman’s“supersmoother”207isanothereﬃcientandﬂex-\\nible nonparametric trend estimator. For both loessand the super smoother\\nthe amount of smoothing can be controlled by the analyst. Hastie and\\nTibshirani275provided an excellent description of smoothing methods and\\ndeveloped a generalized additive model for multiple Xs, in which each\\ncontinuous predictor is ﬁtted with a nonparametric smoother (see Chap-\\nter16). Interactions are not allowed. Cleveland et al.96have extended two- 6\\ndimensional smoothers to multiple dimensions without assuming additivity.\\nTheirlocal regression model is feasible for up to four or so predictors. Local\\nregression models are extremely ﬂexible, allowing parts of the model to be\\nbThis weight is not to be confused with the regression coeﬃcient; rather the weights\\narew1,w2,...,w nand the ﬁtting criterion is∑n\\niwi(Yi−ˆYi)2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26dea817-43d3-4f0e-89ac-87e61ee283d3', embedding=None, metadata={'page_label': '30', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='30 2 General Aspects of Fitting Regression Models\\nparametrically speciﬁed, and allowing the analyst to choose the amount of\\nsmoothing or the eﬀective number of degrees of freedom of the ﬁt.\\nSmoothing splines are related to nonparametric smoothers. Here a knot\\nis placed at every data point, but a penalized likelihood is maximized to\\nderive the smoothed estimates. Gray237,238developed a general method that\\nis halfway between smoothing splines and regressionsplines. He pre-speciﬁed,\\nsay, 10 ﬁxed knots, but uses a penalized likelihood for estimation. This allows\\nthe analyst to control the eﬀective number of degrees of freedom used. 7\\nBesidesusingsmootherstoestimateregressionrelationships,smoothersare\\nvaluable for examining trends in residual plots. See Sections 14.6and21.2\\nfor examples.\\n2.4.8 Advantages of Regression Splines\\nover Other Methods\\nThere are several advantages of regression splines:271\\n1. Parametric splines are piecewise polynomials and can be ﬁtted using any\\nexisting regressionprogramafterthe constructedpredictorsarecomputed.\\nSpline regression is equally suitable to multiple linear regression, survival\\nmodels, and logistic models for discrete outcomes.\\n2. Regression coeﬃcients for the spline function are estimated using stan-\\ndard techniques (maximum likelihood or least squares), and statistical\\ninferences can readily be drawn. Formal tests of no overall association,\\nlinearity, and additivity can readily be constructed. Conﬁdence limits for\\nthe estimated regression function are derived by standard theory.\\n3. The ﬁtted spline function directly estimates the transformation that a\\npredictor should receive to yield linearity in C(Y|X). The ﬁtted spline\\ntransformation sometimes suggests a simple transformation (e.g., square\\nroot) of a predictor that can be used if one is not concerned about the\\npropernumberofdegreesoffreedomfortestingassociationofthepredictor\\nwith the response.\\n4. The spline function can be used to represent the predictor in the ﬁnal\\nmodel. Nonparametric methods do not yield a prediction equation.\\n5. Splines can be extended to non-additive models (see below). Multidimen-\\nsional nonparametric estimators often require burdensome computations.\\n2.5 Recursive Partitioning: Tree-Based Models\\nBreiman et al.69have developed an essentially model-free approach called\\nclassiﬁcation and regression trees (CART), a form of recursive partitioning.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f352ef1c-30ea-487d-8f02-cebf735d9f73', embedding=None, metadata={'page_label': '31', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.6 Multiple Degree of Freedom Tests of Association 31\\nFor some implementations of CART, we say“essentially”model-free since a\\nmodel-based statistic is sometimes chosenas a splitting criterion.The essence\\nof recursive partitioning is as follows.\\n1. Find the predictor so that the best possible binary split on that predictor\\nhas a larger value of some statistical criterion than any other split on any\\nother predictor. For ordinal and continuous predictors, the split is of the\\nformX<cversusX≥c. For polytomous predictors, the split involves\\nﬁnding the best separation of categories, without preserving order.\\n2. Within each previously formed subset, ﬁnd the best predictor and best\\nsplit that maximizes the criterion in the subset of observations passing the\\nprevious split.\\n3. Proceed in like fashion until fewer than kobservations remain to be split,\\nwherekis typically 20 to 100.\\n4. Obtain predicted values using a statistic that summarizes each terminal\\nnode (e.g., mean or proportion).\\n5. Prune the tree backward so that a tree with the same number of nodes\\ndeveloped on 0.9 of the data validates best on the remaining 0.1 of the\\ndata (averageover the 10 cross-validations).Alternatively, shrink the node\\nestimatestowardthe mean,usingaprogressivelystrongershrinkagefactor,\\nuntil the best cross-validation results.\\n8\\nTree models have the advantage of not requiring any functional form for\\nthe predictors and of not assuming additivity of predictors (i.e., recursive\\npartitioning can identify complex interactions). Trees can deal with miss-\\ning data ﬂexibly. They have the disadvantages of not utilizing continuous\\nvariables eﬀectively and of overﬁtting in three directions: searching for best\\npredictors, for best splits, and searching multiple times. The penalty for the\\nextreme amount of data searching required by recursive partitioning surfaces\\nwhen the tree does not cross-validate optimally until it is pruned all the way\\nback to two or three splits. Thus reliable trees are often not very discrimi-\\nnating. 9\\nTree models are especially useful in messy situations or settings in which\\noverﬁttingisnotsoproblematic,suchasconfounderadjustmentusingpropen-\\nsity scores117or in missing value imputation. A majoradvantageof tree mod-\\neling is savings of analyst time, but this is oﬀset by the underﬁtting needed\\nto make trees validate.\\n2.6 Multiple Degree of Freedom Tests of Association\\nWhen a factor is a linear or binary term in the regression model, the test\\nof association for that factor with the response involves testing only a single\\nregression parameter. Nominal factors and predictors that are represented as\\na quadratic or spline function require multiple regression parameters to be', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1aca0651-07b8-4de2-a4d8-8439c8b43cf5', embedding=None, metadata={'page_label': '32', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='32 2 General Aspects of Fitting Regression Models\\ntested simultaneously in order to assess association with the response. For a\\nnominal factor having klevels, the overall ANOVA-type test with k−1 d.f.\\ntests whether there are any diﬀerences in responses between the kcategories.\\nIt is recommended that this test be done before attempting to interpret in-\\ndividual parameter estimates. If the overall test is not signiﬁcant, it can be\\ndangerousto rely on individual pairwise comparisonsbecause the type I error\\nwill be increased. Likewise, for a continuous predictor for which linearity is\\nnot assumed, all terms involving the predictor should be tested simultane-\\nously to check whether the factor is associated with the outcome. This test\\nshould precede the test for linearity and should usually precede the attempt\\nto eliminate nonlinear terms. For example, in the model\\nC(Y|X)=β0+β1X1+β2X2+β3X2\\n2, (2.30)\\none should test H0:β2=β3= 0 with 2 d.f. to assess association between\\nX2and outcome. In the ﬁve-knot restricted cubic spline model\\nC(Y|X)=β0+β1X+β2X′+β3X′′+β4X′′′, (2.31)\\nthe hypothesis H0:β1=...=β4= 0 should be tested with 4 d.f. to\\nassess whether there is any association between XandY. If this 4 d.f. test is\\ninsigniﬁcant,itisdangeroustointerprettheshapeoftheﬁtted splinefunction\\nbecause the hypothesis that the overall function is ﬂat has not been rejected.\\nA dilemma arises when an overall test of association, say one having 4\\nd.f., is insigniﬁcant, the 3 d.f. test for linearity is insigniﬁcant, but the 1 d.f.\\ntest for linear association,after deleting nonlinear terms, becomes signiﬁcant.\\nHad the test for linearity been borderline signiﬁcant, it would not have been\\nwarranted to drop these terms in order to test for a linear association. But\\nwith the evidence for nonlinearity not very great, one could attempt to test\\nfor association with 1 d.f. This however is not fully justiﬁed, because the 1\\nd.f. test statistic does not have a χ2distribution with 1 d.f. since pretesting\\nwas done. The original 4 d.f. test statistic does have a χ2distribution with 4\\nd.f. because it was for a pre-speciﬁed test.\\nFor quadratic regression, Grambsch and O’Brien234showed that the 2\\nd.f. test of association is nearly optimal when pretesting is done, even when\\nthe true relationship is linear. They considered an ordinary regression model\\nE(Y|X)=β0+β1X+β2X2and studied tests of association between Xand\\nY. The strategy they studied was as follows. First, ﬁt the quadratic model\\nand obtain the partial test of H0:β2= 0, that is the test of linearity. If this\\npartialF-test is signiﬁcant at the α=0.05 level, report as the ﬁnal test of\\nassociation between XandYthe 2 d.f. F-test ofH0:β1=β2=0 .I ft h e\\ntest of linearity is insigniﬁcant, the model is reﬁtted without the quadratic\\nterm and the test of association is then a 1 d.f. test, H0:β1=0|β2=0 .\\nGrambsch and O’Brien demonstrated that the type I error from this two-\\nstage test is greater than the stated α, and in fact a fairly accurate P-value\\ncan be obtained if it is computed from an Fdistribution with 2 numerator', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='52d2cdf0-a822-49ae-90e1-41c0edd19a82', embedding=None, metadata={'page_label': '33', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Assessment of Model Fit 33\\nd.f. even when testing at the second stage. This is because in the original\\n2 d.f. test of association, the 1 d.f. corresponding to the nonlinear eﬀect is\\ndeleted if the nonlinear eﬀect is very small; that is one is retaining the most\\nsigniﬁcant part of the 2 d.f. Fstatistic.\\nIf we use a 2 d.f. Fcriticalvalue to assessthe Xeﬀect even when X2is not\\nin the model, it is clear that the two-stage approach can only lose power and\\nhence it has no advantage whatsoever. That is because the sum of squares\\ndue to regressionfromthe quadraticmodel is greaterthan the sum of squares\\ncomputed from the linear model.\\n2.7 Assessment of Model Fit\\n2.7.1 Regression Assumptions\\nIn this section, the regression part of the model is isolated, and methods are\\ndescribed for validating the regression assumptions or modifying the model\\nto meet the assumptions. The general linear regression model is\\nC(Y|X)=Xβ=β0+β1X1+β2X2+...+βkXk.(2.32)\\nThe assumptions of linearity and additivity need to be veriﬁed. We begin\\nwith a special case of the general model,\\nC(Y|X)=β0+β1X1+β2X2, (2.33)\\nwhereX1is binary and X2is continuous. One needs to verify that the prop-\\nerty of the response C(Y) is related to X1andX2according to Figure 2.4.\\nThere are several methods for checking the ﬁt of this model. The ﬁrst\\nmethod below is basedon critiquingthe simple model, and the othermethods\\ndirectly“estimate”the model.\\n1. Fit the simple linear additive model and critically examine residual plots\\nfor evidence of systematic patterns. For least squares ﬁts one can compute\\nestimated residuals e=Y−Xˆβand box plots of estratiﬁed by X1and\\nscatterplots of eversusX1andˆYwith trend curves. If one is assuming\\nconstant conditional variance of Y, the spread of the residual distribution\\nagainst each of the variables can be checked at the same time. If the nor-\\nmality assumption is needed (i.e., if signiﬁcance tests or conﬁdence limits\\nare used), the distribution of ecan be compared with a normal distribu-\\ntion with mean zero. Advantage : Simplicity. Disadvantages : Standard\\nresiduals can only be computed for continuous uncensored response vari-\\nables. The judgment of non-randomness is largely subjective, it is diﬃcult\\nto detect interaction, and if interaction is present it is diﬃcult to check\\nany of the other assumptions. Unless trend lines are added to plots, pat-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7719bb7c-ce02-41df-a257-a19b9bd55335', embedding=None, metadata={'page_label': '34', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='34 2 General Aspects of Fitting Regression Models\\nX2C(Y)\\nX1=0X1=1\\nFig. 2.4 Regression assumptions for one binary and one continuous predictor\\nterns may be diﬃcult to discern if the sample size is very large. Detecting\\npatterns in residuals does not always inform the analyst of what corrective\\naction to take, although partial residual plots can be used to estimate the\\nneeded transformations if interaction is absent.\\n2. Make a scatterplot of YversusX2using diﬀerent symbols according to\\nvalues of X1.Advantages : Simplicity, and one can sometimes see all re-\\ngressionpatterns including interaction. Disadvantages : Scatterplots can-\\nnot be drawn for binary, categorical, or censored Y. Patterns are diﬃcult\\nto see if relationships are weak or if the sample size is very large.\\n3. Stratify the sample by X1and quantile groups (e.g., deciles) of X2. Within\\neachX1×X2stratum an estimate of C(Y|X1,X2) is computed. If X1is\\ncontinuous, the same method can be used after grouping X1into quantile\\ngroups.Advantages : Simplicity, ability to see interaction patterns, can\\nhandle censored Yif care is taken. Disadvantages : Subgrouping requires\\nrelatively large sample sizes and does not use continuous factors eﬀectively\\nasit does not attempt anyinterpolation.The orderingofquantilegroupsis\\nnot utilized by the procedure. Subgroup estimates have low precision (see\\np.488for an example). Each stratum must contain enough information\\nto allow trends to be apparent above noise in the data. The method of\\ngroupingchosen(e.g.,decilesvs.quintilesvs.rounding)canalterthe shape\\nof the plot.\\n4. Fit a nonparametric smoother separately for levels of X1(Section 2.4.7)\\nrelatingX2toY.Advantages : All regression aspects of the model can\\nbe summarized eﬃciently with minimal assumptions. Disadvantages :\\nDoes not easily apply to censored Y, and does not easily handle multiple\\npredictors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6fb1388-9229-4582-b16d-8f2a97318d56', embedding=None, metadata={'page_label': '35', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Assessment of Model Fit 35\\n5. Fit a ﬂexible parametric model that allows for most of the departures from\\nthe linear additive model that you wish to entertain. Advantages :O n e\\nframeworkis usedfor examining the model assumptions, ﬁtting the model,\\nand drawing formal inference. Degrees of freedom are well deﬁned and\\nall aspects of statistical inference “work as advertised.” Disadvantages :\\nComplexity, and it is generally diﬃcult to allow for interactions when\\nassessing patterns of eﬀects.\\nThe ﬁrst four methods each have the disadvantage that if conﬁdence limits\\nor formal inferences are desired it is diﬃcult to know how many degrees of\\nfreedom were eﬀectively used so that, for example, conﬁdence limits will have\\nthe stated coverage probability. For method ﬁve, the restricted cubic spline\\nfunction is an excellent tool for estimating the true relationship between X2\\nandC(Y) for continuous variables without assuming linearity. By ﬁtting a\\nmodel containing X2expanded into k−1t e r m s ,w h e r e kis the number of\\nknots, one can obtain an estimate of the function of X2that could be used\\nlinearly in the model:\\nˆC(Y|X)=ˆβ0+ˆβ1X1+ˆβ2X2+ˆβ3X′\\n2+ˆβ4X′′\\n2\\n=ˆβ0+ˆβ1X1+ˆf(X2), (2.34)\\nwhere\\nˆf(X2)=ˆβ2X2+ˆβ3X′\\n2+ˆβ4X′′\\n2, (2.35)\\nandX′\\n2andX′′\\n2are constructed spline variables (when k= 4) as described\\npreviously.We call ˆf(X2) the spline-estimatedtransformationof X2. Plotting\\nthe estimated spline function ˆf(X2)v e r s u s X2will generally shed light on\\nhow the eﬀect of X2should be modeled. If the sample is suﬃciently large,\\nthe spline function can be ﬁtted separately for X1=0a n d X1= 1, allowing\\ndetection of even unusual interaction patterns. A formal test of linearity in\\nX2is obtained by testing H0:β3=β4= 0, using a computationally eﬃcient\\nscore test, for example (Section 9.2.3).\\nIf the model is nonlinear in X2, either a transformation suggested by the\\nspline function plot (e.g., log( X2)) or the spline function itself (by placing\\nX2,X′\\n2,a n dX′′\\n2simultaneously in any model ﬁtted) may be used to describe\\nX2in the model. If a tentative transformation of X2is speciﬁed, say g(X2),\\nthe adequacy of this transformation can be tested by expanding g(X2)i na\\nspline function and testing for linearity. If one is concerned only with predic-\\ntion and not with statistical inference, one can attempt to ﬁnd a simplifying\\ntransformationforapredictorbyplotting g(X2)against ˆf(X2)(theestimated\\nspline transformation) for a variety of g, seeking a linearizing transformation\\nofX2. When there are nominal or binary predictors in the model in addi-\\ntion to the continuous predictors, it should be noted that there are no shape\\nassumptions to verify for the binary/nominal predictors. One need only test\\nfor interactions between these predictors and the others.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2559f32b-ec3c-494e-9490-401fab7001fe', embedding=None, metadata={'page_label': '36', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='36 2 General Aspects of Fitting Regression Models\\nIf the model contains more than one continuous predictor, all may be ex-\\npanded with spline functions in orderto test linearityor to describe nonlinear\\nrelationships. If one did desire to assess simultaneously, for example, the lin-\\nearity of predictors X2andX3in the presence of a linear or binary predictor\\nX1, the model could be speciﬁed as\\nC(Y|X)=β0+β1X1+β2X2+β3X′\\n2+β4X′′\\n2\\n+β5X3+β6X′\\n3+β7X′′\\n3, (2.36)\\nwhereX′\\n2,X′′\\n2,X′\\n3,an dX′′\\n3representcomponentsoffour knotrestrictedcubic\\nspline functions.\\nThe test of linearity for X2(with 2 d.f.) is H0:β3=β4=0 .T h eo v e r a l l\\ntest of linearity for X2andX3isH0:β3=β4=β6=β7=0 ,w i t h4d . f .\\nBut as described further in Section 4.1, even though there are many reasons\\nfor allowing relationships to be nonlinear, there are reasons for not testing\\nthe nonlinear components for signiﬁcance, as this might tempt the analyst to\\nsimplify the model thus distorting inference.234Testing for linearityis usually\\nbest done to justify to non-statisticians the need for complexity to explain or\\npredict outcomes.\\n2.7.2 Modeling and Testing Complex Interactions\\nFor testing interaction between X1andX2(after a needed transformation\\nmay have been applied), often a product term (e.g., X1X2) can be added\\nto the model and its coeﬃcient tested. A more general simultaneous test of\\nlinearityandlackofinteractionforatwo-variablemodelinwhichonevariable\\nis binary (or is assumed linear) is obtained by ﬁtting the model\\nC(Y|X)=β0+β1X1+β2X2+β3X′\\n2+β4X′′\\n2 (2.37)\\n+β5X1X2+β6X1X′\\n2+β7X1X′′\\n2\\nand testing H0:β3=...=β7= 0. This formulation allows the shape of the\\nX2eﬀect to be completely diﬀerent for each level of X1. There is virtually\\nno departure from linearity and additivity that cannot be detected from this\\nexpanded model formulation if the number of knots is adequate and X1is\\nbinary. For binary logistic models, this method is equivalent to ﬁtting two\\nseparate spline regressions in X2. 10\\nInteractions can be complex when all variables are continuous. An ap-\\nproximate approach is to reduce the variables to two transformed variables,\\ninwhichcaseinteractionmaysometimesbe approximatedbyasingleproduct\\nof the two new variables. A disadvantage of this approach is that the esti-\\nmates of the transformations for the two variables will be diﬀerent depending', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5ceeffd3-8a40-4738-9a28-166dee0c20dc', embedding=None, metadata={'page_label': '37', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Assessment of Model Fit 37\\non whether interaction terms are adjusted for when estimating“main eﬀects.”\\nA good compromisemethod involvesﬁtting interactionsof the form X1f(X2)\\nandX2g(X1):\\nC(Y|X)=β0+β1X1+β2X′\\n1+β3X′′\\n1\\n+β4X2+β5X′\\n2+β6X′′\\n2\\n+β7X1X2+β8X1X′\\n2+β9X1X′′\\n2 (2.38)\\n+β10X2X′\\n1+β11X2X′′\\n1\\n(fork= 4 knots for both variables). The test of additivity is H0:β7=β8=\\n...=β11= 0 with 5 d.f. A test oflackof ﬁt for the simple product interaction\\nwithX2isH0:β8=β9=0,and a test of lack of ﬁt for the simple product\\ninteraction with X1isH0:β10=β11=0 .\\nA general way to model and test interactions, although one requiring a\\nlarger number of parameters to be estimated, is based on modeling the X1×\\nX2×Yrelationship with a smooth three-dimensional surface. A cubic spline\\nsurface can be constructed by covering the X1−X2plane with a grid and\\nﬁtting a patch-wisecubic polynomial in two variables.The gridis ( ui,vj),i=\\n1,...,k,j =1,...,k, where knots for X1are (u1,...,u k) and knots for X2\\nare (v1,...,v k). The number of parameters can be reduced by constraining\\nthe surface to be of the form aX1+bX2+cX1X2in the lower left and\\nupper right corners of the plane. The resulting restricted cubic spline surface\\nis described by a multiple regression model containing spline expansions in\\nX1andX2and all cross-products of the restricted cubic spline components\\n(e.g.,X1X′\\n2). If the same number of knots kis used for both predictors,\\nthe number of interaction terms is ( k−1)2. Examples of various ways of\\nmodeling interaction are given in Chapter 10. Spline functions made up of\\ncross-products of all terms of individual spline functions are called tensor\\nsplines.50,27411\\nThe presence of more than two predictors increases the complexity of tests\\nfor interactions because of the number of two-way interactions and because\\nof the possibility of interaction eﬀects of order higher than two. For example,\\nin a model containing age, sex, and diabetes, the important interaction could\\nbe that older male diabetics have an exaggeratedrisk. However, higher-order\\ninteractions are often ignored unless speciﬁed a priori based on knowledge of\\nthe subject matter. Indeed, the number of two-wayinteractions alone is often\\ntoo large to allow testing them all with reasonable power while controlling\\nmultiple comparison problems. Often, the only two-way interactions we can\\naﬀord to test are those that were thought to be important before examining\\nthe data. A good approach is to test for all such pre-speciﬁed interaction\\neﬀects with a single global (pooled) test. Then, unless interactions involving\\nonly one of the predictors are of special interest, one can either drop all\\ninteractions or retain all of them.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f922e67-1d22-41a2-a784-c6f2cba3c930', embedding=None, metadata={'page_label': '38', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='38 2 General Aspects of Fitting Regression Models\\nFor some problems a reasonable approach is, for each predictor separately,\\nto test simultaneously the joint importance of all interactions involving that\\npredictor. For ppredictors this results in ptests each with p−1 degrees\\nof freedom. The multiple comparison problem would then be reduced from\\np(p−1)/2 tests (if all two-way interactions were tested individually) to p\\ntests.\\nIn the ﬁelds of biostatistics and epidemiology, some types of interactions\\nthat have consistently been found to be important in predicting outcomes\\nand thus may be pre-speciﬁed are the following.\\n1. Interactions between treatment and the severity of disease being treated.\\nPatients with little disease can receive little beneﬁt.\\n2. Interactions involving age and risk factors. Older subjects are generally\\nless aﬀected by risk factors. They had to have been robust to survive to\\ntheir current age with risk factors present.\\n3. Interactionsinvolvingageandtypeofdisease.Somediseasesareincurable\\nand have the same prognosis regardless of age. Others are treatable or\\nhave less eﬀect on younger patients.\\n4. Interactions between a measurement and the state of a subject during a\\nmeasurement. Respiration rate measured during sleep may have greater\\npredictive value and thus have a steeper slope versus outcome than res-\\npiration rate measured during activity.\\n5. Interaction between menopausal status and treatment or risk factors.\\n6. Interactions between race and disease.\\n7. Interactions between calendar time and treatment. Some treatments have\\nlearning curves causing secular trends in the associations.\\n8. Interactions between month of the year and other predictors, due to sea-\\nsonal eﬀects.\\n9. Interaction between the quality and quantity of a symptom, for example,\\ndaily frequency of chest pain ×severity of a typical pain episode.\\n10. Interactions between study center and treatment. 12\\n2.7.3 Fitting Ordinal Predictors\\nFor the case of an ordinal predictor, spline functions are not useful unless\\nthere are so many categories that in essence the variable is continuous. When\\nthe number of categories kis small (three to ﬁve, say), the variable is usu-\\nally modeled as a polytomous factor using indicator variables or equivalently\\nas one linear term and k−2 indicators. The latter coding facilitates testing\\nfor linearity. For more categories, it may be reasonable to stratify the data\\nby levels of the variable and to compute summary statistics (e.g., logit pro-\\nportions for a logistic model) or to examine regression coeﬃcients associated\\nwith indicator variables over categories. T hen one can attempt to summarize\\nthe pattern with a linear or some other simple trend. Later hypothesis tests', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67233597-9d42-41d9-87aa-5081810773b1', embedding=None, metadata={'page_label': '39', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.7 Assessment of Model Fit 39\\nmust take into account this data-driven scoring (by using >1 d.f., for exam-\\nple), but the scoring can save degrees of freedom when testing for interaction\\nwith other factors. In one dataset, the number of comorbid diseases was used\\nto summarize the risk of a set of diseases that was too large to model. By\\nplotting the logit of the proportion of deaths versus the number of diseases,\\nit was clear that the square of the number of diseases would properly score\\nthe variables.\\nSometimes it is useful to code an ordinal predictor with k−1 indicator\\nvariables of the form [ X≥vj], where j=2,...,kand [h]i s1i fhis true,\\n0o t h e r w i s e .648Although a test of linearity does not arise immediately from\\nthis coding, the regression coeﬃcients are interpreted as amounts of change\\nfrom the previous category. A test of whether the last mcategories can be\\ncombined with the category k−mdoes follow easily from this coding.\\n2.7.4 Distributional Assumptions\\nThegenerallinearregressionmodelisstatedas C(Y|X)=Xβtohighlightits\\nregression assumptions. For logistic regression models for binary or nominal\\nresponses, there is no distributional assumption if simple random sampling\\nis used and subjects’ responses are independent. That is, the binary logistic\\nmodel and all of its assumptions are contained in the expression logit {Y=\\n1|X}=Xβ. For ordinary multiple regression with constant variance σ2,w e\\nusuallyassumethat Y−Xβis normallydistributed with mean0and variance\\nσ2. This assumption can be checked by estimating βwithˆβand plotting the\\noverall distribution of the residuals Y−Xˆβ, the residuals against ˆY,a n dt h e\\nresiduals againsteach X. For the latter two, the residuals should be normally\\ndistributedwithineachneighborhoodof ˆYorX.Aweakerrequirementisthat\\nthe overalldistribution of residuals is normal; this will be satisﬁed if all of the\\nstratiﬁedresidualdistributionsarenormal.Notea hidden assumptionin both\\nmodels, namely, that there are no omitted predictors. Other models, such as\\nthe Weibull survival model or the Cox132proportional hazards model, also\\nhavedistributionalassumptionsthatarenotfullyspeciﬁedby C(Y|X)=Xβ.\\nHowever, regression and distributional assumptions of some of these models\\nare encapsulated by\\nC(Y|X)=C(Y=y|X)=d(y)+Xβ (2.39)\\nf o rs o m ec h o i c eo f C.H e r eC(Y=y|X) is a property of the response Y\\nevaluated at Y=y, given the predictor values X,a n dd(y) is a component of\\nthe distribution of Y. For the Cox proportional hazards model, C(Y=y|X)\\ncan be written as the log of the hazard of the event at time y, or equivalently\\nas the log of the −log of the survival probability at time y,a n dd(y)c a nb e\\nthought of as a log hazard function for a“standard”subject.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e48c949d-8833-4dcc-9385-d7dad0b4d396', embedding=None, metadata={'page_label': '40', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='40 2 General Aspects of Fitting Regression Models\\nIf we evaluated the property C(Y=y|X)a tp r e d i c t o rv a l u e s X1andX2,\\nthe diﬀerence in properties is\\nC(Y=y|X1)−C(Y=y|X2)=d(y)+X1β (2.40)\\n−[d(y)+X2β]\\n=(X1−X2)β,\\nwhich is independent of y. One way to verify part of the distributional as-\\nsumption is to estimate C(Y=y|X1)a n dC(Y=y|X2)f o rs e tv a l u e so f\\nX1andX2using a method that does not make the assumption, and to plot\\nC(Y=y|X1)−C(Y=y|X2)v e r s u s y. This function should be ﬂat if the\\ndistributional assumption holds. The assumption can be tested formally if\\nd(y) can be generalized to be a function of Xas well as y. A test of whether\\nd(y|X) depends on Xis a test of one part of the distributional assumption.\\nFor example, writing d(y|X)=d(y)+XΓlog(y)w h e r e\\nXΓ=Γ1X1+Γ2X2+...+ΓkXk (2.41)\\nand testing H0:Γ1=...=Γk= 0 is one way to test whether d(y|X)d e -\\npends on X.ForsemiparametricmodelssuchastheCoxproportionalhazards\\nmodel, the only distributional assumption is the one stated above, namely,\\nthatthediﬀerenceinpropertiesbetweentwosubjectsdependsonlyonthedif-\\nference in the predictorsbetween the twosubjects. Other,parametric,models\\nassume in addition that the property C(Y=y|X) has a speciﬁc shape as a\\nfunction of y, that is that d(y) has a speciﬁc functional form. For example,\\nthe Weibull survival model has a speciﬁc assumption regarding the shape of\\nthe hazard or survival distribution as a function of y.\\nAssessmentsofdistributionalassumptionsarebestunderstoodbyapplying\\nthese methods to individual models as is demonstrated in later chapters.\\n2.8 Further Reading\\n1References [ 152,575,578] have more information about cubic splines.\\n2See Smith578for a good overview of spline functions.\\n3Morematerial aboutnatural splinesmay befoundindeBoor152.McNeiletal.451\\ndiscuss the overall smoothness of natural splines in terms of the integral of the\\nsquare of the second derivative of the regression function, over the range o f\\nthe data. Govindarajulu et al.230compared restricted cubic splines, penalized\\nsplines, and fractional polynomial532ﬁts and found that the ﬁrst two methods\\nagreed with each other more than with estimated fractional polynomials.\\n4A tutorial on restricted cubic splines is in [ 271].\\n5Durrleman and Simon168provide examples in which knots are allowed to be\\nestimated as free parameters, jointly with the regression coeﬃcients. They found\\nthat even though the“optimal”knots were often far from a priori knot locations,\\nthe model ﬁts were virtually identical.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb60291a-eed5-4fde-9942-68ba4329ea7b', embedding=None, metadata={'page_label': '41', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.8 Further Reading 41\\n6Contrast Hastie and Tibshirani’s generalized nonparametric additive models275\\nwith Stone and Koo’s595additive model in which each continuous predictor is\\nrepresented with a restricted cubic spline function.\\n7Gray237,238providedsome comparisons withordinary regression splines,buthe\\ncompared penalized regression splines with non-restricted splines with only two\\nknots.Two knots were chosen so as to limit the degrees of freedom needed by the\\nregression spline method to a reasonable number. Gray argued that regression\\nsplines are sensitive to knot locations, and he is correct when only two knots\\nare allowed and no linear tail restrictions are imposed. Two knots also prevent\\nthe (ordinary maximum likelihood) ﬁt from utilizing some local behavior of\\nthe regression relationship. For penalized likelihood estimation using B-splines,\\nGray238provided extensive simulation studies of type I and II error for testing\\nassociation in which the true regression function, number of knots, and amount\\nof likelihood penalization were varied. He studied both normal regression and\\nCox regression.\\n8Breiman et al.’s original CART method69used the Gini criterion for splitting.\\nLater work has used log-likelihoods.109Segal,562LeBlanc and Crowley,389and\\nCiampiet al.107,108and Kele¸ s andSegal342haveextendedrecursivepartitioning\\nto censored survival data using the log-rank statistic as the criterion. Zhang682\\nextended tree models to handlemultivariate binaryresponses. Schmoor et al.556\\nusedamoregeneral splittingcriterionthatisusefulintherapeutictrials,namely,\\na Cox test for main and interacting eﬀects. Davis and Anderson149used an\\nexponential survival model as the basis for tree construction. Ahn and Loh7\\ndeveloped a Cox proportional hazards model adaptation of recursive par tition-\\ning along with bootstrap and cross-validation-based methods to protect against\\n“over-splitting.”The Cox-based regression tree methods of Ciampi et al.107have\\na unique feature that allows for construction of “treatment interaction trees”\\nwith hierarchical adjustment for baseline variables. Zhang et al.683provided a\\nnew method for handling missing predictor values that is simpler than using\\nsurrogate splits. See [ 34,140,270,629] for examples using recursive partitioning\\nfor binary responses in which the prediction trees did not validate well.\\n9443,629discuss other problems with tree models.\\n10For ordinary linear models, the regression estimates are the same as obtained\\nwith separate ﬁts, but standard errors are diﬀerent (since a pooled standard\\nerror is used for the combined ﬁt). For Cox132regression, separate ﬁts can be\\nslightly diﬀerent since each subset would use a separate ranking of Y.\\n11Gray’s penalized ﬁxed-knot regression splines can be useful for estimating joint\\neﬀects of two continuous variables while allowing the analyst to control the\\neﬀective number of degrees of freedom in the ﬁt [ 237,238, Section 3.2]. When\\nYis a non-censored variable, the local regression model of Cleveland et al.,96\\na multidimensional scatterplot smoother mentioned in Section 2.4.7,p r o v i d e sa\\ngood graphical assessment of the joint eﬀects of several predictors so that the\\nforms of interactions can be chosen. See Wang et al.653and Gustafson248for\\nseveral other ﬂexible approaches to analyzing interactions among continuous\\nvariables.\\n12Study site by treatment interaction is often the interaction that is worried about\\nthe most in multi-center randomized clinical trials, because regulatory agencies\\nare concerned with consistency of treatment eﬀects over study centers. However,\\nthis type of interaction is usually the weakest and is diﬃcult to assess when\\nthere are many centers due to the number of interaction parameters to estimate.\\nSchemper545discussesvarioustypesofinteractions andageneral nonparametric\\ntest for interaction.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b9ca2b11-525a-4be6-801b-5d33ece51742', embedding=None, metadata={'page_label': '42', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='42 2 General Aspects of Fitting Regression Models\\n2.9 Problems\\nFor problems 1 to 3, state each mode l statistically, identifying each predictor\\nwith one or more component variables. Identify and interpret each regression\\nparameter except for coeﬃcients of nonlinear terms in spline functions. State\\neach hypothesis below as a formal statistical hypothesis involving the proper\\nparameters, and give the (numerator) degrees of freedom of the test. State\\nalternative hypotheses carefully with respect to unions or intersections of\\nconditions and list the type of alternatives to the null hypothesis that the\\ntest is designed to detect.c\\n1. A property of Ysuch as the mean is linear in age and blood pressure\\nand there may be an interaction between the two predictors. Test H0:\\nthere is no interaction between age and blood pressure. Also test H0:\\nblood pressure is not associated with Y(in any fashion). State the eﬀect\\nof blood pressure as a function of age, and the eﬀect of age as a function\\nof blood pressure.\\n2. Consider a linear additive model involving three treatments (control, drug\\nZ, and drug Q) and one continuous adjustment variable, age. Test H0:\\ntreatment group is not associated with response, adjusted for age. Also\\ntestH0: response for drug Z has the same property as the response for\\ndrug Q, adjusted for age.\\n3. Consider models each with two predictors, temperature and white blood\\ncount (WBC), for which temperature is always assumed to be linearly\\nrelated to the appropriate property of the response, and WBC may or\\nmay not be linear (depending on the particular model you formulate for\\neach question). Test:\\na.H0: WBC is not associated with the response versus Ha:W B Ci s\\nlinearly associated with the property of the response.\\nb.H0: WBC is not associated with YversusHa: WBC is quadratically\\nassociated with Y. Also write down the formal test of linearity against\\nthis quadratic alternative.\\nc.H0: WBC is not associated with YversusHa:W B Cr e l a t e dt ot h e\\npropertyof the response througha smooth spline function; for example,\\nfor WBC the model requires the variables WBC, WBC′,a n dW B C′′\\nwhere WBC′and WBC′′represent nonlinear components (if there are\\nfour knots in a restricted cubic spline function). Also write down the\\nformal test of linearity against this spline function alternative.\\nd. Test for a lack of ﬁt (combined nonlinearity or non-additivity) in an\\noverallmodelthattakesthe formofaninteractionbetweentemperature\\nand WBC, allowing WBC to be modeled with a smooth spline function.\\n4. For a ﬁtted model Y=a+bX+cX2derive the estimate of the eﬀect on\\nYof changing Xfromx1tox2.\\ncIn other words, under what assumptions does the test have maximum power?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5d4f221b-22e0-4ebf-bf94-cbdb010045a7', embedding=None, metadata={'page_label': '43', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='2.9 Problems 43\\n5. In“The Class of 1988: A Statistical Portrait,”the College Board reported\\nmean SAT scores for each state. Use an ordinary least squares multiple\\nregression model to study the mean verbal SAT score as a function of the\\npercentage of students taking the test in each state. Provide plots of ﬁtted\\nfunctions and defend your choice of the“best”ﬁt. Make sure the shape of\\nthe chosen ﬁt agrees with what you know about the variables. Add the\\nraw data points to plots.\\na. Fit a linear spline function with a knot at X= 50%. Plot the data\\nand the ﬁtted function and do a formal test for linearity and a test\\nfor association between XandY. Give a detailed interpretation of the\\nestimated coeﬃcients in the linear spline model, and use the partial\\nt-test to test linearity in this model.\\nb. Fit a restricted cubic spline function with knots at X= 6, 12, 58, and\\n68% (not percentile).dPlot the ﬁtted function and do a formal test of\\nassociation between XandY. Do two tests of linearity that test the\\nsame hypothesis:\\ni. by using a contrast to simultaneously test the correct set of coeﬃ-\\ncients against zero (done by the anovafunction in rms);e\\nii. bycomparingthe R2fromthecomplexmodelwiththatfromasimple\\nlinear model using a partial F-test.\\nExplain why the tests of linearity have the d.f. they have.\\nc. Using subject matter knowledge, pick a ﬁnal model (from among the\\nprevious models or using another one) that makes sense.\\nThe data are found in Table 2.4and may be created in Rusing the sat.r\\ncode on the RMS course web site.\\n6. Derive the formulas for the restricted cubic spline component variables\\nwithout cubing or squaring any terms.\\n7. Prove that each component variable is linear in XwhenX≥tk,t h e\\nlast knot, using general principles and not algebra or calculus. Derive an\\nexpression for the restricted spline regression function when X≥tk.\\n8. Considera two–stageprocedureinwhichonetestsforlinearityofthe eﬀect\\nof a predictor Xona propertyof the response C(Y|X) againsta quadratic\\nalternative. If the two–tailed test of linearity is signiﬁcant at the αlevel,\\na two d.f. test of association between XandYis done. If the test for\\nlinearity is not signiﬁcant, the square term is dropped and a linear model\\nis ﬁtted. The test of association between XandYis then (apparently) a\\none d.f. test.\\na. Write a formal expression for the test statistic for association.\\ndNote: To pre-specify knots for restricted cubic spline functions, use something like\\nrcs(predictor, c(t1,t2,t3,t4)) , where the knot locations are t1, t2, t3, t4 .\\neNote that anovainrmscomputes all needed test statistics from a single model ﬁt\\nobject.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cab3fddb-aee4-4ad1-929b-80372ff3d404', embedding=None, metadata={'page_label': '44', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='44 2 General Aspects of Fitting Regression Models\\nb. Write an expression for the nominal P–value for testing association\\nusing this strategy.\\nc. Write anexpressionforthe actual P–valueoralternativelyfor the type–\\nI error if using a ﬁxed critical value for the test of association.\\nd. For the same two–stage strategy consider an estimate of the eﬀect on\\nC(Y|X) of increasing Xfromatob. Write a brief symbolic algorithm\\nfor deriving a true two–sided 1 −αconﬁdence interval for the b:aeﬀect\\n(diﬀerence in C(Y)) using the bootstrap.\\nTable 2.4 SAT data from the College Board, 1988\\n% Taking SAT Mean Verbal % Taking SAT Mean Verbal\\n(X)S core(Y)( X)S core(Y)\\n4 482 24 440\\n5 498 29 460\\n5 513 37 448\\n6 498 43 441\\n6 511 44 424\\n7 479 45 417\\n9 480 49 422\\n9 483 50 441\\n10 475 52 408\\n10 476 55 412\\n10 487 57 400\\n10 494 58 401\\n12 474 59 430\\n12 478 60 433\\n13 457 62 433\\n13 485 63 404\\n14 451 63 424\\n14 471 63 430\\n14 473 64 431\\n16 467 64 437\\n17 470 68 446\\n18 464 69 424\\n20 471 72 420\\n22 455 73 432\\n23 452 81 436', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1722f995-1c4c-4c92-8bf1-e4984172a64d', embedding=None, metadata={'page_label': '45', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 3\\nMissing Data\\n3.1 Types of Missing Data\\nThere are missing data in the majority of datasets one is likely to encounter.\\nBefore discussing some of the problems of analyzing data in which some\\nvariables are missing for some subjects, we deﬁne some nomenclature. 1\\nMissing completely at random (MCAR)\\nData are missing for reasons that are unrelated to any characteristics or re-\\nsponses for the subject, including the value of the missing value, were it to\\nbe known. Examples include missing laboratory measurements because of a\\ndropped test tube (if it was not dropped because of knowledge of any mea-\\nsurements), a study that ran out of funds before some subjects could return\\nfor follow-up visits, and a survey in which a subject omitted her response to\\na question for reasons unrelated to the response she would have made or to\\nany other of her characteristics.\\nMissing at random (MAR)\\nData are not missing at random, but the probability that a value is missing\\ndepends on values of variables that were actually measured. As an example,\\nconsider a survey in which females are less likely to provide their personal\\nincome in general (but the likelihood of responding is independent of her\\nactual income). If we know the sex of every subject and have income levels\\nfor some of the females, unbiased sex-speciﬁc income estimates can be made.\\nThat is because the incomes we do have for some of the females are a random\\nsample of all females’ incomes. Another way of saying that a variable is MAR\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 345', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f33fedfc-2116-4e0d-9593-c4c0863eee93', embedding=None, metadata={'page_label': '46', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='46 3 Missing Data\\nis that given the values of other available variables, subjects having missing\\nvalues are only randomly diﬀerent from other subjects.535Or to paraphrase\\nGreenland and Finkle,242for MAR the missingness of a covariable cannot\\ndepend on unobserved covariable values; for example whether a predictor is\\nobserved cannot depend on another predictor when the latter is missing but\\nit can depend on the latter when it is observed. MAR and MCAR data are\\nalso called ignorable non-responses.\\nInformative missing (IM)\\nThe tendency for a variable to be missing is a function of data that are not\\navailable, including the case when data tend to be missing if their true values\\nare systematically higher or lower. An example is when subjects with lower\\nincome levels or very high incomes are less likely to provide their personal in-\\ncomein aninterview.IMisalsocallednonignorablenon-responseandmissing\\nnot at random (MNAR).\\nIMisthemostdiﬃculttypeofmissingdatatohandle.Inmanycases,there\\nis no ﬁx for IM nor is there a way to use the data to test for the existence of\\nIM. External considerations must dictate the choice of missing data models,\\nand there are few clues for specifying a model under IM. MCAR is the easiest\\ncase to handle. Our ability to correctly analyze MAR data depends on the\\navailability of other variables (the sex of the subject in the example above).\\nMost of the methods available for dealing with missing data assume the data\\nare MAR. Fortunately, even though the MAR assumption is not testable, it\\nmay hold approximately if enough variables are included in the imputation\\nmodels256.\\n3.2 Prelude to Modeling\\nNo matter whether one deletes incomplete cases, carefully imputes (esti-\\nmates) missing data, or uses a full maximum likelihood or Bayesian tech-\\nniques to incorporate partial data, it is beneﬁcial to characterize patterns\\nof missingness using exploratory data analysis techniques. These techniques\\ninclude binary logistic models and recursive partitioning for predicting the\\nprobability that a given variable is missing. Patternsof missingness should be\\nreported to help readersunderstand the limitations ofincomplete data.If you\\ndo decide to use imputation, it is alsoimportant to describe how variablesare\\nsimultaneously missing. A cluster analysis of missing value status of all the\\nvariables is useful here. This can uncover cases where imputation is not as ef-\\nfective. For example,if the onlyvariablemoderatelyrelatedto diastolic blood\\npressure is systolic pressure, but both pressures are missing on the same sub-\\njects, systolic pressure cannot be used to e stimate diastolic blood pressure. R', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f691bf2c-660b-49ba-adb6-e54b0e1c1471', embedding=None, metadata={'page_label': '47', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.4 Problems with Simple Alternatives to Imputation 47\\nfunctions naclusandnaplotin theHmiscpackage (see p. 142) can help detect\\nhow variables are simultaneously missing. Recursive partitioning (regression\\ntree) algorithms (see Section 2.5) are invaluable for describing which kinds of\\nsubjects are missing on a variable. Logistic regressionis also an excellent tool\\nfor this purpose. A later example (p. 302) demonstrates these procedures.\\nIt can also be helpful to explore the distribution of non-missing Yby the\\nnumber of missing variables in X(including zero, i.e., complete cases on X).\\n3.3 Missing Values for Diﬀerent Types\\nof Response Variables\\nWhen the response variable Yis collected seriallybut some subjects drop out\\nof the study before completion, there are many ways of dealing with partial\\ninformation42,412,480includingmultipleimputationinphases,381oreﬃciently\\nanalyzingall availableserialdata using afull likelihoodmodel. When Yis the\\ntime until an event, there are actually no missing values of Ybut follow-up\\nwill be curtailed for some subjects. That leaves the case where the response\\nis completely measured once.\\nIt is common practice to discard subjects having missing Y. Before doing\\nso, at minimum an analysis should be done to characterize the tendency\\nforYto be missing, as just described. For example, logistic regression or\\nrecursive partitioning can be used to predict whether Yis missing and to\\ntest for systematic tendencies as opposed to Ybeing missing completely at\\nrandom. In many models, though, more eﬃcient and less biased estimates of\\nregression coeﬃcients can be made by also utilizing observations missing on\\nYthat are non-missing on X. Hence there is a deﬁnite place for imputation\\nofY.v o nH i p p e l645found advantages of using all variables to impute all\\nothers, and once imputation is ﬁnished, discarding those observations having\\nmissingY. However if missing Yvalues are MCAR, up-front deletion of cases\\nhaving missing Ymay sometimes be preferred, as imputation requirescorrect\\nspeciﬁcation of the imputation model. 2\\n3.4 Problems with Simple Alternatives\\nto Imputation\\nIncomplete predictor information is a very common missing data problem.\\nStatisticalsoftwarepackagesusecasewisedeletioninhandlingmissingpredic-\\ntors; that is, any subject having anypredictor or Ymissing will be excluded\\nfrom a regression analysis. Casewise deletion results in regression coeﬃcient\\nestimates that can be terribly biased, imprecise, or both353. First consider an\\nexample where bias is the problem. Suppose that the response is death and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='461d1cc2-8830-48c8-bfc3-a302a57806fd', embedding=None, metadata={'page_label': '48', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='48 3 Missing Data\\nthe predictors are age, sex, and blood pressure, and that age and sex were\\nrecorded for every subject. Suppose that blood pressure was not measured\\nfor a fraction of 0.10 of the subjects, and the most common reason for not\\nobtaining a blood pressure was that the subject was about to die. Deletion\\nof these very sick patients will cause a major bias (downward) in the model’s\\nintercept parameter. In general, casewise deletion will bias the estimate of 3\\nthe model’s intercept parameter (as well as others) when the probability of\\na case being incomplete is related to Yand not just to X[422,E x a m p l e\\n3.3]. van der Heijden et al.628discuss how complete case analysis (casewise\\ndeletion) usually assumes MCAR.\\nNow consider an example in which casewise deletion of incomplete records\\nis ineﬃcient. The ineﬃciency comes from the reduction of sample size, which\\ncausesstandarderrorstoincrease,162conﬁdenceintervalstowiden,andpower\\nof tests of association and tests of lack of ﬁt to decrease. Suppose that the\\nresponse is the presence of coronary artery disease and the predictors are\\nage, sex, LDL cholesterol, HDL cholesterol, blood pressure, triglyceride, and\\nsmoking status. Suppose that age, sex, and smoking are recorded for all sub-\\njects, but that LDL is missing in 0.18 of the subjects, HDL is missing in 0.20,\\nand triglyceride is missing in 0.21. A ssume that all missing data are MCAR\\nand that all of the subjects missing LDL are also missing HDL and that\\noverall 0.28 of the subjects have one or more predictors missing and hence\\nwould be excluded from the analysis.If total cholesterolwere knownon every\\nsubject, even though it does not appear in the model, it (along perhaps with\\nage and sex) can be used to estimate ( impute) LDL and HDL cholesterol and\\ntriglyceride, perhaps using regressionequations from other studies. Doing the\\nanalysis on a“ﬁlled in”dataset will result in more precise estimates because\\nthe sample size would then include the other 0.28 of the subjects.\\nIn general, observations should only be discarded if the MCAR assump-\\ntion is justiﬁed, there is a rarely missing predictor of overriding importance\\nthat cannot be reliably imputed from other information, or if the fraction of\\nobservations excluded is very small and the originalsample size is large.Even\\nthen, there is no advantage of such deletion other than saving analyst time.\\nIf a predictor is MAR but its missingness depends on Y, casewise deletion is\\nbiased.\\nThe ﬁrst blood pressure example points out why it can be dangerous to\\nhandle missing values by adding a dummy variable to the model. Many ana-\\nlysts would set missing blood pressuresto a constant(it doesn’t matter which\\nconstant) and add a variable to the model such as is.na(blood.pressure) in\\nRnotation. The coeﬃcient for the latter dummy variable will be quite large\\nin the earlier example, and the model will appear to have great ability to\\npredict death. This is because some of the left-hand side of the model con-\\ntaminates the right-hand side; that is, is.na(blood.pressure) is correlated\\nwith death. For categorical variables, another common practice is to add a 4\\nnew category to denote missing, adding one more degree of freedom to the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36f960c7-4848-4c3f-a9da-760bf02e5226', embedding=None, metadata={'page_label': '49', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.5 Strategies for Developing an Imputation Model 49\\npredictor and changing its meaning.aJones326, Allison [ 12, pp. 9–11], Don-\\nders et al.161,K n o le ta l .353and van der Heijden et al.628describe why both\\nof these missing-indicator methods are invalid even when MCAR holds. 5\\n3.5 Strategies for Developing an Imputation Model\\nExcept in special circumstances that usually involve only very simple models,\\nthe primary alternative to deleting incomplete observations is imputation of\\nthe missing values. Many non-statisticians ﬁnd the notion of estimating data\\ndistasteful, but the way to think about imputation of missing values is that\\n“making up”data is better than discarding valuable data. It is especially dis-\\ntressing to have to delete subjects who are missing on an adjustment variable\\nwhen a major variable of interest is not missing. So one goal of imputation\\nis to use as much information as possible for examining any one predictor’s\\nadjusted associationwith Y. The overallgoal of imputation is to preservethe\\ninformation and meaning of the non-missing data.\\nAt this point the analyst must make some decisions about the information\\nto use in computing predicted values for missing values.\\n1. Imputation of missing values for one of the variables can ignore all other\\ninformation.Missingvalues canbe ﬁlled in by samplingnon-missingvalues\\nof the variable, or by using a constant such as the median or mean non-\\nmissing value.\\n2. Imputation algorithms can be based only on external information not oth-\\nerwise used in the model for Yin addition to variables included in later\\nmodeling. For example, family income can be imputed on the basis ofloca-\\ntion of residence when such information is to remain conﬁdential for other\\naspects of the analysis or when such information would require too many\\ndegrees of freedom to be spent in the ultimate response model.\\n3. Imputations can be derived by only analyzing interrelationships among\\ntheXs.\\n4. Imputations can use relationships among the Xs and between XandY.\\n5. Imputations can use X,Y, and auxiliary variables not in the model\\npredicting Y.\\n6. Imputations can take into account the reason for non-response if known.\\nThe model to estimate the missing values in a sometimes-missing (target)\\nvariable should include all variables that are either\\naThis may work if values are “missing” because of “not applicable”, e.g. one has a\\nmeasure of marital happiness, dichotomized as high or low, but the sample contains\\nsome unmarried people. One could have a 3-category variable with values high, l ow,\\nand unmarried (Paul Allison, IMPUTE e-mail list, 4Jul09).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6659e11-947b-4a28-a3b6-44c447278ac5', embedding=None, metadata={'page_label': '50', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='50 3 Missing Data\\n1. related to the missing data mechanism;\\n2. havedistributionsthatdiﬀerbetweensubjectsthathavethetargetvariable\\nmissing and those that have it measured;\\n3. are associated with the target variable when it is not missing; or\\n4. are included in the ﬁnal response model43.\\nThe imputation and analysis (response) models should be“congenial”or the\\nimputation model should be more general than the response model or make\\nwell-founded assumptions256.\\nWhen a variable, say Xj, is to be included as a predictor of Y,a n dXj\\nis sometimes missing, ignoring the relationship between XjandYfor those\\nobservations for which both are known will bias regression coeﬃcients for\\nXjtoward zero in the outcome model.421On the other hand, using Yto\\nsingly impute Xjusing a conditional mean will cause a large inﬂation in\\nthe apparent importance of Xjin the ﬁnal model. In other words, when the\\nmissingXjare replaced with a mean that is conditional on Ywithout a\\nrandom component, this will result in a falsely strong relationship between\\nthe imputed Xjvalues and Y.\\nAtﬁrstglanceitmightseemthatusing Yto impute oneormoreof the Xs,\\neven with allowance for the correctamount of random variation, would result\\nin a circular analysis in which the importance of the Xs will be exaggerated.\\nBut the relationship between XandYin the subset of imputed observations\\nwillonly be asstrongasthe associationsbetween XandYthatareevidenced\\nby the non-missing data. In other words, regression coeﬃcients estimated\\nfrom a dataset that is completed by imputation will not in general be biased\\nhigh as long as the imputed values have similar variation as non-missing data\\nvalues.\\nThe next important decision about developing imputation algorithms is\\nthe choice of how missing values are estimated.\\n1. Missings can be estimated using single“best guesses”(e.g., predicted con-\\nditional expected values or means) based on relationships between non-\\nmissing values. This is called single imputation of conditional means.\\n2. Missing Xj(orY) can be estimated using single individual predicted val-\\nues, where by predicted value we mean a random variable value from the\\nwhole conditional distribution of Xj. If one uses ordinary multiple regres-\\nsion to estimate XjfromYand the other Xs, a random residual would\\nbe added to the predicted mean value. If assuming a normal distribution\\nforXjconditional on the other data, such a residual could be computed\\nby a Gaussian random number generator given an estimate of the residual\\nstandard deviation. If normality is not assumed, the residual could be a\\nrandomly chosen residual from the actual computed residuals. When m\\nmissing values need imputation for Xj, the residuals could be sampled\\nwith replacement from the entire vector of residuals as in the bootstrap.\\nBetter still according to Rubin and Schenker535would be to use the“ap-\\nproximate Bayesian bootstrap”which involves sampling nresiduals with', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='12f6860f-2f2d-4dad-b0ed-f51795822582', embedding=None, metadata={'page_label': '51', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.5 Strategies for Developing an Imputation Model 51\\nreplacementfromthe original nestimated residuals(from observationsnot\\nmissing on Xj), then sampling mresiduals with replacement from the ﬁrst\\nsampled set. 6\\n3. More than one random predicted value (as just deﬁned) can be generated\\nfor each missing value. This process is called multiple imputation and it\\nhas many advantages over the other methods in general. This is discussed\\nin Section 3.8.\\n4. Matching methods can be used to obtain random draws of other subject’s\\nvalues to replace missing values. Nearest neighbor matching can be used\\nto select a subject that is “close” to the subject in need of imputation,\\non the basis of a series of variables. This method requires the analyst to\\nmake decisions about what constitutes“closeness.”To simplify the match-\\ning process into a single dimension, Little420proposed the predictive mean\\nmatching method where matching is done on the basis of predicted values\\nfromaregressionmodelforpredictingthesometimes-missingvariable(sec-\\ntion3.7). According to Little, in large samples predictive mean matching\\nmay be more robust to model misspeciﬁcation than the method of adding\\na random residual to the subject’s predicted value, but because of diﬃ-\\nculties in ﬁnding matches the random residual method may be better in\\nsmaller samples. The random residual method may be easier to use when\\nmultiple imputations are needed, but care must be taken to create the\\ncorrect degree of uncertainty in residuals. 7\\nWhat if Xjneeds to be imputed for some subjects based on other variables\\nthat themselves may be missing on the same subjects missing on Xj?T h i si s\\na place where recursive partitioning with“surrogate splits”in case of missing\\npredictors may be a good method for developing imputations (see Section 2.5\\nand p.142). If using regression to estimate missing values, an algorithm\\nto cycle through all sometimes-missing variables for multiple iterations may\\nperform well. This algorithm is used by the Rtranscan function described\\nin Section 4.7.4as well as the to–be–described aregImpute f u n c t i o n .F i r s t ,a l l\\nmissing values are initialized to medians (modes for categorical variables).\\nThen every time missing values are estimated for a certain variable, those\\nestimates are inserted the next time the variable is used to predict other\\nsometimes-missing variables.\\nIf you want to assess the importance of a speciﬁc predictor that is fre-\\nquently missing, it is a good idea to perform a sensitivity analysis in which\\nall observations containing imputed values for that predictor are temporarily\\ndeleted. The test based on a model that included the imputed values may be\\ndiluted by the imputation or it may test the wrong hypothesis, especially if\\nYis not used in imputing X.\\nLittle argues for down-weighting observations containing imputations, to\\nobtain a more accurate variance–covariance matrix. For the ordinary linear\\nmodel, the weights have been worked out for some cases [ 421, p. 1231].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7bcadb43-8d3c-410a-95ae-bfd027612f36', embedding=None, metadata={'page_label': '52', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='52 3 Missing Data\\n3.6 Single Conditional Mean Imputation\\nFor a continuous or binary Xthat is unrelated to all other predictor vari-\\nables, the mean or median may be substituted for missing values without\\nmuch loss of eﬃciency,162although regression coeﬃcients will be biased low\\nsinceYwas not utilized in the imputation. When the variable of interest\\nis related to the other Xs, it is far more eﬃcient to use an individual pre-\\ndictive model for each Xbased on the other variables.79,525,612The “best\\nguess” imputation method ﬁlls in missings with predicted expected values\\nusing the multivariable imputation model based on non-missing datab.I ti s\\ntrue that conditional means are the best estimates of unknown values, but\\nexcept perhaps for binary logistic regression621,623their use will result in bi-\\nased estimates and very biased (low) variance estimates. The latter problem\\narises from the reduced variability of imputed values [ 174, p. 464].\\nTree-based models (Section 2.5) may be very useful for imputation since\\nthey do not require linearity or additivity assumptions,although such models\\noften have poor discrimination when they don’t overﬁt. When a continuous\\nXbeing imputed needs to be non-monotonically transformed to best relate\\nit to the other Xs (e.g., blood pressure vs. heart rate), trees and ordinary\\nregressionare inadequate. Here a general transformationmodeling procedure\\n(Section 4.7) may be needed.\\nSchemper et al.551,553proposed imputing missing binary covariables by\\npredicted probabilities. For categorical sometimes-missing variables, imputa-\\ntion models can be derived using polytomous logistic regression or a classiﬁ-\\ncation tree method. For missing values, the most likely value for each subject\\n(from the series of predicted probabilities from the logistic or recursive par-\\ntitioning model) can be substituted to avoid creating a new category that is\\nfalsely highly correlated with Y. For an ordinal X, the predicted mean value\\n(possibly rounded to the nearest actual data value) or median value from an\\nordinal logistic model is sometimes useful. 8\\n3.7 Predictive Mean Matching\\nInpredictive mean matching422(PMM), one replaces a missing ( NA)v a l u e\\nfor the target variable being imputed with the actual value from a donor\\nobservation.Donorsareidentiﬁedby matchinginonlyone dimension,namely\\nthe predicted value (e.g., predicted mean) of the target. Key considerations\\nare how to\\nbPredictors of the target variable include all the other Xs along with auxiliary\\nvariables that are not included in the ﬁnal outcome model, as long as they p recede\\nthe variable being imputed in the causal chain (unlike with multiple imputation).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2b61cd81-0159-4cf3-a81d-ca556c005bf4', embedding=None, metadata={'page_label': '53', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 Multiple Imputation 53\\n1. model the target when it is not NA\\n2. match donors on predicted values\\n3. avoid overuse of“good”donors to disallow excessive ties in imputed data\\n4. account for all uncertainties (section 3.8).\\nThe predictive model for each target variable uses any outcome variables, all\\npredictors in the ﬁnal outcome model, plus any needed auxiliary variables.\\nThe modeling method should be ﬂexible, not assuming linearity. Many meth-\\nods will suﬃce; parametric additive models are often good choices. Beauties\\nof PMM include the lack of need for distributional assumptions (as no resid-\\nuals are calculated), and predicted values need only be monotonically related\\nto real predicted valuesc\\nIn the original PMM method the donor for an NAwas the complete obser-\\nvation whose predicted target was closest to the predicted value of the target\\nfrom all complete observationsd. This approach can result in some donors\\nbeing used repeatedly. This can be addressed by sampling from a multino-\\nmial distribution, where the probabilities are scaled distances of all potential\\ndonors’ predictions to the predicted value y∗of the missing target. Tukey’s\\ntricube function (used in loess) is a good weighting function, implemented in\\ntheHmisc aregImpute function:\\nwi=( 1−min(di/s,1)3)3,\\ndi=|ˆyi−y∗| (3.1)\\ns=0.2×mean|ˆyi−y∗|.\\nsabove is a good default scale factor, and the wiare scaled so that∑wi=1 .\\n3.8 Multiple Imputation\\nImputingmissingvaluesandthendoinganordinaryanalysisasiftheimputed\\nvalues were real measurements is usually better than excluding subjects with\\nincomplete data. However, ordinary formulas for standard errors and other\\nstatistics are invalid unless imputation is taken into account.651Methods for\\nproperly accounting for having incomplete data can be complex. The boot-\\nstrap(described later)is an easymethod to implement, but the computations\\ncan be slowe.\\ncThus when modeling binary or categorical targets one can frequently take least\\nsquares shortcuts in place of maximum likelihood for binary, ordinal, or multinomial\\nlogistic models.\\nd662discusses an alternative method based on choosing a donor observation at\\nrandom from the qclosest matches ( q= 3, for example).\\neTo use the bootstrap to correctly estimate variances of regression coeﬃcients, one\\nmust repeat the imputation process and the model ﬁtting perhaps 100 t imes using a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e785008b-ec89-4cec-8a0d-1fa1d318b272', embedding=None, metadata={'page_label': '54', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='54 3 Missing Data\\nMultiple imputation uses random draws from the conditional distribu-\\ntion of the target variable given the other variables (and any additional in-\\nformation that is relevant)85,417,421,536. The additional information used to 9\\npredict the missing values can contain any variables that are potentially pre-\\ndictive, including variables measured in the future; the causal chain is not\\nrelevant.421,463When a regression model is used for imputation, the process\\ninvolves adding a random residual to the“best guess”for missing values, to\\nyield the same conditional variance as th e original variable. Methods for esti-\\nmating residualswerelisted in Section 3.5. Toproperlyaccountfor variability\\ndue to unknown values, the imputation is repeated Mtimes, where M≥3.\\nEach repetition results in a “completed” dataset that is analyzed using the\\nstandard method. Parameter estimates are averaged over these multiple im-\\nputations to obtain better estimates than those from single imputation. Th e\\nvariance–covariancematrix of the averagedparameterestimates,adjusted for\\nvariability due to imputation, is estimated using422\\nV=M−1M∑\\niVi+M+1\\nMB, (3.2)\\nwhereViis the ordinary complete data estimate of the variance–covariance\\nmatrix for the model parameters from the ith imputation, and Bis the\\nbetween-imputation sample variance–covariance matrix, the diagonal entries\\nof which are the ordinary sample variances of the Mparameter estimates. 10\\nAfter running aregImpute (orMICE)y o uc a nr u nt h e Hmiscpackages’s\\nfit.mult.impute functiontoﬁtthechosenmodelseparatelyforeachartiﬁcially\\ncompleted dataset corresponding to each imputation. After fit.mult.impute\\nﬁts all of the models, it averages the sets of regression coeﬃcients and com-\\nputes variance and covariance estimates that are adjusted for imputation\\n(using Eq. 3.2).\\nWhite and Royston661provide a method for multiply imputing missing\\ncovariate values using censored survival time data in the context of the Cox\\nproportional hazards model.\\nWhite et al.662recommend choosing the number of imputations Mso\\nthat the key inferential statistics are very reproducible should the imputation\\nanalysis be repeated. They suggest the use of 100 fimputations when fis\\nthe fraction of cases that are incomplete. See also [ 85, Section 2.7] and232.\\nExtreme amount of missing data does not prevent one from using multiple\\nimputation, because alternatives are worse321. Horton and Lipsitz302also\\nhave a good overview of multiple imputation and a review of several software\\npackages that implement PMM.\\nCaution : Multiple imputation methods can generate imputations hav-\\ning very reasonable distributions but still not having the property that ﬁnal\\nresampling procedure174,566(see Section 5.2). Still, the bootstrap can estimate the\\nright variance for the wrong parameter estimates if the imputations are n ot done\\ncorrectly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2496f7a5-5d61-4023-8605-092cd00765bb', embedding=None, metadata={'page_label': '55', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.8 Multiple Imputation 55\\nresponse model regression coeﬃcients have nominal conﬁdence interval cov-\\nerage. Among other things, it is worth checking that imputations generate\\nthe correct collinearities among covariates.\\n3.8.1 The aregImpute and Other Chained Equations\\nApproaches\\nA ﬂexible approach to multiple imputation that handles a wide variety of\\ntarget variables to be imputed and allows for multiple variables to be miss-\\ning on the same subject is the chained equation method. With a chained\\nequations approach, each target variable is predicted by a regression model\\nconditional on all other variables in the model, plus other variables. An it-\\nerative process cycles through all target variables to impute all missing val-\\nues627.Thisapproachisusedinthe MICEalgorithm(multipleimputationusing\\nchained equations) implemented in Rand other systems. The chained equa-\\ntion method does not attempt to use the full Bayesian multivariate model for\\nall target variables, which makes it more ﬂexible and easy to use but leaves it\\nopen to creating improper imputations, e.g., imputing conﬂicting values for\\ndiﬀerent target variables. However, simulation studies627so far have demon-\\nstrated very good performance of imputation based on chained equations in\\nnon-complex situations.\\nThearegImpute algorithm463takes all aspects of uncertainty into account\\nusing the bootstrap while using the same estimation procedures as transcan\\n(section4.7). Diﬀerent bootstrap resamples used for each imputation by ﬁt-\\nting a ﬂexible additive model on a sample with replacement from the original\\ndata. This model is used to predict all ofthe originalmissingand non-missing\\nvalues for the target variable for the current imputation. aregImpute uses ﬂex-\\nible parametric additive regression spline models to predict target variables.\\nThere is anoption to allow targetvariablesto be optimally transformed,even\\nnon-monotonically (but this can overﬁt). The function implements regression\\nimputation based on adding random residuals to predicted means, but its\\nreal value lies in implementing a wide variety of PMM algorithms.\\nThe default method used by aregImpute is (weighted) PMM so that\\nno residuals or distributional assumptions are required. The default PMM\\nmatching used is van Buuren’s“Type 1”matching [ 85, Section 3.4.2] to cap-\\nture the right amount of uncertainty. Here one computes predicted values\\nfor missing values using a regression ﬁt on the bootstrap sample, and ﬁnds\\ndonor observations by matching those predictions to predictions from poten-\\ntial donorsusingthe regressionﬁt from the originalsampleofcomplete obser-\\nvations. When a predictor of the target variable is missing, it is ﬁrst imputed\\nfrom its last imputation when it was a target variable. The ﬁrst 3 iterations', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da28f06a-4224-4b26-8f39-30bb648d7b3b', embedding=None, metadata={'page_label': '56', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='56 3 Missing Data\\nTable 3.1 Summary of Methods for Dealing with Missing Values\\nMethod Deletion Single Multiple\\nAllows nonrandom missing –x x\\nReduces sample size x– –\\nApparent S.E. of ˆβtoo low –x –\\nIncreases real S.E. of ˆβ x– –\\nˆβbiased if not MCAR x –\\nof the process are ignored (“burn-in”). aregImpute seems to perform as well as\\nMICEbut runs signiﬁcantly faster and allows for nonlinear relationships. 11\\nHere is an example using the RHmiscandrmspackages.\\na←aregImpute( ∼age + sex + bp + death +\\nheart.attack.before.death,\\ndata=mydata, n.impute=5)\\nf←fit.mult.impute( death∼rcs(age,3) + sex +\\nrcs(bp,5), lrm, a, data=mydata)\\n3.9 Diagnostics\\nOne diagnostic that can be helpful in assessing the MCAR assumption is to\\ncompare the distribution of non-missing Yfor those subjects having com-\\npleteXwith those having incomplete X. On the other hand, Yucel and\\nZaslavsky681developed a diagnostic that is useful for checking the imputa-\\ntions themselves. In solving a problem related to imputing binary variables\\nusing continuous data models, they proposed a simple approach. Suppose\\nwe were interested in the reasonableness of imputed values for a sometimes-\\nmissing predictor Xj. Duplicate the entire dataset, but in the duplicated\\nobservations set all values of Xjto missing. Develop imputed values for the\\nmissing values of Xj, and in the observations of the duplicated portion of the\\ndataset corresponding to originally non-missing values of Xj,c o m p a r et h e\\ndistribution of imputed Xjwith the original values of Xj. 12\\n3.10 Summary and Rough Guidelines\\nTable3.1summarizes the advantages and disadvantages of three methods of\\ndealing with missing data. Here“Single”refers to single conditional mean im-\\nputation (which cannot utilize Y) and“Multiple”refers to multiple random-\\ndraw imputation (which can incorporate Y).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fe61ec8-db3f-4fb3-85a9-23ff2014978a', embedding=None, metadata={'page_label': '57', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.10 Summary and Rough Guidelines 57\\nThe following contains crude guidelines. Simulation studies are needed to\\nreﬁne the recommendations. Here frefers to the proportion of observations\\nhavinganyvariables missing.\\nf<0.03: It doesn’t matter very much how you impute missings or whether\\nyou adjust variance of regression coeﬃcient estimates for having im-\\nputed data in this case. For continuous variables imputing missings with\\nthe median non-missing value is adequate; for categorical predictors the\\nmost frequent category can be used. Complete case analysis is also an\\noption here. Multiple imputation may be needed to check that the simple\\napproach“worked.”\\nf≥0.03: Use multiple imputation with number of imputations equal to\\nmax(5,100f). Fewer imputations may be possible with very large sample\\nsizes. Type 1 predictive mean matching is usually preferred, with weighted\\nselection of donors. Account for imputation in estimating the covariance\\nmatrix for ﬁnal parameter estimates. Use the tdistribution instead of the\\nGaussian distribution for tests and conﬁdence intervals, if possible, using\\nthe estimated d.f. for the parameter estimates.\\nMultiple predictors frequently missing: Moreimputationsmaybe required.\\nPerform a“sensitivity to order”analysis by creating multiple imputations\\nusing diﬀerent orderings of sometimes missing variables. It may be ben-\\neﬁcial to place the variable with the highest number of NAsﬁ r s ts ot h a t\\ninitialization of other missing variables to medians will have less impact.\\nIt is important to note that the reasons for missing data are more important\\ndeterminants of how missing values should be handled than is the quantity\\nof missing values.\\nIf the main interest is prediction and not interpretation or inference about\\nindividual eﬀects, it is worth trying a simple imputation (e.g., median or nor-\\nmal value substitution) to see if the resulting model predicts the response\\nalmost as well as one developed after using customized imputation. But it\\nis not appropriate to use the dummy variable or extra category method,\\nbecause these methods steal information from Yand bias all ˆβs. Clark and 13\\nAltman110presented a nice example of the use of multiple imputation for\\ndeveloping a prognostic model. Marshall et al.442developed a useful method\\nfor obtaining predictions on future observations when some of the needed\\npredictors are unavailable. Their method uses an approximate re-ﬁt of the\\noriginal model for available predictors only, utilizing only the coeﬃcient esti-\\nmates and covariance matrix from the original ﬁt. Little and An418also have\\nan excellent review of imputation methods and developed several approxi-\\nmate formulas for understanding properties of various estimators. They also\\ndeveloped a method combining imputation of missing values with propensity\\nscore modeling of the probability of missingness.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6290e790-6466-4f12-9cda-34a9b52231fb', embedding=None, metadata={'page_label': '58', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='58 3 Missing Data\\n3.11 Further Reading\\n1These types of missing data are well described in an excellent review article\\non missing data by Schafer and Graham542. A good introductory article on\\nmissing data and imputation is by Donders et al.161and a good overview of\\nmultiple imputation is by White et al.662and Harel and Zhou256. Paul Allison’s\\nbooklet12and van Buuren’s book85are also excellent practical treatments.\\n2Crawford et al.138give an example where responses are not MCAR for which\\ndeleting subjects with missing responses resulted in a biased estimate of the\\nresponse distribution. They found that multiple imputation of the response re-\\nsulted in much improved estimates. Wood et al.673have a good review of how\\nmissing response data are typically handled in randomized trial reports, with\\nrecommendations for improvements. Barnes et al.42have a good overview of\\nimputation methods and a comparison of bias and conﬁdence interval cover-\\nage for the methods when applied to longitudinal data with a small number\\nof subjects. Twist et al.617found instability in using multiple imputation of\\nlongitudinal data, and advantages of using instead full likelihood models.\\n3See van Buuren et al.626for an example in which subjects having missing base-\\nline blood pressure had shorter survival time. Joseph et al.327provide examples\\ndemonstrating diﬃculties with casewise deletion and single imputation, and\\ncomment on the robustness of multiple imputation methods to violations of\\nassumptions.\\n4Another problem with the missingness indicator approach arises when more\\nthan one predictor is missing and these predictors are missing on almost the\\nsame subjects. The missingness indicator variables will be collinear; that is\\nimpossible to disentangle.326\\n5See [623, pp. 2645–2646] for several problems with the “missing category” ap-\\nproach. A clear example is in161where covariates X1,X2have true β1=1,β2=\\n0a n dX1is MCAR. Adding a missingness indicator for X1as a covariate re-\\nsulted in ˆβ1=0.55,ˆβ2=0.51 because in the missing observations the constant\\nX1was uncorrelated with X2. D’Agostino and Rubin146developed methods for\\npropensity score modeling that allow for missing data. They mentioned that ex -\\ntra categories may be added to allow for missing data in propensity models and\\nthat adding indicator variables describing patterns of missingness will also allow\\nthe analyst to match on missingness patterns when comparing non-randomly\\nassigned treatments.\\n6Harel andZhou256andSiddique569discusstheapproximateBayesianbootstrap\\nfurther.\\n7Kalton and Kasprzyk332proposed a hybrid approach to imputation in which\\nmissing values are imputed with the predicted value for the subject plus the\\nresidual from the subject having the closest predicted value to the subject being\\nimputed.\\n8Miller et al.458studied the eﬀect of ignoring imputation when conditional mean\\nﬁll-in methods are used, and showed how to formalize such methods using linear\\nmodels.\\n9Meng455argues against always separating imputation from ﬁnal analysis, and\\nin favor of sometimes incorporating weights into the process.\\n10van Buuren et al.626presented an excellent case study in multiple imputation\\nin the context of survival analysis. Barzi and Woodward43present a nice review\\nof multiple imputation with detailed comparison of results (point estimates and\\nconﬁdence limits for the eﬀect of the sometimes-missing predictor) for various\\nimputation methods. Barnard and Rubin41derived an estimate of the d.f. asso-\\nciated with the imputation-adjusted variance matrix for use in a t-distribution', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0dfcb0d4-a115-4161-9350-f19b53906b5d', embedding=None, metadata={'page_label': '59', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.12 Problems 59\\napproximation for hypothesis tests about imputation-averaged coeﬃcient es-\\ntimates. When d.f. is not very large, the tapproximation will result in more\\naccurate P-values than using a normal approximation that we use with Wald\\nstatistics after inserting Equation 3.2as the variance matrix.\\n11Little and An418present imputation methods based on ﬂexible additive regres-\\nsion models using penalized cubic splines. Horton and Kleinman301compare\\nseveral software packages for handling missing data and have comparisons of\\nresults with that of aregImpute .M o o n se ta l .463compared aregImpute with\\nMICE.\\n12He and Zaslavsky280formalized the duplication approach to imputation\\ndiagnostics.\\n13Agood general reference on missingdataisLittleand Rubin,422and Volume 16,\\nNos. 1 to 3 of Statistics in Medicine , a large issue devoted to incomplete covari-\\nable data. Vach620is an excellent text describing properties of various methods\\nof dealing with missingdata in binary logistic regression (see also [ 621,622,624]).\\nThese references show how to use maximum likelihood to explicitly model the\\nmissing data process. Little and Rubin show how imputation can be avoided\\nif the analyst is willing to assume a multivariate distribution for the joint dis-\\ntribution of XandY.S i n c eXusually contains a strange mixture of binary,\\npolytomous, and continuous buthighlyskewed predictors, itis unlikelythat this\\napproach will work optimally in many problems. That’s the reason the imputa-\\ntion approach is emphasized. See Rubin536for a comprehensive source on mul-\\ntiple imputation. See Little,419Vach and Blettner,623Rubin and Schenker,535\\nZhou et al.,688Greenland and Finkle,242and Hunsberger et al.313for excellent\\nreviews of missing data problems and approaches to solving them. Reilly and\\nPepe have a nice comparison of the“hot-deck”imputation method with a maxi-\\nmum likelihood-based method.523White and Carlin660studied bias of multiple\\nimputation vs. complete case analysis.\\n3.12 Problems\\nThe SUPPORT Study (Study to Understand Prognoses Preferences Out-\\ncomes and Risks of Treatments) was a ﬁve-hospital study of 10,000 critically\\nill hospitalizedadultsf352.Patientswerefollowedforin-hospitaloutcomesand\\nfor long-term survival. We analyze 35 variables and a random sample of 1000\\npatients from the study.\\n1. Explore the variables and patterns of missing data in the SUPPORT\\ndataset.\\na. Print univariable summaries of all variables. Make a plot (showing all\\nvariablesononepage)thatdescribesespeci allythecontinuousvariables.\\nb. Make a plot showing the extent of missing data and tendencies for some\\nvariables to be missing on the same patients. Functions in the Hmisc\\npackage may be useful.\\nfThe dataset is on the book’s dataset wiki and may be automatically fetched over\\nthe internet and loaded using the Hmiscpackage’s command getHdata(support) .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f09290c8-97ef-4192-8f80-77caa3272324', embedding=None, metadata={'page_label': '60', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='60 3 Missing Data\\nc. Total hospital costs (variable totcst) were estimated from hospital-\\nspeciﬁc Medicare cost-to-charge ratios. Characterize what kind of pa-\\ntients have missing totcst. For this characterization use the follow-\\ningpatientdescriptors: age, sex, dzgroup, num.co, edu, income, scoma,\\nmeanbp, hrt, resp, temp .\\n2. Prepare for later development of a model to predict costs by developing\\nreliable imputations for missing costs. Remove the observation having zero\\ntotcst.g\\na. The cost estimates are not available on 105 patients. Total hospital\\ncharges (bills) are available on all but 25 patients. Relate these two\\nvariables to each other with an eye toward using chargesto predict\\ntotcstwhentotcstis missing. Make graphs that will tell whether lin-\\near regression or linear regression after t aking logs of both variables is\\nbetter.\\nb. Impute missing total hospital costs in SUPPORT based on a regression\\nmodel relating charges to costs, when charges are available. You may\\nwant to use a statement like the following in R:\\nsupport ←transform(support,\\ntotcst = ifelse(is.na(totcst),\\n(expression_in_charges), totcst))\\nIf in the previous problem you felt that the relationship between costs\\nand charges should be based on taking logs of both variables, the“ex-\\npression in charges” above may look something like exp(intercept +\\nslope * log(charges)) , where constants are inserted for intercept and\\nslope.\\nc. Compute the likely error in approximating total cost using charges by\\ncomputing the median absolute diﬀerence between predicted and ob-\\nserved total costs in the patients having both variables available. If you\\nused a log transformation, also compute the median absolute percent\\nerror in imputing total costs by anti-logging the absolute diﬀerence in\\npredicted logs.\\n3. State brieﬂy why single conditional medianhimputation is OK here.\\n4. Usetranscan to develop single imputations for total cost, commenting on\\nthe strength of the model ﬁtted by transcan as well as how strongly each\\nvariable can be predicted from all the others.\\n5. Usepredictivemeanmatchingtomultiplyimputecost10timespermissing\\nobservation. Describe graphically the distributions of imputed values and\\nbrieﬂy compare these to distributions of non-imputed values. State in a\\ngYou can usethe Rcommand subset(support, is.na(totcst) | totcst > 0) .T h e\\nis.nacondition tells Rthat it is permissible to include observations having missing\\ntotcstwithout setting all columns of such observations to NA.\\nhWe are anti-logging predicted log costs and we assume log cost has a symmetric\\ndistribution', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c5fc660e-d01a-4640-a904-796b2cd9ef95', embedding=None, metadata={'page_label': '61', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='3.12 Problems 61\\nsimple way what the sample variance of multiple imputations for a single\\nobservation of a continuous predictor is approximating.\\n6. Using the multiple imputed values, develop an overall least squares model\\nfor total cost (using the log transformation) making optimal use of partial\\ninformation, with variances computed so as to take imputation (except for\\ncost) into account. The model should use the predictors in Problem 1 and\\nshould not assume linearity in any predictor but should assume additivity.\\nInterpret one of the resulting ratios of imputation-corrected variance to\\napparent variance and explain why ratios greater than one do not mean\\nthat imputation is ineﬃcient.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d101f5f1-82cf-4705-af65-a8eba7facff4', embedding=None, metadata={'page_label': '63', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 4\\nMultivariable Modeling Strategies\\nChapter 2dealt with aspects of modeling such as transformations of pre-\\ndictors, relaxing linearity assumptions, modeling interactions, and examining\\nlack of ﬁt. Chapter 3dealt with missing data, focusing on utilization of in-\\ncomplete predictorinformation.Alloftheseareasareimportantinthe overall\\nscheme of model development, and they cannot be separated from what is to\\nfollow. In this chapter we concern ourselves with issues related to the whole\\nmodel, with emphasis on deciding on the amount of complexity to allow in\\nthe model and on dealing with large numbers of predictors. The chapter con-\\ncludes with three default modeling strategies depending on whether the goal\\nis prediction, estimation, or hypothesis testing. 1\\nThere are many choices to be made when deciding upon a global modeling\\nstrategy, including choice between\\n•parametric and nonparametric procedures\\n•parsimony and complexity\\n•parsimony and good discrimination ability\\n•interpretable models and black boxes.\\nThis chapter addresses some of these issues. One general theme of what fol-\\nlows is the idea that in statistical inference when a method is capable of\\nworsening performance of an estimator or inferential quantity (i.e., when the\\nmethod is not systematically biased in one’s favor), the analyst is allowed to\\nbeneﬁt from the method. Variable selection is an example where the analysis\\nis systematically tilted in one’s favor by directly selecting variables on the\\nbasis of P-values of interest, and all elements of the ﬁnal result (including\\nregression coeﬃcients and P-values) are biased. On the other hand, the next\\nsection is an example of the “capitalize on the beneﬁt when it works, and\\nthe method may hurt”approach because one may reduce the complexity of\\nan apparently weak predictor by removing its most important component—\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 463', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ede8c085-f39b-4737-8c6c-53f0894397dc', embedding=None, metadata={'page_label': '64', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='64 4 Multivariable Modeling Strategies\\nnonlinear eﬀects—from how the predictor is expressed in the model. The\\nmethod hides tests of nonlinearity that would systematically bias the ﬁnal\\nresult.\\nThe book’swebsite containsanumberofsimulationstudiesand references\\nto others that support the advocated approaches.\\n4.1 Prespeciﬁcation of Predictor Complexity Without\\nLater Simpliﬁcation\\nThere are rare occasions in which one actually expects a relationship to be\\nlinear. For example, one might predict mean arterial blood pressure at two\\nmonths after beginning drug administration using as baseline variables the\\npretreatment mean blood pressure and other variables. In this case one ex-\\npects the pretreatment blood pressure to linearly relate to follow-up blood\\npressure, and modeling is simplea. In the vast majority of studies, however,\\nthere is every reason to suppose that all relationships involving nonbinary\\npredictors are nonlinear. In these cases, the only reason to represent pre-\\ndictors linearly in the model is that there is insuﬃcient information in the\\nsample to allow us to reliably ﬁt nonlinear relationships.b\\nSupposing that nonlinearities are entertained, analysts often use scatter\\ndiagrams or descriptive statistics to decide how to represent variables in a\\nmodel. The result will often be an adequately ﬁtting model, but conﬁdence\\nlimits will be too narrow, P-values too small, R2too large, and calibration\\ntoo good to be true. The reason is that the“phantom d.f.”that represented\\npotential complexities in the model that were dismissed during the subjective\\nassessments are forgotten in computing standard errors, P-values, and R2\\nadj.\\nThe same problem is created when one entertains several transformations\\n(log,√, etc.) and uses the data to see which one ﬁts best, or when one tries\\nto simplify a spline ﬁt to a simple transformation.\\nAn approach that solves this problem is to prespecify the complexity with\\nwhich each predictor is represented in the model, without later simpliﬁcation\\nof the model. The amount ofcomplexity (e.g., number of knots in spline func-\\ntions or order of ordinary polynomials)one can aﬀordto ﬁt is roughly related\\nto the “eﬀective sample size.” It is also very reasonable to allow for greater\\ncomplexity for predictors that are thought to be more powerfully related to\\nY.Forexample,errorsinestimatingthe curvatureofaregressionfunctionare\\nconsequential in predicting Yonly when the regression is somewhere steep.\\nOnce the analyst decides to include a predictor in every model, it is fair to\\naEven then, the two blood pressures may need to be transformed to meet distribu-\\ntional assumptions.\\nbShrinkage (penalized estimation) is a general solution (see Section 4.5). One can\\nalways use complex models that are“penalized towards simplicity,”with the amo unt\\nof penalization being greater for smaller sample sizes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a79c48aa-36c8-4e88-a7ef-39d5e296baa5', embedding=None, metadata={'page_label': '65', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.1 Prespeciﬁcation of Predictor Complexity 65\\nuse general measures of association to quantify the predictive potential for\\na variable. For example, if a predictor has a low rank correlation with the\\nresponse, it will not “pay” to devote many degrees of freedom to that pre-\\ndictor in a spline function having many knots. On the other hand, a potent\\npredictor (with a high rank correlation) not known to act linearly might be\\na s s i g n e dﬁ v ek n o t si ft h es a m p l es i z ea l l o w s .\\nWhen the eﬀective sample size available is suﬃciently large so that a satu-\\nratedmaineﬀectsmodelmaybeﬁtted,agoodapproachtogaugingpredictive\\npotential is the following.\\n•Letallcontinuouspredictorsberepresentedasrestrictedcubicsplineswith\\nkknots, where kis the maximum number of knots the analyst entertains\\nfor the current problem.\\n•Let all categorical predictors retain their original categories except for\\npooling of very low prevalence categories (e.g., ones containing <6o b s e r -\\nvations).\\n•Fit this general main eﬀects model.\\n•Compute the partial χ2statistic for testing the association of each pre-\\ndictor with the response, adjusted for all other predictors. In the case of\\nordinary regression, convert partial Fstatistics to χ2statistics or partial\\nR2values.\\n•Make correctionsfor chanceassociationsto“levelthe playing ﬁeld”for pre-\\ndictors having greatly varying d.f., e.g., subtract the d.f. from the partial\\nχ2(the expected value of χ2\\npispunderH0).\\n•Make certain that tests of nonlinearity are not revealed as this would bias\\nthe analyst.\\n•Sort the partial association statistics in descending order.\\nCommands in the rmspackage can be used to plot only what is needed.\\nHere is an example for a logistic model.\\nf←lrm(y∼sex + race + rcs(age,5) + rcs(weight,5) +\\nrcs(height,5) + rcs( blood.pressure ,5))\\nplot(anova(f))\\nThis approach, and the rank correlation approach about to be discussed,\\ndo not require the analyst to really prespecify predictor complexity, so how\\nare they not biased in our favor? There are two reasons: the analyst has al-\\nready agreed to retain the variable in the model even if the strength of the\\nassociation is very low, and the assessment of association does not reveal\\nthe degree of nonlinearity of the predictor to allow the analyst to “tweak”\\nthe number of knots or to discard nonlinear terms. Any predictive ability a\\nvariable might have may be concentrated in its nonlinear eﬀects, so using\\nthe total association measure for a predictor to save degrees of freedom by\\nrestricting the variable to be linear may result in no predictive ability. Like-\\nwise, a low association measure between a categorical variable and Ymight\\nlead the analyst to co llapse some of the categories based on their frequencies.\\nThis often helps, but sometimes the categories that are so combined are the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02bae1b6-a166-4fa9-92e1-5d594b23190c', embedding=None, metadata={'page_label': '66', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='66 4 Multivariable Modeling Strategies\\nones that are most diﬀerent from one another. So if using partial tests or\\nrank correlationto reduce degrees of freedom can harm the model, one might\\nargue that it is fair to allow this strategy to also beneﬁt the analysis.\\nWhencollinearitiesorconfoundingarenotproblematic,aquickerapproach\\nbased on pairwise measures of association can be useful. This approach will\\nnot have numerical problems (e.g., singular covariance matrix). When Yis\\nbinary or continuous (but not censored), a good general-purpose measure of\\nassociationthatis useful inmakingdecisionsaboutthe number ofparameters\\nto devote to a predictor is an extension of Spearman’s ρrank correlation.\\nThis is the ordinary R2from predicting the rank of Ybased on the rank of\\nXand the square of the rank of X.T h i sρ2will detect not only nonlinear 2\\nrelationships (as will ordinary Spearman ρ) but some non-monotonic ones\\nas well. It is important that the ordinary Spearman ρnot be computed, as\\nthis would tempt the analyst to simplify the regression function (towards\\nmonotonicity) if the generalized ρ2does not signiﬁcantly exceed the square\\nof the ordinarySpearman ρ. For categoricalpredictors,ranks are not squared\\nbut instead the predictor is represented by a series of dummy variables. The\\nresulting ρ2is related to the Kruskal–Wallis test. See p. 460for an example.\\nNote that bivariable correlations can be misleading if marginal relationships\\nvary greatly from ones obtained after adjusting for other predictors. 3\\nOnce one expands a predictor into linear and nonlinear terms and esti-\\nmates the coeﬃcients, the best way to understand the relationship between\\npredictors and response is to graph this estimated relationshipc. If the plot\\nappears almost linear or the test of nonlinearity is very insigniﬁcant there\\nis a temptation to simplify the model. The Grambsch and O’Brien result\\ndescribed in Section 2.6demonstrates why this is a bad idea.\\nFrom the above discussion a general principle emerges. Whenever the re-\\nsponse variable is informally or formally linked, in an unmasked fashion, to\\nparticular parameters that may be deleted from the model, special adjust-\\nm e n t sm u s tb em a d ei n P-values, standard errors, test statistics, and conﬁ-\\ndence limits, in order for these statistics to have the correct interpretation.\\nExamples of strategies that are improper without special adjustments (e.g.,\\nusing the bootstrap) include examining a frequency table or scatterplot to\\ndecide that an association is too weak for the predictor to be included in\\nthe model at all or to decide that the relationship appears so linear that all\\nnonlinear terms should be omitted. It is also valuable to consider the reverse\\nsituation; that is, one posits a simple model and then additional analysis or\\noutside subject matter information makes the analyst want to generalize the\\nmodel. Once the model is generalized (e.g., nonlinear terms are added), the\\ntest of association can be recomputed using multiple d.f. So another general\\nprinciple is that when one makes the model more complex, the d.f. prop-\\nerly increases and the new test statistics for association have the claimed\\ncOne can also perform a joint test of all parameters associated with nonlinear eﬀects.\\nThis can be useful in demonstrating to the reader that some complexity was actually\\nneeded.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0edea731-a6a2-4888-b781-22972655f0d3', embedding=None, metadata={'page_label': '67', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Variable Selection 67\\ndistribution. Thus moving from simple to more complex models presents no\\nproblems other than conservatism if the new complex components are truly\\nunnecessary.\\n4.2 Checking Assumptions of Multiple Predictors\\nSimultaneously\\nBefore developing a multivariable model one must decide whether the as-\\nsumptions of each continuous predictor can be veriﬁed by ignoring the eﬀects\\nof all other potential predictors. In some cases, the shape of the relation-\\nship between a predictor and the property of response will be diﬀerent if an\\nadjustment is made for other correlatedfactors when deriving regressionesti-\\nmates. Also, failure to adjust for an important factor can frequently alter the\\nnature of the distribution of Y. Occasionally, however, it is unwieldy to deal\\nsimultaneously with all predictors at each stage in the analysis, and instead\\nthe regression function shapes are assessed separately for each continuous\\npredictor.\\n4.3 Variable Selection\\nThe material covered to this point dealt with a prespeciﬁed list of variables\\nto be included in the regression model. For reasons of developing a concise\\nmodel or because of a fear of collinearity or of a false belief that it is not\\nlegitimate to include “insigniﬁcant” regression coeﬃcients when presenting\\nresults to the intended audience, stepwisevariable selection is verycommonly\\nemployed. Variable selection is used when the analyst is faced with a series of\\npotential predictors but does not have (or use) the necessary subject matter\\nknowledge to enable her to prespecify the “important” variables to include\\nin the model. But using Yto compute P-values to decide which variables\\nto include is similar to using Yto decide how to pool treatments in a ﬁve–\\ntreatment randomized trial, and then testing for global treatment diﬀerences\\nusing fewer than four degrees of freedom.\\nStepwise variable selection has been a very popular technique for many\\nyears, but if this procedure had just been proposed as a statistical method, it\\nwould most likely be rejected because it violates every principle of statistical\\nestimation and hypothesis testing. Here is a summary of the problems with\\nthis method.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27ea5746-dabf-4a7a-9ce5-e340bf6222a8', embedding=None, metadata={'page_label': '68', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='68 4 Multivariable Modeling Strategies\\n1. It yields R2values that are biased high.\\n2. The ordinary Fandχ2test statistics do not have the claimed distribu-\\ntiond.234Variable selection is based on methods (e.g., Ftests for nested\\nmodels)thatwereintended tobe usedto testonlyprespeciﬁedhypotheses.\\n3. The method yields standard errors of regression coeﬃcient estimates that\\narebiasedlowandconﬁdenceintervalsforeﬀectsand predictedvaluesthat\\nare falsely narrow.16\\n4. Ityields P-valuesthat aretoosmall(i.e., there areseveremultiple compar-\\nison problems) and that do not have the proper meaning, and the proper\\ncorrection for them is a very diﬃcult problem.\\n5. It provides regression coeﬃcients that are biased high in absolute value\\nand need shrinkage. Even if only a single predictor were being analyzed\\nand one only reported the regression coeﬃcient for that predictor if its\\nassociation with Ywere“statistically signiﬁcant,”the estimate of the re-\\ngression coeﬃcient ˆβis biased (too large in absolute value). To put this\\nin symbols for the case where we obtain a positive association ( ˆβ>0),\\nE(ˆβ|P<0.05,ˆβ>0)>β.100\\n6. In observational studies, variable selection to determine confounders for\\nadjustment results in residual confounding241.\\n7. Rather than solving problems caused by collinearity, variable selection is\\nmade arbitrary by collinearity.\\n8. It allows us to not think about the problem.\\nThe problems of P-value-based variable selection are exacerbated when the\\nanalyst (as she so often does) interprets the ﬁnal model as if it were pre-\\nspeciﬁed. Copas and Long125stated one of the most serious problems with\\nstepwise modeling eloquently when they said, “The choice of the variables\\nto be included depends on estimated regression coeﬃcients rather than their\\ntrue values, and so Xjis more likely to be included if its regressioncoeﬃcient\\nis over-estimatedthan if its regressioncoeﬃcient is underestimated.”Derksen\\nand Keselman155studied stepwise variable selection, backward elimination,\\nand forward selection, with these conclusions:\\n1. “The degree of correlation between the predictor variables aﬀected the fre-\\nquency with which authentic predictor variables found their way into the\\nﬁnal model.\\n2. The number of candidate predictor variables aﬀected the number of noise\\nvariables that gained entry to the model.\\n3. The size of the sample was of little practical importance in determining the\\nnumber of authentic variables contained in the ﬁnal model.\\ndLockhart et al.425provide an example with n= 100 and 10 orthogonal predictors\\nwhere all true βs are zero. The test statistic for the ﬁrst variable to enter has type I\\nerror of 0.39 when the nominal αis set to 0.05, in line with what one would expect\\nwith multiple testing using 1 −0.9510=0.40.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='37866c41-0303-4154-ab75-5fd2f67455ba', embedding=None, metadata={'page_label': '69', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Variable Selection 69\\n4. The population multiple coeﬃcient of determination could be faithfully es-\\ntimated by adopting a statistic that is adjusted by the total number of\\ncandidate predictor variables rather than the number of variables in the\\nﬁnal model.”\\nThey found that variables selected for the ﬁnal model represented noise 0.20\\nto 0.74 of the time and that the ﬁnal model usually contained less than half\\nof the actual number of authentic predictors. Hence there are many reasons\\nfor using methods such as full-model ﬁts or data reduction, instead of using\\nany stepwise variable selection algorithm.\\nIf stepwise selection must be used, a global test of no regression should\\nbe made before proceeding, simultaneously testing all candidate predictors\\nand having degrees of freedom equal to the number of candidate variables\\n(plus any nonlinear or interaction terms). If this global test is not signiﬁcant,\\nselection of individually signiﬁcant predictors is usually not warranted.\\nThe method generally used for such variable selection is forward selection\\nof the most signiﬁcant candidate or backward elimination of the least sig-\\nniﬁcant predictor in the model. One of the recommended stopping rules is\\nbased on the “residual χ2” with degrees of freedom equal to the number of\\ncandidate variables remaining at the current step. The residual χ2can be\\ntested for signiﬁcance (if one is able to forget that because of variable selec-\\ntion this statistic does not have a χ2distribution), or the stopping rule can\\nbe based on Akaike’s information criterion (AIC33), here residual χ2−2×\\nd.f.257Of course, use of more insight from knowledge of the subject matter\\nwill generally improve the modeling process substantially. It must be remem-\\nbered that no currently available stopping rule was developed for data-driven\\nvariable selection. Stopping rules such as AIC or Mallows’ Cpare intended\\nfor comparing a limited number of prespeciﬁed models [66, Section 1.3]347e. 4\\nIf the analyst insists on basing the stopping rule on P-values, the optimum\\n(in terms of predictive accuracy) αto use in deciding which variables to\\ninclude in the model is α=1.0 unless there are a few powerful variables\\nand several completely irrelevant variables. A reasonable αthat does allow\\nfor deletion of somevariables is α=0.5.589These values are far from the\\ntraditional choices of α=0.05 or 0.10. 5\\neAIC works successfully when the models being entertained are on a progression\\ndeﬁned by a single parameter, e.g. a common shrinkage coeﬃcient or the single nu m-\\nber of knots to be used by allcontinuous predictors. AIC can also work when the\\nmodel that is best by AIC is much better than the runner-up so that if the proc ess\\nwere bootstrapped the same model would almost always be found. When used for\\none variable at a time variable selection. AIC is just a restatement of the P-value,\\nand as such, doesn’t solve the severe problems with stepwise variable selection other\\nthan forcing us to use slightly more sensible αvalues. Burnham and Anderson84rec-\\nommend selection based on AIC for a limited number of theoretically well-founded\\nmodels. Some statisticians try to deal with multiplicity problems caused by stepwise\\nvariable selection by making αsmaller than 0.05. This increases bias by giving vari-\\nables whose eﬀects are estimated with error a greater relative chance of being selected.\\nVariable selection does not compete well with shrinkage methods that simultaneou sly\\nmodel all potential predictors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe7f3781-a754-4b61-b94a-dbc68a0ec4ce', embedding=None, metadata={'page_label': '70', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='70 4 Multivariable Modeling Strategies\\nEven though forward stepwise variable selection is the most commonly\\nused method, the step-down method is preferred for the following reasons. 6\\n1. It usually performs better than forward stepwise methods, especially when\\ncollinearity is present.437\\n2. It makes one examine a full model ﬁt, which is the only ﬁt providing\\naccurate standard errors, error mean square, and P-values.\\n3. ThemethodofLawlessandSinghal385allowsextremelyeﬃcientstep-down\\nmodeling using Wald statistics, in the context of any ﬁt from least squares\\nor maximum likelihood. This method requires passing through the data\\nmatrix only to get the initial full ﬁt.\\nFor a given dataset, bootstrapping (Efron et al.150,172,177,178)c a nh e l p\\ndecide between using full and reduced models. Bootstrapping can be done\\non the whole model and compared with bootstrapped estimates of predictive\\naccuracy based on stepwise variable selection for each resample. Unless most\\npredictors are either very signiﬁcant or clearly unimportant, the full model\\nusually outperforms the reduced model.\\nFull model ﬁts have the advantage of providing meaningful conﬁdence\\nintervals using standard formulas. Altman and Andersen16gave an example\\nin which the lengths of conﬁdence intervals of predicted survival probabilities\\nwere 60% longer when bootstrapping was used to estimate the simultaneous\\neﬀects of variabilitycaused by variable selectionand coeﬃcient estimation, as\\ncompared with conﬁdence intervals computed ignoring how a “ﬁnal” model\\ncame to be. On the other hand, models developed on full ﬁts after data 7\\n8 reduction will be optimum in many cases.\\nIn some cases you may want to use the full model for prediction and vari-\\nable selection for a “best bet” parsimonious list of independently important\\npredictors. This could be accompanied by a list of variables selected in 50\\nbootstrap samples to demonstrate the imprecision in the“best bet.”\\nSauerbrei and Schumacher541present a method to use bootstrapping to\\nactually select the set of variables.However,there are a number of drawbacks\\nto this approach35:\\n1. The choice of an αcutoﬀ for determining whether a variable is retained in\\na given bootstrap sample is arbitrary.\\n2. The choice of a cutoﬀ for the proportion of bootstrap samples for which a\\nvariable is retained, in order to include that variable in the ﬁnal model, is\\nsomewhat arbitrary.\\n3. Selection from among a set of correlated predictors is arbitrary, and all\\nhighly correlated predictors may have a low bootstrap selection frequency.\\nIt may be the case that none of them will be selected for the ﬁnal model\\neven though when considered individually each of them may be highly\\nsigniﬁcant.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='710b0910-1e5c-4180-a1c1-85ca06fb4c53', embedding=None, metadata={'page_label': '71', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.3 Variable Selection 71\\n4. By using the bootstrap to choose variables, one must use the double boot-\\nstraptoresampletheentiremodelingprocessinordertovalidatethemodel\\nand to derive reliable conﬁdence intervals. This may be computationally\\nprohibitive.\\n5. The bootstrap did not improve upon traditional backward stepdown vari-\\nable selection. Both methods fail at identifying the“correct”variables.\\nFor some applications the list of variables selected may be stabilized by\\ngrouping variables according to subject matter considerations or empirical\\ncorrelations and testing each related group with a multiple degree of freedom\\ntest. Then the entiregroupmaybe keptordeletedand, ifdesired,groupsthat\\nare retained can be summarized into a single variable or the most accurately\\nmeasured variable within the group can replace the group. See Section 4.7\\nfor more on this.\\nKass and Raftery337showed that Bayes factors have several advantages in\\nvariable selection, including the selection of less complex models that may\\nagree better with subject matter knowledge. However, as in the case with\\nmore traditional stopping rules, the ﬁnal model may still have regression\\ncoeﬃcients that are too large. This problem is solved by Tibshirani’s lasso\\nmethod,608,609which is a penalized estimation technique in which the esti-\\nmated regression coeﬃcients are constrained so that the sum of their scaled\\nabsolute values falls below some constant kchosen by cross-validation. This\\nkind of constraint forces some regression coeﬃcient estimates to be exactly\\nzero, thus achieving variable selection while shrinking the remaining coef-\\nﬁcients toward zero to reﬂect the overﬁtting caused by data-based model\\nselection.\\nA ﬁnal problem with variable selection is illustrated by comparing this\\napproach with the sensible way many economists develop regression mod-\\nels. Economists frequently use the strategy of deleting only those variables\\nthat are “insigniﬁcant”and whose regression coeﬃcients have a nonsensible\\ndirection. Standard variable selection on the other hand yields biologically\\nimplausible ﬁndings in many cases by setting certain regression coeﬃcients\\nexactly to zero. In a study of survival time for patients with heart failure,\\nfor example, it would be implausible that patients having a speciﬁc symptom\\nlive exactly as long as those without the symptom just because the symp-\\ntom’s regression coeﬃcient was“insigniﬁcant.”The lasso method shares this\\ndiﬃculty with ordinary variable selection methods and with any method that\\nin the Bayesian context places nonzero prior probability on βbeingexactly\\nzero. 9\\nMany papers claim that there were insuﬃcient data to allow for multivari-\\nable modeling, so they did“univariable screening”wherein only“signiﬁcant”\\nvariables (i.e., those that are separately signiﬁcantly associated with Y)w e r e\\nentered into the model.fThis is just a forward stepwise variable selection in\\nfThis is akin to doing a t-test to compare the two treatments (out of 10, say) that\\nare apparently most diﬀerent from each other.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e88d4f9-ef86-4c0e-b17c-02adda78c594', embedding=None, metadata={'page_label': '72', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='72 4 Multivariable Modeling Strategies\\nwhich insigniﬁcant variables from the ﬁrst step are not reanalyzed in later\\nsteps. Univariable screening is thus even worse than stepwise modeling as\\nit can miss important variables that are only important after adjusting for\\nother variables.598Overall, neither univariable screening nor stepwise vari-\\nable selection in any way solves the problem of“too many variables, too few\\nsubjects,”and they cause severe biases in the resulting multivariable model\\nﬁts while losing valuable predictive information from deleting marginally sig-\\nniﬁcant variables. 10\\nThe online course notes contain a simple simulation study of stepwise\\nselection using R.\\n4.4 Sample Size, Overﬁtting, and Limits on\\nNumber of Predictors\\nWhen a model is ﬁtted that is too complex, that it, has too many free pa-\\nrameters to estimate for the amount of information in the data, the worth\\nof the model (e.g., R2) will be exaggerated and future observed values will\\nnot agree with predicted values. In this situation, overﬁtting is said to be 11\\npresent, and some of the ﬁndings of the analysis come from ﬁtting noise and\\nnot just signal, or ﬁnding spurious associationsbetween XandY. In this sec-\\ntion general guidelines for preventing overﬁtting are given. Here we concern\\nourselves with the reliability orcalibration of a model, meaning the ability of\\nthe model to predict future observations as well as it appeared to predict the\\nresponses at hand. For now we avoid judging whether the model is adequate\\nfor the task, but restrict our attention to the likelihood that the model has\\nsigniﬁcantly overﬁtted the data.\\nIn typical low signal–to–noise ratio situationsg, model validations on in-\\ndependent datasets have found the minimum training sample size for which\\nthe ﬁtted model has an independently validated predictive discrimination\\nthat equals the apparent discrimination seen with in training sample. Similar\\nvalidation experiments have considered the margin of error in estimating an\\nabsolute quantity such as event probability. Studies such as268,270,577have\\nshown that in many situations a ﬁtted regression model is likely to be reli-\\nable when the number of predictors (or candidate predictors if using variable\\nselection) pis less than m/10 orm/20, where mis the“limiting sample size”\\ng i v e ni nT a b l e 4.1. A good average requirement is p<m\\n15. For example, 12\\nSmith et al.577found in one series of simulations that the expected error in\\nCox model predicted ﬁve–year survival probabilities was below 0.05 when\\np<m /20 for “average”subjects and below 0.10 when p<m /20 for “sick”\\ngThese are situations where the true R2is low, unlike tightly controlled experiments\\nand mechanistic models where signal:noise ratios can be quite high. In those situ-\\nations, many parameters can be estimated from small samples, and them\\n15rule of\\nthumb can be signiﬁcantly relaxed.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f019d433-541a-4467-8d06-dee97ef0f27b', embedding=None, metadata={'page_label': '73', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.4 Sample Size, Overﬁtting, and Limits on Number of Predictors 73\\nTable 4.1 Limiting Sample Sizes for Various Response Variables\\nType of Response Variable Limiting Sample Size m\\nContinuous n(total sample size)\\nBinary min( n1,n2)h\\nOrdinal ( kcategories) n−1\\nn2∑k\\ni=1n3\\nii\\nFailure (survival) time number of failuresj\\nsubjects, where mis the number of deaths. For“average”subjects, m/10 was\\nadequate for preventing expected errors >0.1.Note: The number of non-\\nintercept parameters in the model ( p) is usually greater than the number of\\npredictors. Narrowlydistributed predictor variables(e.g., if all subjects’ ages\\nare between 30 and 45 or only 5% of subjects are female) will require even\\nhighersamplesizes.Notethatthenumberofcandidatevariablesmustinclude\\nall variables screened for association with the response, including nonlinear\\nterms and interactions. Instead of relying on the rules of thumb in the table,\\nthe shrinkage factor estimate presented in the next section can be used to\\nguide the analyst in determining how many d.f. to model (see p. 87).\\nRules of thumb such as the 15:1 rule do not consider that a certain min-\\nimum sample size is needed just to estimate basic parameters such as an\\nintercept or residual variance. This is dealt with in upcoming topics about\\nspeciﬁc models. For the case of ordinary linear regression, estimation of the\\nresidual variance is central. All standard errors, P-values, conﬁdence inter-\\nvals, and R2depend on having a precise estimate of σ2.T h eo n e - s a m p l e\\nproblem of estimating a mean, which is equivalent to a linear model contain-\\ning only an intercept, is the easiest case when estimating σ2.W h e nas a m p l e\\nof sizenis drawn from a normal distribution, a 1 −αtwo-sided conﬁdence\\ninterval for the unknown population variance σ2is given by\\nn−1\\nχ2\\n1−α/2,n−1s2<σ2<n−1\\nχ2\\nα/2,n−1s2, (4.1)\\nhSee [487]. If one considers the power of a two-sample binomial test compared\\nwith a Wilcoxon test if the response could be made continuous and the pro por-\\ntional odds assumption holds, the eﬀective sample size for a binary response is\\n3n1n2/n≈3min(n1,n2)i fn1/nis near 0 or 1 [ 664, Eq.10, 15]. Here n1andn2\\nare the marginal frequencies of the two response levels.\\niBased on the power of a proportional odds model two-sample test when the ma rginal\\ncell sizes for the response are n1,...,n k, compared with all cell sizes equal to unity\\n(response is continuous) [ 664, Eq, 3]. If all cell sizes are equal, the relative eﬃciency\\nof having kresponse categories compared with a continuous response is 1 −1/k2[664,\\nEq.14]; for example, a ﬁve-level response is almost as eﬃcient as a continuous one if\\nproportional odds holds across category cutoﬀs.\\njThis is approximate, as the eﬀective sample size may sometimes be boosted some-\\nwhat by censored observations, especially for non-proportional hazards methods such\\nas Wilcoxon-type tests.49', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d579b8f5-eb42-4d7d-a89f-3c2dcb21d32d', embedding=None, metadata={'page_label': '74', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='74 4 Multivariable Modeling Strategies\\nwheres2is the sample variance and χ2\\nα,n−1is theαcritical value of the\\nχ2distribution with n−1 degrees of freedom. We take the fold-change or\\nmultiplicative margin of error (MMOE) for estimating σto be\\n\\ued6a\\ued6b\\ued6b√max(χ2\\n1−α/2,n−1\\nn−1,n−1\\nχ2\\nα/2,n−1) (4.2)\\nTo achieve a MMOE of no worse than 1.2 with 0.95 conﬁdence when\\nestimating σrequires a sample size of 70 subjects.\\nThe linear model case is useful for examining n:pratio another way. As\\ndiscussed in the next section, R2\\nadjis a nearly unbiased estimate of R2, i.e.,\\nis not inﬂated by overﬁtting if the value used for pis“honest”, i.e., includes\\nall variables screened. We can ask the question“for a given R2, what ratio of\\nn:pis required so that R2\\nadjdoes not drop by more than a certain relative or\\nabsolute amount from the value of R2?”This assessment takes into account\\nthat higher signal:noiseratios allowﬁtting more variables. For example,with\\n020406080100\\nR2R2Multiple of p\\n0.0 0.2 0.4 0.6 0.8 1.00.90.950.975\\n020406080100Multiple of p\\n0.0 0.2 0.4 0.6 0.8 1.00.0750.050.040.0250.020.01\\nFig. 4.1 Multiple of pthatnmust be to achieve a relative drop from R2toR2\\nadjby\\nthe indicated relative factor (left panel, 3 factors) or absolute diﬀerence ( right panel,\\n6 decrements)\\nlowR2a 100:1 ratio of n:pmay be required to prevent R2from dropping\\nby more1\\n10or by an absolute amount of 0.01. A 15:1 rule would prevent R2\\nfrom dropping by more than 0.075 for low R2(Figure4.1).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53d6c395-ad73-4e7c-aed5-f33fee4d0f81', embedding=None, metadata={'page_label': '75', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Shrinkage 75\\n4.5 Shrinkage\\nThe term shrinkage is used in regression modeling to denote two ideas. The\\nﬁrst meaning relates to the slope of a calibration plot , which is a plot of\\nobserved responses against predicted responsesk. When a dataset is used to\\nﬁt the model parameters as well as to obtain the calibration plot, the usual\\nestimation process will force the slope of observed versus predicted values to\\nbe one. When, however, parameter estimates are derived from one dataset\\nand then applied to predict outcomes on an independent dataset, overﬁtting\\nwillcausetheslopeofthecalibrationplot(i.e.,the shrinkage factor )t obel ess\\nthan one, a resultof regression to the mean . Typically, low predictions will be\\ntoo low and high predictions too high. Predictions near the mean predicted\\nvalue will usually be quite accurate. The second meaning of shrinkage is a\\nstatistical estimation method that preshrinks regression coeﬃcients towards\\nzero so that the calibration plot for new data will not need shrinkage as its\\ncalibration slope will be one.\\nWe turn ﬁrst to shrinkage as an adverse result of traditional modeling.\\nIn ordinary linear regression, we know that all of the coeﬃcient estimates\\nare exactly unbiased estimates of the true eﬀect when the model ﬁts. Isn’t\\nthe existence of shrinkage and overﬁtting implying that there is some kind\\nof bias in the parameter estimates? The answer is no because each separate\\ncoeﬃcient has the desired expectation. The problem lies in how we use the\\ncoeﬃcients. We tend not to pick out coeﬃcients at random for interpretation\\nbut we tend to highlight very small and very large coeﬃcients.\\nA simple example may suﬃce. Consider a clinical trial with 10 randomly\\nassigned treatments such that the patient responses for each treatment are\\nnormally distributed. We can do an ANOVA by ﬁtting a multiple regres-\\nsion model with an intercept and nine dummy variables. The intercept is an\\nunbiased estimate of the mean response for patients on the ﬁrst treatment,\\nand each of the other coeﬃcients is an unbiased estimate of the diﬀerence\\nin mean response between the treatment in question and the ﬁrst treatment.\\nˆβ0+ˆβ1is an unbiased estimate of the mean response for patients on the\\nsecond treatment. But if we plotted the predicted mean response for patients\\nagainst the observed responses from new data, the slope of this calibration\\nplot would typically be smaller than one. This is because in making this plot\\nwe are not picking coeﬃcients at random but we are sorting the coeﬃcients\\ninto ascending order. The treatment group having the lowest sample mean\\nresponse will usually have a higher mean in the future, and the treatment\\ngroup having the highest sample mean response will typically have a lower\\nmean in the future. The sample mean of the group having the highest sample\\nmean isnotan unbiased estimate of its population mean.\\nkAn even more stringent assessment is obtained by stratifying calibration curves by\\npredictor settings.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06e75caf-6630-4cce-874f-f020bd124452', embedding=None, metadata={'page_label': '76', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"76 4 Multivariable Modeling Strategies\\nAs an illustration, let us draw 20 samples of size n= 50 from a uniform\\ndistribution for which the true mean is 0.5. Figure 4.2displays the 20 means\\nsorted into ascending order, similar to plotting YversusˆY=Xˆβbased\\non least squares after sorting by Xˆβ. Bias in the very lowest and highest\\nestimates is evident.\\nset.seed (123)\\nn←50\\ny←runif(20*n)\\ngroup ←rep(1:20,each=n)\\nybar←tapply(y, group, mean)\\nybar←sort(ybar)\\nplot(1:20, ybar,type= 'n', axes= FALSE, ylim=c(.3,.7),\\nxlab= 'Group ', ylab= 'Group Mean ')\\nlines(1:20, ybar)\\npoints(1:20, ybar, pch=20, cex=.5)\\naxis(2)\\naxis(1, at=1:20, labels=FALSE)\\nfor(j in 1:20) axis(1, at=j, labels=names(ybar)[j])\\nabline(h=.5, col= gray(.85))\\nGroupGroup Mean llllllllllllllllllll\\n0.30.40.50.60.7\\n1661721014209871118543115131912\\nFig. 4.2 Sorted means from 20 samples of size 50 from a uniform [0 ,1] distribution.\\nThe reference line at 0.5 depicts the true population value of all of the me ans.\\nWhen we want to highlight a treatment that is not chosen at random (or a\\npriori), the data-based selection of that treatment needs to be compensated\\nfor in the estimation process.lIt is well known that the use of shrinkage\\nlIt is interesting that researchers are quite comfortable with adjusting P-values for\\npost hoc selection of comparisons using, for example, the Bonferroni inequality, but\\nthey do not realize that post hoc selection of comparisons also biases point es timates.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c90b14b-6675-46dc-b9a6-4fd96aed4741', embedding=None, metadata={'page_label': '77', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.5 Shrinkage 77\\nmethods such as the James–Stein estimator to pull treatment means toward\\nthe grand mean over all treatments results in estimates of treatment-speciﬁc\\nmeans that are far superior to ordinary stratiﬁed means.176\\nTurningfromacellmeansmodeltothegeneralcasewherepredictedvalues\\nare general linear combinations Xˆβ,t h es l o p e γof properly transformed\\nresponses YagainstXˆβ(sorted into ascending order) will be less than one\\non newdata.Estimationof the shrinkagecoeﬃcient γallowsquantiﬁcationof\\nthe amount of overﬁtting present, and it allows one to estimate the likelihood\\nthat the model will reliablypredict new observations.van Houwelingenand le\\nCessie [633, Eq.77] provided a heuristic shrinkage estimate that has worked\\nwell in several examples:\\nˆγ=modelχ2−p\\nmodelχ2, (4.3)\\nwherepis the total degrees of freedom for the predictors and model χ2is 13\\nthe likelihood ratio χ2statistic for testing the joint inﬂuence of all predictors\\nsimultaneously (see Section 9.3.1). For ordinary linear models, van Houwelin-\\ngen and le Cessie proposed a shrinkage factor ˆ γthat can be shown to equal\\nn−p−1\\nn−1R2\\nadj\\nR2, where the adjusted R2is given by 14\\nR2\\nadj=1−(1−R2)n−1\\nn−p−1. (4.4)\\nFor such linear models with an intercept β0, the shrunken estimate of βis\\nˆβs\\n0=( 1−ˆγ)Y+ˆγˆβ0\\nˆβs\\nj=ˆγˆβj,j=1,...,p, (4.5)\\nwhereYis the mean of the response vector. Again, when stepwise ﬁtting is\\nused, the pin these equations is much closer to the number of candidate de-\\ngrees of freedom rather than the number in the“ﬁnal”model. See Section 5.3 15\\nfor methods of estimating γusing the bootstrap (p. 115) or cross-validation.\\nNow turn to the second usage of the term shrinkage . Just as clothing is\\nsometimes preshrunk so that it will not shrink further once it is purchased,\\nbetter calibrated predictions result when shrinkage is built into the estima-\\ntion process in the ﬁrst place. The object of shrinking regression coeﬃcient\\nestimates is to obtain a shrinkage coeﬃcient of γ= 1 on new data. Thus by\\nsomewhat discounting ˆβwe make the model underﬁtted on the data at hand\\n(i.e., apparent γ<1) so that on new data extremely low or high predictions\\nare correct.\\nRidge regression388,633is one technique for placing restrictions on the pa-\\nrameterestimatesthatresultsinshrinkage.A ridge parameter must be chosen\\nto control the amount of shrinkage. Penalized maximum likelihood estima-\\ntion,237,272,388,639a generalization of ridge regression, is a general shrinkage', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58402117-0318-4b3c-b10f-2727b953f13b', embedding=None, metadata={'page_label': '78', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='78 4 Multivariable Modeling Strategies\\nprocedure. A method such as cross-validation or optimization of a modiﬁed\\nAIC must be used to choose an optimal penalty factor. An advantage of pe-\\nnalized estimation is that one can diﬀerentially penalize the more complex\\ncomponents of the model such as nonlinear or interaction eﬀects. A drawback\\nof ridge regression and penalized maximum likelihood is that the ﬁnal model\\nis diﬃcult to validate unbiasedly since the optimal amount of shrinkage is\\nusually determined by examining the entire dataset. Penalization is one of\\nthe best ways to approach the“too many variables, too little data”problem.\\nSee Section 9.10for details.\\n4.6 Collinearity\\nWhen at least one of the predictors can be predicted well from the other\\npredictors, the standard errors of the regression coeﬃcient estimates can be\\ninﬂated and corresponding tests have reduced power.217In stepwise variable\\nselection, collinearitycan cause predictors to compete and make the selection\\nof“important”variables arbitrary. Collinearity makes it diﬃcult to estimate\\nand interpret a particular regression coeﬃcient because the data have little\\ninformation about the eﬀect of changing one variable while holding another\\n(highly correlated) variable constant [ 101, Chap. 9]. However, collinearity\\ndoes not aﬀect the joint inﬂuence of highly correlated variables when tested\\nsimultaneously. Therefore, once groups of highly correlated predictors are\\nidentiﬁed, the problem can be rectiﬁed by testing the contribution of an\\nentire set with a multiple d.f. test rather than attempting to interpret the\\ncoeﬃcient or one d.f. test for a single predictor.\\nCollinearity does not aﬀect predictions made on the same dataset used to\\nestimate the model parameters or on new data that have the same degree\\nof collinearity as the original data [ 470, pp. 379–381] as long as extreme\\nextrapolationis notattempted. Consider as two predictorsthe total andLDL\\ncholesterols that are highly correlated. If predictions are made at the same\\ncombinations of total and LDL cholesterol that occurred in the training data,\\nno problem will arise. However, if one makes a prediction at an inconsistent\\ncombination of these two variables, the predictions may be inaccurate and\\nhave high standard errors.\\nWhen the ordinary truncated power basis is used to derive component\\nvariables for ﬁtting linear and cubic splines, as was described earlier, the\\ncomponent variables can be very collinear. It is very unlikely that this will\\nresult in any problems, however, as the component variables are connected\\nalgebraically. Thus it is not possible for a combination of, for example, xand\\nmax(x−10,0) to be inconsistent with each other. Collinearity problems are\\nthen more likely to result from partially redundant subsets of predictors as\\nin the cholesterol example above.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3aebdbbe-d449-4183-9464-104a03ae36a5', embedding=None, metadata={'page_label': '79', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.7 Data Reduction 79\\nOne way to quantify collinearity is with variance inﬂation factors orVIF,\\nwhichinordinaryleastsquaresarediagonalsoftheinverseofthe X′Xmatrix\\nscaled to have unit variance (except that a column of 1s is retained corre-\\nsponding to the intercept). Note that some authors compute VIF from the\\ncorrelation matrix form of the design matrix, omitting the intercept. VIFiis\\n1/(1−R2\\ni)w h e r eR2\\niis the squared multiple correlation coeﬃcient between\\ncolumniandtheremainingcolumnsofthedesignmatrix.Formodelsthatare\\nﬁtted with maximum likelihood estimation, the information matrix is scaled\\nto correlation form, and VIF is the diagonal of the inverse of this scaled ma-\\ntrix.147,654Then the VIF are similar to those from a weighted correlation\\nmatrix of the original columns in the design matrix. Note that indexes such 16\\nas VIF are not very informative as some variables are algebraicallyconnected\\nto each other.\\nTheSAS VARCLUSprocedure539andRvarclusfunctioncanidentifycollinear\\npredictors. Summarizing collinear variables using a summary score is more\\npowerful and stable than arbitrary selection of one variable in a group of\\ncollinear variables (see the next section). 17\\n4.7 Data Reduction\\nThe sample size need not be as large as shown in Table 4.1if the model\\nis to be validated independently and if you don’t care that the model may\\nfail to validate. However, it is likely that the model will be overﬁtted and\\nwill not validate if the sample size does not meet the guidelines. Use of data\\nreduction methods before model development is strongly recommended if the\\nconditions in Table 4.1are not satisﬁed, and if shrinkage is not incorporated\\ninto parameter estimation. Methods such as shrinkage and data reduction\\nreduce the eﬀective d.f. of the model, making it more likely for the model\\nto validate on future data. Data reduction is aimed at reducing the number\\nof parameters to estimate in the model, without distorting statistical infer-\\nence for the parameters. This is accomplished by ignoring Yduring data\\nreduction. Manipulations of Xin unsupervised learning may result in a loss\\nof information for predicting Y, but when the information loss is small, the\\ngain in power and reduction of overﬁtting more than oﬀset the loss.\\nSome available data reduction methods are given below.\\n1. Use the literature to eliminate unimportant variables.\\n2. Eliminate variables whose distributions are too narrow.\\n3. Eliminate candidate predictors that are missing in a large number of sub-\\njects, especially if those same predictors are likely to be missing for future\\napplications of the model.\\n4. Use a statistical data reduction method such as incomplete principal com-\\nponent regression, nonlinear generalizations of principal components such', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='860a1cac-4604-4f99-95bd-07e2771eeae6', embedding=None, metadata={'page_label': '80', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='80 4 Multivariable Modeling Strategies\\nas principal surfaces, sliced inverse regression, variable clustering, or ordi-\\nnary cluster analysis on a measure of similarity between variables. 18\\nSee Chapters 8and14for detailed case studies in data reduction.\\n4.7.1 Redundancy Analysis\\nTherearemanyapproachestodatareduction.Onerigorousapproachinvolves\\nremoving predictors that are easily predicted from other predictors, using\\nﬂexible parametric additive regression models. This approach is unlikely to\\nresultinamajorreductioninthe numberofregressioncoeﬃcientstoestimate\\nagainstY, but will usually provide insights useful for later data reduction\\nover and above the insights given by methods based on pairwise correlations\\ninstead of multiple R2.\\nTheHmisc redun function implements the following redundancy checking\\nalgorithm.\\n•Expand each continuous predictor into restricted cubic spline basis func-\\ntions. Expand categorical predictors into dummy variables.\\n•Use OLS to predict each predictor with all component terms of all remain-\\ningpredictors(similartowhatthe Hmisc transcan functiondoes).Whenthe\\npredictor is expanded into multiple terms, use the ﬁrst canonical variatem.\\n•Remove the predictor that can be predicted from the remaining set with\\nthe highest adjusted or regular R2.\\n•Predict all remaining predictors from their complement.\\n•Continue in like fashion until no variable still in the list of predictors can\\nbe predicted with an R2or adjusted R2greater than a speciﬁed threshold\\nor until dropping the variable with the highest R2(adjusted or ordinary)\\nwould cause a variable that was dropped earlier to no longer be predicted\\nat the threshold from the now smaller list of predictors.\\nSpecial consideration must be given to categorical predictors. One way to\\nconsider a categorical variable redundant is if a linear combination of dummy\\nvariables representing it can be predicted from a linear combination of other\\nvariables.Forexample,iftherewere4citiesinthedataandeachcity’srainfall\\nwas also present as a variable, with virtually the same rainfall reported for\\nall observations for a city, city would be redundant given rainfall (or vice-\\nversa). If two cities had the same rainfall, ‘city’ might be declared redundant\\neven though tied cities might be deemed non-redundant in another setting. A\\nsecond, more stringent way to check for redundancy of a categoricalpredictor\\nis to ascertain whether all dummy variables created from the predictor are\\nindividually redundant. The redunfunction implements both approaches.\\nExamples of use of redunare given in two case studies. 19\\nmThere is an option to force continuous variables to be linear when they are be ing\\npredicted.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6ba8a6d-cc4d-4a29-ac23-69856362d73e', embedding=None, metadata={'page_label': '81', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.7 Data Reduction 81\\n4.7.2 Variable Clustering\\nAlthough the use of subject matter knowledge is usually preferred, statistical\\nclustering techniques can be useful in determining independent dimensions\\nthat are described by the entire list of candidate predictors. Once each di-\\nmension is scored (see below), the task of regression modeling is simpliﬁed,\\nand one quits trying to separate the eﬀects of factors that are measuring the\\nsame phenomenon. One type of variable clustering539i sb a s e do nat y p eo f\\noblique-rotationprincipalcomponent(PC)analysisthatattemptstoseparate\\nvariables so that the ﬁrst PC of each group is representative of that group\\n(the ﬁrst PC is the linear combination of variables having maximum vari-\\nance subject to normalization constraints on the coeﬃcients142,144). Another\\napproach, that of doing a hierarchical cluster analysis on an appropriate sim-\\nilarity matrix (such as squared correlations) will often yield the same results.\\nFor either approach, it is often advisable to use robust (e.g., rank-based)\\nmeasures for continuous variables if they are skewed, as skewed variables can\\ngreatly aﬀect ordinary correlation coeﬃcients. Pairwise deletion of missing\\nvalues is also advisable for this procedure—casewise deletion can result in a\\nsmall biased sample. 20\\nWhen variables are not monotonically related to each other, Pearson or\\nSpearman squared correlations can miss important associations and thus are\\nnot always good similarity measures. A general and robust similarity mea-\\nsure is Hoeﬀding’s D,295which for two variables XandYis a measure of\\nthe agreement between F(x,y)a n dG(x)H(y), where G,Hare marginal cu-\\nmulative distribution functions and Fis the joint CDF. The Dstatistic will\\ndetect a wide variety of dependencies between two variables.\\nSee pp.330and458for examples of variable clustering. 21\\n4.7.3 Transformation and Scaling Variables Without\\nUsingY\\nScalingtechniquesoftenallowtheanalysttoreducethenumberofparameters\\nto ﬁt by estimating transformationsfor each predictor using only information\\nabout associations with other predictors. It may be advisable to cluster vari-\\nables before scaling so that patterns are derived only from variables that are\\nrelated. For purely categorical predictors, methods such as correspondence\\nanalysis (see, for example, [108,139,239,391,456]) can be useful for data reduc-\\ntion. Often one can use these techniques to scale multiple dummy variables\\ninto a few dimensions. For mixtures of categorical and continuous predictors,\\nqualitative principal component analysis such as the maximum total variance\\n(MTV) method of Young et al.456,680is useful. For the special case of repre-\\nsenting a series of variables with one PC, the MTV method is quite easy to\\nimplement.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='407f5a46-7bf5-405b-be9c-150e94792529', embedding=None, metadata={'page_label': '82', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='82 4 Multivariable Modeling Strategies\\n1. Compute PC1, the ﬁrst PC of the variables to reduce X1,...,X qusing\\nthe correlation matrix of Xs.\\n2. Use ordinary linear regression to predict PC1on the basis of functions of\\ntheXs, such as restricted cubic spline functions for continuous Xso ra\\nseries of dummy variables for polytomous Xs. The expansion of each Xj\\nis regressed separately on PC1.\\n3. These separately ﬁtted regressions specify the working transformations of\\neachX.\\n4. Recompute PC1by doing a PC analysis on the transformed Xs (predicted\\nvalues from the ﬁts).\\n5. Repeat steps 2 to 4 until the proportion of variation explained by PC1\\nreaches a plateau. This typically requires three to four iterations.\\nA transformation procedure that is similar to MTV is the maximum gen-\\neralized variance (MGV) method due to Sarle [ 368, pp. 1267–1268]. MGV\\ninvolves predicting each variable from (the current transformations of) all\\nthe other variables. When predicting variable i, that variable is represented\\nas a set of linear and nonlinear terms (e.g., spline components). Analysis of\\ncanonical variates279can be used to ﬁnd the linear combination of terms for\\nXi(i.e., ﬁnd a new transformation for Xi) and the linear combination of the\\ncurrent transformations of all other variables (representing each variable as\\na single, transformed, variable) such that these two linear combinations have\\nmaximumcorrelation.(Forexample,ifthereareonlytwovariables X1andX2\\nrepresented as quadratic polynomials, solve for a,b,c,dsuch that aX1+bX2\\n1\\nhas maximum correlation with cX2+dX2\\n2.) The process is repeated until the\\ntransformations converge. The goal of MGV is to transform each variable so\\nthat it is most similar to predictions from the other transformed variables.\\nMGV does not use PCs (so one need not precede the analysis by variable\\nclustering), but once all variables have been transformed, you may want to\\nsummarize them with the ﬁrst PC.\\nTheSAS PRINQUAL procedureofKuhfeld368implementstheMTVandMGV\\nmethods, and allows for very ﬂexible transformations of the predictors, in-\\ncluding monotonic splines and ordinary cubic splines.\\nA very ﬂexible automatic procedure for transforming each predictor in\\nturn, based on all remaining predictors, is the ACE (alternating conditional\\nexpectation) procedure of Breiman and Friedman.68Like SAS PROC PRIN-\\nQUAL, ACE handles monotonically restricted transformations and categorical\\nvariables. It ﬁts transformations by maximizing R2between one variable and\\na set of variables. It automatically transforms all variables, using the“super\\nsmoother”207for continuous variables. Unfortunately, ACE does not handle\\nmissing values. See Chapter 16for more about ACE.\\nIt must be noted that at best these automatic transformation procedures\\ngenerallyﬁndonly marginal transformations,nottransformationsofeachpre-\\ndictor adjusted for the eﬀects of all other predictors.When adjusted transfor-\\nmations diﬀer markedly from marginal transformations, only joint modeling\\nof all predictors (and the response) will ﬁnd the correct transformations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1da9588-6653-4858-95ac-37dfe5ff949a', embedding=None, metadata={'page_label': '83', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.7 Data Reduction 83\\nOnce transformations are estimated using only predictor information, the\\nadequacy of each predictor’s transformation can be checked by graphical\\nmethods, by nonparametric smooths of transformed XjversusY,o rb ye x -\\npanding the transformed Xjusing a spline function. This approach of check-\\ning that transformations are optimal with respect to Yuses the response\\ndata, but it accepts the initial transformations unless they are signiﬁcantly\\ninadequate. If the sample size is low, or if PC1for the groupof variables used\\nin deriving the transformations is deemed an adequate summary of those\\nvariables, that PC1can be used in modeling. In that way, data reduction is\\naccomplished two ways: by not using Yto estimate multiple coeﬃcients for\\na single predictor, and by reducing related variables into a single score, after\\ntransforming them. See Chapter 8for a detailed example of these scaling\\ntechniques.\\n4.7.4 Simultaneous Transformation and Imputation\\nAs mentioned in Chapter 3(p.52) if transformations are complex or non-\\nmonotonic, ordinary imputation models may not work. SAS PROC PRINQUAL\\nimplementedamethodforsimultaneouslyimputingmissingvalueswhilesolv-\\ning for transformations. Unfortunately, the imputation procedure frequently\\nconverges to imputed values that are outside the allowable range of the data.\\nThis problem is more likely when multiple variables are missing on the same\\nsubjects, since the transformation al gorithm may simply separate missings\\nand nonmissings into clusters.\\nA simple modiﬁcation of the MGV algorithm of PRINQUAL that simulta-\\nneously imputes missing values without these problems is implemented in\\ntheRfunction transcan. Imputed values are initialized to medians of contin-\\nuous variables and the most frequent category of categorical variables. For\\ncontinuous variables, transformations are initialized to linear functions. For\\ncategorical ones, transformations may be initialized to the identify function,\\nto dummy variables indicating whether the observation has the most preva-\\nlent categorical value, or to random numbers. Then when using canonical\\nvariates to transform each variable in turn, observations that are missing on\\nthe current “dependent”variable are excluded from consideration, although\\nmissing values for the current set of“predictors”are imputed. Transformed\\nvariables are normalized to have mean 0 and standard deviation 1. Although\\ncategorical variables are scored using the ﬁrst canonical variate, transcan has\\nan option to use recursive partitioning to obtain imputed values on the origi-\\nnal scale (Section 2.5) for these variables. It defaults to imputing categorical\\nvariables using the category whose predicted canonical score is closest to the\\npredicted score.\\ntranscan usesrestrictedcubicsplinestomodel continuousvariables.Itdoes\\nnot implement monotonicity constraints. transcan automatically constrains', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b848906b-0429-4572-a850-63fa2f87e571', embedding=None, metadata={'page_label': '84', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='84 4 Multivariable Modeling Strategies\\nimputed values (both on transformed and original scales) to be in the same\\nrange as non-imputed ones. This adds much stability to the resulting esti-\\nmates although it can result in a boundary eﬀect. Also, imputed values can\\noptionally be shrunken using Eq. 4.5to avoid overﬁtting when developing\\nthe imputation models. Optionally, missing values can be set to speciﬁed\\nconstants rather than estimating them. These constants are ignored during\\nthe transformation-estimation phasen. This technique has proved to be help-\\nful when, for example, a laboratory test is not ordered because a physician\\nthinks the patient has returned to normal with respect to the lab parameter\\nmeasured by the test. In that case, it’s better to use a normal lab value for\\nmissings.\\nThe transformation and imputation information created by transcan may\\nbe used to transform/impute variables in datasets not used to develop the\\ntransformationandimputationformulas.Thereis alsoan Rfunction tocreate\\nRfunctions that compute the ﬁnal transformed values of each predictor given\\ninput values on the original scale.\\nAs an example of non-monotonic transformationand imputation, consider\\na sample of 1000 hospitalized patients from the SUPPORTostudy.352Two\\nmean arterial blood pressure measurements were set to missing.\\nrequire( Hmisc)\\ngetHdata(support) # Get data framefrom web site\\nheart.rate ←support$hrt\\nblood.pressure ←support$ meanbp\\nblood.pressure[400:401]\\nMean Arterial Blood Pressure Day 3\\n[1] 151 136\\nblood.pressure [400:401] ←NA# Create two missings\\nd←data.frame(heart.rate, blood.pressure)\\npar(pch=46) # Figure 4.3\\nw←transcan( ∼heart.rate + blood.pressure, transformed=TRUE,\\nimputed=TRUE, show.na= TRUE,data=d)\\nConvergence criterion :2.901 0.035\\n0.007\\nConvergence in 4 iterations\\nR2achieved in predicting each variable:\\nheart.rate blood.pressure\\n0.259 0.259\\nAdjusted R2:\\nheart.rate blood.pressure\\n0.254 0.253\\nnIf one were to estimate transformations without removing observations that had\\nthese constants inserted for the current Y-variable, the resulting transformations\\nwould likely have a spike at Y= imputation constant.\\noStudy to Understand Prognoses Preferences Outcomes and Risks of Treatments', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b100f964-c137-43f0-b595-6fe596a63d96', embedding=None, metadata={'page_label': '85', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"4.7 Data Reduction 85\\nw$imputed$blood.pressure\\n400 401\\n132.4057 109.7741\\nt←w$transformed\\nspe←round(c( spearman( heart.rate, blood.pressure),\\nspearman(t[, 'heart.rate '],\\nt[,'blood.pressure '])), 2)\\n0 50 100 150 200 250 30002468\\nheart.rateTransformed heart.rate\\n0 50 100 150−8−6−4−20\\nblood.pressureTransformed blood.pressure\\nFig. 4.3 Transformations ﬁtted using transcan . Tickmarksindicatethetwoimputed\\nvalues for blood pressure.\\nplot(heart.rate, blood.pressure) # Figure 4.4\\nplot(t[, 'heart.rate '], t[, 'blood.pressure '],\\nxlab= 'Transformed hr ', ylab= 'Transformed bp ')\\nSpearman’s rank correlation ρbetween pairs of heart rate and blood pressure\\nwas-0.02,becausethesevariableseachrequire U-shapedtransformations.Us-\\ning restricted cubic splines with ﬁve knots placed at default quantiles, tran-\\nscanprovided the transformations shown in Figure 4.3. Correlation between\\ntransformed variables is ρ=−0.13. The ﬁtted transformations are similar to\\nthose obtained from relating these two variables to time until death.\\n4.7.5 Simple Scoring of Variable Clusters\\nIf a subset of the predictors is a series of related dichotomous variables, a\\nsimpler data reduction strategy is sometimes employed. First, construct two\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='57f39569-a4ac-452e-9919-9ec75ad3ef7b', embedding=None, metadata={'page_label': '86', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='86 4 Multivariable Modeling Strategies\\n0 50 100 150 200 250 300050100150\\nheart.rateblood.pressure\\n02468−8−6−4−20\\nTransformed hrTransformed bp\\nFig. 4.4 The lower left plot contains raw data (Spearman ρ=−0.02); the lower right\\nis a scatterplot of the corresponding transformed values ( ρ=−0.13). Data courtesy\\nof the SUPPORT study352.\\nnew predictors representingwhether any ofthe factorsis positive and a count\\nof the number of positive factors. For the ordinal count of the number of\\npositive factors, score the summary variable to satisfy linearity assumptions\\nas discussed previously. For the more powerful predictor of the two summary\\nmeasures, test for adequacy of scoring by using all dichotomous variables as\\ncandidatepredictorsafter adjustingforthe new summaryvariable.A residual\\nχ2statistic can be used to test whether the summary variable adequately\\ncaptures the predictive information of the series of binary predictors.pThis\\nstatistic will have degrees of freedom equal to one less than the number of\\nbinarypredictorswhentestingforadequacyofthesummarycount(andhence\\nwill have low power when there are many predictors). Stratiﬁcation by the\\nsummaryscoreandexaminationofresponsesovercells canbe usedto suggest\\na transformation on the score.\\nAnotherapproachtoscoringaseriesofrelateddichotomouspredictorsisto\\nhave“experts”assignseverity points to each condition and then to either sum\\nthese points or use a hierarchical rule that scores according to the condition\\nwith the highest points (see Section 14.3for an example). The latter has the\\nadvantage of being easy to implement for ﬁeld use. The adequacy of either\\ntype of scoring can be checked using tests of linearity in a regression modelq.\\npWhether this statistic should be used to change the model is problematic in view\\nof model uncertainty.\\nqTheRfunction score.binary in theHmiscpackage (see Section 6.2) assists in\\ncomputing a summary variable from the series of binary conditions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='296d4ecc-2042-49ac-9865-95b10c5dca70', embedding=None, metadata={'page_label': '87', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.7 Data Reduction 87\\n4.7.6 Simplifying Cluster Scores\\nIf a variable cluster contains many individual predictors, parsimony may 22\\nsometimes be achieved by predicting the cluster score from a subset of its\\ncomponents (using linear regression or CART (Section 2.5), for example).\\nThen a new cluster score is created and the response model is rerun with the\\nnew score in the place of the original one. If one constituent variable has a\\nvery high R2in predicting the original cluster score, the single variable may\\nsometimes be substituted for the cluster score in reﬁtting the model without\\nloss of predictive discrimination.\\nSometimes it may be desired to simplify a variable cluster by asking the\\nquestion“which variables in the cluster are really the predictive ones?,”even\\nthough this approach will usually cause true predictive discrimination to suf-\\nfer. For clustersthat areretainedafter limited step-downmodeling, the entire\\nlistofvariablescanbeusedascandidatepredictorsandthestep-downprocess\\nrepeated.Allvariablescontainedinclustersthatwerenotselectedinitiallyare\\nignored. A fair way to validate such two-stage models is to use a resampling\\nmethod (Section 5.3) with scores for deleted clusters as candidate variables\\nfor each resample, along with all the individual variables in the clusters the\\nanalyst really wants to retain. A method called battery reduction can be used\\nto delete variables from clusters by determining if a subset of the variables\\ncan explain most of the variance explained by PC1(see [142, Chapter 12]\\nand445). This approach does not require examination of associations with Y.\\nBattery reduction can also be used to ﬁnd a set of individual variables that\\ncapture much of the information in the ﬁrst kprincipal components. 23\\n4.7.7 How Much Data Reduction Is Necessary?\\nIn addition to using the sample size to degrees of freedom ratio as a rough\\nguide to how much data reduction to do before model ﬁtting, the heuristic\\nshrinkage estimate in Equation 4.3can also be informative. First, ﬁt a full\\nmodel with all candidate variables, nonlinear terms, and hypothesized inter-\\nactions. Let pdenote the number of parameters in this model, aside from any\\nintercepts. Let LR denote the log likelihood ratio χ2for this full model. The\\nestimated shrinkage is (LR −p)/LR. If this falls below 0.9, for example, we\\nmay be concerned with the lack of calibration the model may experience on\\nnew data. Eithera shrunkenestimatorordatareductionis needed. Areduced\\nmodel may have acceptable calibration if associations with Yare not used to\\nreduce the predictors.\\nA simple method, with an assumption, can be used to estimate the target\\nnumber of total regression degrees of freedom qin the model. In a “best\\ncase,” the variables removed to arrive at the reduced model would have no\\nassociation with Y. The expected value of the χ2statistic for testing those', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='293d27ae-53b2-440b-8587-fbcf4f89033f', embedding=None, metadata={'page_label': '88', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='88 4 Multivariable Modeling Strategies\\nvariables would then be p−q. The shrinkage for the reduced model is then\\non average [LR −(p−q)−q]/[LR−(p−q)]. Setting this ratio to be ≥0.9\\nand solving for qgivesq≤(LR−p)/9. Therefore, reduction of dimensionality\\ndown to qdegrees of freedom would be expected to achieve <10% shrinkage.\\nWith these assumptions, there is no hope that a reduced model would have\\nacceptable calibration unless LR >p+9. If the information explained by the\\nomitted variables is less than one would expect by chance (e.g., their total\\nχ2is extremely small), a reduced model could still be beneﬁcial, as long as\\nthe conservativebound (LR −q)/LR≥0.9o rq≤LR/10 were achieved. This\\nconservative bound assumes that no χ2is lost by the reduction, that is that\\nthe ﬁnal model χ2≈LR. This is unlikely in practice. Had the p−qomitted\\nvariables had a larger χ2of 2(p−q) (the break-even point for AIC), qmust\\nbe≤(LR−2p)/8.\\nAs an example, suppose that a binary logistic model is being developed\\nfrom a sample containing 45 events on 150 subjects. The 10:1 rule suggests\\nwe can analyze 4.5 degrees of freedom. The analyst wishes to analyze age,\\nsex, and 10 other variables. It is not known whether interaction between age\\nand sex exists, and whether age is linear. A restricted cubic spline is ﬁtted\\nwith four knots, and a linear interaction is allowed between age and sex.\\nTheset w ov ariablesthenneed3+1+1=5degreesoff reedom.Theoth er\\n10 variables are assumed to be linear and to not interact with themselves\\nor age and sex. There is a total of 15 d.f. The full model with 15 d.f. has\\nLR = 50. Expected shrinkage from this model is (50 −15)/50 = 0.7. Since\\nLR>15 + 9 = 24, some reduction might yield a better validating model.\\nReduction to q=( 5 0−15)/9≈4 d.f. would be necessary, assuming the\\nreduced LR is about 50 −(15−4) = 39. In this case the 10:1 rule yields\\nabout the same value for q. The analyst may be forced to assume that age is\\nlinear, modeling 3 d.f. for age and sex. The other 10 variables would have to\\nbe reduced to a single variable using principal components or another scaling\\ntechnique. The AIC-based calculation yields a maximum of 2.5 d.f.\\nIf the goal of the analysis is to make a series of hypothesis tests (adjusting\\nP-values for multiple comparisons)instead of to predict future responses, the\\nfull model would have to be used.\\nA summary of the various data reduction methods is given in Figure 4.5.\\nWhen principal component analysis or related methods are used for data\\nreduction, the model may be harder to describe since internal coeﬃcients are\\n“hidden.” Rcode on p. 141shows how an ordinary linear model ﬁt can be\\nused in conjunction with a logistic model ﬁt based on principal components\\nto draw a nomogram with axes for all predictors. 24', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0fe03acb-8fd4-481b-8175-aac5ba560f5d', embedding=None, metadata={'page_label': '89', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.8 Other Approaches to Predictive Modeling 89\\nFig. 4.5 Summary of Some Data Reduction Methods\\nGoals Reasons Methods\\nGroup predictors so that\\neach group represents a\\nsingle dimension that can\\nbe summarized with a sin-\\ngle score•↓d.f. arising from mul-\\ntiple predictors\\n•Make PC1more reason-\\nable summaryVariable clustering\\n•Subject matter knowl-\\nedge\\n•Group predictors to\\nmaximize proportion of\\nvariance explained by\\nPC1of each group\\n•Hierarchical clustering\\nusing a matrix of simi-\\nlarity measures between\\npredictors\\nTransform predictors•↓d.f. due to nonlin-\\near and dummy variable\\ncomponents\\n•Allows predictors to be\\noptimally combined\\n•Make PC1more reason-\\nable summary\\n•Use in customized\\nmodel for imputing\\nmissing values on each\\npredictor•Maximum total vari-\\nance on a group of re-\\nlated predictors\\n•Canonical variates on\\nthe total set of predic-\\ntors\\nScore a group of predic-\\ntors↓d.f. for group to unity•PC1\\n•Simple point scores\\nMultiple dimensional\\nscoring of all predictors↓d.f. for all predictors\\ncombinedPrincipal components\\n1,2,...,k,k < p com-\\nputed from all trans-\\nformed predictors\\n4.8 Other Approaches to Predictive Modeling\\nThe approaches recommended in this text are\\n•ﬁtting fully pre-speciﬁed models without deletion of“insigniﬁcant”predic-\\ntors\\n•using data reduction methods (masked to Y) to reduce the dimensionality\\nof the predictors and then ﬁtting the number of parameters the data’s\\ninformation content can support', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9e537f3c-66a5-4441-99c4-a55d99ac4a45', embedding=None, metadata={'page_label': '90', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='90 4 Multivariable Modeling Strategies\\n•using shrinkage (penalized estimation) to ﬁt a large model without worry-\\ning about the sample size.\\nData reduction approaches covered in the last section can yield very inter-\\npretable,stablemodels,buttherearemanydecisionstobemadewhenusinga\\ntwo-stage (reduction/model ﬁtting) approach. Newer single stage approaches\\nare evolving. These new approaches, listed on the text’s web site, handle\\ncontinuous predictors well, unlike recursive partitioning.\\nWhen data reduction is not required, generalized additive models277,674\\nshould also be considered.\\n4.9 Overly Inﬂuential Observations\\nEvery observation should inﬂuence the ﬁt of a regression model. It can be\\ndisheartening, however, if a signiﬁcant treatment eﬀect or the shape of a\\nregression eﬀect rests on one or two observations. Overly inﬂuential obser-\\nvations also lead to increased variance of predicted values, especially when\\nvariances are estimated by bootstrapping after taking variable selection into\\naccount. In some cases, overly inﬂuential observations can cause one to aban-\\ndon a model,“change”the data, or get moredata. Observations can be overly\\ninﬂuential for several major reasons.\\n1. The most common reason is having too few observations for the complex-\\nity of the model being ﬁtted. Remedies for this have been discussed in\\nSections 4.7and4.3.\\n2. Data transcription or data entry errors can ruin a model ﬁt.\\n3. Extreme values of the predictor variables can have a great impact, even\\nwhen these values are validated for accuracy. Sometimes the analyst may\\ndeem a subject so atypical of other subjects in the study that deletion\\nof the case is warranted. On other occasions, it is beneﬁcial to truncate\\nmeasurementswhere the datadensity ends. Inone datasetof4000patients\\nand 2000 deaths, white blood count (WBC) ranged from 500 to 100,000\\nwith .05 and .95 quantiles of 2755 and 26,700, respectively. Predictions\\nfrom a linear spline function of WBC were sensitive to WBC >60,000, for\\nwhich there were 16 patients. There were 46 patients with WBC >40,000.\\nPredictions were found to be more stable when WBC was truncated at\\n40,000, that is, setting WBC to 40,000 if WBC >40,000.\\n4. Observations containing disagreements between the predictors and the re-\\nsponsecaninﬂuencethe ﬁt. Suchdisagreementsshouldnot leadtodiscard-\\ning the observations unless the predictor or response values are erroneous\\nas in Reason 3, or the analysis is made conditional on observations being\\nunlike the inﬂuential ones. In one example a single extreme predictor value\\nin a sample of size 8000 that was not on a straight line relationship with', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89f1a11b-2474-4396-8f76-cb420c6ac148', embedding=None, metadata={'page_label': '91', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.9 Overly Inﬂuential Observations 91\\nthe other ( X,Y) pairs caused a χ2of 36 for testing nonlinearity of the pre-\\ndictor. Remember that an imperfectly ﬁtting model is a fact of life, and\\ndiscardingthe observationscan inﬂate the model’s predictive accuracy.On\\nrare occasions, such lack of ﬁt may lead the analyst to make changes in\\nthe model’s structure, but ordinarily this is best done from the “ground\\nup”using formal tests of lack of ﬁt (e.g., a test of linearity or interaction).\\nInﬂuentialobservationsofthe secondandthirdkinds canoftenbe detected\\nby carefulqualitycontrolofthe data.Statisticalmeasurescanalsobe helpful.\\nThe most common measures that apply to a variety of regression models are\\nleverage, DFBETAS, DFFIT, and DFFITS.\\nLeverage measures the capacity of an observation to be inﬂuential due\\nto having extreme predictor values. Such an observation is not necessarily\\ninﬂuential. To compute leverage in ordinary least squares, we deﬁne the hat\\nmatrixHgiven by\\nH=X(X′X)−1X′. (4.6)\\nHis the matrix that when multiplied by the response vector gives the pre-\\ndicted values, so it measures how an observation estimates its own predicted\\nresponse. The diagonals hiiofHare the leverage measures and they are not\\ninﬂuenced by Y. It has been suggested47thathii>2(p+1)/nsignal a high\\nleverage point, where pis the number of columns in the design matrix X\\naside from the intercept and nis the number of observations. Some believe\\nthat the distribution of hiishould be examined for values that are higher\\nthan typical.\\nDFBETAS is the change in the vector of regression coeﬃcient estimates\\nupon deletion of each observation in turn, scaled by their standard errors.47\\nSince DFBETAS encompasses an eﬀect for each predictor’s coeﬃcient, DF-\\nBETAS allows the analyst to isolate the problem better than some of the\\nother measures. DFFIT is the change in the predicted Xβwhen the observa-\\ntion is dropped, and DFFITS is DFFIT standardized by the standard error\\nof the estimate of Xβ. In both cases, the standard error used for normal-\\nization is recomputed each time an observation is omitted. Some classify an\\nobservation as overly inﬂuential when |DFFITS|>2√\\n(p+1)/(n−p−1),\\nwhile others prefer to examine the entire distribution of DFFITS to identify\\n“outliers”.47\\nSection10.7discusses inﬂuence measures for the logistic model, which\\nrequires maximum likelihood estimation. These measures require the use of\\nspecial residuals and information matrices (in place of X′X).\\nIf truly inﬂuential observations are identiﬁed using these indexes, careful\\nthought is needed to decide how (or whether) to deal with them. Most im-\\nportant, there is no substitute for careful examination of the dataset before\\ndoing any analyses.99Spence and Garrison [ 581, p. 16] feel that\\nAlthough the identiﬁcation of aberrations receives considerable attention in\\nmost modern statistical courses, the emphasis sometimes seems to be on dis-\\nposing of embarrassing data by searching for sources of technical error or', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab374f52-4883-488c-bd93-4096636b3c5e', embedding=None, metadata={'page_label': '92', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='92 4 Multivariable Modeling Strategies\\nminimizing the inﬂuence of inconvenient data by the application of resistant\\nmethods. Working scientists often ﬁnd the most interesting aspect of the anal-\\nysis inheres in the lack of ﬁt rather than the ﬁt itself.\\n4.10 Comparing Two Models\\nFrequently one wants to choose between two competing models on the ba-\\nsis of a common set of observations. The methods that follow assume that\\nthe performance of the models is evaluated on a sample not used to develop\\neither one. In this case, predicted values from the model can usually be con-\\nsidered as a single new variable for comparison with responses in the new\\ndataset. These methods listed below will also work if the models are com-\\npared using the same set of data used to ﬁt each one, as long as both models\\nhave the same eﬀective number of (candidate or actual) parameters. This\\nrequirement prevents us from rewarding a model just because it overﬁts the\\ntraining sample (see Section 9.8.1for a method comparing two models of dif-\\nfering complexity). The methods can also be enhanced using bootstrapping\\nor cross-validationon a single sample to get a fair comparison when the play-\\ning ﬁeld is not level, for example, when one model had more opportunity for\\nﬁtting or overﬁtting the responses.\\nSome of the criteria for choosing one model over the other are\\n1. calibration (e.g., one model is well-calibrated and the other is not),\\n2. discrimination,\\n3. face validity,\\n4. measurement errors in required predictors,\\n5. use of continuous predictors (which are usually better deﬁned than cate-\\ngorical ones),\\n6. omission of “insigniﬁcant” variables that nonetheless make sense as risk\\nfactors,\\n7. simplicity (although this is less important with the availability of comput-\\ners), and\\n8. lack of ﬁt for speciﬁc types of subjects.\\nItems 3 through 7 require subjective judgment, so we focus on the other as-\\npects. If the purpose of the models is only to rank-order subjects, calibration\\nis not an issue. Otherwise, a model having poor calibration can be dismissed\\noutright. Given that the two models have similar calibration, discrimination\\nshould be examined critically. Various statistical indexes can quantify dis-\\ncrimination ability (e.g., R2,m o d e lχ2,S o m e r s ’ Dxy,S p e a r m a n ’ s ρ, area un-\\nder ROC curve—see Section 10.8). Rank measures ( Dxy,ρ, ROC area) only\\nmeasure how well predicted values can rank-order responses. For example,\\npredicted probabilities of 0.01 and 0.99 for a pair of subjects are no better\\nthan probabilities of 0.2 and 0.8 using rank measures, if the ﬁrst subject had', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eaca757b-0838-4856-8217-fcc06282ca5e', embedding=None, metadata={'page_label': '93', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.10 Comparing Two Models 93\\na lower response value than the second. Therefore, rank measures such as\\nROC area ( cindex), although ﬁne for describing a given model, may not be\\nvery sensitive in choosing between two models118,488,493.T h i si se s p e c i a l l y\\ntrue when the models are strong, as it is easier to move a rank correlation\\nfrom 0.6 to 0.7 than it is to move it from 0.9 to 1.0. Measures such as R2and\\nthe model χ2statistic (calculated fromt he predicted and observedresponses)\\nare more sensitive. Still, one may not know how to interpret the added utility\\nof a model that boosts the R2from 0.80 to 0.81.\\nAgain given that both models are equally well calibrated, discrimination\\ncan be studied more simply by examining the distribution of predicted values\\nˆY. Suppose that the predicted value is the probability that a subject dies.\\nThen high-resolution histograms of the predicted risk distributions for the\\ntwo models can be very revealing. If one model assigns 0.02 of the sample to\\na risk of dying above 0.9 while the other model assigns 0.08 of the sample to\\nthe high risk group, the second model is more discriminating. The worth of a\\nmodel can be judged by how far it goes out on a limb while still maintaining\\ngood calibration. 25\\nFrequently, one model will have a similar discrimination index to another\\nmodel,but thelikelihoodratio χ2statisticismeaningfullygreaterforone.As-\\nsuming correctionshave been made for complexity, the model with the higher\\nχ2usually has a better ﬁt for somesubjects, although not necessarily for the\\naverage subject. A crude plot of predictions from the ﬁrst model against\\npredictions from the second, possibly stratiﬁed by Y, can help describe the\\ndiﬀerences in the models. More speciﬁc analyses will determine the charac-\\nteristics of subjects where the diﬀerences are greatest. Large diﬀerences may\\nbe caused by an omitted, underweighted, or improperly transformed predic-\\ntor, among other reasons. In one example, two models for predicting hospital\\nmortality in critically ill patients had the same discrimination index (to two\\ndecimal places). Forthe relativelysmall subsetofpatients with extremelylow\\nwhite blood counts or serum albumin, the model that treated these factors\\nas continuous variables provided predictions that were very much diﬀerent\\nfrom a model that did not.\\nWhen comparing predictions for two models that may not be calibrated\\n(from overﬁtting, e.g.), the two sets of predictions may be shrunk so as to\\nnot give credit for overﬁtting (see Equation 4.3).\\nSometimes one wishes to compare two models that used the response vari-\\nable diﬀerently, a much more diﬃcult problem. For example, an investigator\\nmay want to choose between a survival model that used time as a continuous\\nvariable,and a binarylogisticmodelfor dead/aliveatsix months.Here,othe r\\nconsiderations are also important (see Section 17.1). A model that predicts\\ndead/alive at six months does not use the response variable eﬀectively, and\\nit provides no information on the chance of dying within three months.\\nWhen one or both of the models is ﬁtted using least squares, it is useful\\nto compare them using an error measure that was not used as the optimiza-\\ntion criterion, such as mean absolute error or median absolute error. Mean', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='719c0de6-2d5e-40aa-961a-55bc8e07a539', embedding=None, metadata={'page_label': '94', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='94 4 Multivariable Modeling Strategies\\nand median absolute errors are excellent measures for judging the value of a\\nmodel developed without transforming the response to a model ﬁtted after\\ntransforming Y, then back-transforming to get predictions. 26\\n4.11 Improving the Practice of Multivariable Prediction\\nStandards for published predictive modeling and feature selection in high-\\ndimensional problems are not very high. There are several things that a good\\nanalyst can do to improve the situation.\\n1. Insist on validation of predictive models and discoveries, using rigorous\\ninternal validation based on resampling or using external validation.\\n2. Show collaborators that split-sample validation is not appropriate unless\\nthe number of subjects is huge\\n•This can be demonstrated by spliting the data more than once and\\nseeing volatile results, and by calculating a conﬁdence interval for the\\npredictive accuracy in the test dataset and showing that it is very wide.\\n3. Run a simulation study with no real associations and show that asso-\\nciations are easy to ﬁnd if a dangerous data mining procedure is used.\\nAlternately, analyze the collaborator’s data after randomly permuting the\\nYvector and show some“positive”ﬁndings.\\n4. Show that alternative explanations are easy to posit. For example:\\n•The importance of a risk factor may disappear if 5“unimportant”risk\\nfactors are added back to the model\\n•Omitted main eﬀects can explain away apparent interactions.\\n•Perform a uniqueness analysis : attempt to predict the predicted val-\\nues from a model derived by data torture from all of the features not\\nused in the model. If one can obtain R2=0.85 in predicting the“win-\\nning”feature signature(predicted values) from the“losing”features, the\\n“winning”pattern is not unique and may be unreliable.\\n4.12 Summary: Possible Modeling Strategies\\nSome possible global modeling strategies are to\\n•Use a method known not to work well (e.g., stepwise variable selection\\nwithout penalization; recursivepartitioningresulting in a single tree), doc-\\nument how poorly the model performs (e.g. using the bootstrap), and use\\nthe model anyway\\n•Developablackboxmodelthatperformspoorlyandisdiﬃculttointerpret\\n(e.g., does not incorporate penalization)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b65fd16a-11cb-4357-935e-d4a248e465f1', embedding=None, metadata={'page_label': '95', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.12 Summary: Possible Modeling Strategies 95\\n•Develop a black box model that performs well and is diﬃcult to interpret\\n•Develop interpretable approximations to the black box\\n•Develop an interpretable model (e.g. give priority to additive eﬀects) that\\nperforms well and is likely to perform equally well on future data from the\\nsame stream.\\nAs stated in the Preface, the strategy emphasized in this text, stemming\\nfrom the last philosophy, is to decide how many degrees of freedom can be\\n“spent,” where they should be spent, and then to spend them. If statistical\\ntests or conﬁdence limits are required, later reconsideration of how d.f. are\\nspent is not usually recommended. In what follows some default strategies\\nare elaborated. These strategies are far from failsafe, but they should allow\\nthe reader to develop a strategy that is tailored to a particular problem. At\\nthe least these default strategies are concrete enough to be criticized so that\\nstatisticians can devise better ones.\\n4.12.1 Developing Predictive Models\\nThe following strategy is generic although it is aimed principally at the de-\\nvelopment of accurate predictive models.\\n1. Assemble as much accurate pertinent data as possible, with wide distri-\\nbutions for predictor values. For survival time data, follow-up must be\\nsuﬃcient to capture enough events as well as the clinically meaningful\\nphases if dealing with a chronic process.\\n2. Formulate good hypotheses that lead to speciﬁcation of relevant candi-\\ndate predictors and possible interactions. Don’t use Y(either informally\\nusing graphs, descriptive statistics, or tables, or formally using hypothe-\\nsis tests or estimates of eﬀects such as odds ratios) in devising the list of\\ncandidate predictors.\\n3. If there are missing Yvalues on a small fraction of the subjects but Y\\ncan be reliably substituted by a surrogate response, use the surrogate to\\nreplace the missing values. Characterize tendencies for Yto be missing\\nusing, for example, recursive partitioning or binary logistic regression.\\nDepending on the model used, even the information on Xfor observa-\\ntions with missing Ycan be used to improve precision of ˆβ, so multiple\\nimputation of Ycan sometimes be eﬀective. Otherwise, discard observa-\\ntions having missing Y.\\n4. Impute missing Xs if the fraction of observations with any missing Xsi s\\nnot tiny. Characterize observations that had to be discarded. Special im-\\nputation models may be needed ifa continuous Xneeds a non-monotonic\\ntransformation (p. 52). These models can simultaneously impute missing\\nvalueswhile determiningtransformations.In most cases,multiply impute\\nmissingXs based on other Xsa n dY, and other available information\\nabout the missing data mechanism.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ed46aff-3de1-465f-89bf-3c0afbb128d7', embedding=None, metadata={'page_label': '96', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='96 4 Multivariable Modeling Strategies\\n5. For each predictor specify the complexity or degree of nonlinearity that\\nshould be allowed (see Section 4.1). When prior knowledge does not in-\\ndicate that a predictor has a linear eﬀect on the property C(Y|X) (the\\nproperty of the response that canbe linearly related to X), specify the\\nnumber of degrees of freedom that should be devoted to the predictor.\\nThe d.f. (or number of knots) can be largerwhen the predictor is thought\\nto be more important in predicting Yor when the sample size is large.\\n6. If the number of terms ﬁtted or tested in the modeling process (counting\\nnonlinear and cross-product terms) is too large in comparison with the\\nnumber of outcomes in the sample, use data reduction (ignoring Y)u n t i l\\nthe number of remaining free variables needing regression coeﬃcients is\\ntolerable. Use the m/10 orm/15 rule or an estimate of likely shrinkage\\nor overﬁtting (Section 4.7) as a guide. Transformations determined from\\nthe previous step may be used to reduce each predictor into 1 d.f., or the\\ntransformed variables may be clustered into highly correlated groups if\\nmore data reduction is required. Alternatively, use penalized estimation\\nwith the entire set of variables. This will also eﬀectively reduce the total\\ndegrees of freedom.272\\n7. Use the entire sample in the model development as data are too precious\\ntowaste.Ifstepslistedbelowaretoodiﬃculttorepeatforeachbootstrap\\norcross-validationsample,holdouttestdatafrom allmodeldevelopment\\nsteps that follow.\\n8. When you can test for model complexity in a very structured way, you\\nmay be able to simplify the model without a great need to penalize the\\nﬁnal model for having made this initial look. For example, it can be\\nadvisable to test an entire group of variables (e.g., those more expensive\\nto collect) and to either delete or retain the entire group for further\\nmodeling, based on a single P-value (especially if the Pvalue is not\\nbetween 0.05 and 0.2). Another example of structured testing to simplify\\nthe “initial” model is making allcontinuous predictors have the same\\nnumber of knots k,v a r y i n g kfrom 0 (linear), 3 ,4,5,..., and choosing\\nthevalueof kthatoptimizesAIC.Acompositetestofallnonlineareﬀects\\nin a model can also be used, and statistical inferences are not invalidated\\nif the global test of nonlinearity yields P>0.2o rs oa n dt h ea n a l y s t\\ndeletes all nonlinear terms.\\n9. Make tests of linearity of eﬀects in the model only to demonstrate to\\nothers that such eﬀects are often statistically signiﬁcant. Don’t remove\\ninsigniﬁcant eﬀects from the model when tested separately by predictor.\\nAny examination of the response that might result in simplifying the\\nmodelneedsto be accountedforincomputingconﬁdencelimits andother\\nstatistics. It is preferable to retain the complexity that was prespeciﬁed\\nin Step5regardless of the results of assessments of nonlinearity.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='64178afe-6fe9-4f9c-b7be-ffe21b7b481c', embedding=None, metadata={'page_label': '97', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.12 Summary: Possible Modeling Strategies 97\\n10. Check additivity assumptions by testing prespeciﬁed interaction terms.\\nIf the global test for additivity is signiﬁcant or equivocal, all prespeciﬁed\\ninteractions should be retained in the model. If the test is decisive (e.g.,\\nP>0.3), all interaction terms can be omitted, and in all likelihood there\\nis no need to repeat this pooled test for each resample during model\\nvalidation.Inotherwords,onecanassumethathadtheglobalinteraction\\ntest been carried out for each bootstrap resample it would have been\\ninsigniﬁcant at the 0.05 level more than, say, 0.9 of the time. In this large\\nP-value case the pooled interaction test did not induce an uncertainty in\\nmodel selection that needed accounting.\\n11. Check to see if there are overly inﬂuential observations.\\n12. Check distributional assumptions and choose a diﬀerent model if needed.\\n13. Do limited backwards step-down variable selection if parsimony is more\\nimportant than accuracy.582The cost of doing any aggressive variable\\nselection is that the variable selection algorithm must also be included\\nin a resampling procedure to properly validate the model or to compute\\nconﬁdence limits and the like.\\n14. This is the“ﬁnal”model.\\n15. Interpret the model graphically(Section 5.1)and by examining predicted\\nvaluesandusingappropriatesigniﬁcancetests withouttryingtointerpret\\nsome of the individual model parameters. For collinear predictors obtain\\npooled tests of association so that competition among variables will not\\ngive misleading impressions of their total signiﬁcance.\\n16. Validate the ﬁnal model for calibration and discrimination ability, prefer-\\nablyusingbootstrapping(seeSection 5.3).Steps9to13mustberepeated\\nforeachbootstrapsample,atleastapproximately.Forexample,ifagewas\\ntransformed when building the ﬁnal model, and the transformation was\\nsuggested by the data using a ﬁt involving age and age2, each bootstrap\\nrepetition should include both age variables with a possible step-down\\nfrom the quadratic to the linear model based on automatic signiﬁcance\\ntesting at each step.\\n17. Shrink parameter estimates if there is overﬁtting but no further data\\nreductionisdesired,ifshrinkagewasnotbuiltintotheestimationprocess.\\n18. When missing values were imputed, adjust ﬁnal variance–covariancema-\\ntrix for imputation wherever possible (e.g., using bootstrap or multiple\\nimputation). This may aﬀect some of the other results.\\n19. When all steps of the modeling strategy can be automated, consider\\nusing Faraway’s method186to penalize for the randomness inherent in\\nthe multiple steps. 27\\n20. Develop simpliﬁcations to the full model by approximating it to any\\ndesired degrees of accuracy (Section 5.5).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='887d40d7-f095-4eea-b99d-72dcf4b16c12', embedding=None, metadata={'page_label': '98', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='98 4 Multivariable Modeling Strategies\\n4.12.2 Developing Models for Eﬀect Estimation\\nBy eﬀect estimation is meant point and interval estimation of diﬀerences in\\npropertiesoftheresponsesbetweentwoormoresettingsofsomepredictors,or\\nestimating some function of these diﬀerences such as the antilog. In ordinary\\nmultiple regression with no transformation of Ysuch diﬀerences are absolute\\nestimates. Inregressioninvolvinglog( Y) orinlogisticorproportionalhazards\\nmodels, eﬀect estimation is, at least initially, concerned with estimation of\\nrelative eﬀects. As discussed on pp. 4and224, estimation of absolute eﬀects\\nfor these models must involve accurate prediction of overall response values,\\nso the strategy in the previous section applies.\\nWhen estimating diﬀerences or relative eﬀects, the bias in the eﬀect es-\\ntimate, besides being inﬂuenced by the study design, is related to how well\\nsubject heterogeneity and confounding are taken into account. The variance\\nof the eﬀect estimate is related to the distribution of the variable whose levels\\nare being compared, and, in least squares estimates, to the amount of vari-\\nation “explained” by the entire set of predictors. Variance of the estimated\\ndiﬀerence can increase if there is overﬁtting. So for estimation, the previous\\nstrategy largely applies.\\nThe following are diﬀerences in the modeling strategy when eﬀect estima-\\ntion is the goal.\\n1. There is even less gain from having a parsimonious model than when de-\\nveloping overall predictive models, as estimation is usually done at the\\ntime of analysis. Leaving insigniﬁcant predictors in the model increases\\nthe likelihood that the conﬁdence interval for the eﬀect of interest has the\\nstated coverage. By contrast, overall predictions are conditional on the\\nvalues of all predictors in the model. The variance of such predictions is\\nincreased by the presence of unimportant varia bles, as predictions are still\\nconditional on the particular values of these variables (Section 5.5.1)a n d\\ncancellation of terms (which occurs when diﬀerences are of interest) does\\nnot occur.\\n2. Careful consideration of inclusion of interactions is still a major consid-\\neration for estimation. If a predictor whose eﬀects are of major interest\\nis allowed to interact with one or more other predictors, eﬀect estimates\\nmust be conditional on the values of the other predictors and hence have\\nhigher variance.\\n3. A major goal of imputation is to avoid lowering the sample size because\\nof missing values in adjustment variables. If the predictor of interest is the\\nonly variable having a substantial number of missing values, multiple im-\\nputation is less worthwhile, unless it corrects for a substantial bias caused\\nby deletion of nonrandomly missing data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0539810a-72b2-44e2-ae52-a6400b34a03d', embedding=None, metadata={'page_label': '99', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.12 Summary: Possible Modeling Strategies 99\\n4. The analyst need not be very concerned about conserving degrees of free-\\ndom devoted to the predictor of interest. The complexity allowed for this\\nvariable is usually determined by priorbeliefs, with compromises that con-\\nsider the bias-variance trade-oﬀ.\\n5. If penalized estimation is used, the analyst may wish to not shrink param-\\neter estimates for the predictor of interest.\\n6. Model validation is not necessary unless the analyst wishes to use it to\\nquantify the degree of overﬁtting.\\n4.12.3 Developing Models for Hypothesis Testing\\nA default strategy for developing a multivariable model that is to be used\\nas a basis for hypothesis testing is almost the same as the strategy used for\\nestimation.\\n1. There is little concern for parsimony. A full model ﬁt, including insigniﬁ-\\ncant variables, will result in more accurate P-values for tests for the vari-\\nables of interest.\\n2. Careful consideration of inclusion of interactions is still a major consid-\\neration for hypothesis testing. If one or more predictors interacts with a\\nvariable of interest, either separate hypothesis tests are carried out over\\nthe levels of the interacting factors, or a combined“main eﬀect + interac-\\ntion” test is performed. For example, a very well–deﬁned test is whether\\ntreatment is eﬀective for anyrace group.\\n3. If the predictor of interest is the only variable having a substantial number\\nof missing values, multiple imputation is less worthwhile. In some cases,\\nmultiple imputation may increase power (e.g., in ordinary multiple regres-\\nsion one can obtain larger degrees of freedom for error)but in others there\\nwill be little net gain. However, the test can be biased due to exclusion of\\nnonrandomly missing observations if imputation is not done.\\n4. Asbefore,theanalystneednotbev eryconcernedaboutconservingdegrees\\nof freedom devoted to the predictor of interest. The degrees of freedom\\nallowed for this variable is usually determined by prior beliefs, with careful\\nconsideration of the trade-oﬀ between bias and power.\\n5. If penalized estimation is used, the analyst should not shrink parameter\\nestimates for the predictors being tested.\\n6. Model validation is not necessary unless the analyst wishes to use it to\\nquantify the degree of overﬁtting. This may shed light on whether there is\\noveradjustment for confounders.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f0c06f6-0614-4b59-85f9-39ea8569035d', embedding=None, metadata={'page_label': '100', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='100 4 Multivariable Modeling Strategies\\n4.13 Further Reading\\n1Some good general references that address modeling strategies are [ 216,269,476,\\n590].\\n2Even though they used a generalized correlation index for screening variables\\nand not for transforming them, Hall and Miller249present a related idea, com-\\nputing the ordinary R2against a cubic spline transformation of each potential\\npredictor.\\n3Simulation studies are needed to determine the eﬀects of modifying the model\\nbased on assessments of “predictor promise.” Although it is unlikely that this\\nstrategy will result in regression coeﬃcients that are biased high in absolute\\nvalue, it may on some occasions result in somewhat optimistic standard errors\\nand a slight elevation in type I error probability. Some simulation results may\\nbe found on the Web site. Initial promising ﬁndings for least squares models\\nfor two uncorrelated predictors indicate that the procedure is conservative in\\nits estimation of σ2and in preserving type I error.\\n4Verweij and van Houwelingen640and Shao565describe how cross-validation can\\nbe used in formulating a stopping rule. Luo et al.430developed an approach to\\ntuning forward selection by adding noise to Y.\\n5Roecker528compared forward variable selection (FS) and all possible subsets\\nselection (APS) with full model ﬁts in ordinary least squares. APS had a greater\\ntendency to select smaller, less accurate models than FS. Neither selection tech-\\nniquewas as accurate as the full model ﬁt unlessmore than half of the candidate\\nvariables was redundant or unnecessary.\\n6Wiegand668showed that it is not very fruitful to try diﬀerent stepwise algo-\\nrithms and then to be comforted by agreements in some of the variables selected.\\nIt is easy for diﬀerent stepwise methods to agree on the wrong set of variables.\\n7Other resultson howvariable selection aﬀects inference may be foundin Hurvich\\nand Tsai316and Breiman [ 66, Section 8.1].\\n8Goring et al.227presented an interesting analysis of the huge bias caused by\\nconditioning analyses on statistical signiﬁcance in a high-dimensional genetics\\ncontext.\\n9Steyerberg et al.589have comparisons of smoothly penalized estimators with\\nthe lasso and with several stepwise variable selection algorithms.\\n10See Weiss,656Faraway,186and Chatﬁeld100for more discussions of the eﬀect of\\nnot prespecifying models, for example, dependence of point estimates of eﬀects\\non the variables used for adjustment.\\n11Greenland241provides an example in which overﬁtting a logistic model resulted\\nin far too many predictors with P<0.05.\\n12See Peduzzi et al.486,487for studies of the relationship between “events per\\nvariable”and types I and II error, accuracy of variance estimates, and accuracy\\nof normal approximations for regression coeﬃcient estimators. Their ﬁndings\\nare consistent with those given in the text (but644has a slightly diﬀerent take).\\nvan der Ploeg et al.629did extensive simulations to determine the events per\\nvariable ratio needed to avoid a drop-oﬀ (in an independenttest sample) in more\\nthan 0.01 in the c-index, for a variety of predictive methods. They concluded\\nthat support vector machines, neural networks, and random forests needed far\\nmore events per variable to achieve freedom from overﬁtting than does logistic\\nregression, and that recursive partitioning was not competitive. Logistic regres-\\nsion required between 20 and 50 events per variable to avoid overﬁtting. Diﬀer-\\nent results might have been obtained had the authors used a proper accuracy\\nscore.\\n13Copas [122, Eq.8.5] adds 2 to the numerator of Equation 4.3(see also [ 504,631]).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b497432e-d00d-478c-9145-263566c71ce5', embedding=None, metadata={'page_label': '101', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='4.13 Further Reading 101\\n14An excellent discussion about such indexes may be found in http://r.789695.\\nn4.nabble.com/Adjusted-R-squared-formula-in-lm-td4656857.html where\\nJ. Lucke points out that R2tends top\\nn−1when the population R2is zero,\\nbutR2\\nadjconverges to zero.\\n15Efron [173, Eq.4.23] and van Houwelingen and le Cessie633showed that the av-\\nerage expected optimism in a mean logarithmic quality score for a p-predictor\\nbinary logistic model is p/n. Taylor et al.600showed that the ratio of variances\\nfor certain quantities is proportional to the ratio of the number of parameters\\nin two models. Copas stated that“Shrinkage can be particularly marked when\\nstepwise ﬁtting is used: the shrinkage is then closer to that expected of the\\nfull regression rather than of the subset regression actually ﬁtted.”122,504,631\\nSpiegelhalter,582in arguing against variable selection, states that better predic-\\ntion will often be obtained by ﬁtting all candidate variables in the ﬁnal mod el,\\nshrinking the vector of regression coeﬃcient estimates towards zero.\\n16See Belsley [ 46, pp. 28–30] for some reservations about using VIF.\\n17Friedman and Wall208discuss and provide graphical devices for explaining sup-\\npression by a predictor not correlated with the response but that is correlated\\nwith another predictor. Adjusting for a suppressor variable will increase the\\npredictive discrimination of the model. Meinshausen453developed a novel hier-\\narchical approach to gauging the importance of collinear predictors.\\n18For incomplete principal component regression see [ 101,119,120,142,144,320,\\n325]. See396,686for sparse principal component analysis methods in which con-\\nstraints are applied to loadings so that some of them are set to zero. The latter\\nreference provides a principal component method for binary data. See246for\\na type of sparse principal component analysis that also encourages loadings\\nto be similar for a group of highly correlated variables and allows for a type\\nof variable clustering.See [ 390] for principal surfaces. Sliced inverse regression\\nis described in [ 104,119,120,189,403,404]. For material on variable cluster-\\ning see [ 142,144,268,441,539]. A good general reference on cluster analysis\\nis [634, Chapter 11]. de Leeuw and Mair in their Rhomalspackage [ 153]h a v e\\none of the most general approaches to data reduction related to optimal scaling.\\nTheir approach includes nonlinear principal component analysis among several\\nother multivariate analyses.\\n19The redundancy analysis described here is related to principal variables448but\\nis faster.\\n20Meinshausen453developed a method of testing the importance of competing\\n(collinear) variables using an interesting automatic clustering procedure.\\n21TheRClustOfVar package by Marie Chavent, Vanessa Kuentz, Benoit Liquet,\\nand Jerome Saracco generalizes variable clustering and explicitly handles a mix-\\nture of quantitative and categorical predictors. It also implements bootstrap\\ncluster stability analysis.\\n22Principal components are commonly used to summarize a cluster of variables.\\nVines643developed a method to constrain the principal component coeﬃcients\\nto be integers without much loss of explained variability.\\n23Jolliﬀe324presented a way to discard some of the variables making up principal\\ncomponents. Wang and Gehan649presented a new method for ﬁndingsubsetsof\\npredictors that approximate a set of principal components, and surveyed other\\nmethods for simplifying principal components.\\n24See D’Agostino et al.144for excellent examples of variable clustering (including\\natwo-stage approach) andotherdata reductiontechniquesusingboth statistical\\nmethods and subject-matter expertise.\\n25Cook118and Pencina et al.490,492,493present an approach for judging the\\nadded value of new variables that is based on evaluating the extent to which\\nthe new information moves predicted probabilities higher for subjects having\\nevents and lower for subjects not having events. But see292,592.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1f72da8b-6afd-48f9-8f48-8b87d0f2365f', embedding=None, metadata={'page_label': '102', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='102 4 Multivariable Modeling Strategies\\n26TheHmisc abs.error.pred function computes a variety of accuracy measures\\nbased on absolute errors.\\n27Shen et al.567developed an “optimal approximation” method to make correct\\ninferences after model selection.\\n4.14 Problems\\nAnalyze the SUPPORT dataset ( getHdata(support) ) as directed below to re-\\nlate selected variables to total cost of the hospitalization. Make sure this\\nresponse variableis utilized in a way that approximatelysatisﬁesthe assump-\\ntions of normality- based multiple regression so that statistical inferences will\\nbe accurate.See problems at the end of Chapters 3and7of the text for more\\ninformation. Consider as predictors mean arterial blood pressure, heart rate,\\nage, disease group, and coma score.\\n1. Do ananalysisto understandinterrelationshipsamongpredictors,andﬁnd\\noptimal scaling (transformations) that make the predictors better relate\\nto each other (e.g., optimize the variation explained by the ﬁrst principal\\ncomponent).\\n2. Do a redundancy analysis of the predictors, using both a less stringent and\\namorestringentapproachtoassessingtheredundancyofthemultiple-level\\nvariable disease group.\\n3. Do an analysis that helps one determine how many d.f. to devote to each\\npredictor.\\n4. Fit a model, assuming the above predictors act additively, but do not as-\\nsume linearity for the age and blood pressure eﬀects. Use the truncated\\npower basis for ﬁtting restricted cubic spline functions with 5 knots. Esti-\\nmate the shrinkage coeﬃcient ˆ γ.\\n5. Make appropriate graphical diagnostics for this model.\\n6. Testlinearityin age,linearityin bloodpressure,andlinearityin heartrate,\\nand also do a joint test of linearity simultaneously in all three predictors.\\n7. Expand the model to not assume additivity of age and blood pressure.\\nUse a tensor natural spline or an appropriate restricted tensor spline. If\\nyou run into any numerical diﬃculties, use 4 knots instead of 5. Plot in an\\ninterpretable fashion the estimated 3-D relationship between age, blood\\npressure, and cost for a ﬁxed disease group.\\n8. Test for additivity of age and blood pressure. Make a joint test for the\\noverall absence of complexity in the model (linearity and additivity simul-\\ntaneously).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='32fb5042-5649-4470-9ba7-15b9d273c327', embedding=None, metadata={'page_label': '103', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 5\\nDescribing, Resampling, Validating,\\nand Simplifying the Model\\n5.1 Describing the Fitted Model\\n5.1.1 Interpreting Eﬀects\\nBefore addressing issues related to describing and interpreting the model\\nand its coeﬃcients, one can never apply too much caution in attempting to\\ninterpret results in a causal manner. Regression models are excellent tools\\nfor estimating and inferring associations between an XandYgiven that the\\n“right”variables are in the model. Any ability of a model to provide causal\\ninference rests entirely on the faith of the analyst in the experimental design,\\ncompleteness of the set of variables that are thought to measure confounding\\nand are used for adjustment when the experiment is not randomized, lack of\\nimportant measurement error, and lastly the goodness of ﬁt of the model.\\nTheﬁrstlineofattackininterpretingtheresultsofamultivariableanalysis\\nis to interpret the model’s parameter estimates. For simple linear, additive\\nmodels, regression coeﬃcients may be readily interpreted. If there are in-\\nteractions or nonlinear terms in the model, however, simple interpretations\\nare usually impossible. Many programs ignore this problem, routinely print-\\ning such meaningless quantities as the eﬀect of increasing age2by one day\\nwhile holding age constant. A meaningful age change needs to be chosen, and\\nconnections between mathematically related variables must be taken into\\naccount. These problems can be solved by relying on predicted values and\\ndiﬀerences between predicted values.\\nEven when the model contains no nonlinear eﬀects, it is diﬃcult to com-\\npare regression coeﬃcients across predictors having varying scales. Some an-\\nalysts like to gauge the relative contributions of diﬀerent predictors on a\\ncommon scale by multiplying regression coeﬃcients by the standard devia-\\ntions of the predictors that pertain to them. This does not make sense for\\nnonnormally distributed predictors (and regression models should not need\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 5103', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9fc9ee67-3a79-427c-a1ac-3e8c0a27c2a9', embedding=None, metadata={'page_label': '104', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='104 5 Describing, Resampling, Validating, and Simplifying the Model\\nto make assumptions about the distributions of predictors). When a predic-\\ntor is binary (e.g., sex), the standard deviation makes no sense as a scaling\\nfactor as the scale would depend on the prevalence of the predictor.a1\\nIt is more sensible to estimate the change in YwhenXjis changed by\\nan amount that is subject-matter relevant. For binary predictors this is a\\nchange from 0 to 1. For many continuous predictors the interquartile range\\nis a reasonable default choice. If the 0.25 and 0.75 quantiles of Xjaregand\\nh, linearity holds, and the estimated coeﬃcient of Xjisb;b×(h−g)i st h e\\neﬀect of increasing Xjbyh−gunits, which is a span that contains half of\\nthe sample values of Xj.\\nFor the more general case of continuous predictors that are monotonically\\nbut not linearly related to Y, a useful point summary is the change in Xβ\\nwhen the variable changes from its 0.25 quantile to its 0.75 quantile. For\\nmodels for which exp( Xβ) is meaningful, antilogging the predicted change in\\nXβresults in quantities such as interquartile-range odds and hazards ratios.\\nWhen the variable is involved in interactions, these ratios are estimated sep-\\narately for various levels of the interacting factors. For categoricalpredictors,\\nordinary eﬀects are computed by comparing each level of the predictor with\\na reference level. See Section 10.10and Chapter 11for tabular and graphical\\nexamples of this approach. 2\\nThe model can be described using partial eﬀect plots by plotting each X\\nagainstXˆβholdingotherpredictorsconstant.Modiﬁedversionsofsuchplots,\\nby nonlinearly rank-transforming the predictor axis, can show the relative\\nimportance of a predictor336.\\nFor anXthat interacts with other factors, separate curves are drawn on\\nthe same graph, one for each level of the interacting factor.\\nNomograms40,254,339,427provide excellent graphical depictions of all the 3\\nvariables in the model, in addition to enabling the user to obtain predicted\\nvalues manually. Nomograms are especially good at helping the user envision\\ninteractions. See Section 10.10and Chapter 11for examples. 4\\n5.1.2 Indexes of Model Performance\\n5.1.2.1 Error Measures\\nCare must be taken in the choice of accuracy scores to be used in validation.\\nIndexes can be broken down into three main areas.\\nCentral tendency of prediction errors: These measures include mean abso-\\nlute diﬀerences, mean squared diﬀe rences, and logarithmic scores. An ab-\\nsolute measure is mean |Y−ˆY|. The mean squared error is a commonly\\nused and sensitive measure if there are no outliers. For the special case\\naThe s.d. of a binary variable is, aside from a multiplier ofn\\nn−1,e q u a lt o√\\na(1−a),\\nwhereais the proportion of ones.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95971376-18ae-4be8-9c8a-9b0f376b81ae', embedding=None, metadata={'page_label': '105', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.1 Describing the Fitted Model 105\\nwhereYis binary, such a measure is the Brier score, which is a quadratic\\nproper scoring rule that combines calibration and discriminationb.T h e\\nlogarithmic proper scoring rules (related to average log-likelihood) is even\\nmore sensitive but can be harder to interpret and can be destroyed by a\\nsingle predicted probability of 0 or 1 that was incorrect.\\nDiscrimination measures: A measure of pure discrimination is a rank corre-\\nlation of ˆYandY, including Spearman’s ρ, Kendall’s τ, and Somers’ Dxy.\\nWhenYis binary, Dxy=2×(c−1\\n2)w h e r e cis the concordance prob-\\nability or area under the receiver operating characteristic curve, a linear\\ntranslation of the Wilcoxon-Mann-Whitney statistic. R2ismostlyam e a -\\nsure of discrimination, and R2\\nadjis is a good overﬁtting-correctedmeasure,\\nif the model is pre-speciﬁed. See Section 10.8for more information about\\nrank-based measures.\\nDiscrimination measures based on variation in ˆY: Theseincludetheregres-\\nsion sum of squares and the g–Index (see below).\\nCalibration measures: These assess absolute prediction accuracy.\\nCalibration–in–the–large compares the average ˆYwith the average Y.\\nAhigh-resolution calibration curve orcalibration–in–the–small assessesthe\\nabsolute forecast accuracy of predictions at individual levels of ˆY.W h e n\\nthe calibration curve is linear, this can be summarized by the calibration\\nslope andintercept.A moregeneralapproachuses the loessnonparametric\\nsmoother to estimate the calibration curve37. For any shape of calibration\\ncurve, errors can be summarized by quantities such as the maximum ab-\\nsolute calibration error, mean absolute calibration error, and 0.9 quantile\\nof calibration error.\\nTheg-index is a new measure of a model’s predictive discrimination based\\nonly on Xˆβ=ˆYthat applies quite generally. It is based on Gini’s mean\\ndiﬀerence for a variable Z, which is the mean over all possible i̸=jof|Zi−\\nZj|.T h eg-index is an interpretable, robust, and highly eﬃcient measure of\\nvariation.Forexample,whenpredictingsystolicbloodpressure, g= 11mmHg\\nrepresents a typical diﬀerence in ˆY.gis independent of censoring and other\\ncomplexities. For models in which the anti-log of a diﬀerence in ˆYrepresents\\nmeaningful ratios (e.g., odds ratios, hazard ratios, ratio of medians), grcan\\nbe deﬁned as exp( g). For models in which ˆYcan be turned into a probability 5\\nestimate (e.g., logistic regression), gpis deﬁned as Gini’s mean diﬀerence of\\nˆP.T h e s e g–indexes represent e.g. “typical” odds ratios, and “typical” risk\\ndiﬀerences. Partial gindexes can also be deﬁned. More details may be found\\nin the documentation for the Rrmspackage’s gIndexfunction.\\nbThere are decompositions of the Brier score into discrimination and calibration\\ncomponents.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b5a1e47-153d-4582-a7d4-de04775d06bb', embedding=None, metadata={'page_label': '106', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='106 5 Describing, Resampling, Validating, and Simplifying the Model\\n5.2 The Bootstrap\\nWhen one assumes that a random variable Yhas a certain population dis-\\ntribution, one can use simulation or analytic derivations to study how a sta-\\ntistical estimator computed from samples from this distribution behaves. For\\nexample, when Yhas a log-normal distribution, the variance of the sample\\nmedian for a sample of size nfrom that distribution can be derived analyt-\\nically. Alternatively, one can simulate 500 samples of size nfrom the log-\\nnormal distribution, compute the sample median for each sample, and then\\ncompute the sample variance of the 500 sample medians. Either case requires\\nknowledge of the population distribution function.\\nEfron’sbootstrap150,177,178is a general-purposetechnique for obtaining es-\\ntimatesofthepropertiesofstatisticalestimatorswithoutmakingassumptions\\nabout thedistributiongivingrisetothe data.Supposethatarandomvariable\\nYcomes from a cumulative distribution function F(y)=P r o b {Y≤y}and\\nthatwehaveasampleofsize nfromthisunknowndistribution, Y1,Y2,...,Y n.\\nThe basic idea is to repeatedly simulate a sample of size nfromF, computing\\nthe statistic of interest, and assessing how the statistic behaves over Brep-\\netitions. Not having Fat our disposal, we can estimate Fby the empirical\\ncumulative distribution function\\nFn(y)=1\\nnn∑\\ni=1[Yi≤y]. (5.1)\\nFncorresponds to a density function that places probability 1 /nat each\\nobserved datapoint ( k/nif that point were duplicated ktimes and its value\\nlisted only once).\\nAs an example, consider a random sample of size n= 30 from a normal\\ndistribution with mean 100 and standard deviation 10. Figure 5.1shows the\\npopulation and empirical cumulative distribution functions.\\nNow pretend that Fn(y) is the original population distribution F(y). Sam-\\npling from Fnis equivalent to sampling with replacement from the observed\\ndataY1,...,Y n. For large n, the expected fraction of original datapoints that\\nare selected for each bootstrap sample is 1 −e−1=0.632. Some points are\\nselected twice, some three times, a few four times, and so on. We take Bsam-\\nples of size nwith replacement, with Bchosen so that the summary measure\\nof the individual statistics is nearly as good as taking B=∞.T h eb o o t s t r a p 6\\nis basedon the fact that the distributionofthe observed diﬀerencesbetween a\\nresampled estimate of a parameter of interest and the originalestimate of the\\nparameter from the whole sample tells us about the distribution of unobserv-\\nablediﬀerences between the original estimate and the unknown population\\nvalue of the parameter.\\nAsanexample,considerthedata(1 ,5,6,7,8,9)andsupposethatwewould\\nlike to obtain a 0.80 conﬁdence interval for the population median, as well as\\nan estimate of the population expected value ofthe samplemedian (the latter', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4f6a9b60-e8c5-4549-9565-8f30c209aeda', embedding=None, metadata={'page_label': '107', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.2 The Bootstrap 107\\n60 80 100 120 1400.00.20.40.60.81.0\\nxProb[X ≤x]\\nFig. 5.1 Empirical and population cumulative distribution function\\nis only used to estimate bias in the sample median). The ﬁrst 20 bootstrap\\nsamples (after sorting data values) and the corresponding sample medians\\nare shown in Table 5.1.\\nFor a given number Bof bootstrap samples, our estimates are simply\\nthe sample 0.1 and 0.9 quantiles of the sample medians, and the mean of\\nthe sample medians. Not knowing how large Bshould be, we could let B\\nrange from, say, 50 to 1000, stopping when we are sure the estimates have\\nconverged. In the left plot of Figure 5.2,Bvaries from 1 to 400 for the mean\\n(10 to 400 for the quantiles). It can be seen that the bootstrap estimate of the\\npopulation mean of the sample median can be estimated satisfactorily when\\nB>50. For the lower and upper limits of the 0.8 conﬁdence interval for the\\npopulation median Y,Bmust be at least 200. For more extreme conﬁdence\\nlimits,Bmust be higher still.\\nFor the ﬁnal set of 400 sample medians, a histogram (right plot in Fig-\\nure5.2) can be used to assess the form of the sampling distribution of the\\nsample median. Here, the distribution is almost normal, although there is a\\nslightlyheavylefttailthatcomesfromthedatathemselveshavingaheavyleft\\ntail. For large samples, sample medians are normally distributed for a wide\\nvariety of population distributions. Therefore we could use bootstrapping to\\nestimate the variance of the sample median and then take ±1.28 standard\\nerrors as a 0.80 conﬁdence interval. In other cases (e.g., regression coeﬃcient\\nestimates for certain models), estimates are asymmetrically distributed, and\\nthe bootstrap quantiles are better estimates than conﬁdence intervals that\\nare based on a normality assumption. Note that because sample quantiles\\nare more or less restricted to equal one of the values in the sample, the boot-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f89fd99-e17d-4c46-be49-3dd6ce16f435', embedding=None, metadata={'page_label': '108', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='108 5 Describing, Resampling, Validating, and Simplifying the Model\\n0 100 200 300 4003456789\\nBootstrap Samples UsedMean and 0.1, 0.9 Quantiles\\nFrequency\\n24680204060\\nFig. 5.2 Estimating properties of sample median using the bootstrap\\nTable 5.1 First 20 bootstrap samples\\nBootstrap Sample Sample Median\\n166789 6 . 5\\n155568 5 . 0\\n578999 8 . 5\\n777889 7 . 5\\n157799 7 . 0\\n156678 6 . 0\\n788888 8 . 0\\n555799 6 . 0\\n155779 6 . 0\\n155778 6 . 0\\n115577 5 . 0\\n115578 5 . 0\\n155778 6 . 0\\n156788 6 . 5\\n156799 6 . 5\\n667789 7 . 0\\n157889 7 . 5\\n668999 8 . 5\\n115569 5 . 0\\n168999 8 . 5', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b0119d45-6bf8-4294-a54c-e7d63ab75ebb', embedding=None, metadata={'page_label': '109', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Model Validation 109\\nstrap distribution is discrete and can be dependent on a small number of\\noutliers. For this reason, bootstrapping quantiles does not work particularly\\nwell for small samples [ 150, pp. 41–43].\\nThe method just presented for obtaining a nonparametric conﬁdence in-\\nterval for the population median is called the bootstrap percentile method .I t\\nis the simplest but not necessarily the best performing bootstrap method. 7\\nIn this text we use the bootstrap primarily for computing statistical esti-\\nmates that are much diﬀerent from standard errors and conﬁdence intervals,\\nnamely, estimates of model performance.\\n5.3 Model Validation\\n5.3.1 Introduction\\nThe surest method to have a model ﬁt the data at hand is to discard much\\nof the data. A p-variable ﬁt to p+1 observations will perfectly predict Yas\\nlong as no two observations have the same Y. Such a model will, however,\\nyield predictions that appear almost random with respect to responses on\\na diﬀerent dataset. Therefore, unbiased estimates of predictive accuracy are\\nessential.\\nModel validation is done to ascertain whether predicted values from the\\nmodel are likely to accurately predict responses on future subjects or sub-\\njects not used to develop our model. Three major causes of failure of the 8\\nmodel to validate are overﬁtting, changes in measurement methods/changes\\nin deﬁnition of categorical variables, and major changes in subject inclusion\\ncriteria.\\nTherearetwomajormodesofmodelvalidation, external andinternal.The\\nmost stringent external validation involves testing a ﬁnal model developed in\\none country or setting on subjects in another country or setting at another\\ntime. This validation would test whether the data collection instrument was\\ntranslated into another language properly, whether cultural diﬀerences make\\nearlier ﬁndings nonapplicable, and whether secular trends have changed as-\\nsociations or base rates. Testing a ﬁnished model on new subjects from the 9\\nsame geographic area but from a diﬀerent institution as subjects used to ﬁt\\nthe model is a less stringent form of external validation. The least stringent\\nform of external validation involves using the ﬁrst mofnobservations for\\nmodel training and using the remaining n−mobservations as a test sample.\\nThis is very similar to data-splitting (Section 5.3.3). For details about meth-\\nods for external validation see the Rval.prob andval.surv functions in the\\nrmspackage.\\nEven though external validation is frequently favored by non-statisticians,\\nit is often problematic. Holding back data from the model-ﬁtting phase re-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7af3653e-e494-4149-9308-b5d93a2a7508', embedding=None, metadata={'page_label': '110', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='110 5 Describing, Resampling, Validating, and Simplifying the Model\\nsults in lower precision and power, and one can increase precision and learn\\nmore about geographic or time diﬀerences by ﬁtting a uniﬁed model to the\\nentire subject series including, for example, country or calendar time as a\\nmain eﬀect and/or as an interacting eﬀect. Indeed one could use the follow-\\ning working deﬁnition of external validation: validation of a prediction to ol\\nusing data that were not available when the tool needed to be completed. An\\nalternate deﬁnition could be taken as the validation of a prediction tool by\\nan independent research team.\\nOne suggested hierarchy of the quality of various validation methods is as\\nfollows, ordered from worst to best.\\n1. Attempting several validations (internal or external) and reporting only\\nthe one that“worked”\\n2. Reporting apparent performance on the training dataset (no validation)\\n3. Reporting predictive accuracy on an undersized independent test sample\\n4. Internal validation using data-splitting where at least one of the training\\nand test samples is not huge and the investigator is not aware of the\\narbitrariness of variable selection done on a single sample\\n5. Strong internal validation using 100 repeats of 10-fold cross-validation or\\nseveralhundred bootstrapresamples,repeating allanalysissteps involving\\nYafresh at each re-sample and the arbitrariness of selected “important\\nvariables”is reported (if variable selection is used)\\n6. External validation on a large test sample, done by the original research\\nteam\\n7. Re-analysis by an independent research team using strong internal valida-\\ntion of the original dataset\\n8. External validation using new test data, done by an independent research\\nteam\\n9. External validation using new test data generated using diﬀerent instru-\\nments/technology, done by an independent research team\\nInternal validation involves ﬁtting and validating the model by carefu lly\\nusing one series of subjects. One uses the combined dataset in this way to\\nestimate the likely performance of the ﬁnal model on new subjects, which\\nafter all is often of most interest. Most of the remainder of Section 5.3deals\\nwith internal validation.\\n5.3.2 Which Quantities Should Be Used\\nin Validation?\\nFor ordinary multiple regression models, the R2index is a good measure\\nof the model’s predictive ability, especially for the purpose of quantifying\\ndrop-oﬀ in predictive ability when applying the model to other datasets.\\nR2is biased, however. For example, if one used nine predictors to predict\\noutcomes of 10 subjects, R2=1.0 but the R2that will be achieved on future', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1cf59f2b-2edb-49c8-9ec5-f7cf04acbdf2', embedding=None, metadata={'page_label': '111', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Model Validation 111\\nsubjects will be close to zero. In this case, dramatic overﬁtting has occurred.\\nTheadjusted R2(Equation 4.4) solves this problem, at least when the model\\nhas been completely prespeciﬁed and no variables or parameters have been\\n“screened”out of the ﬁnal model ﬁt. That is, R2\\nadjis only valid when pin its\\nformula is honest— when it includes all parameters ever examined (formally\\nor informally, e.g., using graphs or tables) whether these parameters are in\\nthe ﬁnal model or not.\\nQuite often we need to validate indexes other than R2for which adjust-\\nments for phave not been created.cWe also need to validate models contain-\\ning “phantom degrees of freedom” that were screened out earlier, formally\\nor informally. For these purposes, we obtain nearly unbiased estimates of R2\\nor other indexes using data splitting, cross-validation, or the bootstrap. The\\nbootstrap provides the most precise estimates.\\nTheg–index is another discrimination measure to validate. But gandR2\\nmeasures only one aspect of predictive ability. In general,there are two major\\naspects of predictive accuracy that need to be assessed. As discussed in Sec-\\ntion4.5,calibration orreliability is the ability of the model to make unbiased\\nestimates of outcome. Discrimination is the model’s ability to separate sub-\\njects’ outcomes. Validation of the model is recommended even when a data\\nreduction technique is used. This is a way to ensure that the model was not\\noverﬁtted or is otherwise inaccurate.\\n5.3.3 Data-Splitting\\nThe simplest validation method is one-time data-splitting .H e r ead a t a s e ti s\\nsplit into training (model development) and test(model validation) samples\\nby a random process with or without balancing distributions of the response\\nand predictor variables in the two samples. In some cases, a chronological\\nsplit is used so that the validation is prospective. The model’s calibration\\nand discrimination are validated in the test set.\\nIn ordinary least squares, calibration may be assessed by, for example,\\nplotting Yagainst ˆY. Discrimination here is assessed by R2and it is of\\ninterest in comparing R2in the training sample with that achieved in the\\ntest sample. A drop in R2indicates overﬁtting, and the absolute R2in the\\ntest sample is an unbiased estimate of predictive discrimination. Note that\\nin extremely overﬁtted models, R2in the test set can be negative, since it is\\ncomputed on“frozen”intercept and regression coeﬃcients using the formula\\n1−SSE/SST ,w h e r eSSEis the error sum of squares, SSTis the total sum\\ncFor example, in the binary logistic model, there is a generalization of R2available,\\nbut no adjusted version. For logistic models we often validate other indexes such\\nas the ROC area or rank correlation between predicted probabilities and observed\\noutcomes. We also validate the calibration accuracy of ˆYin predicting Y.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='45f5515a-be7e-48f4-860b-5f8e043ea080', embedding=None, metadata={'page_label': '112', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='112 5 Describing, Resampling, Validating, and Simplifying the Model\\nof squares, and SSEcan be greater than SST(when predictions are worse\\nthan the constant predictor Y). 10\\nTo be able to validate predictions from the model over an entire test sam-\\nple (without validating it separately i n particular subsets such as in males\\nand females), the test sample must be large enough to precisely ﬁt a model\\ncontaining one predictor. For a study with a continuous uncensored response\\nvariable, the test sample size should ordinarily be ≥100 at a bare minimum.\\nFor survival time studies, the test sample should at least be large enough\\nto contain a minimum of 100 outcome events. For binary outcomes, the test\\nsample should contain a bare minimum of 100 subjects in the least frequent\\noutcome category.Once the size of the test sample is determined, the remain-\\ning portion ofthe originalsample can be used asa trainingsample. Evenwith\\nthese test sample sizes, validation of extreme predictions is diﬃcult.\\nData-splitting has the advantage of allowing hypothesis tests to be con-\\nﬁrmed in the test sample. However, it has the following disadvantages.\\n1. Data-splitting greatly reduces the sample size for both model development\\nand model testing. Becauseof this, Roecker528foundthis method“appears\\nto be a costly approach, both in terms of predictive accuracy of the ﬁtted\\nmodel and the precision of our estimate of the accuracy.” Breiman [ 66,\\nSection 1.3] found that bootstrap validation on the original sample was as\\neﬃcient as having a separate test sample twice as large36.\\n2. It requires a larger sample to be held out than cross-validation (see be-\\nlow) to be able to obtain the same precision of the estimate of predictive\\naccuracy.\\n3. The split may be fortuitous; if the process were repeated with a diﬀerent\\nsplit, diﬀerent assessments of predictive accuracy may be obtained.\\n4. Data-splitting does not validate the ﬁnal model, but rather a model devel-\\nopedononlyasubsetofthedata.Thetrainingandtestsetsarerecombined\\nfor ﬁtting the ﬁnal model, which is not validated.\\n5. Data-splitting requires the split before the ﬁrstanalysis of the data. With\\nother methods, analyses can proceed in the usual way on the complete\\ndataset. Then, after a “ﬁnal” model is speciﬁed, the modeling process is\\nrerun on multiple resamples from the original data to mimic the process\\nthat produced the“ﬁnal”model.\\n5.3.4 Improvements on Data-Splitting: Resampling\\nBootstrapping, jackkniﬁng, and other resampling plans can be used to obtain\\nnearly unbiased estimates of model performance without sacriﬁcing sample\\nsize. These methods work when either the model is completely speciﬁed ex-\\ncept for the regression coeﬃcients, or all important steps of the modeling\\nprocess, especially variable selection, are auto mated. Only then can each', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='091c8e78-cc2c-4c26-b11a-2fbe4c5a4c02', embedding=None, metadata={'page_label': '113', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Model Validation 113\\nbootstrap replication be a reﬂection of all sources of variability in model-\\ning. Note that most analyses involve examination of graphs and testing for\\nlack of model ﬁt, with many intermediate decisions by the analyst such as\\nsimpliﬁcation of interactions. These processes are diﬃcult to automate. But\\nvariable selection alone is often the greatest source of variability because of\\nmultiple comparison problems, so the analyst must go to great lengths to\\nbootstrap or jackknife variable selection.\\nThe ability to study the arbitrariness of how a stepwise variable selection\\nalgorithm selects“important”factors is a major beneﬁt of bootstrapping. A\\nuseful display is a matrix of blanks and asterisks, where an asterisk is placed\\nin column xof rowiif variable xis selected in bootstrap sample i(see p.\\n263for an example). If many variables appear to be selected at random,\\nthe analyst may want to turn to a data reduction method rather than using\\nstepwise selection (see also [541]).\\nCross-validationisageneralizationofdata-splittingthatsolvessomeofthe\\nproblems of data-splitting. Leave-out-one cross-validation ,565,633the limit of\\ncross-validation, is similar to jackkniﬁng.675Here one observation is omitted\\nfrom the analytical process and the response for that observationis predicted\\nusing a model derived from the remaining n−1 observations. The process\\nis repeated ntimes to obtain an average accuracy. Efron172reports that\\ngrouped cross-validation is more accurate; here groups of kobservations are\\nomitted at a time. Suppose, for example, that 10 groups are used. The orig-\\ninal dataset is divided into 10 equal subsets at random. The ﬁrst 9 subsets\\nare used to develop a model (transformation selection, interaction testing,\\nstepwise variable selection, etc. are all done). The resulting model is assessed\\nfor accuracy on the remaining 1 /10th of the sample. This process is repeated\\nat least 10 times to get an average of 10 indexes such as R2. 11\\nA drawback of cross-validation is the choice of the number of observations\\nto hold out from each ﬁt. Another is that the number of repetitions needed to\\nachieve accurate estimates of accuracy often exceeds 200. For example, one\\nmay have to omit1\\n10th of the sample 500 times to accurately estimate the\\nindex of interest Thus the sample would need to be split into tenths 50 times. 12\\nAnother possible problem is that cross-validation may not fully represent the\\nvariability of variable selection. If 20 subjects are omitted each time from a\\nsample of size 1000, the lists of variables selected from each training sample\\nof size 980 are likely to be much more similar than lists obtained from ﬁtting\\nindependent samples of 1000 subjects. Finally, as with data-splitting, cross-\\nvalidation does not validate the full 1000-subject model.\\nAn interesting way to study overﬁtting could be called the randomization\\nmethod. Here we ask the question“How well can the response be predicted\\nwhen we use our best procedure on random responses when the predictive\\naccuracy should be near zero?”The better the ﬁt on random Y, the worsethe\\noverﬁtting. The method takes a random permutation of the response variable\\nand develops a model with optional vari ableselectionbasedon the original X\\nand permuted Y. Suppose this yields R2=.2 for the ﬁtted sample. Apply the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e36e6f18-663a-41a0-a637-d24c9c9476f2', embedding=None, metadata={'page_label': '114', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='114 5 Describing, Resampling, Validating, and Simplifying the Model\\nﬁt to the original data to estimate optimism. If overﬁtting is not a problem,\\nR2would be the same for both ﬁts and it will ordinarily be very near zero. 13\\n5.3.5 Validation Using the Bootstrap\\nEfron,172,173Efron and Gong,175Gong,224Efron and Tibshirani,177,178Lin-\\nnet,416and Breiman66describe several bootstrapping procedures for obtain-\\ning nearly unbiased estimates of future model performance without holding\\nback data when making the ﬁnal estimates of model parameters. With the\\n“simple bootstrap”[ 178, p. 247], one repeatedly ﬁts the model in a bootstrap\\nsample and evaluates the performance of the model on the original sample.\\nThe estimate of the likely performance of the ﬁnal model on future data\\nis estimated by the average of all of the indexes computed on the original\\nsample.\\nEfron showed that an enhanced bootstrap estimates future model per-\\nformance more accurately than the simple bootstrap. Instead of estimating\\nan accuracy index directly from averaging indexes computed on the original\\nsample, the enhanced bootstrap uses a slightly more indirect approach by\\nestimating the bias due to overﬁtting or the “optimism” in the ﬁnal model\\nﬁt. After the optimism is estimated, it can be subtracted from the index\\nof accuracy derived from the original sample to obtain a bias-corrected or\\noverﬁtting-corrected estimate of predictive accuracy. The bootstrap method\\nis as follows. From the original XandYin the sample of size n,d r a wa\\nsample with replacement also of size n.D e r i v eam o d e li nt h eb o o t s t r a ps a m -\\nple and apply it without change to the original sample. The accuracy index\\nfrom the bootstrap sample minus the index computed on the original sampl e\\nis an estimate of optimism. This process is repeated for 100 or so bootstrap\\nreplicationsto obtainanaverageoptimism,which is subtractedfromthe ﬁnal\\nmodel ﬁt’s apparent accuracy to obtain the overﬁtting-corrected estimate. 14\\nNote that bootstrappingvalidates the processthat was used to ﬁt the orig-\\ninal model (as does cross-validation). It provides an estimate of the expected\\nvalueof the optimism, which when subtracted from the original index, pro-\\nvides an estimate of the expected bias-corrected index. If stepwise variable 15\\nselection is part of the bootstrap process (as it must be if the ﬁnal model\\nis developed that way), and not all resamples (samples with replacement or\\ntraining samples in cross-validation) resulted in the same model (which is\\nalmost always the case), this internal validation process actually provides an\\nunbiased estimate of the future performance of the processused to identify\\nmarkers and scoring systems; it does not validate a single ﬁnal model. But\\nresampling does tend to provide good estimates of the future performance of\\nthe ﬁnal model that was selected using the same procedure repeated in the\\nresamples.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb5471bc-f031-41a6-9d82-77274c05975a', embedding=None, metadata={'page_label': '115', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.3 Model Validation 115\\nNote that by drawing samples from XandY, we are estimating aspects\\nof theunconditional distribution of statistical quantities. One could instead\\ndraw samples from quantities such as residuals from the model to obtain a\\ndistribution that is conditional on X. However, this approach requires that\\nthe model be speciﬁed correctly, whereas the unconditional bootstrap does\\nnot. Also, the unconditional estimators are similar to conditional estimators\\nexcept for very skewed or very small samples [ 186, p. 217].\\nBootstrappingcanbeusedtoestimatetheoptimisminvirtuallyanyindex.\\nBesidesdiscriminationindexessuchas R2,slopeandinterceptcalibrationfac-\\ntorscanbeestimated.Whenoneﬁtsthemodel C(Y|X)=Xβ,andthenreﬁts\\nthe model C(Y|X)=γ0+γ1Xˆβon the same data, where ˆβis an estimate of\\nβ,ˆγ0and ˆγ1will necessarilybe 0 and 1, respectively.However,when ˆβis used\\nto predict responses on another dataset, ˆ γ1may be<1 if there is overﬁtting,\\nand ˆγ0will be diﬀerent from zero to compensate. Thus a bootstrap estimate\\nofγ1will not only quantify overﬁtting nicely, but can also be used to shrink\\npredicted values to make them more calibrated (similar to [582]). Efron’s op-\\ntimism bootstrap is used to estimate the optimism in (0 ,1) and then ( γ0,γ1)\\nare estimated by subtracting the optimism in the constant estimator (0 ,1).\\nNote that in cross-validationone estimates βwithˆβfrom the training sample\\nand ﬁtsC(Y|X)=γXˆβon the test sample directly. Then the γestimates are\\naveragedover all test samples. This approach does not require the choice of a 16\\nparameter that determines the amount of shrinkage as does ridge regression\\nor penalized maximum likelihood estimation; instead one estimates how to\\nmake the initial ﬁt well calibrated.123,633However, this approach is not as\\nreliable as building shrinkage into the original estimation process. The latter\\nallows diﬀerent parameters to be shrunk by diﬀerent factors.\\nOrdinary bootstrapping can sometimes yield overly optimistic estimates\\nof optimism, that is, may underestimate the amount of overﬁtting. This is\\nespecially true when the ratio of the number of observations to the number\\nof parameters estimated is not large.205A variation on the bootstrap that\\nimproves precision of the assessment is the“ .632”method, which Efron found\\nto be optimal in several examples.172This method provides a bias-corrected\\nestimate of predictive accuracy by substituting 0 .632×[apparent accuracy\\n−ˆǫ0]fortheestimateofoptimism,whereˆ ǫ0isaweightedaverageofaccuracies\\ne v a l u a t e do no b s e r v a t i o n s omitted from bootstrap samples [ 178, Eq.17.25,\\np. 253]. 17\\nFor ordinary least squares, where the genuine per-observation.632 estima-\\ntor can be used, several simulations revealed close agreement with the mod-\\niﬁed .632 estimator, even in small, highly overﬁtted samples. In these over-\\nﬁtted cases, the ordinary bootstrap bias-corrected accuracy estimates were\\nsigniﬁcantly higher than the .632 estimates. Simulations259,591have shown,\\nhowever, that for most types of indexes of accuracy of binary logistic regres-\\nsion models, Efron’s original bootstrap has lower mean squared error than\\nthe .632 bootstrap when n= 200,p= 30. Bootstrap overﬁtting-corrected es- 18\\ntimates of model performance can be biased in favor of the model. Although', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2d9babb3-48dc-4aef-94a8-c98b54aa372f', embedding=None, metadata={'page_label': '116', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='116 5 Describing, Resampling, Validating, and Simplifying the Model\\nTable 5.2 Example validation with and without variable selection\\nMethod Apparent Rank Over- Bias-Corrected\\nCorrelation of Optimism Correlation\\nPredicted vs.\\nObserved\\nFull Model 0.50 0.06 0.44\\nStepwise Model 0.47 0.05 0.42\\ncross-validationis less biased than the bootstrap, Efron172showed that it has\\nmuch higher variance in estimating overﬁtting-corrected predictive accuracy\\nthan bootstrapping. In other words, cross-validation, like data-splitting, can\\nyield signiﬁcantly diﬀerent estimates when the entire validation process is\\nrepeated.\\nIt is frequently very informative to estimate a measure of predictive accu-\\nracy forcing all candidate factors into the ﬁt and then to separately estimate\\naccuracy allowing stepwise variable selection, possibly with diﬀerent stop-\\nping rules. Consistent with Spiegelhalter’s proposal to use all factors and\\nthen to shrink the coeﬃcients to adjust for overﬁtting,582the full model ﬁt\\nwill outperform the stepwise model more often than not. Even though step-\\nwise modeling has slightly less optimism in predictive discrimination, this\\nimprovement is not enough to oﬀset the loss of information from deleting\\neven marginally important variables. Table 5.2shows a typical scenario. In\\nthis example, stepwise modeling lost a possible 0 .50−0.4 7=0.03 predictive\\ndiscrimination. The full model ﬁt will especially be an improvement when\\n1. the stepwise selection deletes several variab les that are almost signiﬁcant;\\n2. these marginalvariables have somereal predictive value, even if it’s slight;\\nand\\n3. there is no small set of extremely dominant variables that would be easily\\nfound by stepwise selection.\\n19\\nFaraway186has a fascinating study showing how resampling methods can\\nbe used to estimate the distributions of predicted values and of eﬀects of a\\npredictor,adjustingforanautomatedmultistepmodelingprocess.Bootstrap-\\nping can be used, for example, to penalize the variance in predicted values for\\nchoosing a transformation for Yand for outlier and inﬂuential observation\\ndeletion, in additionto variableselection. Estimationofthe transformationof\\nYgreatly increased the variance in Faraway’s examples. Brownstone [ 77,p .\\n74] states that“In spite of considerable eﬀorts, theoretical statisticians have\\nbeen unable to analyze the sampling properties of [usual multistep modeling\\nstrategies] under realistic conditions”and concludes that the modeling strat-\\negy must be completely speciﬁed and then bootstrapped to get consistent\\nestimates of variances and other sampling properties. 20', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86d3cce0-6dfd-4891-b64e-3c10d9bb894e', embedding=None, metadata={'page_label': '117', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"5.4 Bootstrapping Ranks of Predictors 117\\n5.4 Bootstrapping Ranks of Predictors\\nWhen the order of importance of predictors is not pre-speciﬁed but the re-\\nsearcher attempts to determine that order by assessing multiple associations\\nwithY, the process of selecting “winners” and “losers” is unreliable. The\\nbootstrap can be used to demonstrate the diﬃculty of this task, by estimat-\\ning conﬁdence intervals for the ranks of all the predictors. Even though the\\nbootstrap intervals are wide, they actually underestimate the true widths250.\\nThe following exampling uses simulated data with known ranks of impor-\\ntance of12predictors,usinganordinarylinearmodel. Theimportancemetric\\nis the partial χ2minus its degrees of freedom, while the true metric is the\\npartialβ, as all covariates have U(0,1) distributions.\\n# Use the plot method for anova, with pl=FALSE to suppress\\n# actual plotting of chi-square - d.f. for each bootstrap\\n# repetition. Rank the negative of the adjusted chi-squares\\n# so that a rank of 1 is assigned to the highest. It is\\n# important to tellplot.anova.rms not to sort the results,\\n# or every bootstrap replication would have ranks of 1,2,3,\\n# ... for the partial teststatistics.\\nrequire(rms)\\nn←300\\nset.seed(1)\\nd←data.frame(x1=runif (n), x2= runif(n), x3= runif(n),\\nx4=runif (n), x5= runif(n), x6= runif(n), x7= runif(n),\\nx8=runif (n), x9= runif(n), x10= runif(n), x11=runif (n),\\nx12=runif(n))\\nd$y←with(d, 1*x1 + 2*x2 + 3*x3 + 4*x4 + 5*x5 + 6*x6 +\\n7*x7 + 8*x8 + 9*x9 + 10*x10 + 11*x11 +\\n12*x12 + 9*rnorm(n))\\nf←ols(y∼x1+x2+x3+x4+x5+x6+x7+x8+x9+x10+x11+x12, data=d)\\nB←1000\\nranks ←matrix(NA, nrow=B, ncol=12)\\nrankvars ←function(fit)\\nrank(plot(anova(fit), sort= 'none ', pl=FALSE))\\nRank←rankvars(f)\\nfor(i in 1:B) {\\nj←sample(1:n, n, TRUE)\\nbootfit ←update(f, data=d, subset=j)\\nranks[i,] ←rankvars(bootfit)\\n}\\nlim←t(apply(ranks, 2, quantile, probs=c( .025,.975)))\\npredictor ←factor(names(Rank), names(Rank))\\nw←data.frame(predictor, Rank,lower=lim[,1], upper=lim[ ,2])\\nrequire(ggplot2)\\nggplot(w, aes(x=predictor, y=Rank)) + geom_point() +\\ncoord_flip() + scale_y_continuous( breaks=1:12) +\\ngeom_errorbar(aes( ymin=lim[,1], ymax=lim[,2]), width=0)\\nWith a sample size of n= 300 the observed ranks of predictor importance do\\nnot coincide with population βs, even when there are no collinearities among\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='987e68ac-eb9b-4cb9-8f83-87ed852363ec', embedding=None, metadata={'page_label': '118', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='118 5 Describing, Resampling, Validating, and Simplifying the Model\\nx1x2x3x4x5x6x7x8x9x10x11x12\\n123456789 1 0 1 1 1 2\\nRankpredictor\\nFig. 5.3 Bootstrap percentile 0.95 conﬁdence limits for ranks of predictors in an OLS\\nmodel. Ranking is on the basis of partial χ2minus d.f. Point estimates are original\\nranks\\nthe predictors. Conﬁdence intervals are wide; for example the 0.95 conﬁdence\\ninterval for the rank of x7(which has a true rank of 7) is [1 ,8], so we are\\nonly conﬁdent that x7is not one of the 4 most inﬂuential predictors. The\\nconﬁdence intervals do include the true ranks in each case (Figure 5.3).\\n5.5 Simplifying the Final Model by Approximating It\\n5.5.1 Diﬃculties Using Full Models\\nA model that contains all prespeciﬁed terms will usually be the one that pre-\\ndicts the most accurately on new data. It is also a model for which conﬁdence\\nlimits and statistical tests have the claimed properties. Often, however, this\\nmodel will not be very parsimonious. The full model may require more pre-\\ndictors than the researchers care to collect in future samples. It also requires\\npredicted values to be conditional on all of the predictors, which can increase\\nthe variance of the predictions.\\nAs an example suppose that least squares has been used to ﬁt a model\\ncontaining several variables including race (with four categories). Race may\\nbe an insigniﬁcant predictor and may explain a tiny fraction of the observed\\nvariation in Y. Yet when predictions are requested, a value for race must be\\ninserted.If the subject is of the majorityrace,andthis racehas a majorityof,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a8c1a508-1769-4ccd-85cb-5565d32f5b1e', embedding=None, metadata={'page_label': '119', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.5 Simplifying the Final Model by Approximating It 119\\nsay 0.75, the variance of the predicted value will not be signiﬁcantly greater\\nthan the variance for a predicted value from a model that excluded race\\nfor its list of predictors. If, however, the subject is of a minority race (say\\n“other”with a prevalence of 0.01), the predicted value will have much higher\\nvariance. One approach to this problem, that does not require development\\nof a second model, is to ignore the subject’s race and to get a weighted\\naverage prediction. That is, we obtain predictions for each of the four races\\nand weight these predictions by the relative frequencies of the four races.d\\nThis weighted average estimates the expected value of Yunconditional on\\nrace. It has the advantage of having exactly correct conﬁdence limits when\\nmodel assumptions are satisﬁed, because the correct “error term” is being\\nused (one that deducts 3 d.f. for having ever estimated the race eﬀect). In\\nregression models having nonlinear link functions, this process does not yield\\nsuch a simple interpretation.\\nWhen predictors are collinear, their competition results in larger P-values\\nwhenpredictorsare(ofteninappropriately)testedindividually.Likewise,con-\\nﬁdence intervals for individual eﬀects will be wide and uninterpretable (can\\nother variables really be held constant when one is changed?).\\n5.5.2 Approximating the Full Model\\nWhen the full model contains several predictors that do not appreciably af-\\nfect the predictions, the above process of“unconditioning”is unwieldy. In the\\nsearch for a simple solution, the most commonly used procedure for making\\nthe model parsimonious is to remove variables on the basis of P-values, but\\nthis results in a variety of problems as we have seen. Our approach instead\\nis to consider the full model ﬁt as the“gold standard”model, especially the\\nmodel from which formal inferences are made. We then proceed to approxi-\\nmate this full model to any desired degree of accuracy. For any approximate\\nmodel we calculate the accuracy with which it approximates the best model.\\nOne goal this process accomplishes is that it provides diﬀerent degrees of\\nparsimony to diﬀerent audiences, based on thei r needs. One investigator may\\nbe able to collect only three variables, another one seven. Each investigator\\nwill know how much she is giving up by using a subset of the predictors.\\nIn approximating the gold standard model it is very important to note that\\nthere is nothing gained in removing certain nonlinear terms; gains in parsi-\\nmony come only from removing entire predictors. Another accomplishment\\nof model approximation is that when the full model has been ﬁtted using\\ndUsing the rmspackage described in Chapter 6, such estimates and their\\nconﬁdence limits can easily be obtained, using for example contrast(fit,\\nlist(age=50, disease=’hypertension’, race=levels(race)), type=’average’,\\nweights=table(race)).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='191813c9-a357-4cbc-8746-887b7e6c9450', embedding=None, metadata={'page_label': '120', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='120 5 Describing, Resampling, Validating, and Simplifying the Model\\nshrinkage (penalized estimation, Section 9.10), the approximate models will\\ninherit the shrinkage (see Section 14.10for an example).\\nApproximating complex models with simpler ones has been used to de-\\ncode“black boxes”such as artiﬁcial neu ral networks. Recursive partitioning\\ntrees (Section 2.5) may sometimes be used in this context. One develops a\\nregression tree to predict the predicted value Xˆβon the basis of the unique\\nvariables in X,u s i n gR2, the average absolute prediction error, or the max-\\nimum absolute prediction error as a stopping rule, for example184. The user\\ndesiring simplicity may use the tree to obtain predicted values, using the ﬁrst\\nknodes, with kjust large enough to yield a low enough absolute error in pre-\\ndicting the more comprehensive prediction. Overﬁtting is not a problem as it\\nis when the tree procedure is used to predict the outcome, because (1) given\\nthe predictor values the predictions are deterministic and (2) the variable be-\\ning predicted is a continuous, completely observed variable. Hence the best\\ncross-validating tree approximation will be one with one subject per node.\\nOne advantage of the tree-approximation procedure is that data collection\\non an individual subject whose outcome is being predicted may be abbrevi-\\nated by measuring only those Xs that are used in the top nodes, until the\\nprediction is resolved to within a tolerable error.\\nWhen principal component regression is being used, trees can also be used\\nto approximate the components or to make them more interpretable.\\nFull models may also be approximated using least squares as long as the\\nlinear predictor Xˆβis the target, and not some nonlinear transformation of\\nit such as a logistic model probability. When the original model was ﬁtted\\nusingunpenalizedleastsquares,submodelsﬁttedagainst ˆYwillhavethesame\\ncoeﬃcients as if least squares had been used to ﬁt the subset of predictors\\ndirectlyagainst Y.T ose et h i s,n ot et h ati f Xdenotesthe entiredesignmatrix\\nandTdenotes a subset of the columns of X, the coeﬃcient estimates for the\\nfull model are ( X′X)−1X′Y,ˆY=X(X′X)−1X′Y, estimates for a reduced\\nmodel ﬁtted against Yare (T′T)−1T′Y, and coeﬃcients ﬁtted against ˆYare\\n(T′T)T′X(X′X)−1X′Yw h i c hc a nb es h o w nt oe q u a l( T′T)−1T′Y.\\nWhen least squares is used for both the full and reduced models, the\\nvariance–covariancematrixofthecoeﬃcientestimatesofthereducedmodelis\\n(T′T)−1σ2, where the residual variance σ2is estimated using the fullmodel.\\nWhenσ2is estimated by the unbiased estimator using the d.f. from the\\nfull model, which provides the only unbiased estimate of σ2,t h ee s t i m a t e d\\nvariance–covariance matrix of the reduced model will be appropriate (unlike\\nthat from stepwise variable selection) although the bootstrap may be needed\\nto fully take into account the source of variation due to how the approximate\\nmodel was selected.\\nSo if in the least squares case the approximate model coeﬃcients are iden-\\ntical to coeﬃcients obtained upon ﬁtting the reduced model against Y,h o w\\nis model approximation any diﬀerent from stepwise variable selection? There\\nare several diﬀerences, in addition to how σ2is estimated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc464b94-94f9-4c20-abdd-b5128ea09bcc', embedding=None, metadata={'page_label': '121', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6 Further Reading 121\\n1. When the full model is approximated by a backward step-down procedure\\nagainstˆY, the stopping rule is less arbitrary. One stops deleting variables\\nwhen deleting any further variable would make the approximation inad-\\nequate (e.g., the R2for predictions from the reduced model against the\\noriginal ˆYdrops below 0.95).\\n2. Because the stopping rule is diﬀerent (i.e., is not based on P-values), the\\napproximate model will have a diﬀerent number of predictors than an\\nordinary stepwise model.\\n3. If the original model used penalization, approximate models will inherit\\nthe amount of shrinkage used in the full ﬁt.\\nTypically, though, if one performed ordinary backward step-down against Y\\nusing a large cutoﬀ for α(e.g., 0.5), the approximate model would be very\\nsimilar to the step-down model. The main diﬀerence would be the use of\\na larger estimate of σ2and smaller error d.f. than are used for the ordinary\\nstep-down approach (an estimate that pretended the ﬁnal reduced model was\\nprespeciﬁed).\\nWhen the full model was not ﬁtted using least squares, least squares can\\nstill easily be used to approximate the full model. If the coeﬃcient estimates\\nfrom the full model are ˆβ, estimates from the approximate model are ma-\\ntrix contrasts of ˆβ,n a m e l y , Wˆβ,w h e r eW=(T′T)−1T′X. So the variance–\\ncovariance matrix of the reduced coeﬃcient estimates is given by\\nWVW′, (5.2)\\nwhereVis the variancematrixfor ˆβ. See Section 19.5for anexample. Ambler\\net al.21studied model simpliﬁcation using simulation studies based on several\\nclinical datasets, and compared it with ordinary backward stepdown variable\\nselectionandwithshrinkagemethodssuchasthe lasso(seeSection 4.3).They\\nfound that ordinary backwards variable selection can be competitive when\\nthere is a large fraction of truly irrelevant predictors (something that can be\\ndiﬃcult to know in advance). Paul et al.485found advantages to modeling\\nthe response with a complex but reliable approach, and then developing a\\nparsimoneous model using the lassoor stepwise variable selection against ˆY.\\nSee Section 11.7for a case study in model approximation.\\n5.6 Further Reading\\n1Gelman213argues that continuous variables should be scaled by two standard\\ndeviationstomake themcomparable tobinarypredictors. However hisapproach\\nassumeslinearity in thepredictor eﬀectand assumes theprevalenceof thebinary\\npredictor is near 0.5. John Fox [ 202, p. 95] points out that if two predictors are\\non the same scale and have the same impact (e.g., years of employment and\\nyears of education), standardizing the coeﬃcients will make them appear t o\\nhave diﬀerent impacts.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3887a326-1779-4b01-b3a1-50f60ce3cbd7', embedding=None, metadata={'page_label': '122', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='122 5 Describing, Resampling, Validating, and Simplifying the Model\\n2Levine et al.401have a compelling argument for graphing eﬀect ratios on a\\nlogarithmic scale.\\n3Hankins254is a deﬁnitive reference on nomograms and has multi-axis examples\\nof historical signiﬁcance. According to Hankins, Maurice d’Ocagne could be\\ncalled the inventor of the nomogram, starting with alignment diagrams in 1884\\nand declaring a new science of “nomography” in 1899. d’Ocagne was at ´Ecole\\ndes Ponts et Chauss´ ees, a French civil engineering school. Julien and Hanley328\\nhave a nice example of adding axes to a nomogram to estimate the absolute\\neﬀect of a treatment estimated using a Cox proportional hazards model. Kattan\\nand Marasco339have several clinical examples and explain advantages to the\\nuser of nomograms over“black box”computerized prediction.\\n4Graham and Clavel231discuss graphical and tabular ways of obtaining risk\\nestimates. van Gorp et al.630have a nice example of a score chart for manually\\nobtaining estimates.\\n5Larsen and Merlo375developed a similar measure—the median odds ratio. G ¨o-\\nnen and Heller223developed a c-index that like gis a function of the covariate\\ndistribution.\\n6Booth and Sarkar61have a nice analysis of the number of bootstrap resamples\\nneeded to guarantee with 0.95 conﬁdence that a variance estimate has a suf-\\nﬁciently small relative error. They concentrate on the Monte Carlo simulation\\nerror, showing that small errors in variance estimates can lead to important\\ndiﬀerences in P-values. Canty et al.91provide a number of diagnostics to check\\nthe reliability of bootstrap calculations.\\n7There are many variations on the basic bootstrap for computing conﬁdence\\nlimits.150,178See Booth and Sarkar61for useful information about choosing\\nthe number of resamples. They report the number of resamples necessary to\\nnot appreciably change P-values, for example. Booth and Sarkar propose a\\nmore conservative number of resamples than others use (e.g., 800 resamples)\\nfor estimating variances. Carpenter and Bithell92have an excellent overview of\\nbootstrap conﬁdence intervals, with practical guidance. They also have a go od\\ndiscussion of the unconditional nonparametric bootstrap versus the conditional\\nsemiparametric bootstrap.\\n8Altman and Royston18have a good general discussion of what it means to\\nvalidate a predictive model, including issues related to study design and con-\\nsideration of uses to which the model will be put.\\n9AnexcellentpaperonexternalvalidationandgeneralizabilityisJusticeetal.329.\\nBleeker et al.58providean example where internal validation ismisleading when\\ncompared with a true external validation done using subjects from diﬀerent\\ncenters in a diﬀerent time period. Vergouwe et al.638give good guidance about\\nthe number of events needed in sample used for external validation of binary\\nlogistic models.\\n10See Picard and Berk505for more about data-splitting.\\n11In the context of variable selection where one attempts to select the set of vari-\\nables with nonzero true regression coeﬃcients in an ordinary regression model,\\nShao565demonstrated that leave-out-one cross-validation selects models that\\nare“too large.”Shao also showed that the number of observations held back for\\nvalidation should often be larger than the number used to train the model. This\\nis because in this case one is not interested in an accurate model (you ﬁt the\\nwhole sample to do that), but an accurate estimate of prediction error is manda-\\ntory so as to know which variables to allow into the ﬁnal model. Shao suggests\\nusing a cross-validation strategy in which approximately n3/4observations are\\nused in each training sample and the remaining observations are used in the\\ntest sample. A repeated balanced or Monte Carlo splitting approach is used,\\nand accuracy estimates are averaged over 2 n(for the Monte Carlo method)\\nrepeated splits.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26ae7f28-1840-49ae-b8b2-d4e845b4b5ba', embedding=None, metadata={'page_label': '123', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.6 Further Reading 123\\n12Picard and Cook’s Monte Carlo cross-validation procedure506is an improve-\\nment over ordinary cross-validation.\\n13The randomization method is related to Kipnis’“chaotization relevancy princi-\\nple”348in which one chooses between two models by measuring how far each is\\nfrom a nonsense model. Tibshirani and Knight also use a randomization method\\nfor estimating the optimism in a model ﬁt.611\\n14This method used here is a slight change over that presented in [ 172], where\\nEfron wrote predictive accuracy as a sum of per-observation components (such\\nas 1 if the observation is classiﬁed correctly, 0 otherwise). Here we are writing\\nm×the unitless summary index of predictive accuracy in the place of Efron’s\\nsum ofmper-observation accuracies [ 416, p. 613].\\n15See [633]a n d[66, Section 4] for insight on the meaning of expected optimism.\\n16See Copas,123van Houwelingen and le Cessie [ 633, p. 1318], Verweij and van\\nHouwelingen,640and others631for other methods of estimating shrinkage coef-\\nﬁcients.\\n17Efron172developed the“ .632”estimator only for the case where the index being\\nbootstrapped is estimated on a per-observation basis. A natural generalization\\nof this method can be derived by assuming that the accuracy evaluated on\\nobservation ithat is omitted from a bootstrap sample has the same expectation\\nas the accuracy of any other observation that would be omitted from the sample.\\nThe modiﬁed estimate of ǫ0is then given by\\nˆǫ0=B∑\\ni=1wiTi, (5.3)\\nwhereTiis the accuracy estimate derived from ﬁtting a model on the ith boot-\\nstrap sample and evaluating it on the observations omitted from that bootstrap\\nsample, and wiare weights derived for the Bbootstrap samples:\\nwi=1\\nnn∑\\nj=1[bootstrap sample iomits observation j]\\n#bootstrap samples omitting observation j.(5.4)\\nNote that ˆ ǫ0is undeﬁned if any observation is included in every bootstrap\\nsample. Increasing Bwill avoid this problem. This modiﬁed “ .632” estimator\\nis easy to compute if one assembles the bootstrap sample assignments and\\ncomputes the wibefore computing the accuracy indexes Ti.F o rl a r g e n,t h ewi\\napproach 1 /Band so ˆǫ0becomes equivalent to the accuracy computed on the\\nobservations not contained in the bootstrap sample and then averaged over the\\nBrepetitions.\\n18Efron and Tibshirani179have reduced the bias of the “.632” estimator further\\nwith only a modest increase in its variance. Simulation has, however, shown no\\nadvantage of this “.632+” method over the basic optimism bootstrap for most\\naccuracy indexes used in logistic models.\\n19van Houwelingen and le Cessie633have several interesting developments in\\nmodel validation. See Breiman66for a discussion of the choice of Xfor which\\nto validate predictions. Steyerberg et al.587present simulations showing the\\nnumber of bootstrap samples needed to obtain stable estimates of optimism of\\nvarious accuracy measures. They demonstrate that bootstrap estimates of op-\\ntimism are nearly unbiased when compared with simulated external estimates.\\nThey also discuss problems with precision of estimates of accuracy, especially\\nwhen using external validation on small samples.\\n20Blettner and Sauerbrei also demonstrate the variability caused by data-driven\\nanalytic decisions.59Chatﬁeld100has more results on the eﬀects of using the\\ndata to select the model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6a761923-83d8-4727-a76b-64864ed4ccae', embedding=None, metadata={'page_label': '124', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='124 5 Describing, Resampling, Validating, and Simplifying the Model\\n5.7 Problem\\nPerforma simulationstudyto understandthe performanceofvariousinternal\\nvalidation methods for binary logistic models. Modify the Rcode below in at\\nleast two meaningful ways with regard to covariate distribution or number,\\nsample size, true regression coeﬃcients, number of resamples, or number of\\ntimes certain strategies are averaged. Interpret your ﬁndings and give recom-\\nmendations for best practice for the type of conﬁguration you studied. The\\nRcode from this assignment may be downloaded from the RMS course wiki\\npage.\\nFor each of 200 simulations, the code below generates a training sample\\nof 200 observations with ppredictors ( p= 15 or 30) and a binary response.\\nThe predictors are independently U(−0.5,0.5). The response is sampled so\\nas to follow a logistic model where the intercept is zero and all regression\\ncoeﬃcients equal 0.5. The “gold standard” is the predictive ability of the\\nﬁtted model on a test sample containing 50,000 observations generated from\\nthesamepopulationmodel.Foreachofthe200simulations,severalvalidation\\nmethods are employed to estimate how the training sample model predicts\\nresponsesinthe 50,000observations.Thesevalidationmethods involveﬁtting\\n40 or 200 models in resamples.\\ng-fold cross-validation is done using the command validate(f, method=\\n’cross’, B=g) using the rmspackage. This was repeated and averaged using\\nan extra loop, shown below.\\nFor bootstrap methods, validate(f, method=’boot’ or ’.632’, B=40 or\\nB=200)was used. method=’.632’ does Efron’s“.632”method179, labeled 632ain\\nthe output. An ad-hoc modiﬁcation of the .632 method, 632bwas also done.\\nHere a“bias-corrected”index of accuracy is simply the index evaluated in the\\nobservation omitted from the bootstrap resample.The“gold standard”exter-\\nnal validations were obtained from the val.prob function in the rmspackage.\\nThe following indexes of predictive accuracy are used:\\nDxy: Somers’rankcorrelationbetweenpredictedprobabilitythat Y=1v s .\\nthe binary Yv a l u e s .T h i se q u a l s2 ( C−0.5) where Cis the “ROC Area”\\nor concordance probability.\\nD: Discrimination index — likelihood ratio χ2divided by the sample size\\nU: Unreliability index — unitless index of how far the logit calibration\\ncurve intercept and slope are from (0 ,1)\\nQ: Logarithmic accuracy score — a scaled version of the log-likelihood\\nachieved by the predictive model\\nIntercept: Calibration intercept on logit scale\\nSlope: Calibration slope (slope of p redicted log odds vs. true log odds)\\nAccuracy of the various resampling procedures may be estimated by com-\\nputing the mean absolute errors and the root mean squared errors of esti-\\nmates (e.g., of Dxyfrom the bootstrap on the 200 observations) against the\\n“gold standard” (e.g., Dxyfor the ﬁtted 200-observation model achieved in\\nthe 50,000 observations).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60e436a5-745a-4d6d-ba17-65163d0dbda8', embedding=None, metadata={'page_label': '125', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='5.7 Problem 125\\nrequire(rms)\\nset.seed(1) # so can reproduce results\\nn←200 # Size of training sample\\nreps←200 # Simulations\\nnpop←50000 # Size of validation goldstandard sample\\nmethods ←c(\\'Boot 40 \\',\\'Boot 200 \\',\\'632a 40 \\',\\'632a 200 \\',\\n\\'632b 40 \\',\\'632b 200 \\',\\'10-fold x 4 \\',\\'4-fold x 10 \\',\\n\\'10-fold x 20 \\',\\'4-fold x 50 \\')\\nR←expand.grid(sim = 1:reps,\\np = c(15,30),\\nmethod = methods)\\nR$Dxy ←R$Intercept ←R$Slope ←R$D←R$U←R$Q←\\nR$repmeth ←R$B←NA\\nR$n←n\\n## Function to do r overall reps of B resamples, averaging to\\n## get estimates similar to as if r *B resamples were done\\nval←function(fit, method, B, r) {\\ncontains ←function(m) length(grep(m, method)) > 0\\nmeth←if(contains( \\'Boot \\'))\\'boot \\'else\\nif(contains( \\'fold \\'))\\'crossvalidation \\'else\\nif(contains( \\'632\\'))\\'.632 \\'\\nz←0\\nfor(i in 1:r) z ←z + validate(fit, method=meth, B=B)[\\nc(\"Dxy\",\"Intercept\",\"Slope\",\"D\",\"U\",\"Q\"),\\n\\'index.corrected \\']\\nz/r\\n}\\nfor(p in c(15, 30)) {\\n## For each p create the true betas, the design matrix,\\n## and realizations of binary y in the goldstandard\\n## large sample\\nBeta←rep(.5, p) # True betas\\nX←matrix(runif(npop*p), nrow=npop) - 0.5\\nLX←matxv(X, Beta)\\nY←ifelse(runif(npop) ≤plogis(LX), 1, 0)\\n## For each simulation create the datamatrix and\\n## realizations of y\\nfor(j in 1:reps) {\\n## Make training sample\\nx←matrix(runif(n*p), nrow=n) - 0.5\\nL←matxv(x, Beta)\\ny←ifelse(runif(n) ≤plogis(L), 1, 0)\\nf←lrm(y∼x, x=TRUE, y= TRUE)\\nbeta←f$coef\\nforecast ←matxv(X, beta)\\n## Validate in population', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0442e543-b989-4a70-aecf-a33310fc14e4', embedding=None, metadata={'page_label': '126', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='126 5 Describing, Resampling, Validating, and Simplifying the Model\\nv←val.prob( logit=forecast, y=Y, pl=FALSE)[\\nc(\"Dxy\",\"Intercept\",\" Slope\",\"D\",\"U\",\"Q\")]\\nfor(method in methods) {\\nrepmeth ←1\\nif(method %in% c( \\'Boot 40 \\',\\'632a 40 \\',\\'632b 40 \\'))\\nB←40\\nif(method %in% c( \\'Boot 200 \\',\\'632a 200 \\',\\'632b 200 \\'))\\nB←200\\nif(method == \\'10-fold x 4 \\'){\\nB←10\\nrepmeth ←4\\n}\\nif(method == \\'4-fold x 10 \\'){\\nB←4\\nrepmeth ←10\\n}\\nif(method == \\'10-fold x 20 \\'){\\nB←10\\nrepmeth ←20\\n}\\nif(method == \\'4-fold x 50 \\'){\\nB←4\\nrepmeth ←50\\n}\\nz←val(f, method, B, repmeth)\\nk←which(R$sim == j & R$p == p & R$method == method)\\nif(length(k) != 1) stop( \\'program logic error \\')\\nR[k, names(z)] ←z-v\\nR[k, c( \\'B\\',\\'repmeth \\')]←c(B=B, repmeth=repmeth)\\n}# end over methods\\n}# end over reps\\n}# end over p\\nResults are best summarized in a multi-waydot chart. Bootstrapnonpara-\\nmetric percentile 0.95 conﬁdence limits are included.\\nstatnames ←names(R )[6:11]\\nw←reshape(R, direction= \\'long \\', varying= list(statnames),\\nv.names= \\'x\\', timevar= \\'stat \\', times= statnames)\\nw$p←paste( \\'p\\', w$p, sep= \\'=\\')\\nrequire(lattice)\\ns←with(w, summarize(abs(x), llist(p, method, stat),\\nsmean.cl.boot,stat.name= \\'mae\\'))\\nDotplot( method ∼Cbind(mae, Lower, Upper) | stat*p, data=s,\\nxlab= \\'Mean |error| \\')\\ns←with(w, summarize(x∧2, llist(p, method, stat),\\nsmean.cl.boot, stat.name= \\'mse\\'))\\nDotplot( method ∼Cbind(sqrt(mse), sqrt(Lower), sqrt(Upper)) |\\nstat*p, data=s,\\nxlab=expression( sqrt(MSE)))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e8a426b1-63af-4afe-9458-040e84aa8577', embedding=None, metadata={'page_label': '127', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 6\\nRSoftware\\nThe methods described in this book are useful in any regression model that\\ninvolves a linear combination of regression parameters. The software that is\\ndescribed below is useful in the same situations.Functions in R520allowinter-\\nactionsplinefunctionsaswellasawidevarietyofpredictorparameterizations\\nfor any regression function, and facilitate model validation by resampling. 1\\nRis the most comprehensive tool for general regression models for the\\nfollowing reasons.\\n1. It is very easy to write Rfunctions for new models, so Rhas implemented\\na wide variety of modern regression models.\\n2. Designs can be generated for any model. There is no need to ﬁnd out\\nwhether the particular modeling function handles what SAS calls“class”\\nvariables—dummy variables are generated automatically when an Rcate-\\ngory,factor,ordered,o rcharacter variable is analyzed.\\n3. A single Robject can contain all information needed to test hypotheses\\nand to obtain predicted values for new data.\\n4.Rhas superior graphics.\\n5.ClassesinRmake possible the use of generic function names (e.g., predict,\\nsummary,anova) to examine ﬁts from a large set of speciﬁc model–ﬁtting\\nfunctions.\\nR44,601,635is a high-level object-oriented language for statistical anal-\\nysis with over six thousand packages and tens of thousands of functions\\navailable. The Rsystem318,520is the basis for Rsoftware used in this\\ntext, centered around the Regression Modeling Strategies ( rms) package261.\\nSee the Appendix and the Web site for more information about software\\nimplementations.\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 6127', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bacedbe0-1b17-4c85-8c89-0025981af00f', embedding=None, metadata={'page_label': '128', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='128 6RSoftware\\n6.1 The RModeling Language\\nRhas a battery of functions that make up a statistical modeling language.96\\nAt the heart of the modeling functions is an Rformula of the form 2\\nresponse ∼terms\\nThetermsrepresent additive components of a general linear model. Although\\nvariables and functions of variables make up the terms, the formula refers\\nto additive combinations; for example, when termsisage + blood.pressure ,\\nit refers to β1×age+β2×blood.pressure . Some examples of formulas are\\nbelow.\\ny∼age + sex # age + sex main effects\\ny∼age + sex + age:sex # add second-order interaction\\ny∼age*sex # second-order interaction +\\n# all main effects\\ny∼(age + sex + pressure)∧2\\n# age+sex+pressure+age:sex+age:pressure...\\ny∼(age + sex + pressure)∧2 - sex:pressure\\n# all main effects and all 2nd order\\n# interactions except sex:pressure\\ny∼(age + race)*sex # age+race+sex+age:sex+race:sex\\ny∼treatment*(age* race + age*sex)\\n# no interact. withrace,sex\\nsqrt(y) ∼sex*sqrt(age) + race\\n# functions, withdummy variables generated if\\n# race is an R factor (classification) variable\\ny∼sex + poly(age,2) # poly makes orthogonal polynomials\\nrace.sex ←interaction( race,sex)\\ny∼age + race.sex # if desire dummy variables for all\\n# combinations of the factors\\nThe formula for a regression model is given to a modeling function; for\\nexample,\\nlrm(y∼rcs(x,4))\\nis read“use a logistic regression model to model yas a function of x,r e p r e -\\nsenting xby a restricted cubic spline with four default knots.”aYou can use\\ntheRfunction updateto reﬁt a model with changes to the model terms or the\\ndata used to ﬁt it:\\nf←lrm(y∼rcs(x,4) + x2 + x3)\\nf2←update(f, subset=sex==\"male\")\\nf3←update(f, . ∼.-x2) # remove x2 frommodel\\nf4←update(f, . ∼. + rcs(x5,5)) # add rcs(x5,5) to model\\nf5←update(f, y2 ∼.) # same terms, new response var.\\nalrmandrcsare in the rmspackage.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2bd571d6-7a1e-4c7b-9b9c-47039ef52a5f', embedding=None, metadata={'page_label': '129', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.2 User-Contributed Functions 129\\n6.2 User-Contributed Functions\\nIn addition to the many functions that are packaged with R,aw i d ev a r i e t y\\nof user-contributed functions is available on the Internet (see the Appendix\\nor Web site for addresses). Two packages of functions used extensively in\\nthis text are Hmisc20andrmswritten by the author. The Hmiscpackage con-\\ntains miscellaneous functions such as varclus,spearman2 ,transcan,hoeffd,\\nrcspline.eval ,impute,cut2,describe,sas.get,latex,a n ds e v e r a lp o w e ra n d\\nsample size calculation functions. The varclusfunction uses the Rhclusthi-\\nerarchical clustering function to do variable clustering, and the Rplclust\\nfunction to draw dendrograms depicting the clusters. varclusoﬀers a choice\\nof three similarity measures (Pearson r2,S p e a r m a n ρ2, and Hoeﬀding D)\\nand uses pairwise deletion of missing values. varclusautomatically generates\\na series of dummy variables for categorical factors. The Hmisc hoeffd function\\ncomputes a matrix of Hoeﬀding Ds for a series of variables. The spearman2\\nfunction will do Wilcoxon, Spearman, and Kruskal–Wallis tests and general-\\nizes Spearman’s ρto detect non-monotonic relationships.\\nHmisc’stranscan function (see Section 4.7) performs a similar function to\\nPROC PRINQUAL inSAS—itusesrestrictedsplines,dummyvariables,andcanon-\\nical variates to transform each of a series of variables while imputing missing\\nvalues. An option to shrink regression coeﬃcients for the imputation models\\navoids overﬁtting for small samples or a large number of predictors. transcan\\ncan also do multiple imputation and adjust variance–covariance matrices for\\nimputation. See Chapter 8for an example of using these functions for data\\nreduction.\\nSee the Web site for a list of Rfunctions for correspondence analysis,\\nprincipal component analysis, and missing data imputation available from\\nother users. Venables and Ripley [ 635, Chapter 11] provide a nice description\\nof the multivariate methods that are available in R, and they provide several\\nnew multivariate analysis functions.\\nA basic function in Hmiscis thercspline.eval function, which creates a\\ndesignmatrixforarestricted(natural)cubicsplineusingthetruncatedpower\\nbasis. Knot locations are optionally estimated using methods described in\\nSection2.4.6, and two types of normalizations to reduce numerical problems\\nare supported. You can optionally obtain the design matrix for the anti-\\nderivative of the spline function. The rcspline.restate function computes\\nthe coeﬃcients (after un-normalizing if needed) that translate the restricted\\ncubic spline function to unrestricted form (Equation 2.27).rcspline.restate\\nalso outputs L ATEXa n d Rrepresentations of spline functions in simpliﬁed\\nform.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca41ee73-7c5c-44bb-909d-8015390562b1', embedding=None, metadata={'page_label': '130', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='130 6RSoftware\\n6.3 The rmsPackage\\nA package of Rfunctions called rmscontains several functions that extend\\nRto make the analyses described in this book easy to do. A central func-\\ntion inrmsisdatadist, which computes statistical summaries of predictors to\\nautomateestimationandplottingofeﬀects. datadist existsasaseparatefunc-\\ntion so that the candidate predictors may be summarized once, thus saving\\ntime when ﬁtting several models using subsets or diﬀerent transformations of\\npredictors. If datadist is called before model ﬁtting, the distributional sum-\\nmaries are stored with the ﬁt so that the ﬁt is self-contained with respect\\nto later estimation. Alternatively, datadist may be called after the ﬁt to cre-\\nate temporary summaries to use as plot ranges and eﬀect intervals, or these\\nranges may be speciﬁed explicitly to Predictandsummary(see below), without\\never calling datadist. The input to datadist may be a data frame, a list of\\nindividual predictors, or a combination of the two.\\nThe characteristicssaved by datadist include the overallrange and certain\\nquantiles for continuous variables, and the distinct values for discrete vari-\\nables (i.e., Rfactorvariablesor variableswith 10orfewer unique values).The\\nquantiles and set of distinct values facilitate estimation and plotting, as de-\\nscribed later. When a function of a predictoris used (e.g., pol(pmin(x,50),2) ),\\nthe limits saved apply to the innermost variable (here, x). When a plot is re-\\nquested for how xrelates to the response, the plot will have xon thex-axis,\\nnotpmin(x,50) . The way that defaults are computed can be controlled by\\ntheq.effect andq.display parameters to datadist. By default, continuous\\nvariables are plotted with ranges determined by the tenth smallest and tenth\\nlargest values occurring in the data (if n<200, the 0.05 and 0.95 quantiles\\nare used). The default range for estimating eﬀects such as odds and hazard\\nratios is the lower and upper quartiles. When a predictor is adjusted to a\\nconstant so that the eﬀects of changes in other predictors can be studied, the\\ndefault constant used is the median for continuous predictors and the most\\nfrequent category for factor variables. The Rsystem option datadist is used\\nto point to the result returned by the datadist function. See the help ﬁles for\\ndatadist for more information.\\nrmsﬁttingfunctionssavedetailedinformationforlaterprediction,plotting,\\nand testing. rmsalso allows for special restricted interactions and sets the\\ndefault method of generating contrasts for categorical variables to \"contr.-\\ntreatment\" , the traditional dummy-variable approach.\\nrmshas a special operator %ia%in the terms of a formula that allows for\\nrestricted interactions. For example, one may specify a model that contains\\nsex and a ﬁve-knot linear spline for age, but restrict the age ×sex interaction\\nto be linear in age. To be able to connect this incomplete interaction with the\\nmain eﬀects for later hypothesis testing and estimation, the followingformula\\nwould be given:\\ny∼sex + lsp(age,c(20,30,40,50,60)) +\\nsex %ia% lsp(age,c(20,30,40,50,60))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='474eb1fd-7c92-4484-b7a5-cffe2db116d6', embedding=None, metadata={'page_label': '131', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3 The rmsPackage 131\\nTable 6.1 rmsFitting Functions\\nFunction Purpose Related R\\nFunctions\\nols Ordinary least squares linear model lm\\nlrm Binary and ordinal logistic regression model glm\\nHas options for penalized MLE\\norm Ordinal semi-parametric regression model with polr,lrm\\nseveral link functions\\npsm Accelerated failure time parametric survival survreg\\nmodels\\ncph Cox proportional hazards regression coxph\\nbj Buckley–James censored least squares model survreg,lm\\nGlm General linear models glm\\nGls Generalized least squares gls\\nRq Quantile regression rq\\nThe following expression would restrict the age ×cholesterol interaction to\\nbe of the form AF(B)+BG(A) by removing doubly nonlinear terms.\\ny∼lsp(age,30) + rcs(cholesterol ,4) +\\nlsp(age,30) %ia% rcs(cholesterol ,4)\\nrmshas special ﬁtting functions that facilitate many of the procedures de-\\nscribed in this book, shown in Table 6.1.\\nGlmis a slight modiﬁcation of the built-in Rglmfunction so that rmsmeth-\\nods can be run on the resulting ﬁt object. glmﬁts general linear models under\\na wide variety of distributions of Y.Glsis a modiﬁcation of the glsfunction\\nfrom the nlmepackageof Pinheiroand Bates509, for repeatedmeasures(longi-\\ntudinal) andspatiallycorrelateddata.The Rqfunction isamodiﬁcationofthe\\nquantreg package’s rqfunction356,357. Functions related to survival analysis\\nmake heavy use of Therneau’s survival package482.\\nYou may want to specify to the ﬁtting functions an option for how missing\\nvalues ( NAs) are handled. The method for handling missing data in Ris to\\nspecify an na.action function.Some possible na.action saregiv eninT able 6.2.\\nThe default na.action isna.delete when you use rms’s ﬁtting functions. An\\neasy way to specify a new default na.action is, for example,\\noptions(na.action=\"na.omit\") # don \\'t report frequency of NAs\\nbeforeusingaﬁttingfunction.Ifyouuse na.delete youcanalsousethesystem\\noptionna.detail.response that makes model ﬁts store information about Y\\nstratiﬁed by whether each Xis missing. The default descriptive statistics for\\nYare the sample size and mean. For a survival time response object the\\nsample size and proportion of events are used. Other summary functions can\\nbe speciﬁed using the na.fun.response option.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b6b387eb-cc8e-4588-93d5-2bdf18369f55', embedding=None, metadata={'page_label': '132', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='132 6RSoftware\\nTable 6.2 Somena.action sU s e di n rms\\nFunction Name Method Used\\nna.fail Stop with error message if any missing\\nvalues present\\nna.omit Function to remove observations with\\nany predictors or responses missing\\nna.delete Modiﬁed version of na.omitto also\\nreport on frequency of NAsf o re a c h\\nvariable\\noptions(na.action=\"na.delete\", na.detail.response= TRUE,\\nna.fun.response=\"mystats\")\\n# Just use na.fun.response=\"quantile\" if don \\'t care about n\\nmystats ←function(y) {\\nz←quantile(y, na.rm=T)\\nn←sum(!is.na(y))\\nc(N=n, z) # elements named N, 0%, 25%, etc.\\n}\\nWhenRdeletes missing values during the model–ﬁtting procedure, residuals,\\nﬁtted values, and other quantities stored with the ﬁt will not correspondrow-\\nfor-rowwithobservationsintheoriginaldataframe(whichretained NAs). This\\nis problematic when, for example, agein the dataset is plotted against the\\nresidual from the ﬁtted model. Fortunately, for many na.action s including\\nna.delete and a modiﬁed version of na.omit,ac l a s so f Rfunctions called\\nnaresidwritten by Therneau works behind the scenes to put NAsb a c ki n t o\\nresiduals,predictedvalues,andotherquantitieswhenthe predictorresiduals\\nfunctions (see below) are used. Thus for some of the na.action s, predicted\\nvalues and residuals will automatically be arranged to match the original\\ndata.\\nAnyRfunction can be used in the terms for formulas given to the ﬁt-\\nting function, but if the function represents a transformation that has data-\\ndependent parameters (such as the standard Rfunctions polyorns),Rwill\\nnot in general be able to compute predicted values correctly for new obser-\\nvations. For example, the function nsthat automatically selects knots for a\\nB-spline ﬁt will not be conducive to obtaining predicted values if the knots\\nare kept“secret.”For this reason, a set of functions that keep track of trans-\\nformation parameters, exists in rmsfor use with the functions highlighted\\nin this book. These are shown in Table 6.3. Of these functions, asis,catg,\\nscored,a n dmatrxare almost always called implicitly and are not mentioned\\nby the user. catgis usually called explicitly when the variable is a numeric\\nvariable to be used as a polytomous factor, and it has not been converted to\\nanRcategorical variable using the factorfunction.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c6ac2671-86f7-48a7-8b63-7550f2afb204', embedding=None, metadata={'page_label': '133', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3 The rmsPackage 133\\nTable 6.3 rmsTransformation Functions\\nFunction Purpose Related R\\nFunctions\\nasis No post-transformation (seldom used explicitly) I\\nrcs Restricted cubic spline ns\\npol Polynomial using standard notation poly\\nlsp Linear spline\\ncatg Categorical predictor (seldom) factor\\nscored Ordinal categorical variables ordered\\nmatrx Keep variables as group for anovaandfastbw matrix\\nstrat Nonmodeled stratiﬁcation factors strata\\n(used for cphonly)\\nThese functions can be usedwith any function of a predictor.For example,\\nto obtain a four-knot cubic spline expansion of the cube root of x, specify\\nrcs(x∧(1/3),4).\\nWhen the transformation functions are called, they are usually given one\\nor two arguments, such as rcs(x,5). The ﬁrst argument is the predictor vari-\\nable or some function of it. The second argument is an optional vector of\\nparameters describing a transformation, for example location or number of\\nknots. Other arguments may be provided.\\nTheHmiscpackage’s cut2function is sometimes used to create a categorical\\nvariable from a continuous variable x. You can specify the actual interval\\nendpoints ( cuts), the number of observations to have in each interval on\\nthe average ( m), or the number of quantile groups ( g). Use, for example,\\ncuts=c(0,1,2) to cut into the intervals [0 ,1),[1,2].\\nA key concept in ﬁtting models in Ris that the ﬁtting function returns an\\nobject that is an Rlist. This object contains bas ic information about the ﬁt\\n(e.g.,regressioncoeﬃcientestimatesandcovariancematrix,model χ2)asw el l\\nas information about how each parameter of the model relates to each factor\\nin the model. Components of the ﬁt object are addressed by, for example,\\nfit$coef, fit$var, fit$loglik .rmscauses the following information to also\\nbe retained in the ﬁt object: the limits for plotting and estimating eﬀects\\nfor each factor (if options(datadist=\"name\") was in eﬀect), the label for each\\nfactor, and a vector of values indicating which parameters associated with a\\nfactor arenonlinear(if any).Thus the“ﬁtobject”containsallthe information\\nneeded to get predicted values, plots, odds or hazard ratios, and hypothesis\\ntests, and to do “smart” variable selection that keeps parameters together\\nwhen they are all associated with the same predictor.\\nRuses the notion of the classof an object. The object-oriented class idea\\nallows one to write a few generic functions that decide which speciﬁc func-\\ntions to call based on the class of the object passed to the generic function.\\nAn example is the function for printing the main results of a logistic model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7ffd9c10-008d-4240-a8af-413b6082313b', embedding=None, metadata={'page_label': '134', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='134 6RSoftware\\nThelrmfunction returns a ﬁt object of class \"lrm\". If you specify the Rcom-\\nmandprint(fit) (or just fitif using Rinteractively—this invokes print), the\\nprintfunctioninvokesthe print.lrm function todothe actualprintingspeciﬁc\\nto logistic models. To ﬁnd out which particular methods are implemented for\\na given generic function, type methods(generic.name) .\\nGeneric functions that are used in this book include those in Table 6.4.\\nTable 6.4 rmsPackage and RGeneric Functions\\nFunction Purpose Related Functions\\nprint Print parameters and statistics of ﬁt\\ncoef Fitted regression coeﬃcients\\nformula Formula used in the ﬁt\\nspecs Detailed speciﬁcations of ﬁt\\nvcov Fetch covariance matrix\\nlogLik Fetch maximized log-likelihood\\nAIC Fetch AIC\\nlrtest Likelihood ratio test for two nested models\\nunivarLR Compute all univariable LR χ2\\nrobcov Robust covariance matrix estimates\\nbootcov Bootstrap covariance matrix estimates\\nand bootstrap distributions of estimates\\npentrace Find optimum penalty factors by tracing\\neﬀective AIC for a grid of penalties\\neffective.df Print eﬀective d.f. for each type of variable\\nin model, for penalized ﬁt or pentrace result\\nsummary Summary of eﬀects of predictors\\nplot.summary Plot continuously shaded conﬁdence bars\\nfor results of summary\\nanova Wald tests of most meaningful hypotheses\\nplot.anova Graphical depiction of anova\\ncontrast General contrasts, C.L., tests\\nPredict Predicted values and conﬁdence limits easily\\nvarying a subset of predictors and leaving the\\nrest set at default values\\nplot.Predict Plot the result of Predict usinglattice\\nggplot Plot the result of Predict usingggplot2\\nbplot 3-dimensional plot when Predict varied\\ntwo continuous predictors over a ﬁne grid\\ngendata Easily generate predictor combinations\\npredict Obtain predicted values or design matrix\\nfastbw Fast backward step-down variable selection step\\nresiduals (orresid) Residuals, inﬂuence stats from ﬁt\\nsensuc Sensitivity analysis for unmeasured\\nconfounder\\nwhich.influence Which observations are overly inﬂuential residuals\\nlatex LATEX representation of ﬁtted model Function\\ncontinued on next page', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe5948ea-2c4d-4333-a744-ee04ea62da90', embedding=None, metadata={'page_label': '135', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3 The rmsPackage 135\\ncontinued from previous page\\nFunction Purpose Related Functions\\nFunction Rfunction analytic representation of Xˆβ latex\\nfrom a ﬁtted regression model\\nHazard Rfunction analytic representation of a ﬁtted\\nhazard function (for psm)\\nSurvival Rfunction analytic representation of ﬁtted\\nsurvival function (for psm,cph)\\nExProb Rfunction analytic representation of\\nexceedance probabilities for orm\\nQuantile Rfunction analytic representation of ﬁtted\\nfunction for quantiles of survival time\\n(forpsm,cph)\\nMean Rfunction analytic representation of ﬁtted\\nfunction for mean survival time or for ordinal logistic\\nnomogram Draws a nomogram for the ﬁtted model latex,plot\\nsurvest Estimate survival probabilities ( psm,cph) survfit\\nsurvplot Plot survival curves ( psm,cph) plot.survfit\\nvalidate Validate indexes of model ﬁt using resampling\\ncalibrate Estimate calibration curve using resampling val.prob\\nvif Variance inﬂation factors for ﬁtted model\\nnaresid Bring elements corresponding to missing data\\nback into predictions and residuals\\nnaprint Print summary of missing values\\nimpute Impute missing values transcan\\nThe ﬁrst argument of the majority of functions is the object returned from\\nthe model ﬁtting function. When used with ols,lrm,orm,psm,cph,Glm,Gls,Rq,\\nbj, these functions do the following. specsprints the design speciﬁcations, for\\nexample, number of parameters for each factor, levels of categorical factors,\\nknot locations in splines, and so on. vcovreturns the variance-covariance\\nmatrix for the model. logLikretrieves the maximized log-likelihood, whereas\\nAICcomputes the Akaike Information Criterion for the model on the minus\\ntwice log-likelihoodscale (with an option to compute it on the χ2scale if you\\nspecifytype=’chisq’ ).lrtest, when given two ﬁt objects from nested models,\\ncomputes the likelihood ratio test for the extra variables. univarLR computes\\nall univariable likelihood ratio χ2statistics, one predictor at a time.\\nTherobcovfunction computes the Huber robust covariance matrix esti-\\nmate.bootcovuses the bootstrap to estimate the covariance matrix of pa-\\nrameter estimates. Both robcovandbootcovassume that the design matrix\\nand response variable were stored with the ﬁt. They have options to adjust\\nfor cluster sampling. Both replace the original variance–covariance matrix\\nwith robust estimates and return a new ﬁt object that can be passed to any\\nof the other functions. In that way, robust Wald tests, variable selection, con-\\nﬁdence limits, and many other quantities may be computed automatically.\\nThe functions do save the old covariance estimates in component orig.var\\nof the new ﬁt object. bootcovalso optionally returns the matrix of param-\\neter estimates over the bootstrap simulations. These estimates can be used\\nto derive bootstrap conﬁdence intervals that don’t assume normality or sym-\\nmetry. Associated with bootcovare plotting functions for drawing histogram', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5353fc8d-476a-4b70-9a36-7f49358bd38d', embedding=None, metadata={'page_label': '136', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='136 6RSoftware\\nand smooth density estimates for bootstrap distributions. bootcovalso has\\na feature for deriving approximate nonparametric simultaneous conﬁdence\\nsets. For example, the function can get a simultaneous 0.90 conﬁdence region\\nfor the regression eﬀect of age over its entire range.\\nThepentrace function assists in selection of penalty factors for ﬁtting re-\\ngression models using penalized maximum likelihood estimation (see Sec-\\ntion9.10). Diﬀerent types of model terms can be penalized by diﬀerent\\namounts. For example, one can penalize interaction terms more than main\\neﬀects. The effective.df function prints details about the eﬀective degrees\\nof freedom devoted to each type of model term in a penalized ﬁt.\\nsummaryprints a summary of the eﬀects of each factor. When summaryis\\nused to estimate eﬀects (e.g., odds or hazard ratios) for continuous variables,\\nit allows the levels of interacting factors to be easily set, as well as allowing\\nthe userto choosethe intervalforthe eﬀect. This method ofestimatingeﬀects\\nallows for nonlinearity in the predictor. By default, interquartile range eﬀects\\n(diﬀerencesin Xˆβ,oddsratios,hazardsratios,etc.)areprintedforcontinuous\\nfactors, and all comparisons with the reference level are made for categorical\\nfactors.Seetheexampleattheendofthe summarydocumentationforamethod\\nof quickly computing pairwise treatment eﬀects and conﬁdence intervals for\\na large series of values of factors that interact with the treatment variable.\\nSayingplot(summary(fit)) will depict the eﬀects graphically, with bars for a\\nlist of conﬁdence levels.\\nTheanovafunction automatically tests most meaningful hypotheses in a\\ndesign. For example, suppose that age and cholesterol are predictors, and\\nthat a general interaction is modeled using a restricted spline surface. anova\\nprints Wald statistics for testing linearity of age, linearity of cholesterol, age\\neﬀect (age + age ×cholesterol interaction), cholesterol eﬀect (cholesterol +\\nage×cholesterol interaction), linearity of the age ×cholesterol interaction\\n(i.e., adequacy of the simple age ×cholesterol 1 d.f. product), linearity of the\\ninteraction in age alone, and linearity of the interaction in cholesterol alone.\\nJoint tests of all interactionterms in the model and all nonlinear terms in the\\nmodel arealso performed.The plot.anova function draws a dot chart showing\\nthe relative contribution ( χ2,χ2minus d.f., AIC, partial R2,P-value, etc.)\\nof each factor in the model.\\nThecontrast function is used to obtain general contrasts and correspond-\\ning conﬁdence limits and test statistics. This is most useful for testing eﬀects\\nin the presence of interactions (e.g., type II and type III contrasts). See the\\nhelp ﬁle for contrast for several examples of how to obtain joint tests of mul-\\ntiple contrasts (see Section 9.3.2) as well as double diﬀerences (interaction\\ncontrasts).\\nThepredictfunction is used to obtain a variety of values or predicted\\nvalues from either the data used to ﬁt the model or a new dataset. The\\nPredictfunction is easier to use for most purposes, and has a special plot\\nmethod. The gendatafunctionmakesiteasyto obtainadataframecontaining\\npredictor combinations for obtaining selected predicted values.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cf576a49-3949-495d-a937-f31df7772204', embedding=None, metadata={'page_label': '137', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3 The rmsPackage 137\\nThefastbwfunction performs a slightly ineﬃcient but numerically stable\\nversion of fast backward elimination on factors, using a method based on\\nLawless and Singhal.385This method uses the ﬁtted complete model and\\ncomputes approximate Wald statistics by computing conditional (restricted)\\nmaximum likelihood estimates assuming multivariate normality of estimates.\\nIt can be used in simulations since it returns indexes of factors retained and\\ndropped:\\nfit←ols(y∼x1*x2*x3)\\n# run, and print results:\\nfastbw(fit, optional_arguments)\\n# typically used in simulations:\\nz←fastbw(fit, optional_args)\\n# least squares fit of reduced model:\\nlm.fit(X[,z$ parms.kept], Y)\\nfastbwdeletes factors, not columns of the design matrix. Factors requiring\\nmultiple d.f. will be retained or dropped as a group. The function prints the\\ndeletionstatisticsforeachvariableinturn,andprintsapproximateparameter\\nestimates for the model after deleting variables. The approximation is better\\nwhen the number of factors deleted is not large. For ols, the approximation\\nis exact.\\nThewhich.influence function creates a list with a component for each\\nfactor in the model. The names of the components are the factor names.\\nEach component contains the observation identiﬁers of all observations that\\nare“overlyinﬂuential”with respect to that factor, meaning that |dfbetas|>u\\nfor at least one βiassociated with that factor, for a given u. The default u\\nis .2. You must have speciﬁed x=TRUE, y=TRUE in the ﬁtting function to use\\nwhich.influence . Theﬁrstargumentistheﬁtobject,andthesecondargument\\nis the cutoﬀ u.\\nThe following Rprogram will print the set of predictor values that were\\nvery inﬂuential for each factor. It assumes that the data frame containing the\\ndata used in the ﬁt is called df.\\nf←lrm(y∼x1 + x2 + ..., data=df, x= TRUE, y= TRUE)\\nw←which.influence(f, .4)\\nnam←names(w)\\nfor(i in 1:length(nam)) {\\ncat(\"Influential observations for effect of\",\\nnam[i],\"\\\\n\")\\nprint(df[w[[i]],])\\n}\\nThelatexfunction is a generic function available in the Hmiscpackage. It\\ninvokes a speciﬁc latexfunction for most of the ﬁt objects created by rmsto\\ncreate a L ATEX algebraic representation of the ﬁtted model for inclusion in a\\nreportorviewingonthescreen.Thisrepresentationdocumentsallparameters\\nin the model and the functional form being assumed for Y, and is especially\\nuseful for getting a simpliﬁed version of restricted cubic spline functions. On', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00d8c426-f666-409e-95c9-516c66c13b7b', embedding=None, metadata={'page_label': '138', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='138 6RSoftware\\nthe other hand, the printmethod with optional argument latex=TRUE is used\\nto output L ATEX code representing the model results in tabular form to the\\nconsole. This is intended for use with knitr677orSweave399.\\nTheFunction function composesan Rfunction that youcanuse to evaluate\\nXˆβanalytically from a ﬁtted regression model. The documentation for Func-\\ntionalso shows how to use a subsidiary function sascodethat will (almost)\\ntranslate such an Rfunction into SAS code for evaluating predicted values in\\nnew subjects. Neither Function norlatexhandles third-order interactions.\\nThenomogram function draws a partial nomogram for obtaining predictions\\nfrom the ﬁtted model manually. It constructs diﬀerent scales when interac-\\ntions (up to third-order) are present. The constructed nomogram is not com-\\nplete, in that point scores are obtained for each predictor and the user must\\nadd the point scores manually before reading predicted values on the ﬁnal\\naxis of the nomogram. The constructed nomogram is useful for interpreting\\nthe model ﬁt, especially for non-monotonically transformed predictors (their\\nscales wrap around an axis automatically).\\nTheviffunction computes variance inﬂation factors from the covariance\\nmatrix of a ﬁtted model, using [147,654].\\nTheimputefunction is another generic function. It does simple imputation\\nby default. It can also work with the transcan function to multiply or singly\\nimpute missing values using a ﬂexible additive model.\\nAs an example of using many of the functions, suppose that a categorical\\nvariable treathas values \"a\", \"b\", and\"c\", an ordinal variable num.diseases\\nhas values 0,1,2,3,4, and that there are two continuous variables, ageand\\ncholesterol .ageis ﬁtted with a restricted cubic spline, while cholesterol\\nis transformed using the transformation log(cholesterol+10) . Cholesterol is\\nmissing on three subjects, and we impute these using the overall median\\ncholesterol. We wish to allow for interaction between treatandcholesterol .\\nThe following Rprogramwill ﬁt a logistic model, test all eﬀects in the design,\\nestimate eﬀects, and plot estimated transformations. The ﬁt for num.diseases\\nreally considers the variable to be a ﬁve-level categorical variable. The only\\ndiﬀerence isthat a 3d.f. test oflinearityisdone toassesswhether the variable\\ncan be remodeled “asis”. Here we also show statements to attach the rms\\npackage and store predictor characteristics from datadist.\\nrequire(rms) # make new functions available\\nddist ←datadist( cholesterol, treat, num.dis eases, age)\\n# Could have used ddist ←datadist(data.frame.name)\\noptions(datadist=\" ddist\") # defines datadist. to rms\\ncholesterol ←impute( cholesterol)\\nfit←lrm(y∼treat + scored( num.diseases) + rcs(age) +\\nlog(cholesterol +10) +\\ntreat:log( cholesterol +10))\\ndescribe(y ∼treat + scored( num.diseases) + rcs(age))\\n# or use describe(formula(fit)) for all variables used in\\n# fit. describe function (in Hmisc) getssimple statistics\\n# on variables\\n# fit ←robcov(fit)# Would make all statistics thatfollow', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d9878152-908f-4a10-a551-2f28cb1f0ec2', embedding=None, metadata={'page_label': '139', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.3 The rmsPackage 139\\n# use a robust covariance matrix\\n# would need x= TRUE, y= TRUE in lrm()\\nspecs(fit) # Describe the design characteristics\\nanova(fit)\\nanova(fit, treat, cholesterol) # Test these 2 by themselves\\nplot(anova(fit)) # Summarize anova graphically\\nsummary(fit) # Est. effects; default ranges\\nplot(summary(fit)) # Graphical display of effects with C.I.\\n# Specific reference cell and adjustment value:\\nsummary(fit, treat=\"b\", age=60)\\n# Estimate effect of increasing age: 50->70\\nsummary(fit, age=c(50 ,70))\\n# Increase age 50->70, adjust to 60 whenestimating\\n# effects of other factors:\\nsummary(fit, age=c(50,60 ,70))\\n# If had not defined datadist, wouldhave to define\\n# ranges for all variables\\n# Estimate and testtreatment (b-a) effect averaged\\n# over 3 cholesterols:\\ncontrast(fit, list(treat= \\'b\\', cholesterol=c (150,200,250)),\\nlist(treat= \\'a\\', cholesterol=c (150,200,250)),\\ntype= \\'average \\')\\np←Predict(fit, age=seq(20,80, length=100), treat,\\nconf.int= FALSE)\\nplot(p) # Plot relationship between age and\\n# or ggplot(p) # log odds,separate curve for each\\n# treat, no C.I.\\nplot(p, ∼age | treat) # Same but 2 panels\\nggplot(p, groups=FALSE)\\nbplot(Predict(fit, age, cholesterol, np=50))\\n# 3-dimensional perspective plot for\\n# age, cholesterol, and log odds\\n# using default ranges for both\\n# Plot estimated probabilities instead of log odds:\\nplot(Predict(fit, num.dise ases,\\nfun=function(x) 1/(1+exp(-x)),\\nconf.int=.9), ylab=\"Prob\")\\n# Again, if no datadist were defined, wouldhave to tell\\n# plot all limits\\nlogit ←predict(fit, expand.grid( treat=\"b\",num.dis =1:3,\\nage=c(20,40,60),\\ncholesterol=seq (100,300,length=10)))\\n# Could obtain list of predictor settings interactively\\nlogit ←predict(fit, gendata(fit, nobs=12))\\n# An easier approach is\\n# Predict(fit, treat= \\'b\\',num.dis =1:3,...)\\n# Since age doesn \\'t interact with anything, we can quickly\\n# and interactively try various transformations of age,\\n# taking the spline function of age as the goldstandard.\\n# We are seeking a linearizing transformation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5cca5e2a-8ba5-4c9e-85d4-79be3bd83762', embedding=None, metadata={'page_label': '140', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='140 6RSoftware\\nag←10:80\\nlogit ←predict(fit, expand.grid( treat=\"a\",num.dis=0,\\nage=ag,\\ncholesterol= median( cholesterol)),\\ntype=\"terms\")[,\"age\"]\\n# Note: if age interacted with anything, this would be the\\n# age `main effect \\'ignoring interaction terms\\n# Could also use logit ←Predict(f, age=ag, ...) $yhat,\\n# which allows evaluation of the shape for any level of\\n# interacting factors. When age does not interact with\\n# anything, the result frompredict(f, ..., type=\"terms\")\\n# would equal the result fromPredict if all other terms\\n# were ignored\\n# Could alsospecify:\\n# logit ←predict(fit,\\n# gendata(fit, age=ag, cholesterol=...))\\n# Unmentioned variables are set to reference values\\nplot(ag∧.5, logit) # try square root vs. spline transform.\\nplot(ag∧1.5, logit) # try 1.5 power\\n# Pretty printing of table of estimates and\\n# summary statistics:\\nprint(fit, latex=TRUE) # print LATEXcode to console\\nlatex(fit) # invokes latex.lrm, creates fit.tex\\n# Draw a nomogram for the model fit\\nplot(nomogram(fit))\\n# Compose R function to evaluate linear predictors\\n# analytically\\ng←Function(fit)\\ng(treat= \\'b\\', cholesterol =260, age =50)\\n# Letting num.diseases default to reference value\\nTo examine interactions in a simpler way, you may want to group age into\\ntertiles:\\nage.tertile ←cut2(age, g=3)\\n# For auto ranges later, specify age.tertile to datadist\\nfit←lrm(y∼age.tertile * rcs(cholesterol))\\nExample output from these functions is shown in Chapter 10and later\\nchapters.\\nNote that type=\"terms\" inpredictscores each factor in a model with its\\nﬁtted transformation. This may be used to compute, for example, rank cor-\\nrelation between the response and each transformed factor, pretending it has\\n1 d.f.\\nWhen regression is done on principal components, one may use an ordi-\\nnary linear model to decode “internal”regression coeﬃcients for helping to\\nunderstand the ﬁnal model. Here is an example.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1af6efc7-bddd-4119-97bb-9d8e731903f5', embedding=None, metadata={'page_label': '141', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='6.4 Other Functions 141\\nrequire(rms)\\ndd←datadist(my.data)\\noptions(datadist= \\'dd\\')\\npcfit ←princomp( ∼pain.symptom1 + pain.symptom2 + sign1 +\\nsign2 + sign3 + smoking)\\npc2←pcfit$scores[,1:2] # first 2 PCs as matrix\\nlogistic.fit ←lrm(death ∼rcs(age,4) + pc2)\\npredicted.logit ←predict(logistic.fit)\\nlinear.mod ←ols(predicted.logit ∼rcs(age,4) +\\npain.symptom1 + pain.symptom2 +\\nsign1 + sign2 + sign3 + smoking)\\n# This modelwill have R-squared=1\\nnom←nomogram( linear.mod, fun= function(x)1/(1+exp(-x)),\\nfunlabel=\"Probability of Death\")\\n# can use fun=plogis\\nplot(nom)\\n# 7 Axes showing effects of all predictors, plus a reading\\n# axis converting to predicted probability scale\\nIn addition to many of the add-on functions described above, there are\\nseveral other Rfunctions that validate models. The ﬁrst, predab.resample ,\\nis a general-purpose function that is used by functions for speciﬁc models\\ndescribed later. predab.resample computes estimates of optimism and bias-\\ncorrected estimates of a vector of indexes of predictive accuracy, for a model\\nwith a speciﬁed design matrix, with or without fast backward step-down of\\npredictors. If bw=TRUE,predab.resample prints a matrix of asterisks showing\\nwhich factors were selected at each repetition, along with a frequency dis-\\ntribution of the number of factors retained across resamples. The function\\nhas an optional parameter that may be speciﬁed to force the bootstrap al-\\ngorithm to do sampling with replacement from clusters rather than from\\noriginal records, which is useful when each subject has multiple records in\\nthe dataset. It also has a parameter that can be used to validate predictions\\nin a subset of the records even though models are reﬁt using all records.\\nThe generic function validate invokes predab.resample with model-speciﬁc\\nﬁts and measuresof accuracy.The function calibrate invokes predab.resample\\nto estimatebias-correctedmodelcalibrationand toplot the calibrationcurve.\\nModel calibration is estimated at a sequence of predicted values.\\n6.4 Other Functions\\nFor principal component analysis, Rhas the princomp andprcompfunctions.\\nCanonical correlations and canonical variates can be easily computed us-\\ning the cancorfunction. There are many other Rfunctions for examining\\nassociations and for ﬁtting models. The supsmufunction implements Fried-\\nman’s“super smoother.”207Thelowessfunction implements Cleveland’s two-\\ndimensionalsmoother.111Theglmfunctionwillﬁtgenerallinearmodelsunder', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='009c46be-42e8-4c42-92b8-b55149247fbd', embedding=None, metadata={'page_label': '142', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='142 6RSoftware\\na wide variety of distributions of Y. There arefunctions to ﬁt Hastie and Tib-\\nshirani’s275generalized additive model for a variety of distributions. More is\\nsaid about parametric and nonparametric additive multiple regression func-\\ntions in Chapter 16.T h eloessfunction ﬁts a multidimensional scatterplot\\nsmoother (the local regression model of Cleveland et al.96).loessprovides\\napproximate test statistics for normal or symmetrically distributed Y:\\nf←loess(y ∼age * pressure)\\nplot(f) # cross-sectional plots\\nages←seq(20,70,length=40)\\npressures ←seq(80,200, length=40)\\npred←predict(f,\\nexpand.grid(age= ages,pressure=pressures))\\npersp(ages,pressures, pred) # 3-D plot\\nloesshas a large number of options allowing various restrictions to be placed\\non the ﬁtted surface.\\nAtkinson and Therneau’s rpartrecursive partitioning package and related\\nfunctions implement classiﬁcation and regression trees69algorithms for bi-\\nnary, continuous, and right-censored response variables (assuming an expo-\\nnential distribution for the latter). rpartdeals eﬀectively with missing predic-\\ntor values using surrogate splits. The rmspackage has a validate function for\\nrpartobjects for obtaining cross-validated mean squared errors and Somers’\\nDxyrank correlations (Brier score and ROC areas for probability models).\\nFor displaying which variables tend to be missing on the same subjects,\\ntheHmisc naclus f u n c t i o nc a nb eu s e d( e . g . , plot(naclus(dataframename)) or\\nnaplot(naclus( dataframename)) ). For characterizing what type of subjects\\nhaveNA’s on a given predictor (or response) variable, a tree model whose\\nresponse variable is is.na(varname) can be quite useful.\\nrequire( rpart)\\nf←rpart(is.na( cholesterol) ∼age + sex + trig + smoking)\\nplot(f) # plots the tree\\ntext(f) # labels the tree\\nTheHmisc rcorr.cens function can compute Somers’ Dxyrank correla-\\ntion coeﬃcient and its standard error, for binary or continuous (and possibly\\nright-censored) responses. A simple transformation of Dxyyields the cindex\\n(generalized ROC area). The Hmisc improveProb function is useful for compar-\\ning two probability models using the methods of Pencina etal490,492,493in an\\nexternal validation setting. See also the rcorrp.cens function in this context.\\n6.5 Further Reading\\n1Harrell and Goldstein263list components of statistical languages or packages\\nand compare several popular packages for survival analysis capabilities.\\n2Imai et al.319have further generalized Ras a statistical modeling language.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89dbebe0-20f7-40f6-801e-def9aebad38d', embedding=None, metadata={'page_label': '143', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 7\\nModeling Longitudinal Responses using\\nGeneralized Least Squares\\nIn this chapter we consider models for a multivariate response variable repre-\\nsented by serial measurements over time within subject. This setup induces\\ncorrelations between measurements on the same subject that must be taken\\ninto account to have optimal model ﬁts and honest inference. Full likelihood\\nmodel-based approaches have advantages including (1) optimal handling of\\nimbalanced data and (2) robustness to missing data (dropouts) that occur\\nnot completely at random. The three most popular model-based full like-\\nlihood approaches are mixed eﬀects mod els, generalized least squares, and\\nBayesian hierarchical models. For continuous Y, generalized least squares\\nhas a certain elegance, and a case study will demonstrate its use after sur-\\nveying competing approaches. As OLS is a special case of generalized least\\nsquares, the case study is also helpful in developing and interpreting OLS\\nmodelsa.\\nSome good references on longitudinal data analysis\\ninclude148,159,252,414,509,635,637.\\n7.1 Notation and Data Setup\\nSuppose there are Nindependent subjects, with subject i(i=1,2,...,N)\\nhavingniresponsesmeasuredattimes ti1,ti2,...,tini. The responseattime t\\nfor subject iis denoted by Yit. Suppose that subject ihas baseline covariates\\nXi. Generally the response measured at time ti1= 0 is a covariate in Xi\\ninstead of being the ﬁrst measured response Yi0.\\nFor ﬂexible analysis, longitudinal data are usually arranged in a“tall and\\nthin”layout. This allows measurement times to be irregular. In studies com-\\naA case study in OLS—Chapter 7 from the ﬁrst edition—may be found on the text’s\\nweb site.\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 7143', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f7d2551c-75d5-4bd0-b66c-9293136a17fc', embedding=None, metadata={'page_label': '144', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='144 7 Modeling Longitudinal Responses using Generalized Least Squares\\nparing two or more treatments, a response is often measured at baseline\\n(pre-randomization). The analyst has the option to use this measurement as\\nYi0or as part of Xi. There are many reasons to put initial measurements of\\nYinX, i.e., to use baseline measurements as baseline . 1\\n7.2 Model Speciﬁcation for Eﬀects on E(Y)\\nLongitudinal data can be used to estimate overall means or the mean at the\\nlastscheduledfollow-up,makingmaximumuseofincompleterecords.Butthe\\nreal value of longitudinal data comes from modeling the entire time course.\\nEstimating the time course leads to understanding slopes, shapes, overall\\ntrajectories, and periods of treatment eﬀectiveness. With continuous Yone\\ntypically speciﬁes the time course by a m ean time-response proﬁle. Common\\nrepresentations for such proﬁles include\\n•kdummy variables for k+1 unique times (assumes no functional form for\\ntime but assumes discrete measurement times and may spend many d.f.)\\n•k= 1 for linear time trend, g1(t)=t\\n•k–order polynomial in t\\n•k+1–knot restricted cubic spline (one linear term, k−1 nonlinear terms)\\nSuppose the time trend is modeled with kparameters so that the time\\neﬀect has kd.f. Let the basis functions modeling the time eﬀect be g1(t),\\ng2(t),...,g k(t) to allow it to be nonlinear. A model for the time proﬁle with-\\nout interactions between time and any Xis given by\\nE[Yit|Xi]=Xiβ+γ1g1(t)+γ2g2(t)+...+γkgk(t).(7.1)\\nTo allow the slope or shape of the time-response proﬁle to depend on some\\nof theXs we add product terms for desired interaction eﬀects. For example,\\nto allow the mean time trend for subjects in group 1 (reference group) to\\nbe arbitrarily diﬀerent from the time trend for subjects in group 2, have a\\ndummy variable for group 2, a time“main eﬀect”curve with kd.f. and all k\\nproducts of these time components with the dummy variable for group 2.\\nOnce the right hand side of the model is formulated, predicted values,\\ncontrasts, and ANOVAs are obtained just as with a univariate model. For\\nthese purposes time is no diﬀerent than any other covariate except for what\\nis described in the next section.\\n7.3 Modeling Within-Subject Dependence\\nSometimes understanding within-subject correlation patterns is of interest\\nin itself. More commonly, accounting for intra-subject correlation is crucial\\nfor inferences to be valid. Some methods of analysis cover up the correlation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc52f15a-688a-45f8-ab05-c6aada9e1d52', embedding=None, metadata={'page_label': '145', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.3 Modeling Within-Subject Dependence 145\\npattern while others assume a restrictive form for the pattern. The following\\ntable is an attempt to brieﬂy survey available longitudinal analysis meth-\\nods. LOCF and the summary statistic method are not modeling methods. 2\\nLOCF is an ad hoc attempt to account for longitudinal dropouts, and sum-\\nmary statisticscanconvertmultivariateresponsesto univariateones withfew\\nassumptions (other than minimal dropouts), with some information loss.\\nWhat Methods To Use for Repeated Measurements /\\nSerial Data?ab\\nRepeated GEE Mixed GLS LOCF Summary\\nMeasures Eﬀects Statisticc\\nANOVA Model\\nAssumes normality ×× ×\\nAssumes independence of ×d×e\\nmeasurements within subject\\nAssumes a correlation structuref××g××\\nRequires same measurement × ?\\ntimes for all subjects\\nDoes not allow smooth modeling ×\\nof time to save d.f.\\nDoes not allow adjustment for ×\\nbaseline covariates\\nDoes not easily extend to ××\\nnon-continuous Y\\nLoses information by not using ×h×\\nintermediate measurements\\nDoes not allow widely varying # ××i××j\\nof observations per subject\\nDoes not allow for subjects ×× × ×\\nto have distinct trajectoriesk\\nAssumes subject-speciﬁc eﬀects ×\\nare Gaussian\\nBadly biased if non-random ? ××\\ndropouts\\nBiased in general ×\\nHarder to get tests & CLs ×l×m\\nRequires large # subjects/clusters ×\\nSEs are wrong ×n×\\nAssumptions are not veriﬁable ×N/A×××\\nin small samples\\nDoes not extend to complex ×× ×× ?\\nsettings such as time-dependent\\ncovariates and dynamicomodels\\naThanks to Charles Berry, Brian Cade, Peter Flom, Bert Gunter, and Leena Choi\\nfor valuable input.\\nbGEE: generalized estimating equations; GLS: generalized least squares; LOCF: last\\nobservation carried forward.\\ncE.g., compute within-subject slope, mean, or area under the curve over time. As-\\nsumes that the summary measure is an adequate summary of the time proﬁle and\\nassesses the relevant treatment eﬀect.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88ff8011-9c5e-4f6b-bda4-1ec500aad124', embedding=None, metadata={'page_label': '146', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='146 7 Modeling Longitudinal Responses using Generalized Least Squares\\nThemostprevalentfullmodelingapproachismixedeﬀectsmodelsinwhich\\nbaseline predictors are ﬁxed eﬀects, and random eﬀects are used to describe\\nsubject diﬀerences and to induce within-subject correlation. Some disadvan-\\ntages of mixed eﬀects models are\\n•The induced correlation structure for Ym a yb eu n r e a l i s t i ci fc a r ei sn o t\\ntaken in specifying the model.\\n•Random eﬀects require complex approximations for distributions of test\\nstatistics.\\n•The most commonly used models assume that random eﬀects follow a\\nnormal distribution. This assumption may not hold.\\nIt could be argued that an extended linear model (with no random eﬀects)\\nis a logical extension of the univariate OLS modelb.T h i sm o d e l ,c a l l e dt h e\\ngeneralizedleastsquaresorgrowthcurvemodel221,509,510,wasdevelopedlong\\nbefore mixed eﬀect models became popular.\\nWe will assume that Yit|Xihas a multivariate normal distribution with\\nmean given above and with variance-covariance matrix Vi,a nni×nimatrix\\nthat is a function of ti1,...,tini. We further assume that the diagonals of Vi\\nare all equalb.T h i sextended linear model has the following assumptions:\\n•all the assumptions of OLS at a single time point including correct mod-\\neling of predictor eﬀects and univariate normality of responses conditional\\nonX\\ndUnless one uses the Huynh-Feldt or Greenhouse-Geisser correction\\neFor full eﬃciency, if using the working independence model\\nfOr requires the user to specify one\\ngFor full eﬃciency of regression coeﬃcient estimates\\nhUnless the last observation is missing\\niThe cluster sandwich variance estimator used to estimate SEs in GEE does not\\nperform well in this situation, and neither does the working independence model\\nbecause it does not weight subjects properly.\\njUnless one knows how to properly do a weighted analysis\\nkOr uses population averages\\nlUnlike GLS, does not use standard maximum likelihood methods yielding simple\\nlikelihood ratio χ2statistics. Requires high-dimensional integration to marginalize\\nrandom eﬀects, using complex approximations, and if using SAS, unintuitive d.f. for\\nthe various tests.\\nmBecause there is no correct formula for SE of eﬀects; ordinary SEs are not pena lized\\nfor imputation and are too small\\nnIf correction not applied\\noE.g., a model with a predictor that is a lagged value of the response variable\\nbE.g., few statisticians use subject random eﬀects for univariate Y. Pinheiro and\\nBates [509, Section 5.1.2] state that “in some applications, one may wish to avoid\\nincorporating random eﬀects in the model to account for dependence among obser -\\nvations, choosing to use the within-group component Λito directly model variance-\\ncovariance structure of the response.”\\nbThis procedure can be generalized to allow for heteroscedasticity over time or with\\nrespect to X, e.g., males may be allowed to have a diﬀerent variance than females.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1c66130-8af6-448f-90f7-0fbad7f07bd0', embedding=None, metadata={'page_label': '147', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.5 Common Correlation Structures 147\\n•the distribution of two responses at two diﬀerent times for the same sub-\\nject, conditional on X, is bivariate normal with a speciﬁed correlation\\ncoeﬃcient\\n•the joint distribution of all niresponses for the ithsubject is multivariate\\nnormal with the given correlation pattern (which implies the previous two\\ndistributional assumptions)\\n•responses from two diﬀerent subjects are uncorrelated.\\n7.4 Parameter Estimation Procedure\\nGeneralized least squares is like weighted least squares but uses a covariance\\nmatrix that is not diagonal. Each subject can have her own shape of Vidue\\nto each subject being measured at a diﬀerent set of times. This is a maximum\\nlikelihood procedure. Newton-Raphson or other trial-and-error methods are\\nused for estimating parameters. For a small number of subjects, there are ad-\\nvantages in using REML (restrictedmaximum likelihood) instead of ordinary\\nMLE [159, Section 5.3] [ 509, Chapter 5]221(especially to get a more unbiased\\nestimate of the covariance matrix).\\nWhenimbalancesofmeasurementtimesarenotsevere,OLSﬁttedignoring\\nsubject identiﬁers may be eﬃcient for estimating β. But OLS standard errors\\nwill be too small as they don’t take intra-cluster correlation into account.\\nThis may be rectiﬁed by substituting a covariance matrix estimated using\\nthe Huber-White cluster sandwich estimator or from the cluster bootstrap.\\nWhen imbalances are severe and intra-subject correlations are strong, OLS\\n(or GEE using a working independence model) is not expected to be eﬃcient\\nbecause it gives equal weight to each observation; a subject contributing two\\ndistant observations receives1\\n5the weight of a subject having 10 tightly-\\nspaced observations.\\n7.5 Common Correlation Structures\\nWe usually restrictourselves to isotropic correlationstructures which assume\\nthecorrelationbetweenresponseswithinsubjectattwotimesdependsonlyon\\na measure of the distance between the two times, not the individual times.\\nWe simplify further and assume it depends on |t1−t2|c. Assume that the\\ncorrelation coeﬃcient for Yit1vs.Yit2conditional on baseline covariates Xi\\nfor subject iish(|t1−t2|,ρ), where ρis a vector (usually a scalar) set of\\nfundamental correlation parameters. Some commonly used structures when\\ncWe can speak interchangeably of correlations of residuals within subjects or correla-\\ntions between responses measured at diﬀerent times on the same subject, conditional\\non covariates X.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e870e728-afef-4491-883c-fcb9646388d7', embedding=None, metadata={'page_label': '148', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='148 7 Modeling Longitudinal Responses using Generalized Least Squares\\ntimes arecontinuousandarenot equallyspaced[ 509,Section 5.3.3]areshown\\nbelow, along with the correlation function names from the Rnlmepackage.\\nCompound symmetry: h=ρift1̸=t2,1i ft1=t2 nlme corCompSymm\\n(Essentially what two-way ANOVA assumes)\\nAutoregressive-moving average lag 1: h=ρ|t1−t2|=ρscorCAR1\\nwheres=|t1−t2|\\nExponential: h=e x p (−s/ρ) corExp\\nGaussian: h=e x p [−(s/ρ)2] corGaus\\nLinear: h=( 1−s/ρ)[s<ρ] corLin\\nRational quadratic: h=1−(s/ρ)2/[1+(s/ρ)2] corRatio\\nSpherical: h=[ 1−1.5(s/ρ)+0.5(s/ρ)3][s<ρ] corSpher\\nLinear exponent AR(1): h=ρdmin+δs−dmin\\ndmax−dmin,1i ft1=t2572\\nThe structures 3–7 use ρas a scaling parameter, not as something re-\\nstricted to be in [0 ,1]\\n7.6 Checking Model Fit\\nThe constant variance assumption may be checked using typical residual\\nplots. The univariate normality assumption (but not multivariate normal-\\nity) may be checked using typical Q-Q plots on residuals. For checking the\\ncorrelation pattern, a variogram is a very helpful device based on estimating\\ncorrelations of all possible pairs of residuals at diﬀerent time pointsd.P a i r s\\nof estimates obtained at the same absolute time diﬀerence sare pooled. The\\nvariogramis a plot with y=1−ˆh(s,ρ)v s .sonthex-axis, and the theoretical\\nvariogramof the correlationmodel currently being assumed is superimposed.\\n7.7 Sample Size Considerations\\nSection4.4provided some guidance about sample sizes needed for OLS.\\nA goodwayto think about sample size adequacy for generalizedleastsquares\\nis to determine the eﬀective number of independent observations that a given\\nconﬁguration of repeated measurements has. For example, if the standard er-\\nrorofanestimate fromthree measurementsoneachof20subjectsis the same\\nas the standard error from 27 subjects measured once, we say that the 20 ×3\\nstudy has an eﬀective sample size of 27, and we equate power from the uni-\\nvariate analysis on nsubjects measured once to20n\\n27subjects measured three\\ntimes. Faes et al.181have a nice approach to eﬀective sample sizes with a\\nvariety of correlation patterns in longitudinal data. For an AR(1) correlation\\nstructure with nequally spaced measurement times on each of Nsubjects,\\ndVariograms can be unstable.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d059f78b-67cd-4b07-9e0f-15727b687120', embedding=None, metadata={'page_label': '149', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.9 Case Study 149\\nwith the correlation between two consecutive times being ρ, the eﬀective\\nsample size isn−(n−2)ρ\\n1+ρN. Under compound symmetry, the eﬀective size is\\nnN\\n1+ρ(n−1).\\n7.8RSoftware\\nThe nonlinear mixed eﬀects model package nlmeof Pinheiro & Bates in\\nRprovides many useful functions. For ﬁtting linear models, ﬁtting functions\\narelmefor mixed eﬀects models and glsfor generalized least squares without\\nrandom eﬀects. The rmspackage has a front-end function Glsso that many\\nfeatures of rmscan be used:\\nanova: all partial Wald tests, test of linearity, pooled tests\\nsummary: eﬀect estimates (diﬀerences in ˆY) and conﬁdence limits\\nPredictandplot: partial eﬀect plots\\nnomogram: nomogram\\nFunction: generate Rfunction code for the ﬁtted model\\nlatex:LATEX representation of the ﬁtted model.\\nIn addition, Glshas a cluster bootstrap option (hence you do not use rms’s\\nbootcovforGlsﬁts). When Bis provided to Gls( ), bootstrapped regression\\ncoeﬃcients and correlation estimates are saved, the former setting up for\\nbootstrap percentile conﬁdence limitseThenlmepackage has many graphics\\nand ﬁt-checking functions. Several functions will be demonstrated in the case\\nstudy.\\n7.9 Case Study\\nConsider the dataset in Table 6.9 of Davis [ 148, pp. 161–163] from a multi-\\ncenter, randomized controlled trial of botulinum toxin type B (BotB) in pa-\\ntients with cervical dystonia from nine U.S. sites. Patients were randomized\\nto placebo ( N= 36), 5000 units of BotB ( N= 36), or 10,000 units of BotB\\n(N= 37). The response variable is the total score on the Toronto Western\\nSpasmodicTorticollisRatingScale(TWSTRS), measuringseverity,pain, and\\ndisabilityofcervicaldystonia(highscoresmeanmoreimpairment).TWSTRS\\nis measured at baseline (week 0) and weeks 2, 4, 8, 12, 16 after treatment\\nbegan. The dataset name on the dataset wiki page is cdystonia .\\neTo access regular glsfunctions named anova(for likelihood ratio tests, AIC, etc.)\\norsummary useanova.gls orsummary.gls .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e20881fd-fd13-4eb9-9072-deecf41f9859', embedding=None, metadata={'page_label': '150', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='150 7 Modeling Longitudinal Responses using Generalized Least Squares\\n7.9.1 Graphical Exploration of Data\\nGraphics which follow display raw data as well as quartiles of TWSTRS by\\ntime, site, and treatment. A table shows the realized measurement schedule.\\nrequire(rms)\\ngetHdata(cdystonia)\\nattach( cdystonia)\\n# Construct unique subject ID\\nuid←with(cdystonia, factor(paste( site, id)))\\n# Tabulate patterns of subjects \\'timepoints\\ntable(tapply( week, uid,\\nfunction(w) paste(sort(unique(w)), collapse= \\'\\')))\\n0 024 0241 21 6 0248 02481 2\\n11311\\n02481 21 6 02481 6 0281 21 6 0481 21 6 0481 6\\n941241\\n# Plot raw data,superposing subjects\\nxl←xlab( \\'Week \\'); yl ←ylab( \\'TWSTRS-total score \\')\\nggplot(cdystonia, aes(x= week, y= twstrs, color=factor(id))) +\\ngeom_line() + xl + yl + facet_grid( treat∼site) +\\nguides(color=FALSE) # Fig. 7.1\\n# Show quartiles\\nggplot(cdystonia, aes(x= week, y= twstrs)) + xl + yl +\\nylim(0, 70) + stat_summary(fun.data=\"median_hilow\",\\nconf.int=0.5, geom= \\'smooth \\')+\\nfacet_wrap( ∼treat, nrow=2) # Fig. 7.2\\nNext the data are rearranged so that Yi0is a baseline covariate.\\nbaseline ←subset( data.frame( cdystonia,uid), week == 0,\\n-week)\\nbaseline ←upData(baseline, rename=c(twstrs= \\'twstrs0 \\'),\\nprint=FALSE)\\nfollowup ←subset( data.frame( cdystonia,uid), week > 0,\\nc(uid,week, twstrs))\\nrm(uid)\\nboth ←merge(baseline, followup, by= \\'uid\\')\\ndd ←datadist( both)\\noptions(datadist= \\'dd\\')', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='831c8454-0c0c-48c7-b795-52d36c465b9d', embedding=None, metadata={'page_label': '151', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"7.9 Case Study 151\\n1 2 3 4 5 6 7 8 9\\n204060\\n204060\\n20406010000U 5000U Placebo\\n0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15 05 1 0 1 5 0 5 10 15 0 5 10 15 0 5 10 15 0 5 10 15\\nWeekTWSTRS−total score\\nFig. 7.1 Time proﬁles for individual subjects, stratiﬁed by study site and dose\\n7.9.2 Using Generalized Least Squares\\nWe stay with baseline adjustment and use a variety of correlation structures,\\nwith constant variance. Time is modeled as a restricted cubic spline with\\n3 knots, because there are only 3 unique interior values of week.B e l o w ,s i x\\ncorrelation patterns are attempted. In general it is better to use scientiﬁc\\nknowledge to guide the choice of the correlation structure.\\nrequire( nlme)\\ncp←list(corCAR1,corExp,corCompSymm,c orLin,corGaus, corSpher)\\nz←vector( 'list ',length(cp))\\nfor(k in 1:length(cp)) {\\nz[[k]] ←gls(twstrs ∼treat * rcs( week, 3) +\\nrcs(twstrs0, 3) + rcs(age, 4) * sex, data=both,\\ncorrelation=cp[[k]](form = ∼week | uid))\\n}\\nanova(z[[1]],z[[2]],z[[3]],z[[4]],z[[5]],z[[6]])\\nModel df AIC BIC logLik\\nz[[1]] 1 20 3553.906 3638.357 -1756.953\\nz[[2]] 2 20 3553.906 3638.357 -1756.953\\nz[[3]] 3 20 3587.974 3672.426 -1773.987\\nz[[4]] 4 20 3575.079 3659.531 -1767.540\\nz[[5]] 5 20 3621.081 3705.532 -1790.540\\nz[[6]] 6 20 3570.958 3655.409 -1765.479\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be149232-4119-424b-9a63-d17ed1584878', embedding=None, metadata={'page_label': '152', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='152 7 Modeling Longitudinal Responses using Generalized Least Squares\\n10000U 5000U\\nPlacebo0204060\\n0204060\\n0 5 10 15\\nWeekTWSTRS−total score\\nFig. 7.2 Quartiles of TWSTRSstratiﬁed by dose\\nAIC computed above is set up so that smaller values are best. From this\\nthe continuous-time AR1 and exponential structures are tied for the best.\\nFor the remainder of the analysis we use corCAR1,u s i n gGls. 3\\na←Gls(twstrs ∼treat * rcs( week, 3) + rcs( twstrs0, 3) +\\nrcs(age, 4) * sex, data= both,\\ncorrelation=corCAR1( form=∼week | uid))\\nprint(a, latex=TRUE)\\nGeneralized Least Squares Fit by REML\\nGls(model = twstrs ~ treat * rcs(week, 3) + rcs(twstrs0, 3) +\\nrcs(age, 4) * sex, data = both, correlation = corCAR1\\n(form = ~week | uid))\\nObs 522 Log-restricted-likelihood -1756.95\\nClusters 108 Model d.f. 17\\ng11.334σ 8.5917\\nd.f. 504', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ac2aa08-b283-450a-a60e-9176abc77f34', embedding=None, metadata={'page_label': '153', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.9 Case Study 153\\nCoef S.E. tPr(>|t|)\\nIntercept -0.3093 11.8804 -0.03 0.9792\\ntreat=5000U 0.4344 2.5962 0.17 0.8672\\ntreat=Placebo 7.1433 2.6133 2.73 0.0065\\nweek 0.2879 0.2973 0.97 0.3334\\nweek’ 0.7313 0.3078 2.38 0.0179\\ntwstrs0 0.8071 0.1449 5.57 <0.0001\\ntwstrs0’ 0.2129 0.1795 1.19 0.2360\\nage -0.1178 0.2346 -0.50 0.6158\\nage’ 0.6968 0.6484 1.07 0.2830\\nage” -3.4018 2.5599 -1.33 0.1845\\nsex=M 24.2802 18.6208 1.30 0.1929\\ntreat=5000U * week 0.0745 0.4221 0.18 0.8599\\ntreat=Placebo * week -0.1256 0.4243 -0.30 0.7674\\ntreat=5000U * week’ -0.4389 0.4363 -1.01 0.3149\\ntreat=Placebo * week’ -0 .6459 0.4381 -1.47 0.1411\\nage * sex=M -0.5846 0.4447 -1.31 0.1892\\nage’ * sex=M 1.4652 1.2388 1.18 0.2375\\nage”* sex=M -4.0338 4.8123 -0.84 0.4023\\nCorrelation Structure: Continuous AR(1)\\nFormula: ~week | uid\\nParameter estimate(s):\\nPhi\\n0.8666689\\nˆρ=0.867, the estimate of the correlation between two measurements\\ntaken one week apart on the same subject. The estimated correlation for\\nmeasurements 10 weeks apart is 0 .86710=0.24.\\nv←Variogram(a, form=∼week | uid)\\nplot(v) # Figure 7.3\\nThe empirical variogramis largely in agreement with the pattern dictated by\\nAR(1).\\nNext check constant variance and normality assumptions.\\nboth$resid ←r←resid(a); both$fitted ←fitted(a)\\nyl←ylab( \\'Residuals \\')\\np1←ggplot( both, aes(x=fitted, y= resid)) + geom_point() +\\nfacet_grid( ∼treat) + yl\\np2←ggplot( both, aes(x=twstrs0, y= resid)) + geom_point()+yl\\np3←ggplot( both, aes(x=week, y= resid)) + yl + ylim(-20,20) +\\nstat_summary( fun.data=\"mean_sdl\", geom= \\'smooth \\')\\np4←ggplot( both, aes( sample=resid)) + stat_qq() +\\ngeom_abline(intercept=mean (r),slope=sd(r)) + yl\\ngridExtra:: grid.arrange(p1, p2, p3, p4, ncol=2) # Figure 7.4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b499e11-66b6-4dbf-90e1-2badef54d23a', embedding=None, metadata={'page_label': '154', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"154 7 Modeling Longitudinal Responses using Generalized Least Squares\\nDistanceSemivariogram\\n0.20.40.6\\n2 4 6 8 10 12 14\\nFig. 7.3 Variogram, with assumed correlation pattern superimposed\\nThese model assumptions appear to be well satisﬁed, so inferences are likely\\nto be trustworthy if the more subtle multivariate assumptions hold.\\nNow get hypothesis tests, estimates, and graphically interpret the model.\\nplot(anova(a)) # Figure 7.5\\nylm←ylim(25, 60)\\np1←ggplot( Predict(a, week, treat, conf.int= FALSE),\\nadj.subtitle= FALSE, legend.position= 'top') + ylm\\np2←ggplot( Predict(a, twstrs0), adj.subtitle= FALSE) + ylm\\np3←ggplot( Predict(a, age, sex), adj.subtitle= FALSE,\\nlegend.position= 'top') + ylm\\ngridExtra::grid.arrange(p1, p2, p3, ncol=2) # Figure 7.6\\nlatex(summary(a),file= '', table.env=FALSE) # Shows for week 8\\nLow High ΔEﬀect S.E. Lower 0.95 Upper 0.95\\nweek 4 12 8 6.69100 1.10570 4.5238 8.8582\\ntwstrs0 39 53 14 13.55100 0.88618 11.8140 15.2880\\nage 46 65 19 2.50270 2.05140 -1.5179 6.5234\\ntreat — 5000U:10000U 1 2 0.59167 1.99830 -3.3249 4.5083\\ntreat — Placebo:10000U 1 3 5.49300 2.00430 1.5647 9.4212\\nsex — M:F 1 2 -1.08500 1.77860 -4.5711 2.4011\\n# To get results for week 8 for a different reference group\\n# for treatment, use e.g.summary(a, week=4, treat= 'Placebo ')\\n# Compare low dose with placebo, separately at each time\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d7cb971c-c3a6-4bfb-852c-098447fcbfd1', embedding=None, metadata={'page_label': '155', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.9 Case Study 155\\n10000U 5000U Placebo\\n−40−20020\\n20 30 40 50 60 7020 30 40 50 60 7020 30 40 50 60 70\\nfittedResiduals\\n−40−20020\\n30 40 50 60\\ntwstrs0Residuals\\n−20−1001020\\n4 8 12 16\\nweekResiduals\\n−40−20020\\n−2 0 2\\ntheoreticalResiduals\\nFig. 7.4 Three residual plots to check for absence of trends in central tendency\\nand in variability. Upper right panel shows the baseline score on the x-axis. Bottom\\nleft panel shows the mean ±2×SD. Bottom right panel is the QQ plot for checking\\nnormality of residuals from the GLS ﬁt.\\nsex\\nage * sex\\nage\\ntreat * week\\ntreat\\nweek\\ntwstrs0\\n0 50 100 150 200\\nχ2− df\\nFig. 7.5 Results of anovafrom generalized least squares ﬁt with continuous time\\nAR1 correlation structure. As expected, the baseline version of Ydominates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2830da9a-a5ac-40e3-b12f-3084eb91d1d4', embedding=None, metadata={'page_label': '156', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"156 7 Modeling Longitudinal Responses using Generalized Least Squares\\n36404448\\n4 8 12 16\\nWeekXβ^Treatment 10000U 5000U Placebo\\n10203040506070\\n30 40 50 60\\nTWSTRS−total scoreXβ^\\n253035404550\\n40 50 60 70 80\\nAge,yearsXβ^Sex FM\\nFig. 7.6 Estimated eﬀects of time, baseline TWSTRS, age, and sex\\nk1←contrast(a, list(week=c(2,4,8,12,16), treat= '5000U '),\\nlist(week=c(2,4,8,12,16), treat= 'Placebo '))\\noptions( width=80)\\nprint(k1, digits=3)\\nweek twstrs0 age sex Contrast S.E.Lower Upper Z Pr(>|z|)\\n1 2 46 56 F -6.31 2.10 -10.43 -2.186 -3.00 0.0027\\n2 4 46 56 F -5.91 1.82-9.47-2.349 -3.25 0.0011\\n3 8 46 56 F -4.90 2.01-8.85-0.953 -2.43 0.0150\\n4* 12 46 56 F -3.07 1.75-6.49 0.361-1.75 0.0795\\n5* 16 46 56 F -1.02 2.10-5.14 3.092-0.49 0.6260\\nRedundant contrasts are denoted by *\\nConfidence intervals are 0.95individual intervals\\n# Compare high dose with placebo\\nk2←contrast(a, list(week=c(2,4,8,12,16), treat= '10000U '),\\nlist(week=c(2,4,8,12,16), treat= 'Placebo '))\\nprint(k2, digits=3)\\nweek twstrs0 age sex Contrast S.E.Lower Upper Z Pr(>|z|)\\n1 2 46 56 F -6.89 2.07 -10.96 -2.83 -3.32 0.0009\\n2 4 46 56 F -6.64 1.79 -10.15 -3.13 -3.70 0.0002\\n3 8 46 56 F -5.49 2.00-9.42 -1.56 -2.74 0.0061\\n4* 12 46 56 F -1.76 1.74-5.17 1.65-1.01 0.3109\\n5* 16 46 56 F 2.62 2.09 -1.47 6.71 1.25 0.2099\\nRedundant contrasts are denoted by *\\nConfidence intervals are 0.95individual intervals\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='feeecd4a-6001-476d-808d-e499104c8f57', embedding=None, metadata={'page_label': '157', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"7.9 Case Study 157\\nk1←as.data.frame(k1[c( 'week ','Contrast ','Lower ',\\n'Upper ')])\\np1←ggplot(k1, aes(x= week, y= Contrast)) + geom_point() +\\ngeom_line() + ylab( 'Low Dose - Placebo ')+\\ngeom_errorbar(aes( ymin=Lower, ymax=Upper), width=0)\\nk2←as.data.frame(k2[c( 'week ','Contrast ','Lower ',\\n'Upper ')])\\np2←ggplot(k2, aes(x= week, y= Contrast)) + geom_point() +\\ngeom_line() + ylab( 'High Dose - Placebo ')+\\ngeom_errorbar(aes( ymin=Lower, ymax=Upper), width=0)\\ngridExtra::grid.arrange(p1, p2, ncol=2) # Figure 7.7\\n−8−40\\n4 8 12 16\\nweekLow Dose − Placebo\\n−8−404\\n4 8 12 16\\nweekHigh Dose − Placebo\\nFig. 7.7 Contrasts and 0.95 conﬁdence limits from GLS ﬁt\\nAlthough multiple d.f. tests such as total treatment eﬀects or treatment\\n×time interaction tests are comprehensive, their increased degrees of free-\\ndom can dilute power. In a treatment comparison, treatment contrasts at\\nthe last time point (single d.f. tests) are often of major interest. Such con-\\ntrasts are informed by all the measurements made by all subjects (up until\\ndropout times) when a smooth time trend is assumed. They use appropriate\\nextrapolation past dropout times based on observed trajectories of subjects\\nfollowed the entire observation period. In agreement with the top left panel\\nof Figure 7.6, Figure 7.7shows that the treatment, despite causing an early\\nimprovement, wears oﬀ by 16 weeks at which time no beneﬁt is seen.\\nA nomogram can be used to obtain predicted values, as well as to better\\nunderstand the model, just as with a univariate Y.\\nn←nomogram(a, age=c(seq(20, 80, by =10),85))\\nplot(n, cex.axis=.55, cex.var=.8, lmgp=.25) # Figure 7.8\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='635f7a95-eccb-4243-862f-35f36b5dca2b', embedding=None, metadata={'page_label': '158', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='158 7 Modeling Longitudinal Responses using Generalized Least Squares\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 00\\nTWSTRS−total score\\n20 25 30 35 40 45 50 55 60 65 70\\nage (sex=F)\\n40 2050 60\\n85 70\\nage (sex=M)\\n50 40 30 2060 70\\n85\\ntreat (week=2)\\n10000U Placebo5000U\\ntreat (week=4)\\n10000U Placebo5000U\\ntreat (week=8)\\n10000U Placebo5000U\\ntreat (week=12)\\n5000U10000U\\ntreat (week=16)\\n5000UPlacebo\\nTotal Points\\n0 10 20 30 40 50 60 70 80 90 100 110 120 130 140\\nLinear Predictor\\n15 20 25 30 35 40 45 50 55 60 65 70\\nFig. 7.8 Nomogram from GLS ﬁt. Second axis is the baseline score.\\n7.10 Further Reading\\n1Jim Rochon (Rho, Inc., Chapel Hill NC) has the following comments about\\nusing the baseline measurement of Yas the ﬁrst longitudinal response.\\nFor RCTs [randomized clinical trials], I draw a sharp line at the point\\nwhen the intervention begins. The LHS [left hand side of the model equa-\\ntion] is reserved for something that is a response to treatment. Anything\\nbefore this point can potentially be included as a covariate in the regres-\\nsion model. This includes the “baseline” value of the outcome variable.\\nIndeed, the best predictor of the outcome at the end of the study is typ-\\nically where the patient began at the beginning. It drinks up a lot of\\nvariability in the outcome; and, the eﬀect of other covariates is typically\\nmediated through this variable.\\nI treat anything after the intervention begins as an outcome. In the west-\\nern scientiﬁc method, an“eﬀect”must follow the“cause”even if by a split\\nsecond.\\nNote that an RCT is diﬀerent than a cohort study. In a cohort study,\\n“Time 0” is not terribly meaningful. If we want to model, say, the trend\\nover time, it would be legitimate, in my view, to include the “baseline”\\nvalue on the LHS of that regression model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='119ba85c-5068-440f-85b1-bce28f31786c', embedding=None, metadata={'page_label': '159', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='7.10 Further Reading 159\\nNow, even if the intervention, e.g., surgery, has an immediate eﬀect, I\\nwould include still reserve the LHS for anything that might legitimately\\nbe considered as the response to the intervention. So, if we cleared a\\nblocked artery and then measured the MABP, then that would still be\\nincluded on the LHS.\\nNow, it could well be that most of the therapeutic eﬀect occurred by\\nthe time that the ﬁrst repeated measure was taken, and then levels oﬀ.\\nThen, a plot of the means would essentially be two parallel lines and the\\ntreatment eﬀect is the distance between the lines, i.e., the diﬀerence in\\nthe intercepts.\\nIf the linear trend from baseline to Time 1 continues beyond Time 1, then\\nthe lines will have a common intercept but the slopes will diverge. Then,\\nthe treatment eﬀect will the diﬀerence in slopes.\\nOne point to remember is that the estimated intercept is the valueat time\\n0 that we predict from the set of repeated measures post randomization.\\nIn the ﬁrst case above, the model will predict diﬀerent intercepts even\\nthough randomization would suggest that they would start from the same\\nplace. This is because we were asleep at the switch and didn’t record the\\n“action”from baselineto time 1. In the second case, the model will predict\\nthe same intercept values because the linear trend from baseline to time\\n1 was continued thereafter.\\nMore importantly, there are considerable beneﬁts to including it as a co-\\nvariate on the RHS. The baseline value tends to be the best predictor of\\nthe outcome post-randomization, and this maneuver increases the preci-\\nsion of the estimated treatment eﬀect. Additionally, any other prognostic\\nfactors correlated with the outcome variable will also be correlated with\\nthe baseline value of that outcome, and this has two important conse-\\nquences. First, this greatly reduces the need to enter a large number of\\nprognostic factors as covariates in the linear models.Their eﬀect is already\\nmediated through the baseline value of the outcome variable. Secondly,\\nany imbalances across the treatment arms in important prognostic factors\\nwill induce an imbalance across the treatment arms in the baseline value\\nof the outcome. Including the baseline value thereby reduces the need to\\nenter these variables as covariates in the linear models.\\nStephen Senn563states that temporally and logically, a “baseline cannot be\\naresponse to treatment”, so baseline and response cannot be modeled in an\\nintegrated framework.\\n...one should focus clearly on ‘outcomes’ as being the only values that\\ncan be inﬂuenced by treatment and examine critically any schemes that\\nassume that these are linked in some rigid and deterministic view to\\n‘baseline’ values. An alternative tradition sees a baseline as being merely\\none of a number of measurements capable of improving predictions of\\noutcomes and models it in this way.\\nThe ﬁnal reason that baseline cannot be modeled as the response at time zero is\\nthat many studies have inclusion/exclusion criteria that include cutoﬀs on the\\nbaseline variable yielding a truncated distribution. In general it is not appropri-\\nate to model the baseline with the same distributional shape as the follow-up', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='07924b24-2828-4ac2-b249-385794bae09d', embedding=None, metadata={'page_label': '160', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='160 7 Modeling Longitudinal Responses using Generalized Least Squares\\nmeasurements. Thus the approach recommended by Liang and Zeger405and\\nLiu et al.423are problematicf.\\n2Gardineretal.211compared severallongitudinaldata models,especiallywith re-\\ngard to assumptionsand howregression coeﬃcients areestimated. Peters etal.500\\nhave an empirical study conﬁrming that the“use all available data”approach of\\nlikelihood–based longitudinal models makes imputation of follow-up measure-\\nments unnecessary.\\n3Keselman et al.347did a simulation study to study the reliability of AIC for\\nselecting the correct covariance structure in repeated measurement models. In\\nchoosing from among 11 structures, AIC selected the correct structure 47% of\\nthe time. Gurka et al.247demonstrated that ﬁxed eﬀects in a mixed eﬀects\\nmodel can be biased, independent of sample size, when the speciﬁed covariate\\nmatrix is more restricted than the true one.\\nfIn addition to this, one of the paper’s conclusions that analysis of covariance is not\\nappropriate if the population means of the baseline variable are not identical in the\\ntreatment groups is arguable563.S e e346for a discussion of423.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2f059a20-57da-4721-b01d-62601f5181bd', embedding=None, metadata={'page_label': '161', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 8\\nCase Study in Data Reduction\\nRecallthattheaimofdatareductionistoreduce(withoutusingtheoutcome)\\nthe number of parameters needed in the outcome model. The following case\\nstudy illustrates these techniques:\\n1. redundancy analysis;\\n2. variable clustering;\\n3. data reduction using principal component analysis (PCA), sparse PCA,\\nand pretransformations;\\n4. restricted cubic spline ﬁtting using ordinary least squares, in the context\\nof scaling; and\\n5. scaling/variabletransformationsusingcanonicalvariatesandnonparamet-\\nric additive regression.\\n8.1 Data\\nConsiderthe 506-patientprostatec ancerdatasetfromByarandGreen.87The\\ndata are listed in [ 28, Table 46] and are available in ASCII form from StatLib\\n(lib.stat.cmu.edu )i nt h e Datasets area from this book’s Web page. These\\ndata were from a randomized trial comparing four treatments for stage 3\\nand 4 prostate cancer, with almost equal numbers of patients on placebo and\\neach of three doses of estrogen. Four pat ients had missing values on all of the\\nfollowing variables: wt, pf, hx, sbp, dbp, ekg, hg, bm ; two of these patients\\nwere also missing sz. These patients are excluded from consideration. The\\nultimate goal of an analysis of the dataset might be to discover patterns in\\nsurvival or to do an analysis of covariance to assess the eﬀect of treatment\\nwhile adjusting for patient heterogeneity. See Chapter 21for such analyses.\\nThe data reductions developed here are general and can be used for a variety\\nof dependent variables.\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 8161', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97db54cb-ca9b-43d7-8beb-d5dad39ebc90', embedding=None, metadata={'page_label': '162', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"162 8 Case Study in Data Reduction\\nThe variable names, labels, and a summary of the data are printed below.\\nrequire( Hmisc)\\ngetHdata(prostate) # Download and makeprostate accessible\\n# Convert an old dateformat to R format\\nprostate$ sdate ←as.Date(prostate$ sdate)\\nd←describe(prostate[2:17])\\nlatex(d, file= '')\\nprostate[2:17]\\n16 Variables 502 Observations\\nstage : Stage\\nn missing unique Info Mean\\n502 0 2 0.73 3.424\\n3 (289, 58%), 4 (213, 42%)\\nrxn missing unique\\n502 0 4\\nplacebo (127, 25%), 0.2 mg estrogen (124, 25%)\\n1.0 mg estrogen (126, 25%), 5.0 mg estrogen (125, 25%)\\ndtime : Months of Follow-up\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n502 0 76 1 36.13 1.05 5.00 14.25 34.00 57.75 67.00 71.00\\nlowest : 0 1 2 3 4, highest: 72 73 74 75 76\\nstatusn missing unique\\n502 0 10\\nalive (148, 29%), dead - prostatic ca (130, 26%)\\ndead - heart or vascular (96, 19%), dead - cerebrovascular (31, 6%)\\ndead - pulmonary embolus (14, 3%), dead - other ca (25, 5%)\\ndead - respiratory disease (16, 3%)\\ndead - other specific non-ca (28, 6%), dead - unspecified non-ca (7, 1%)\\ndead - unknown cause (7, 1%)\\nage : Age in Years\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n501 1 41 1 71.46 56 60 70 73 76 78 80\\nlowest : 48 49 50 51 52, highest: 84 85 87 88 89\\nwt : Weight Index = wt(kg)-ht(cm)+200\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n500 2 67 1 99.03 77.95 82.90 90.00 98.00 107.00 116.00 123.00\\nlowest : 69 71 72 73 74, highest: 136 142 145 150 152\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='62e0fa7a-876c-44e7-ba21-fcdae0ecb0d9', embedding=None, metadata={'page_label': '163', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.1 Data 163\\npf\\nn missing unique\\n502 0 4\\nnormal activity (450, 90%), in bed < 50% daytime (37, 7%)\\nin bed > 50% daytime (13, 3%), confined to bed (2, 0%)\\nhx : History of Cardiovascular Disease\\nn missing unique Info Sum Mean\\n502 0 2 0.73 213 0.4243\\nsbp : Systolic Blood Pressure/10\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n502 0 18 0.98 14.35 11 12 13 14 16 17 18\\n891 01 11 21 31 41 51 61 71 81 92 02 12 22 32 43 0\\nFrequency 1 3 14 27 65 74 98 74 72 34 17 12 3 2 3 1 1 1\\n% 01 3 51 31 52 01 51 4 7 3 2 1 0 1 0 0 0\\ndbp : Diastolic Blood Pressure/10\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n502 0 12 0.95 8.149 6 6 7 8 9 10 10\\n45 6 7 8 91 01 11 21 31 41 8\\nFrequency 4 5 43 107 165 94 66 9 5 2 1 1\\n% 1 1 9 21 33 19 13 2 1 0 0 0\\nekg\\nn missing unique\\n494 8 7\\nnormal (168, 34%), benign (23, 5%)\\nrhythmic disturb & electrolyte ch (51, 10%)\\nheart block or conduction def (26, 5%), heart strain (150, 30%)\\nold MI (75, 15%), recent MI (1, 0%)\\nhg : Serum Hemoglobin (g/100ml)\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n502 0 91 1 13.45 10.2 10.7 12.3 13.7 14.7 15.8 16.4\\nlowest : 5.899 7.000 7.199 7.800 8.199\\nhighest: 17.297 17.500 17.598 18.199 21.199\\nsz: Size of Primary Tumor (cm2)\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n497 5 55 1 14.63 2.0 3.0 5.0 11.0 21.0 32.0 39.2\\nlowest : 0 1 2 3 4, highest: 54 55 61 62 69\\nsg : Combined Index of Stage and Hist. Grade\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n491 11 11 0.96 10.31 8 8 9 10 11 13 13\\n5 6 7 8 9 10 11 12 13 14 15\\nFrequency 3876 71 3 73 31 1 42 67 5 51 6\\n% 1211 4 2 8 7 2 3 51 5 1 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='885def39-5d3f-4242-bfac-3f9d0faaf12c', embedding=None, metadata={'page_label': '164', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='164 8 Case Study in Data Reduction\\nap : Serum Prostatic Acid Phosphatase\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n502 0 128 1 12.18 0.300 0.300 0.500 0.700 2.975 21.689 38.470\\nlowest : 0.09999 0.19998 0.29999 0.39996 0.50000\\nhighest: 316.00000 353.50000 367.00000 596.00000 999.87500\\nbm : Bone Metastases\\nn missing unique Info Sum Mean\\n502 0 2 0.41 82 0.1633\\nstageis deﬁned by apas well as X-ray results. Of the patients in stage 3,\\n0.92 have ap≤0.8. Of those in stage 4, 0.93 have ap>0.8. Since stagecan\\nbe predicted almost certainly from ap, we do not consider stagein some of\\nthe analyses.\\n8.2 How Many Parameters Can Be Estimated?\\nThere are354 deaths among the 502 patients.If predicting survivaltime were\\nof major interest, we could develop a reliable model if no more than about\\n354/15 = 24 parameters were examined againstYin unpenalized modeling.\\nSuppose that a full model with no interactions is ﬁtted and that linearity is\\nnot assumed for any continuous predictors. Assuming ageis almost linear,\\nwe could ﬁt a restricted cubic spline function with three knots. For the other\\ncontinuous variables, let us use ﬁve knots. For categorical predictors, the\\nmaximum number of degrees of freedom needed would be one fewer than\\nthe number of categories. For pfwe could lump the last two categories since\\nthe last category has only 2 patients. Likewise, we could combine the last\\ntwo levels of ekg.T a b l e8.1lists the candidate predictors with the maximum\\nnumber of parameters we consider for each.\\nTable 8.1 Degrees of freedom needed for predictors\\nPredictor: rx age wt pf hx sbp dbp ekg hg sz sg ap bm\\n# P a r a m e t e r s :324214 4 544441\\n8.3 Redundancy Analysis\\nAs described in Section 4.7.1, it is occasionally useful to do a rigorous re-\\ndundancy analysis on a set of potential predictors. Let us run the algorithm\\ndiscussed there, on the set of predictors we are considering. We will use a low\\nthreshold (0.3) for R2for demonstration purposes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34c91c7e-1086-4f0b-9800-9aac6022a639', embedding=None, metadata={'page_label': '165', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.3 Redundancy Analysis 165\\n# Allow only 1 d.f. for three of the predictors\\nprostate ←\\ntransform(prostate,\\nekg.norm = 1*(ekg %in% c(\"normal\",\"benign\")),\\nrxn = as.numeric(rx),\\npfn = as.numeric(pf))\\n# Force pfn, rxn to be linear because of difficulty of placing\\n# knots with so many ties in the data\\n# Note: all incomplete cases are deleted (inefficient)\\nredun(∼stage + I(rxn) + age + wt + I(pfn) + hx +\\nsbp + dbp + ekg.norm + hg + sz + sg + ap + bm,\\nr2=.3, type= \\'adjusted \\', data=prostate)\\nRedundancy Analysis\\nredun(formula = ∼stage + I(rxn) + age + wt + I(pfn) + hx +\\nsbp + dbp + ekg.norm + hg + sz + sg + ap + bm,\\ndata = prostate, r2 = 0.3, type = \" adjusted\")\\nn: 483 p: 14 nk: 3\\nNumber of NAs: 19\\nFrequencies of Missing Values Due to Each Variable\\nstage I(rxn) age wt I(pfn) hx sbp\\ndbp\\n0012000\\n0\\nekg.norm hg sz sg ap bm\\n005 1 100\\nTransformation of target variables forced to be linear\\nR2cutoff: 0.3 Type: adjusted\\nR2with which each variable can be predicted from all other\\nvariables:\\nstage I(rxn) age wt I(pfn) hx sbp\\ndbp\\n0.658 0.000 0.073 0.111 0.156 0.062 0.452\\n0.417\\nekg.norm hg sz sg ap bm\\n0.055 0.146 0.192 0.540 0.147 0.391\\nRendundant variables:\\nstage sbp bm sg\\nPredicted fromvariables:\\nI(rxn) age wt I(pfn) hx dbp ekg.norm hg sz ap', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1643db3c-4134-4cef-a997-32e5895971fd', embedding=None, metadata={'page_label': '166', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='166 8 Case Study in Data Reduction\\nVariable Deleted R2R2after later deletions\\n1 stage 0.658 0.658 0.646 0.494\\n2 sbp 0.452 0.453 0.455\\n3 bm 0.374 0.367\\n4 sg 0.342\\nBy any reasonable criterion on R2, none of the predictors is redundant. stage\\ncan be predicted with an R2=0.658 from the other 13 variables, but only\\nwithR2=0.493after deletion of 3 variableslater declared to be“redundant.”\\n8.4 Variable Clustering\\nFrom Table 8.1, the total number of parameters is 42, so some data reduction\\nshould be considered. We resist the temptation to take the“easy way out”us-\\ning stepwise variable selection so that we can achieve a more stable modeling\\nprocess and obtain unbiased standard errors. Before using a variable cluster- 1\\ning procedure, note that apis extremely skewed. To handle skewness, we use\\nSpearman rank correlationsfor continuous variables (later we transformeach\\nvariable using transcan, which will allow ordinary correlation coeﬃcients to\\nbe used). After classifying ekgas“normal/benign”versus everything else, the\\nSpearman correlations are plotted below.\\nx←with(prostate,\\ncbind(stage, rx, age, wt, pf, hx, sbp, dbp,\\nekg.norm, hg, sz, sg, ap, bm))\\n# If no missing data,could use cor(apply(x, 2, rank))\\nr←rcorr(x, type=\" spearman \")$r # rcorr in Hmisc\\nmaxabsr ←max(abs(r[row(r) != col(r)]))\\np←nrow(r)\\nplot(c( -.35,p+.5),c(.5,p+.25), type= \\'n\\', axes= FALSE,\\nxlab= \\'\\',ylab= \\'\\')# Figure 8.1\\nv←dimnames(r )[[1]]\\ntext(rep(.5,p), 1:p, v, adj=1)\\nfor(i in 1:(p-1)) {\\nfor(j in (i+1):p) {\\nlines(c(i,i),c(j ,j+r[i,j]/ maxabsr/2),\\nlwd=3, lend= \\'butt \\')\\nlines(c( i-.2,i+.2),c(j,j), lwd=1, col= gray(.7))\\n}\\ntext(i, i, v[i], srt=-45, adj=0)\\n}\\nWe perform a hierarchical cluster analysis based on a similarity matrix\\nthat contains pairwise Hoeﬀding Dstatistics.295Dwill detect nonmonotonic\\nassociations.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd7cb339-a91b-46c2-96c2-c139b929ecc3', embedding=None, metadata={'page_label': '167', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"8.5 Transformation and Single Imputation Using transcan 167\\nvc←varclus( ∼stage + rxn + age + wt + pfn + hx +\\nsbp + dbp + ekg.norm + hg + sz + sg + ap + bm,\\nsim= 'hoeffding ', data= prostate)\\nplot(vc) # Figure 8.2\\nWe combine sbpanddbp, and tentatively combine ap, sg, sz ,a n dbm.\\n8.5 Transformation and Single Imputation Using\\ntranscan\\nNow we turn to the scoringofthe predictorsto potentially reducethe number\\nofregressionparametersthatareneededlaterbydoingawaywiththeneedfor\\nstagerxagewtpfhxsbpdbpekg.normhgszsgapbm\\nstagerxagewtpfhxsbpdbpekg.normhgszsgap\\nFig. 8.1 Matrix of Spearman ρrank correlation coeﬃcients between predictors. Hor-\\nizontal gray scale lines correspond to ρ= 0. The tallest bar corresponds to |ρ|=0.78.\\nnonlinear terms and multiple dummy variables. The RHmiscpackage transcan\\nfunction defaults to using a maximum generalized variance method368that\\nincorporates canonical variates to optimally transform both sides of a mul-\\ntiple regression model. Each predictor is treated in turn as a variable being\\npredicted, and all variables are expanded into restricted cubic splines (for\\ncontinuous variables) or dummy variables (for categorical ones).\\n# Combine 2 levels of ekg (one had freq. 1)\\nlevels( prostate$ekg)[ levels( prostate$ekg) %in%\\nc('old MI ','recent MI ')]←'MI'\\nprostate$pf.coded ←as.integer(prostate$pf)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f4466bb-b801-4d4f-a692-bd275eceecd3', embedding=None, metadata={'page_label': '168', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='168 8 Case Study in Data Reductionekg.norm\\nage\\nhx\\nrxn\\nsbp\\ndbp\\nwt\\nhg\\npfn\\nsz\\nbm\\nstage\\nsg\\nap0.140.120.100.080.060.040.02 0.0030 * Hoeffding D\\nFig. 8.2 Hierarchical clustering using Hoeﬀding’s Das a similarity measure. Dummy\\nvariables were used for the categorical variable ekg. Some of the dummy variables\\ncluster together since they are by deﬁnition negatively correlated.\\n# make a numeric version; combine last 2 levels of original\\nlevels( prostate$pf) ←levels( prostate$pf)[c(1,2,3 ,3)]\\nptrans ←\\ntranscan( ∼sz + sg + ap + sbp + dbp +\\nage + wt + hg + ekg + pf + bm + hx, imputed= TRUE,\\ntransformed= TRUE,trantab= TRUE, pl=FALSE,\\nshow.na= TRUE,data=prostate, frac=.1, pr=FALSE)\\nsummary( ptrans, digits=4)\\ntranscan(x = ∼sz + sg + ap + sbp + dbp + age + wt + hg + ekg +\\npf + bm + hx, imputed = TRUE, trantab = TRUE, transformed = TRUE,\\npr = FALSE, pl = FALSE, show.na = TRUE, data = prostate ,\\nfrac = 0.1)\\nIterations: 8\\nR2achieved in predicting each variable:\\nsz sg ap sbp dbp age wt hg ekg pf bm hx\\n0.207 0.556 0.573 0.498 0.485 0.095 0.122 0.158 0.092 0.113 0.349 0.108\\nAdjusted R2:\\nsz sg ap sbp dbp age wt hg ekg pf bm hx\\n0.180 0.541 0.559 0.481 0.468 0.065 0.093 0.129 0.059 0.086 0.331 0.083\\nCoefficients of canonical variates for predicting each (row) variable\\nsz sg ap sbp dbp age wt hg ekg pf bm\\nsz 0.66 0.20 0.33 0.33 −0.01−0.01 0.11 0.11 0.03 −0.36\\nsg 0.23 0.84 0.08 0.07 −0.02 0.01 −0.01−0.07 0.02 −0.20\\nap 0.07 0.80 −0.11−0.05 0.03 −0.02 0.01 0.01 0.00 −0.83\\nsbp 0.13 0.10 −0.14 −0.94 0.14 −0.09 0.03 0.10 0.10 −0.03\\ndbp 0.13 0.09 −0.06−0.98 0.14 0.07 0.05 0.03 0.04 0.03\\nage−0.02−0.06 0.18 0.58 0.57 0.14 0.46 0.43 −0.03 1.05\\nwt−0.02 0.06 −0.08−0.31 0.23 0.12 0.51 −0.06 0.21 −1.09\\nhg 0.13 −0.02 0.03 0.09 0.15 0.33 0.43 −0.02 0.24 −1.53\\nekg 0.20 −0.38 0.10 0.42 0.12 0.41 −0.04−0.04 0.15 −0.42\\npf 0.04 0.08 0.02 0.36 0.14 −0.03 0.22 0.29 0.13 −1.75\\nbm−0.02−0.03−0.13 0.00 0.00 0.03 −0.04−0.06−0.01−0.06', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3d6a0bb-675c-4bb3-a81e-4cf01942d90a', embedding=None, metadata={'page_label': '169', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.5 Transformation and Single Imputation Using transcan 169\\nhx 0.04 0.05 −0.01−0.04 0.00 −0.06 0.02 −0.01−0.09−0.04−0.05\\nhx\\nsz 0.34\\nsg 0.14\\nap−0.03\\nsbp−0.14\\ndbp−0.01\\nage−0.76\\nwt 0.27\\nhg−0.12\\nekg−1.23\\npf−0.46\\nbm−0.02\\nhx\\nSummary of imputed values\\nsz\\nn missing unique Info Mean\\n5 0 4 0.95 12.86\\n6 (2, 40%), 7.416 (1, 20%), 20.18 (1, 20%), 24.69 (1, 20%)\\nsg\\nn missing unique Info Mean .05 .10 .25 .50\\n11 0 10 1 10.1 6.900 7.289 7.697 10.270\\n.75 .90 .95\\n10.560 15.000 15.000\\n6.511 7.289 7.394 8 10.25 10.27 10.32 10.39 10.73 15\\nFrequency111 111111 2\\n% 999 999999 18\\nage\\nn missing unique Info Mean\\n1010 71.65\\nwt\\nn missing unique Info Mean\\n2021 97.77\\n91.24 (1, 50%), 104.3 (1, 50%)\\nekg\\nn missing unique Info Mean\\n804 0 .92.625\\n1 (3, 38%), 3 (3, 38%), 4 (1, 12%), 5 (1, 12%)\\nStarting estimates for imputed values:\\nsz sg ap sbp dbp age wt hg ekg pf bm hx\\n11.0 10.0 0.7 14.0 8.0 73.0 98.0 13.7 1.0 1.0 0.0 0.0\\nggplot(ptrans, scale=TRUE) +\\ntheme(axis.text.x=element_text( size=6)) # Figure 8.3\\nThe plotted output is shown in Figure 8.3. Note that at face value the trans-\\nformation of apwas derived in a circular manner, since the combined index\\nof stage and histologic grade, sg, uses in its stage component a cutoﬀ on ap.\\nHowever, if sgis omitted from consideration,the resulting transformationfor\\napdoes not change appreciably.Note that bmandhxare representedas binary\\nvariables, so their coeﬃcients in the ta ble of canonical variable coeﬃcients\\nare on a diﬀerent scale. For the variables that were actually transformed, the\\ncoeﬃcients are for standardized transformed vari ables (mean 0, variance 1).\\nFrom examining the R2s,age, wt, ekg, pf ,a n dhxare not strongly related\\nto other variables. Imputations for age, wt, ekg are thus relying more on the\\nmedian ormodalvaluesfromthemarginaldistributions.Fromthecoeﬃcients\\nof ﬁrst (standardized) canonical variates, sbpis predicted almost solely from\\ndbp;bmis predicted mainly from ap, hg,a n dpf. 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8627c015-3474-4fdf-b2fc-696f6ccb2e5f', embedding=None, metadata={'page_label': '170', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='170 8 Case Study in Data Reduction\\nR2=0.21   5 missing R2=0.56 11 missing R2=0.57R2=0.5\\nR2=0.49 R2=0.1   1 missingR2=0.12   2 missingR2=0.16\\nR2=0.09   8 missingR2=0.11R2=0.35 R2=0.11sz sg ap sbp\\ndbp age wt hg\\nekg pf bm hx0.000.250.500.751.00\\n0.000.250.500.751.00\\n0.000.250.500.751.0002 0 4 0 6 0 5.0 7.5 10.0 12.5 15.0 0 250 500 750 1000 10 15 20 25 30\\n4 8 12 16 50 60 70 80 90 80 100 120 140 10 15 20\\n246 1.0 1.5 2.0 2.5 3.0 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00Transformed\\nFig. 8.3 Simultaneous transformation and single imputation of all candidate predic-\\ntors using transcan . Imputed values are shown as red plus signs. Transformed values\\nare arbitrarily scaled to [0 ,1].\\n8.6 Data Reduction Using Principal Components\\nThe ﬁrst PC, PC 1, is the linear combination of standardized variables having\\nmaximum variance. PC 2is the linear combination of predictors having the\\nsecond largest variance such that PC 2is orthogonal to (uncorrelated with)\\nPC1.I ft h e r ea r e praw variables, the ﬁrst kPCs, where k<p, will explain\\nonly part of the variation in the whole system of pvariables unless one or\\nmore of the originalvariables is exactly a linear combinationof the remaining\\nvariables. Note that it is common to scale and center variables to have mean\\nzero and variance 1 before computing PCs.\\nThe response variable (here, time until death due to any cause) is not\\nexamined during data reduction, so that if PCs are selected by variance ex-\\nplained in the X-space and not by variation explained in Y, one needn’t\\ncorrect for model uncertainty or multiple comparisons.\\nPCA results in data reduction when the analyst uses only a subset of the\\nppossible PCs in predicting Y.T h i si sc a l l e d incomplete principal component\\nregression . When one sequentially enters PCs into a predictive model in a\\nstrict pre-speciﬁed order (i.e., by descending amounts of variance explained', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c40c9cb-54f3-4447-b109-ebc2045f6da1', embedding=None, metadata={'page_label': '171', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"8.6 Data Reduction Using Principal Components 171\\nfor the system of pvariables), model uncertainty requiring bootstrap adjust-\\nment is minimized. In contrast, model uncertainty associated with stepwise\\nregression (driven by associations with Y) is massive.\\nFor the prostate dataset, consider PCs on raw candidate predictors, ex-\\npanding polytomous factors using dummy variables. The Rfunction princomp\\nis used, after singly imputing missing raw values using transcan’s optimal\\nadditive nonlinear models. In this series of analyses we ignore the treatment\\nvariable, rx.\\n# Impute all missing values in all variables given to transcan\\nimputed ←impute(ptrans, data=prostate, list.out=TRUE)\\nImputed missing values with the following frequencies\\nand stored them in variables with their original names:\\nsz sg age wt ekg\\n5 11128\\nimputed ←as.data.frame(imputed)\\n# Compute principal components on imputed data.\\n# Create a design matrix from ekg categories\\nEkg←model.matrix( ∼ekg, data=imputed)[, -1]\\n# Use correlation matrix\\npfn←prostate$pfn\\nprin.raw ←princomp( ∼sz + sg + ap + sbp + dbp + age +\\nw t+h g+E k g+p f n+b m+h x ,\\ncor=TRUE, data=imputed)\\nplot(prin.raw, type= 'lines ', main= '', ylim=c(0,3)) #Figure 8.4\\n# Add cumulative fraction of variance explained\\naddscree ←function(x, npcs=min(10, length(x$sdev)),\\nplotv=FALSE,\\ncol=1, offset=.8, adj=0, pr=FALSE) {\\nvars←x$sdev∧2\\ncumv←cumsum(vars)/sum(vars)\\nif(pr) print(cumv)\\ntext(1: npcs,vars[1:npcs] + offset*par( 'cxy')[2],\\nas.character( round(cumv[1:npcs], 2)),\\nsrt=45, adj=adj, cex=.65, xpd=NA, col=col)\\nif(plotv) lines(1: npcs,vars[1:npcs], type= 'b', col=col)\\n}\\naddscree(prin.raw)\\nprin.trans ←princomp( ptrans$transfo rmed, cor= TRUE)\\naddscree( prin.trans, npcs=10, plotv= TRUE, col= 'red',\\noffset=-.8, adj=1)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7f0319f5-7bfd-4730-b37f-4dbd37e0d4f3', embedding=None, metadata={'page_label': '172', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='172 8 Case Study in Data ReductionVariances\\n0.00.51.01.52.02.53.0\\nComp.1 Comp.3 Comp.5 Comp.7 Comp.90.15\\n0.26\\n0.35\\n0.420.49 0.56 0.63 0.70.750.80.23\\n0.38\\n0.5\\n0.59\\n0.660.73 0.79 0.85 0.91\\n0.95\\nFig. 8.4 Variance of the system of raw predictors (black) explained by individual\\nprincipal components (lines) along with cumulative proportion of variance explained\\n(text), and variance explained by components computed on transcan -transformed\\nvariables (red)\\nThe resulting plot shown in Figure 8.4is called a“scree”plot [ 325, pp. 96–99,\\n104,106].Itshowsthe variationexplainedby theﬁrst kprincipalcomponents\\naskincreases all the way to 16 parameters (no data reduction). It requires\\n10 of the 16 possible components to explain >0.8 of the variance, and the\\nﬁrst 5 components explain 0 .49 of the variance of the system. Two of the 16\\ndimensions are almost totally redundant.\\nAfter repeating this process when transforming all predictors via transcan,\\nwe have only 12 degrees of freedom for the 12 predictors. The variance ex-\\np l a i n e di sd e p i c t e di nF i g u r e 8.4in red.It requiresat least9 ofthe 12 possible\\ncomponents to explain ≥0.9 of the variance, and the ﬁrst 5 components ex-\\nplain 0.66 of the variance as opposed to 0 .49 for untransformed variables.\\nLet us see how the PCs“explain”the times until death using the Cox re-\\ngression132function from rms,cph, described in Chapter 20. In what follows\\nwe vary the number of components used in the Cox models from 1 to all 16,\\ncomputing the AIC for each model. AIC is related to model log likelihood\\npenalized for number of parameters estimated, and lower is better. For refer-\\nence, the AIC of the model using all of the original predictors, and the AIC\\nof a full additive spline model are shown as horizontal lines.\\nrequire(rms)\\nS←with(prostate, Surv( dtime, status != \"alive\"))\\n# two-column response var.\\npcs←prin.raw$ scores # pick off all PCs\\naic←numeric (16)\\nfor(i in 1:16) {', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='48dbbb8b-7fdf-4a8e-a990-aad52dd98bc6', embedding=None, metadata={'page_label': '173', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"8.6 Data Reduction Using Principal Components 173\\nps←pcs[,1:i]\\naic[i] ←AIC(cph(S ∼ps))\\n}# Figure 8.5\\nplot(1:16, aic, xlab= 'Number of Components Used ',\\nylab= 'AIC', type= 'l', ylim=c(3950,4000))\\nf←cph(S∼sz + sg + log(ap) + sbp + dbp + age + wt + hg +\\nekg + pf + bm + hx, data= imputed)\\nabline(h=AIC(f), col= 'blue ')\\nf←cph(S∼rcs(sz,5) + rcs(sg,5) + rcs(log(ap),5) +\\nrcs(sbp,5) + rcs(dbp,5) + rcs(age,3) + rcs(wt,5) +\\nrcs(hg,5) + ekg + pf + bm + hx,\\ntol=1e-14, data=imputed)\\nabline(h=AIC(f), col= 'blue ', lty=2)\\nFor the money, the ﬁrst 5 components adequately summarizes all variables,\\nif linearly transformed, and the full linear model is no better than this. The\\nmodel allowing all continuous predictors to be nonlinear is not worth its\\nadded degrees of freedom.\\nNext check the performance of a model derived from cluster scores of\\ntransformed variables.\\n# Compute PC1 on a subset of transcan-transformed predictors\\npco←function(v) {\\nf←princomp( ptrans$ transformed[,v], cor= TRUE)\\nvars←f$sdev∧2\\ncat( 'Fraction of variance explained by PC1: ',\\nround(vars[1]/sum(vars),2), '\\\\n')\\nf$scores[,1]\\n}\\ntumor ←pco(c( 'sz','sg','ap','bm'))\\nFraction of variance explained by PC1: 0.59\\nbp ←pco(c( 'sbp','dbp'))\\nFraction of variance explained by PC1: 0.84\\ncardiac ←pco(c( 'hx','ekg'))\\nFraction of variance explained by PC1: 0.61\\n# Get transformed individual variables that are not clustered\\nother ←ptrans$transformed[,c( 'hg','age','pf','wt')]\\nf←cph(S∼tumor + bp + cardiac + other) # other is matrix\\nAIC(f)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b66232a-358b-4105-8250-efbbe132eb34', embedding=None, metadata={'page_label': '174', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"174 8 Case Study in Data Reduction\\n51 0 1 5395039603970398039904000\\nNumber of Components UsedAIC\\nFig. 8.5 AIC of Cox models ﬁtted with progressively more principal components.\\nThe solid blue line depicts the AIC of the model with all original covariates. The\\ndotted blue line is positioned at the AIC of the full spline model.\\n[1] 3954.393\\nprint(f, latex= TRUE,long=FALSE, title= '')\\nModel Tests Discrimination\\nIndexes\\nObs 502 LRχ281.11R20.149\\nEvents 354 d.f. 7 Dxy0.286\\nCenter 0 Pr(>χ2) 0.0000 g 0.562\\nScoreχ286.81gr1.755\\nPr(>χ2) 0.0000\\nCoef S.E. Wald ZPr(>|Z|)\\ntumor -0.1723 0.0367 -4.69 <0.0001\\nbp -0.0251 0.0424 -0.59 0.5528\\ncardiac -0.2513 0.0516 -4.87 <0.0001\\nhg -0.1407 0.0554 -2.54 0.0111\\nage -0.1034 0.0579 -1.79 0.0739\\npf -0.0933 0.0487 -1.92 0.0551\\nwt -0.0910 0.0555 -1.64 0.1012\\nThetumorandcardiacclusters seem to dominate prediction of mortality,\\nand the AIC of the model built from cluster scores of transformed variables\\ncompares favorably with other models (Figure 8.5).\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='41c1a95f-b89d-4636-a70b-ff94f5810a90', embedding=None, metadata={'page_label': '175', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"8.6 Data Reduction Using Principal Components 175\\n8.6.1 Sparse Principal Components\\nA disadvantage of principal components is that every predictor receives a\\nnonzero weight for every component, so many coeﬃcients are involved even\\nthrough the eﬀective degrees of freedom with respect to the response model\\narereduced. Sparse principal components672usesapenaltyfunctiontoreduce\\nthe magnitude of the loadings variables receive in the components. If an L1\\npenalty is used (as with the lasso), some loadings are shrunk to zero, result-\\ning in some simplicity. Sparse principal components combines some elements\\nof variable clustering, scoring of variables within clusters, and redundancy\\nanalysis.\\nFilzmoser, Fritz, and Kalcher191have written a nice Rpackage pcaPPfor\\ndoing sparse PC analysis.aThe following example uses the prostate data\\nagain. To allow for nonlinear transformations and to score the ekgvariable\\nin the prostate dataset down to a scalar, we use the transcan-transformed\\npredictors as inputs.\\nrequire( pcaPP)\\ns←sPCAgrid( ptrans$transformed, k=10, method= 'sd',\\ncenter= mean,scale=sd, scores= TRUE,\\nmaxiter =10)\\nplot(s, type= 'lines ', main= '', ylim=c(0,3)) # Figure 8.6\\naddscree(s)\\ns$loadings # These loadings are on the orig. transcan scale\\nLoadings:\\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8 Comp.9 Comp.10\\nsz 0.248 0.950\\nsg 0.620 0.522\\nap 0.634 −0.305\\nsbp −0.707\\ndbp 0.707\\nage 1.000\\nwt 1.000\\nhg 1.000\\nekg 1.000\\npf 1.000\\nbm−0.391 0.852\\nhx 1.000\\nComp.1 Comp.2 Comp.3 Comp.4 Comp.5 Comp.6 Comp.7 Comp.8\\nSS loadings 1.000 1.000 1.000 1.000 1.000 1.000 1.000 1.000\\nProportion Var 0.083 0.083 0.083 0.083 0.083 0.083 0.083 0.083\\nCumulative Var 0.083 0.167 0.250 0.333 0.417 0.500 0.583 0.667\\nComp.9 Comp.10\\nSS loadings 1.000 1.000\\nProportion Var 0.083 0.083\\nCumulative Var 0.750 0.833\\nOnly nonzero loadings are shown. The ﬁrst sparse PC is the tumorcluster\\nused above, and the second is the blood pressure cluster. Let us see how well\\nincomplete sparse principal component regression predicts time until death.\\naThespcapackage is a new sparse PC package that should also be considered.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='60176d58-7c52-47e9-bccc-a19500c0a06a', embedding=None, metadata={'page_label': '176', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"176 8 Case Study in Data ReductionVariances\\n0.00.51.01.52.02.53.0\\n123456789 1 00.2\\n0.35\\n0.44 0.53 0.61 0.7 0.79 0.88\\n0.951\\nFig. 8.6 Variance explained by individual sparse principal components (lines) along\\nwith cumulative proportion of variance explained (text)\\npcs←s$scores # pick off sparse PCs\\naic←numeric (10)\\nfor(i in 1:10) {\\nps←pcs[,1:i]\\naic[i] ←AIC(cph(S ∼ps))\\n}# Figure 8.7\\nplot(1:10, aic, xlab= 'Number of Components Used ',\\nylab= 'AIC', type= 'l', ylim=c(3950,4000))\\nMore components are required to optimize AIC than were seen in Figure 8.5,\\nbut a model built from 6–8sparse PCs performedas wellas the other models.\\n8.7 Transformation Using Nonparametric Smoothers\\nThe ACE nonparametric additive regression method of Breiman and Fried-\\nman68transforms both the left-hand-side variable and all the right-hand-side\\nvariables so as to optimize R2. ACE can be used to transform the predic-\\ntors using the Racefunction in the acepackpackage, called by the transace\\nfunction in the Hmiscpackage. transace does not impute data but merely\\ndoes casewise deletion of missing values. Here transace is run after single im-\\nputation by transcan.binaryis used to tell transace which variables not to\\ntry to predict (because they need no transformation). Several predictors are\\nrestricted to be monotonically transformed.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='27fe5709-44b0-4547-8af6-19a20c4db33c', embedding=None, metadata={'page_label': '177', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.8 Further Reading 177\\n2468 1 0395039603970398039904000\\nNumber of Components UsedAIC\\nFig. 8.7 Performance of sparse principal components in Cox models\\nx←with(imputed,\\ncbind(sz, sg, ap, sbp, dbp, age, wt, hg, ekg, pf,\\nbm, hx))\\nmonotonic ←c(\"sz\",\"sg\",\"ap\",\"sbp\",\"dbp\",\"age\",\"pf\")\\ntransace(x, monotonic, # Figure 8.8\\ncategorical=\"ekg\", binary=c(\"bm\",\"hx\"))\\nR2achieved in predicting each variable:\\nsz sg ap sbp dbp age wt\\n0.2265824 0.5762743 0.5717747 0.4823852 0.4580924 0.1514527 0.1732244\\nhg ekg pf bm hx\\n0.2001008 0.1110709 0.1778705 NA NA\\nExcept for ekg,age, and for arbitrary sign reversals, the transformations in\\nFigure8.8determined using transace were similar to those in Figure 8.3.T h e\\ntranscan transformation for ekgmakes more sense.\\n8.8 Further Reading\\n1Sauerbrei and Schumacher541used the bootstrap to demonstrate the variability\\nof a standard variable selection procedure for the prostate cancer dataset.\\n2Schemper and Heinze551used logistic models to impute dichotomizations of the\\npredictors for this dataset.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cc5ed58b-c7c4-4995-b34b-1471b8528f91', embedding=None, metadata={'page_label': '178', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='178 8 Case Study in Data Reduction\\n0 1 02 03 04 05 06 07 0−10123\\nsz6 8 10 12 14−1.5−1.0−0.50.00.51.01.5\\nsg0 200 400 600 800 1000−10123\\nap\\n10 15 20 25 30−202468\\nsbp4 6 8 10 12 14 16 18−202468\\ndbp50 60 70 80 90−3−2−101\\nage\\n80 100 120 140−3−2−1012\\nwt10 15 20−6−5−4−3−2−101\\nhg123456−1.0−0.50.00.51.01.5\\nekg\\n1.0 1.5 2.0 2.5 3.001234\\npf\\nFig. 8.8 Simultaneous transformation of all variables using ACE.\\n8.9 Problems\\nThe Mayo Clinic conducted a randomized trial in primary biliary cirrhosis\\n(PBC) of the liver between January 1974 and May 1984, to compare D-\\npenicillamine with placebo. The drug was found to be ineﬀective [ 197,p .\\n2], and the trial was done before liver transplantation was common, so this\\ntrialconstitutesanaturalhistorystudyforPBC.Followupcontinuedthrough\\nJuly,1986.Forthe19patientsthatdidundergotransplant,followuptimewas\\ncensored ( status=0) at the day of transplant. 312 patients were randomized,\\nand another 106 patients were entered into a registry. The nonrandomized\\npatients have most of their laboratory values missing, except for bilirubin,\\nalbumin, and prothrombin time. 28 randomized patients had both serum\\ncholesterol and triglycerides missing. The data, which consist of clinical, bio-\\nchemical, serologic, and histologic information, are listed in [ 197, pp. 359–\\n375]. The PBC data are discussed and analyzed in [ 197, pp. 2–7, 102–104,\\n153–162], [158],[7](a tree-based analysis which on its p. 480 mentions some\\npossible lack of ﬁt of the earlier analyses), and [361]. The data are stored in\\nthe datasets web site so may be accessed using the Hmisc getHdata function\\nwith argument pbc. Use only the data onrandomizedpatientsfor all analyses.\\nFor Problems 1–6, ignore followup time, status, and drug.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4444288f-d5cb-4b59-8470-7a58edfc3cc6', embedding=None, metadata={'page_label': '179', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='8.9 Problems 179\\n1. Do an initial variable clustering based on ranks, using pairwise deletion of\\nmissing data. Comment on the potential for one-dimensionalsummaries of\\nsubsets of variables being adequate summaries of prognostic information.\\n2.cholesterol, triglycerides, platelets ,a n dcopperaremissingonsomepa-\\ntients. Impute them using a method you recommend. Use some or all of\\nthe remaining predictors and possibly the outcome. Provide a correlation\\ncoeﬃcient describing the usefulness of each imputation model. Provide\\nthe actual imputed values, specifying observation numbers. For all later\\nanalyses, use imputed values for missing values.\\n3. Perform a scaling/transformationanalysis to better measure how the pre-\\ndictorsinterrelateandtopossiblypretransformsomeofthem.Use transcan\\nor ACE. Repeat the variable clustering using the transformed scores and\\nPearsoncorrelationor using anobliquerotationprincipalcomponent anal-\\nysis. Determine if the correlation structure (or variance explained by the\\nﬁrst principal component) indicates whether it is possible to summarize\\nmultiple variables into single scores.\\n4. Do a principal component analysis of all transformed variables simulta-\\nneously. Make a graph of the number of components versus the cumula-\\ntive proportionof explained variation.Repeat this for laboratoryvariables\\nalone.\\n5. Repeat the overall PCA using sparse principal components. Pay atten-\\ntion to how best to solve for sparse components, e.g., consider the lambda\\nparameter in sPCAgrid.\\n6. How well can variables (lab and otherwise) that are routinely collected\\n(on nonrandomized patients) capture the information (variation) of the\\nvariablesthatareoftenmissing?Itwouldbehelpfultoexplorethestrength\\nof interrelationships by\\na. correlating two PC 1s obtained from untransformed variables,\\nb. correlating two PC 1s obtained from transformed variables,\\nc. correlating the best linear combination of one set of variables with the\\nbest linear combination of the other set, and\\nd. doing the same on transformed variables.\\nFor this problem consider only complete cases, and transform the 5 non-\\nnumeric categorical predictors to binary 0–1 variables.\\n7. Consider the patients having complete data who were randomized to\\nplacebo. Consider only models that are linear in all the covariates.\\na. Fit a survival model to predict time of death using the following covari-\\nates:bili, albumin, stage, protime, age, alk.phos, sgot, chol, trig,\\nplatelet, copper .\\nb. Perform an ordinary principal component analysis. Fit the survival\\nmodel using only the ﬁrst 3 PCs. Compare the likelihood ratio χ2and\\nAIC with that of the model using the original variables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d18271d-f884-47ec-bd3d-b134aeccf02d', embedding=None, metadata={'page_label': '180', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='180 8 Case Study in Data Reduction\\nc. Considering the PCs are ﬁxed, use the bootstrap to estimate the 0.95\\nconﬁdence interval of the inter-quartile-range age eﬀect on the original\\nscale,and the sametype ofconﬁdenceinterval forthe coeﬃcient ofPC 1.\\nd. Now accounting for uncertainty in the PCs, compute the same two\\nconﬁdence intervals. Compare and interpret the two sets. Take into\\naccount the fact that PCs are not unique to within a sign change.\\nRprogramming hints for this exercise are found on the course web site.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='53342351-c1fb-4af0-a643-aaf584d911ef', embedding=None, metadata={'page_label': '181', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 9\\nOverview of Maximum Likelihood\\nEstimation\\n9.1 General Notions—Simple Cases\\nIn ordinary least squares multiple regression, the objective in ﬁtting a model\\nis to ﬁnd the values of the unknown parameters that minimize the sum of\\nsquared errorsof prediction. When the response variable is non-normal,poly-\\ntomous, or not observed completely, one needs a more general objective func-\\ntion to optimize.\\nMaximum likelihood (ML) estimation is a general technique for estimat-\\ning parameters and drawing statistical inferences in a variety of situations,\\nespecially nonstandard ones. Before laying out the method in general, ML\\nestimation is illustrated with a standard situation, the one-sample binomial\\nproblem. Here, independent binary responses are observed and one wishes to\\ndraw inferences about an unknown parameter, the probability of an event in\\na population.\\nSuppose that in a population of individuals, each individual has the same\\nprobability Pthat an event occurs. We could also say that the event has\\nalready been observed, so that Pis the prevalence of some condition in the\\npopulation. For each individual, let Y= 1 denote the occurrence of the\\nevent and Y= 0 denote nonoccurrence. Then Prob {Y=1}=Pfor each\\nindividual. Suppose that a random sample of size 3 from the population is\\ndrawn and that the ﬁrst individual had Y= 1, the second had Y=0 ,a n dt h e\\nthird had Y= 1. The respective probabilities of these outcomes are P,1−P,\\nandP. The joint probability of observing the independent events Y=1,0,1\\nisP(1−P)P=P2(1−P). Now the value of Pis unknown, but we can solve\\nfor the value of Pthat makes the observed data ( Y=1,0,1)most likely\\nto have occurred . In this case, the value of Pthat maximizes P2(1−P)i s\\nP=2/3. This value for Pis themaximum likelihood estimate (MLE)o ft h e\\npopulation probability.\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 9181', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='213a0a7e-53e9-4f13-91a9-e47c5369c362', embedding=None, metadata={'page_label': '182', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='182 9 Overview of Maximum Likelihood Estimation\\nLet us now study the situation of independent binary trials in general. Let\\nt h es a m p l es i z eb e nand the observed responses be Y1,Y2,...,Y n. The joint\\nprobability of observing the data is given by\\nL=n∏\\ni=1PYi(1−P)1−Yi. (9.1)\\nNow let sdenote the sum of the Ys or the number of times that the event\\noccurred ( Yi= 1), that is the number of “successes.” The number of non-\\noccurrences (“failures”) is n−s. The likelihood of the data can be simpliﬁed\\nto\\nL=Ps(1−P)n−s. (9.2)\\nIt is easier to work with the log likelihood function , which also has desirable\\nstatistical properties. For the one-sample binary response problem, the log\\nlikelihood is\\nlogL=slog(P)+(n−s)log(1−P). (9.3)\\nThe MLE of Pis that value of Pthat maximizes Lor logL. Since log L\\nis a smooth function of P, its maximum value can be found by ﬁnding the\\npoint at which log Lhas a slope of 0. The slope or ﬁrst derivative of log L,\\nwith respect to P,i s\\nU(P)=∂logL/∂P=s/P−(n−s)/(1−P). (9.4)\\nThe ﬁrst derivative of the log likelihood function with respect to the parame-\\nter(s), here U(P), is called the score function . Equating this function to zero\\nrequires that s/P=(n−s)/(1−P). Multiplying both sides of the equation\\nbyP(1−P) yieldss(1−P)=(n−s)Por thats=(n−s)P+sP=nP.\\nThus the MLE of Pisp=s/n.\\nAnother important function is called the Fisher information about the\\nunknown parameters. The information function is the expected value of the\\nnegative of the curvature in log L, which is the negative of the slope of the\\nslope as a function of the parameter, or the negative of the second derivative\\nof logL. Motivation for consideration of the Fisher information is as follows.\\nIf the log likelihood function has a distinct peak, the sample provides infor-\\nmation that allows one to readily discriminate between a good parameter\\nestimate (the location of the obvious peak) and a bad one. In such a case the\\nMLE will have good precisionor small variance. If on the other hand the like-\\nlihood function is relatively ﬂat, almost any estimate will do and the chosen\\nestimate will have poor precision or large variance. The degree of peakedness\\nof a function at a given point is the speed with which the slope is changing at\\nthat point, that is, the slope of the slope or second derivative of the function\\nat that point.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='43df7a20-f438-41bd-8273-d39e5233def5', embedding=None, metadata={'page_label': '183', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.1 General Notions—Simple Cases 183\\nHere, the information is\\nI(P)=E{−∂2logL/∂P2}\\n=E{s/P2+(n−s)/(1−P)2} (9.5)\\n=nP/P2+n(1−P)/(1−P)2=n/[P(1−P)].\\nWeestimatetheinformationbysubstitutingtheMLEof PintoI(P),yielding\\nI(p)=n/[p(1−p)].\\nFigures9.1,9.2,a n d9.3depict, respectively, log L,U(P), andI(P), all\\nas a function of P. Three combinations of nandswere used in each graph.\\nThese combinations correspond to p=.5,.6, and.6, respectively.\\n0.0 0.2 0.4 0.6 0.8 1.0−140−120−100−80−60−40−20\\nPLog Likelihood\\ns=50  n=100\\ns=60  n=100\\ns=12  n=20\\nFig. 9.1 log likelihood functions for three one-sample binomial problems\\nIn each case it can be seen that the value of Pthat makes the data most\\nlikely to have occurred (the value that maximizes Lor logL)i spgiven\\nabove. Also, the score function (slope of log L) is zero at P=p.N o t et h a t\\nthe information function I(P) is highest for Papproaching 0 or 1 and is\\nlowest for Pnear.5, where there is maximum uncertainty about P.N o t e\\nalso that while log Lhas the same shape for the s=6 0a n d s=1 2c u r v e s\\nin Figure 9.1, the range of log Lis much greater for the larger sample size.\\nFigures9.2and9.3show that the larger sample size produces a sharper\\nlikelihood. In other words, with larger n, one can zero in on the true value of\\nPwith more precision.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='24388a93-2355-4de7-a412-b33844a1055d', embedding=None, metadata={'page_label': '184', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='184 9 Overview of Maximum Likelihood Estimation\\n0.0 0.2 0.4 0.6 0.8 1.0−400−2000200400600\\nPScore\\ns=50  n=100\\ns=60  n=100\\ns=12  n=20\\nFig. 9.2 Score functions ( ∂L/∂P)\\n0.0 0.2 0.4 0.6 0.8 1.020040060080010001200\\nPInformationn=100\\nn=20\\nFig. 9.3 Information functions ( −∂2logL/∂P2)\\nIn this binary response one-sample example let us now turn to inference\\nabout the parameter P. First, we turn to the estimation of the variance of the\\nMLE,p.Anestimateofthisvarianceisgivenbythe inverseoftheinformation\\natP=p:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e2119e8-c363-4425-abe2-d2a13e8828ef', embedding=None, metadata={'page_label': '185', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Hypothesis Tests 185\\nVar(p)=I(p)−1=p(1−p)/n. (9.6)\\nNote that the variance is smallest when the information is greatest ( p=0\\nor 1).\\nThe variance estimate forms a basis for conﬁdence limits on the unknown\\nparameter. For large n,t h eM L E pis approximately normally distributed\\nwith expected value (mean) Pand variance P(1−P)/n.S i n c ep(1−p)i sa\\nconsistent estimate of P(1−P)/n, it follows that p±z[p(1−p)/n]1/2is an\\napproximate 1 −αconﬁdence interval for Pifzis the 1−α/2c r i t i c a lv a l u e\\nof the standard normal distribution.\\n9.2 Hypothesis Tests\\nNow let us turn to hypothesis tests about the unknown population parameter\\nP—H0:P=P0. There are three kinds of statistical tests that arise from\\nlikelihood theory.\\n9.2.1 Likelihood Ratio Test\\nThis test statistic is the ratio of the likelihood at the hypothesized parameter\\nvalues to the likelihood of the data at the maximum (i.e., at parameter values\\n= MLEs). It turns out that −2×the log of this likelihood ratio has desirable\\nstatistical properties. The likelihood ratio test statistic is given by\\nLR=−2log(LatH0/Lat MLEs)\\n=−2(logLatH0)−[−2(logLat MLEs)] . (9.7)\\nTheLRstatistic, for large enough samples, has approximately a χ2distribu-\\ntion with degrees of freedom equal to th e number of para meters estimated, if\\nthe null hypothesis is“simple,”that is, doesn’t involve any unknown param-\\neters. Here LRhas 1 d.f.\\nThe value of log LatH0is\\nlogL(H0)=slog(P0)+(n−s)log(1−P0). (9.8)\\nThe maximum value of log L(at MLEs) is\\nlogL(P=p)=slog(p)+(n−s)log(1−p). (9.9)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c312b7a2-440d-4ab7-a985-4e702b2537b7', embedding=None, metadata={'page_label': '186', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='186 9 Overview of Maximum Likelihood Estimation\\nFor the hypothesis H0:P=P0, the test statistic is\\nLR=−2{slog(P0/p)+(n−s)log[(1−P0)/(1−p)]}. (9.10)\\nNotethatwhen phappenstoequal P0,LR=0.When pisfarfrom P0,LRwill\\nbe large. Suppose that P0=1/2, so that H0isP=1/2. Forn= 100,s= 50,\\nLR=0 .F o r n= 100,s= 60,\\nLR=−2{60log(.5/.6)+40log( .5/.4)}=4.03. (9.11)\\nForn=2 0,s= 12,\\nLR=−2{12log(.5/.6)+8log( .5/.4)}=.81 = 4.03/5. (9.12)\\nTherefore, even though the best estimate of Pis the same for these two cases,\\nthe test statistic is more impressive when the sample size is ﬁve times larger.\\n9.2.2 Wald Test\\nThe Wald test statistic is a generalizationof a t-o rz-statistic. It is a function\\nof the diﬀerence in the MLE and its hypothesized value, normalized by an\\nestimate of the standard deviation of the MLE. Here the statistic is\\nW=[p−P0]2/[p(1−p)/n]. (9.13)\\nFor large enough n,Wis distributed as χ2with 1 d.f. For n= 100,s= 50,\\nW= 0. For the other samples, Wis, respectively, 4.17 and 0.83 (note 0 .83 =\\n4.17/5).\\nMany statistical packages treat√\\nWas having a tdistribution instead of\\na normal distribution. As pointed out by Gould,228there is no basis for this\\noutside of ordinary linear modelsa.\\n9.2.3 Score Test\\nIf the MLE happens to equal the hypothesized value P0,P0maximizes the\\nlikelihood and so U(P0) = 0. Rao’s score statistic measures how far from zero\\nthescorefunctioniswhenevaluatedatthenullhypothesis.Thescorefunction\\naIn linear regression, a tdistribution is used to penalize for the fact that the variance\\nofY|Xis estimated. In models such as the logistic model, there is no separate vari-\\nance parameter to estimate. Gould has done simulations that show that the normal\\ndistribution provides more accurate P-values than the tfor binary logistic regression.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9fd6d94d-6603-4a7f-8b7b-f985f819b584', embedding=None, metadata={'page_label': '187', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.2 Hypothesis Tests 187\\n(slope or ﬁrst derivative of log L) is normalized by the information (curvature\\nor second derivative of −logL). The test statistic for our example is\\nS=U(P0)2/I(P0), (9.14)\\nwhich formally does not involve the MLE, p. The statistic can be simpliﬁed\\nas follows.\\nU(P0)=s/P0−(n−s)/(1−P0)\\nI(P0)=s/P2\\n0+(n−s)/(1−P0)2(9.15)\\nS=(s−nP0)2/[nP0(1−P0)] =n(p−P0)2/[P0(1−P0)].\\nNote that the numerator of Sinvolves s−nP0, the diﬀerence between the\\nobservednumberofsuccessesandthenumberofsuccessesexpectedunder H0.\\nAs with the other two test statistics, S= 0 for the ﬁrst sample. For the\\nlast two samples Sis, respectively, 4 and .8 = 4 /5. 1\\n9.2.4 Normal Distribution—One Sample\\nSuppose that a sample of size nis taken from a population for a random\\nvariable Ythat is known to be normally distributed with unknown mean\\nμand variance σ2. Denote the observed values of the random variable by\\nY1,Y2,...,Y n. Now unlike the binary response case ( Y= 0 or 1), we cannot\\nuse the notion of the probability that Yequals an observed value. This is\\nbecauseYis continuous and the probability that it will take on a given value\\nis zero. We substitute the density function for the probability. The density\\nat a point yis the limit as dapproaches zero of\\nProb{y<Y≤y+d}/d=[F(y+d)−F(y)]/d, (9.16)\\nwhereF(y) is the normal cumulative distribution function (for a mean of μ\\nand variance of σ2). The limit of the right-hand side of the above equation as\\ndapproaches zero is f(y), the density function of a normal distribution with\\nmeanμand variance σ2. This density function is\\nf(y)=( 2πσ2)−1/2exp{−(y−μ)2/2σ2}. (9.17)\\nThe likelihood of observing the observed sample values is the joint density\\nof theYs. The log likelihood function here is a function of two unknowns, μ\\nandσ2.\\nlogL=−.5nlog(2πσ2)−.5n∑\\ni=1(Yi−μ)2/σ2. (9.18)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f322c4b8-090b-439c-a308-75a9dafd54dd', embedding=None, metadata={'page_label': '188', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='188 9 Overview of Maximum Likelihood Estimation\\nItcanbeshownthatthevalueof μthatmaximizeslog Listhevaluethatmin-\\nimizes the sum of squared deviations about μ, which is the sample mean Y.\\nThe MLE of σ2is\\ns2=n∑\\ni=1(Yi−Y)2/n. (9.19)\\nRecall that the sample variance uses n−1 instead of nin the denominator. It\\ncan be shown that the expected value of the MLE of σ2,s2,i s[ (n−1)/n]σ2;\\nin other words, s2is too small by a factor of ( n−1)/non the average. The\\nsample variance is unbiased, but being unbiased does not necessarily make\\nit a better estimator. The MLE has greater precision (smaller mean squared\\nerror) in many cases.\\n9.3 General Case\\nSuppose we need to estimate a vector of unknown parameters B={B1,B2,\\n...,Bp}from a sample of size nb a s e do no b s e r v a t i o n s Y1,...,Y n.D e n o t et h e\\nprobability or density function of the random variable Yfor theith observa-\\ntion byfi(y;B). The likelihood for the ith observation is Li(B)=fi(Yi;B).\\nIn the one-sample binary response case, recall that Li(B)=Li(P)=\\nPYi[1−P]1−Yi. The likelihood function, or joint likelihood of the sample,\\nis given by\\nL(B)=n∏\\ni=1fi(Yi;B). (9.20)\\nThe log likelihood function is\\nlogL(B)=n∑\\ni=1logLi(B). (9.21)\\nThe MLE of Bis that value of the vector Bthat maximizes log L(B)a s\\na function of B. In general, the solution for Brequires iterative trial-and-\\nerror methods as outlined later. Denote the MLE of Basb={b1,...,b p}.\\nThescore vector is the vector of ﬁrst derivatives of log L(B) with respect to\\nB1,...,B p:\\nU(B)={∂/∂B1logL(B),...,∂/∂B plogL(B)}\\n=(∂/∂B)logL(B). (9.22)\\nThe Fisher information matrix is thep×pmatrix whose elements are the\\nnegative of the expectation of all second partial derivatives of log L(B):\\nI∗(B)=−{E[(∂2logL(B)/∂Bj∂Bk)]}p×p\\n=−E{(∂2/∂B∂B′)logL(B)}. (9.23)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='06db60f0-885a-4518-9f7d-1d39a7521a09', embedding=None, metadata={'page_label': '189', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 General Case 189\\nTheobserved information matrix I(B)i sI∗(B) without taking the expecta-\\ntion. In other words, observed values remain in the second derivatives:\\nI(B)=−(∂2/∂B∂B′)logL(B). (9.24)\\nThis information matrix is often estimated from the sample using the es-\\ntimated observed information I(b), by inserting b,t h eM L Eo f B,i n t ot h e\\nformula for I(B).\\nUnder suitable conditions, which are satisﬁed for most situations likely\\nto be encountered, the MLE bfor large samples is an optimal estimator\\n(has as great a chance of being close to the true parameter as all other\\ntypes ofestimators)andhas anapproximatemultivariatenormaldistribution\\nwith mean vector Band variance–covariance matrix I∗−1(B), where C−1\\ndenotes the inverse of the matrix C.(C−1is the matrix such that C−1Cis\\nthe identity matrix, a matrix with ones on the diagonal and zeros elsewhere.\\nIfCis a 1×1m a t r i x , C−1=1/C.) A consistent estimator of the variance–\\ncovariance matrix is given by the matrix V, obtained by inserting bforBin\\nI(B):V=I−1(b).\\n9.3.1 Global Test Statistics\\nSuppose we wish to test the null hypothesis H0:B=B0. The likelihood\\nratio test statistic is\\nLR=−2log(LatH0/Lat MLEs)\\n=−2[logL(B0)−logL(b)]. (9.25)\\nThe corresponding Wald test statistic, using the estimated observed informa-\\ntion matrix, is\\nW=(b−B0)′I(b)(b−B0)=(b−B0)′V−1(b−B0).(9.26)\\n(A quadratic form a′Vais a matrix generalization of a2V.) Note that if the\\nnumber of estimated parameters is p=1 ,Wreduces to ( b−B0)2/V,w h i c h\\nis the squareof a z-o rt-type statistic (estimate −hypothesized value divided\\nby estimated standard deviation of estimate).\\nThe score statistic for H0is\\nS=U′(B0)I−1(B0)U(B0). (9.27)\\nNotethatasbefore, SdoesnotrequiresolvingfortheMLE.Forlargesamples,\\nLR,W,a n dShave aχ2distribution with pd.f. under suitable conditions.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='69c5f115-b9c3-48a2-8b4f-6971851afc8c', embedding=None, metadata={'page_label': '190', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='190 9 Overview of Maximum Likelihood Estimation\\n9.3.2 Testing a Subset of the Parameters\\nLetB={B1,B2}and suppose that we wish to test H0:B1=B0\\n1.W e\\nare treating B2as a nuisance parameter. For example, we may want to test\\nwhether blood pressure and cholesterol are risk factors after adjusting for\\nconfounders age and sex. In that case B1is the pair of regression coeﬃcients\\nfor blood pressure and cholesterol and B2is the pair of coeﬃcients for age\\nand sex. B2must be estimated to allow adjustment for age and sex, although\\nB2is a nuisance parameter and is not of primary interest.\\nLet the number of parameters of interest be kso thatB1is a vector of\\nlengthk. Let the number of“nuisance”or“adjustment”parameters be q,t h e\\nlength of B2(notek+q=p).\\nLetb∗\\n2be the MLE of B2under the restriction that B1=B0\\n1. Then the\\nlikelihood ratio statistic is\\nLR=−2[logLatH0−logLat MLE]. (9.28)\\nNowlogLatH0ismorecomplexthanbeforebecause H0involvesanunknown\\nnuisance parameter B2that must be estimated. log LatH0is the maximum\\nof the likelihood function for any value of B2but subject to the condition\\nthatB1=B0\\n1.T h u s\\nLR=−2[logL(B0\\n1,b∗\\n2)−logL(b)], (9.29)\\nwhere as before bis the overall MLE of B.N o t et h a t LRrequires maximiz-\\ning two log likelihood functions. The ﬁrst component of LRis a restricted\\nmaximum likelihood and the second component is the overall or unrestricted\\nmaximum.\\nLRis often computed by examining successively more complex models in\\na stepwise fashion and calculating the increment in likelihood ratio χ2in the\\noverall model. The LR χ2for testing H0:B2=0w h e n B1is not in the\\nmodel is\\nLR(H0:B2=0|B1=0 )=−2[logL(0,0)−logL(0,b∗\\n2)].(9.30)\\nHere we are specifying that B1is not in the model by setting B1=B0\\n1=0 ,\\nand we are testing H0:B2= 0. (We are also ignoring nuisance parameters\\nsuch as an intercept term in the test for B2=0 . )\\nTheLR χ2for testing H0:B1=B2= 0 is given by\\nLR(H0:B1=B2=0 )=−2[logL(0,0)−logL(b)].(9.31)\\nSubtracting LR χ2for the smaller model from that of the larger model yields\\n−2[logL(0,0)−logL(b)]−−2[logL(0,0)−logL(0,b2∗)]\\n= −2[logL(0,b∗\\n2)−logL(b)], (9.32)\\nwhich is the same as above (letting B0\\n1=0 ) .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9c6a3fb-72c0-41d0-a5c4-9cc726c4adc1', embedding=None, metadata={'page_label': '191', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 General Case 191\\nTable 9.1 Example tests\\nVariables (Parameters) LR χ2Number of\\nin Model Parameters\\nIntercept, age 1000 2\\nIntercept, age, age21010 3\\nIntercept, age, age2, sex 1013 4\\nFor example, suppose successively larger models yield the LR χ2si n\\nTable9.1.T h eLR χ2for testing for linearity in age (not adjusting for sex)\\nagainst quadratic alternatives is 1010 −1000 = 10 with 1 d.f. The LR χ2\\nfor testing the added information provided by sex, adjusting for a quadratic\\neﬀect ofage,is 1013 −1010= 3 with 1 d.f. The LR χ2for testing the joint im-\\nportance ofsex and the nonlinear (quadratic)eﬀect ofage is 1013 −1000= 13\\nwith 2 d.f.\\nTo derive the Wald statistic for testing H0:B1=B0\\n1withB2being a\\nnuisance parameter, let the MLE bbe partitioned into b={b1,b2}.W ec a n\\nlikewise partition the estimated variance–covariance matrix Vinto\\nV=[V11V12\\nV′\\n12V22]\\n. (9.33)\\nThe Wald statistic is\\nW=(b1−B0\\n1)′V−1\\n11(b1−B0\\n1), (9.34)\\nwhich when k= 1 reduces to (estimate −hypothesized value)2/estimated\\nvariance, with the estimates adjusted for the parameters in B2.\\nThe score statistic for testing H0:B1=B0\\n1does not require solving for\\nthe full set of unknown parameters.Only the MLEs of B2must be computed,\\nunder the restriction that B1=B0\\n1. This restricted MLE is b∗\\n2from above.\\nLetU(B0\\n1,b∗\\n2) denote the vector of ﬁrst derivatives of log Lwith respect to\\nall parameters in B, evaluated at the hypothesized parameter values B0\\n1for\\nthe ﬁrstkparameters and at the restricted MLE b∗\\n2for the last qparameters.\\n(Since the last qestimates are MLEs, the last qelements of Uare zero, so\\nthe formulas that follow simplify.) Let I(B0\\n1,b∗\\n2) be the observed information\\nmatrix evaluated at the same values of Bas isU. The score statistic for\\ntestingH0:B1=B0\\n1is\\nS=U′(B0\\n1,b∗\\n2)I−1(B0\\n1,b∗\\n2)U(B0\\n1,b∗\\n2). (9.35)\\nUnder suitable conditions, the distribution of LR,W,a n dScan be ade-\\nquately approximated by a χ2distribution with kd.f. 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f8dbd027-f84d-4521-82c7-6e9bc5919d31', embedding=None, metadata={'page_label': '192', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='192 9 Overview of Maximum Likelihood Estimation\\n9.3.3 Tests Based on Contrasts\\nWald tests are also done by setting up a general linear contrast. H0:CB=0\\nis tested by a Wald statistic of the form\\nW=(Cb)′(CVC′)−1(Cb), (9.36)\\nwhereCis a contrast matrix that“picks oﬀ”the proper elements of B.T h e\\ncontrasts can be much more general by allowing elements of Cto be other\\nthan zero and one. For the normal linear model, Wis converted to an F-\\nstatistic by dividing by the rank rofC(normally the number of rows in\\nC), yielding a statistic with an F-distribution with rnumerator degrees of\\nfreedom.\\nMany interesting contrasts are tested by forming diﬀerences in predicted\\nvalues. By forming more contrasts than are really needed, one can develop\\na surprisingly ﬂexible approach to hypothesis testing using predicted values.\\nThis has the major advantage of not requiring the analyst to account for how\\nthe predictors are coded. Suppose that one wanted to assess the diﬀerence\\nin two vectors of predicted values, X1b−X2b=(X1−X2)b=Δbto test\\nH0:ΔB=0 ,w h e r e Δ=X1−X2. The covariance matrix for Δbis given by\\nvar(Δb)=ΔVΔ′. (9.37)\\nLetrbe the rank of var( Δb), i.e., the number of non-linearly-dependent\\n(non-redundant) diﬀerences of predicted values of Δ. The value of rand the\\nrows ofΔthat are not redundant may easily be determined using the QR\\ndecomposition as done by the Rfunction qrb.T h eχ2statistic with rdegrees\\noffreedom(or F-statisticupondividingthestatisticby r)ma ybeobtainedb y\\ncomputing Δ∗V∗Δ∗′whereΔ∗is the subset of elements of Δcorresponding\\nto non-redundant contrasts and V∗is the corresponding sub-matrix of V.\\nThe “diﬀerence in predictions” approach can be used to compare means\\nin a 30 year old male with a 40 year old femalec. But the true utility of\\nthe approach is most obvious when the contrast involves multiple nonlinear\\nterms for a single predictor, e.g., a spline function. To test for a diﬀerence\\nin two curves, one can compare predictions at one predictor value against\\npredictions at a series of values with at least one value that pertains to each\\nbasis function. Points can be placed between every pair of knots and beyond\\nthe outer knots, or just obtain predictions at 100 equally spaced X-values.\\nbFor example, in a 3-treatment comparison one could examine contrasts between\\ntreatments A and B, A and C, and B and C by obtaining predicted values for those\\ntreatments, even though only two diﬀerences are required.\\ncThermscommand could be contrast(fit, list(sex=’male’,age=30),\\nlist(sex=’female’,age=40)) where all other predictors are set to medians or\\nmodes.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1ba95c92-808a-4680-9013-fa228042288f', embedding=None, metadata={'page_label': '193', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.3 General Case 193\\nSuppose that there arethreetreatment groups(A, B,C) interactingwith a\\ncubic spline function of X. If one wants to test the multiple degree of freedom\\nhypothesis that the proﬁle for Xis the same for treatment A and B vs. the\\nalternative hypothesis that there is a diﬀerence between A and B for at least\\nonevalueof X,onecancomparepredictedvaluesat treatmentAand avector\\nofXvalues against predicted values at treatment B and the same vector of\\nXvalues. If the Xrelationship is linear, any two Xvalues will suﬃce, and\\nifXis quadratic, any three points will suﬃce. It would be diﬃcult to test\\ncomplex hypotheses involving only 2 of 3 treatments using other methods.\\nThecontrast function in rmscan estimate a wide variety of contrasts and\\nmake joint tests involvingthem, automaticallycomputing the number ofnon-\\nlinearly-dependent contrasts as the test’s degrees of freedom. See its help ﬁle\\nfor several examples.\\n9.3.4 Which Test Statistics to Use When\\nAt this point, one may ask why three types of test statistics are needed. The\\nanswer lies in the statistical properties of the three tests as well as in com-\\nputational expense in diﬀerent situations. From the standpoint of statistical\\nproperties, LRis the best statistic, followed by SandW.T h em a j o rs t a -\\ntistical problem with Wis that it is sensitive to problems in the estimated\\nvariance–covariancematrix in the full model. For some models, most notably\\nthe logistic regressionmodel,278the variance–covarianceestimates can be too\\nlarge as the eﬀects in the model become very strong, resulting in values of\\nWthat are too small (or signiﬁcance levels that are too large). Wis also\\nsensitive to the way the parameter appears in the model. For example, a test\\nofH0: log odds ratio = 0 will yield a diﬀerent value of Wthan will H0:\\nodds ratio = 1.\\nRelativecomputationaleﬃciencyofthe three typesoftestsis alsoan issue.\\nComputation of LRandWrequires estimating all punknown parameters,\\nand in addition LRrequires re-estimating the last qparameters under that\\nrestriction that the ﬁrst kparameters = B0\\n1. Therefore, when one is contem-\\nplating whether a set of parameters should be added to a model, the score\\ntest is the easiest test to carry out. For example, if one were interested in\\ntesting all two-way interactions among 4 predictors, the score test statistic\\nforH0:“no interactions present”could be computed without estimating the\\n4×3/2 = 6 interaction eﬀects. Swould also be appealing for testing linearity\\nof eﬀects in a model—the nonlinear spline terms could be tested for signiﬁ-\\ncance after adjusting for the linear eﬀects (with estimation of only the linear\\neﬀects). Only parameters for linear eﬀects must be estimated to compute\\nS, resulting in fewer numerical problems such as lack of convergence of the\\nNewton–Raphson algorithm.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cb6f90f8-5489-4de1-8e18-d5effbea9fbb', embedding=None, metadata={'page_label': '194', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='194 9 Overview of Maximum Likelihood Estimation\\nTable 9.2 Choice of test statistics\\nType of Test Recommended Test Statistic\\nGlobal association LR (S for large no. parameters)\\nPartial association W (LR or S if problem with W)\\nLack of ﬁt, 1 d.f. W or S\\nLack of ﬁt, >1d . f . S\\nInclusion of additional predictors S\\nThe Wald tests are very easy to make after all the parameters in a model\\nhave been estimated. Wald tests are thus appealing in a multiple regression\\nsetup when one wants to test whether a given predictor or set of predic-\\ntors is “signiﬁcant.”A score test would require re-estimating the regression\\ncoeﬃcients under the restriction that the parameters of interest equal zero.\\nLikelihood ratio tests are used often for testing the global hypothesis that\\nno eﬀects are signiﬁcant, as the log likelihood evaluated at the MLEs is al-\\nready available from ﬁtting the model and the log likelihood evaluated at\\na “null model” (e.g., a model containing only an intercept) is often easy to\\ncompute. Likelihood ratio tests should also be used when the validity of a\\nWald test is in question as in the example cited above.\\nTable9.2summarizes recommendations for choice of test statistics for\\nvarious situations.\\n9.3.5 Example: Binomial—Comparing Two\\nProportions\\nSupposethat abinaryrandomvariable Y1representsresponsesforpopulation\\n1a n dY2represents responses for population 2. Let Pi=P r o b{Yi=1}\\nand assume that a random sample has been drawn from each population\\nwith respective sample sizes n1andn2. The sample values are denoted by\\nYi1,...,Y ini,i= 1 or 2. Let\\ns1=n1∑\\nj=1Y1js2=n2∑\\nj=1Y2j, (9.38)\\nthe respective observed number of“successes”in the two samples. Let us test\\nthe null hypothesis H0:P1=P2based on the two samples.\\nThe likelihood function is\\nL=2∏\\ni=1ni∏\\nj=1PYij\\ni(1−Pi)1−Yij', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa615a07-a563-4ebc-b056-ebac1f68ff67', embedding=None, metadata={'page_label': '195', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.4 Iterative ML Estimation 195\\n=2∏\\ni=1Psi\\ni(1−Pi)ni−si(9.39)\\nlogL=2∑\\ni=1{silog(Pi)+(ni−si)log(1−Pi)}. (9.40)\\nUnderH0,P1=P2=P,s o\\nlogL(H0)=slog(P)+(n−s)log(1−P), (9.41)\\nwheres=s1+s2,n=n1+n2. The (restricted) MLE of this common Pis\\np=s/nand logLat this value is slog(p)+(n−s)log(1−p).\\nSince the original unrestricted log likelihood function contains two terms\\nwith separate parameters, the two parts may be maximized separately giving\\nMLEs\\np1=s1/n1andp2=s2/n2. (9.42)\\nlogLevaluated at these (unrestricted) MLEs is\\nlogL=s1log(p1)+(n1−s1)log(1−p1)\\n+s2log(p2)+(n2−s2)log(1−p2). (9.43)\\nThe likelihood ratio statistic for testing H0:P1=P2is then\\nLR=−2{slog(p)+(n−s)log(1−p)\\n−[s1log(p1)+(n1−s1)log(1−p1) (9.44)\\n+s2log(p2)+(n2−s2)log(1−p2)]}.\\nThis statistic for large enough n1andn2has aχ2distribution with 1 d.f.\\nsince the null hypothesis involves the estimation of one fewer parameter than\\ndoes the unrestrictedcase.This LRstatisticis the likelihoodratio χ2statistic\\nfor a 2×2 contingency table. It can be shown that the corresponding score\\nstatistic is equivalent to the Pearson χ2statistic. The better LRstatistic can\\nbe used routinely over the Pearson χ2for testing hypotheses in contingency\\ntables.\\n9.4 Iterative ML Estimation\\nIn most cases, one cannot explicitly solve for MLEs but must use trial-and-\\nerror numerical methods to solve for parameter values Bthat maximize\\nlogL(B) or yield a score vector U(B) = 0. One of the fastest and most ap-\\nplicable methods for maximizing a function is the Newton–Raphson method,\\nwhich is based on approximating U(B) by a linear function of Bin a small', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8d22b182-42d3-4f03-b5b1-cab26ed515d2', embedding=None, metadata={'page_label': '196', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='196 9 Overview of Maximum Likelihood Estimation\\nregion.Astartingestimate b0oftheMLE bismade.Thelinearapproximation\\n(a ﬁrst-order Taylor series approximation)\\nU(b)=U(b0)−I(b0)(b−b0) (9.45)\\nis equated to 0 and solved by byielding\\nb=b0+I−1(b0)U(b0). (9.46)\\nThe process is continued in like fashion. At the ith step the next estimate is\\nobtained from the previous estimate using the formula\\nbi+1=bi+I−1(bi)U(bi). (9.47)\\nIf the log likelihood actually worsened at bi+1, “step halving” is used; bi+1\\nis replaced with ( bi+bi+1)/2. Further step halving is done if the log like-\\nlihood still is worse than the log likelihood at bi, after which the original\\niterative strategy is resumed. The Newton–Raphson iterations continue until\\nthe−2log likelihood changes by only a small amount over the previous iter-\\nation (say .025). The reasoning behind this stopping rule is that estimates of\\nBthat change the −2log likelihood by less than this amount do not aﬀect\\nstatistical inference since −2log likelihood is on the χ2scale. 3\\n9.5 Robust Estimation of the Covariance Matrix\\nTheestimatorforthecovariancematrixof bfoundinSection 9.3assumesthat\\nthe model is correctly speciﬁed in terms of distribution, regression assump-\\ntions, and independence assumptions. The model may be incorrect in a va-\\nriety of ways such as non-independence (e.g., repeated measurements within\\nsubjects), lack of ﬁt (e.g., omitted covariable, incorrect covariable transfor-\\nmation, omitted interaction), and distributional (e.g., Yhas aΓdistribution\\ninstead of a normal distribution). Variances and covariances, and hence con-\\nﬁdence intervals and Wald tests, will be incorrect when these assumptions\\nare violated.\\nFor the case in which the observations are independent and identically\\ndistributed but other assumptions are possibly violated, Huber312provided\\na covariance matrix estimator that is consistent. His“sandwich”estimator is\\ngiven by\\nH=I−1(b)[n∑\\ni=1UiU′\\ni]I−1(b), (9.48)\\nwhereI(b) is the observed information matrix (Equation 9.24)a n dUiis the\\nvector of derivatives, with respect to all parameters, of the log likelihood\\ncomponent for the ith observation (assuming the log likelihood can be par-\\ntitioned into per-observation contributions). For the normal multiple linear\\nregression case, Hwas derived by White:659', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9d76135-1931-44c9-b8b9-2cb616faff16', embedding=None, metadata={'page_label': '197', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.5 Robust Estimation of the Covariance Matrix 197\\n(X′X)−1[n∑\\ni=1(Yi−Xib)2XiX′\\ni](X′X)−1, (9.49)\\nwhereXis the design matrix (including an intercept if appropriate) and Xi\\nis the vector of predictors (including an intercept) for the ith observation.\\nThis covariance estimator allows for any pattern of variances of Y|Xacross\\nobservations. Note that even though Himproves the bias of the covariance 4\\nmatrix of b, it may actually have largermean squared errorthan the ordinary\\nestimate in some cases due to increased variance.164,529\\nWhen observations are dependent within clusters, and the number of ob-\\nservations within clusters is very small in comparison to the total sample\\nsize, a simple adjustment to Equation 9.48c a nb eu s e dt od e r i v ea p p r o -\\npriate covariance matrix estimates (see Lin [ 407, p. 2237], Rogers,529and\\nLee et al. [ 393, Eq.5.1, p.246]). One merely accumulates sums of elements of\\nUwithin clusters before computing cross-product terms:\\nHc=I−1(b)[c∑\\ni=1{(ni∑\\nj=1Uij)(ni∑\\nj=1Uij)′}]I−1(b), (9.50)\\nwherecis the number of clusters, niis the number of observations in the ith\\ncluster,Uijis the contribution of the jth observationwithin the ith cluster to\\nthe scorevector,and I(b)is computedasbeforeignoringclusters.Foramodel\\nsuch as the Cox model which has no per-observation score contributions,\\nspecial score residuals393,407,410,605are used for U.\\nBootstrapping can also be used to derive robust covariance matrix esti-\\nmates177,178in many cases, especially if covariances of bthat are not condi-\\ntionalon Xareappropriate.Onemerelygeneratesapproximately200samples\\nwith replacement from the original dataset, computes 200 sets of parameter\\nestimates, and computes the sample covariance matrix of these parameter es-\\ntimates. Sampling with replacementfromentireclusterscan be used toderive\\nvariance estimates in the presence of intracluster correlation.188Bootstrap 5\\nestimates of the conditional variance–covariance matrix given Xare harder\\nto obtain and depend on the model assumptions being satisﬁed. The simpler\\nunconditionalestimatesmay bemoreappropriatefor manynon-experimental\\nstudies where one may desire to“penalize”for the Xbeing random variables.\\nIt isinterestingthattheseunconditionalestimatesmaybeverydiﬃculttoob-\\ntain parametrically,since a multivariate distribution may need to be assumed\\nforX.\\nThe previous discussion addresses the use of a “working independence\\nmodel”with clustered data. Here one estimates regression coeﬃcients assum-\\ning independence of all records (observations). Then a sandwich or bootstrap\\nmethod is used to increase standard errors to reﬂect some redundancy in the\\ncorrelated observations. The parameter estimates w ill often be consistent es-\\ntimates of the true parameter values, but they may be ineﬃcient for certain\\ncluster or correlation structures. 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86e49317-d8c1-44a8-bafc-6f888f0dbca2', embedding=None, metadata={'page_label': '198', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='198 9 Overview of Maximum Likelihood Estimation\\nThermspackage’s robcovfunction computes the Huber robust covariance\\nmatrixestimator,andthe bootcovfunctioncomputesthebootstrapcovariance\\nestimator. Both of these functions allow for clustering.\\n9.6 Wald, Score, and Likelihood-Based Conﬁdence\\nIntervals\\nA1−αconﬁdence interval for a parameter βiis the set of all values β0\\ni\\nthat if hypothesized would be accepted in a test of H0:βi=β0\\niat the\\nαlevel. What test should form the basis for the conﬁdence interval? The\\nWald test is most frequently used because of its simplicity. A two-sided 1 −α\\nconﬁdenceintervalis bi±z1−α/2s,wherezisthecriticalvaluefromthenormal\\ndistribution and sis the estimated standard error of the parameter estimate\\nbi.dThe problem with sdiscussed in Section 9.3.4p o i n t so u tt h a tW a l d\\nstatisticsmaynotalwaysbe agoodbasis.Wald-basedconﬁdenceintervalsare\\nalso symmetric even though the coverage probability may not be.160Score-\\nand LR-based conﬁdence limits have deﬁnite advantages. When Wald-type 7\\nconﬁdence intervals are appropriate, the analyst may consider insertion of\\nrobustcovarianceestimates(Section 9.5)intotheconﬁdenceintervalformulas\\n(note that adjustments for heterogeneity and correlated observations are not\\navailable for score and LR statistics).\\nWald– (asymptotic normality) based statistics are convenient for deriving\\nconﬁdence intervals for linear or more complex combinations of the model’s\\nparameters.AsinEquation 9.36,thevariance–covariancematrixof Cb,where\\nCis an appropriate matrix and bis the vector of parameter estimates, is\\nCVC′,whereVisthevariancematrixof b.Inregressionmodelswecommonly\\nsubstitute a vector of predictors (and optional intercept) for Cto obtain the\\nvariance of the linear predictor Xbas\\nvar(Xb)=XVX′. (9.51)\\nSee Section 9.3.3for related information.\\ndThis is the basis for conﬁdence limits computed by the Rrmspackage’s Predict,\\nsummary,a n dcontrast functions. When the robcovfunction has been used to replace\\nthe information-matrix-based covariance matrix with a Huber robust covariance esti-\\nmate with an optional cluster sampling correction, the functions are using a“robust”\\nWald statistic basis. When the bootcov function has been used to replace the model\\nﬁt’s covariance matrix with a bootstrap unconditional covariance matrix estimate,\\nthe two functions are computing conﬁdence limits based on a normal distribution but\\nusing more nonparametric covariance estimates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='04812c11-d0ce-4e34-ae6a-fc5cb4ee24d6', embedding=None, metadata={'page_label': '199', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.7 Bootstrap Conﬁdence Regions 199\\n9.6.1 Simultaneous Wald Conﬁdence Regions\\nThe conﬁdence intervals just discussed are pointwise conﬁdence intervals.\\nFor OLS regression there are methods for computing conﬁdence intervals\\nwith exact simultaneous conﬁdence coveragefor multiple estimates374.T h e r e\\nare approximate methods for simultaneous conﬁdence limits for all models\\nfor which the vector of estimates bis approximately multivariately normally\\ndistributed. The method of Hothorn et al.307is quite general; in their R\\npackage multcomp’sglhtfunction,theusercanspecifyanycontrastmatrixover\\nwhich the individual conﬁdence limits will be simultaneous. A special case\\nof a contrast matrix is the design matrix Xitself, resulting in simultaneous\\nconﬁdence bands for any number of predicted values. An example is shown\\nin Figure 9.5. See Section 9.3.3for a good use for simultaneous contrasts.\\n9.7 Bootstrap Conﬁdence Regions\\nA more nonparametric method for computing conﬁdence intervals for func-\\ntions of the vector of parameters Bcan be based on bootstrap percentile\\nconﬁdencelimits.Foreachsamplewithreplacementfromtheoriginaldataset,\\none computes the MLE of B,b, and then the quantity of interest g(b). Then\\nthegs are sorted and the desired quantiles are computed. At least 1000 boot-\\nstrap samples will be needed for accurate assessment of outer conﬁdence\\nlimits. This method is suitable for obtaining pointwise conﬁdence bands for 8\\na nonlinear regression function, say, the relationship between age and the log\\nodds of disease. At each of 100 age values the predicted logits are computed\\nfor each bootstrap sample. Then separately for each age point the 0.025 and\\n0.975 quantiles of 1000 estimates of the logit are computed to derive a 0.95\\nconﬁdence band. Other more complex bootstrap schemes will achieve some-\\nwhat greater accuracy of conﬁdence interval coverage,178and as described\\nin Section 9.5one can use variations on the basic bootstrap in which the\\npredictors are considered ﬁxed and/or cluster sampling is taken into account.\\nTheRfunction bootcovin thermspackage bootstraps model ﬁts to obtain\\nunconditional (with respect to predictors) bootstrap distributions with or\\nwithout cluster sampling. bootcovstores the matrix of bootstrap regression\\ncoeﬃcients so that the bootstrapped quantities of interest can be computed\\nin one sweep of the coeﬃcient matrix once bootstrapping is completed. 9\\nFor many regression models. the rmspackage’s Predict,summary,a n d\\ncontrast functions make it easy to compute pointwise bootstrap conﬁdence\\nintervals in a variety of contexts. As an example, consider 200 simulated\\nxvalues from a log-normal distribution and simulate binary yfrom a true\\npopulation binary logistic model given by', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ce85254b-0431-4427-8002-e3c86867ba54', embedding=None, metadata={'page_label': '200', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"200 9 Overview of Maximum Likelihood Estimation\\nProb(Y=1|X=x)=1\\n1+exp[−(1+x/2)]. (9.52)\\nNot knowing the true model, a quadratic logistic model is ﬁtted. The Rcode\\nneeded to generate the data and ﬁt the model is given below.\\nrequire(rms)\\nn←200\\nset.seed (15)\\nx1←rnorm(n)\\nlogit ←x1/2\\ny←ifelse(runif(n) ≤plogis(logit), 1, 0)\\ndd←datadist(x1); options(datadist= 'dd')\\nf←lrm(y∼pol(x1,2), x=TRUE, y= TRUE)\\nprint(f, latex=TRUE)\\nLogistic Regression Model\\nlrm(formula = y ~ pol(x1, 2), x = TRUE, y = TRUE)\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 200 LRχ216.37R20.105C 0.642\\n09 7d.f. 2 g 0.680Dxy0.285\\n1 103 Pr(>χ2) 0.0003 gr1.973γ 0.286\\nmax|∂logL\\n∂β|3×10−9gp0.156τa0.143\\nBrier 0.231\\nCoef S.E. Wald ZPr(>|Z|)\\nIntercept -0.0842 0.1823 -0.46 0.6441\\nx1 0.5902 0.1580 3.74 0.0002\\nx120.1557 0.1136 1.37 0.1708\\nlatex(anova(f), file= '', table.env= FALSE)\\nχ2d.f.P\\nx1 13 .99 2 0.0009\\nNonlinear 1.88 1 0.1708\\nTOTAL 13 .99 2 0.0009\\nThebootcovfunction is used to draw 1000 resamples to obtain bootstrap\\nestimates of the covariance matrix of the regression coeﬃcients as well as\\nto save the 1000 ×3 matrix of regression coeﬃcients. Then, because indi-\\nvidual regression coeﬃcients for xdo not tell us much, we summarize the\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1379c9a6-3030-4e63-a43c-ede9020e41ae', embedding=None, metadata={'page_label': '201', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"9.7 Bootstrap Conﬁdence Regions 201\\nx-eﬀect by computing the eﬀect (on the logit scale) of increasing xfrom 1\\nto 5. We ﬁrst compute bootstrap nonparametric percentile conﬁdence inter-\\nvals the long way. The 1000 bootstrap estimates of the log odds ratio are\\ncomputed easily using a single matrix multiplication with the diﬀerence in\\npredictions approach, multiplying the diﬀerence in two design matrices, and\\nwe obtain the bootstrap estimate of the standard error of the log odds ratio\\nby computing the sample standard deviation of the 1000 valuese. Bootstrap\\npercentile conﬁdence limits are just sample quantiles from the bootstrapped\\nlog odds ratios.\\n# Get 2-row design matrix for obtaining predicted values\\n# for x = 1 and 5\\nX←cbind(Intercept=1,\\npredict(f, data.frame(x1=c(1 ,5)),type= 'x'))\\nXdif←X[2,,drop=FALSE] - X[1,,drop=FALSE]\\nXdif\\nIntercept pol(x1, 2)x1 pol(x1, 2)x1∧2\\n20 4 2 4\\nb←bootcov(f, B =1000)\\nboot.log.odds.ratio ←b$boot.Coef %*% t( Xdif)\\nsd(boot.log.odds.ratio)\\n[1] 2.752103\\n# This is the same as from summary(b, x=c(1 ,5)) as summary\\n# uses the bootstrap covariance matrix\\nsummary(b, x1=c(1 ,5))[1, 'S.E. ']\\n[1] 2.752103\\n# Compare this s.d. with one from information matrix\\nsummary(f, x1=c(1 ,5))[1, 'S.E. ']\\n[1] 2.988373\\n# Compute percentiles of bootstrap oddsratio\\nexp(quantile(boot.log.odds.ratio, c( .025,.975)))\\n2.5% 97.5%\\n2.795032e+00 2.067146e+05\\n# Automatic:\\nsummary(b, x1=c(1 ,5))[ 'Odds Ratio ',]\\neAs indicated below, this standard deviation can also be obtained by using the\\nsummary function on the object returned by bootcov,a sbootcov returns a ﬁt object\\nlike one from lrmexcept with the bootstrap covariance matrix substituted for the\\ninformation-based one.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='841dc6ae-59c7-4c92-8e2a-0679ade0def6', embedding=None, metadata={'page_label': '202', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"202 9 Overview of Maximum Likelihood Estimation\\nLow High Diff. Effect S.E.\\n1.000000e+00 5.000000e+00 4.000000e+00 4.443932e+02 NA\\nLower 0.95 Upper 0.95 Type\\n2.795032e+00 2.067146e+05 2.000000e+00\\nprint(contrast(b, list(x1=5), list(x1=1), fun=exp))\\nContrast S.E. Lower Upper Z Pr(>|z|)\\n11 6.09671 2.752103 1.027843 12.23909 2.22 0.0267\\nConfidence intervals are 0.95bootstrap nonparametric percentile intervals\\n# Figure 9.4\\nhist(boot.log.odds.ratio, nclass=100,xlab= 'log(OR) ',\\nmain= '')\\nlog(OR)Frequency\\n0 5 10 15010203040\\nFig. 9.4 Distribution of 1000 bootstrap x=1:5 log odds ratios\\nFigure9.4shows the distribution of log odds ratios.\\nNow consider conﬁdence bands for the true log odds that y=1 ,a c r o s s\\na sequence of xvalues. The Predictfunction automatically calculates point-\\nby-point bootstrap percentiles, basic bootstrap, or BCa203conﬁdence limits\\nwhen the ﬁt has passed through bootcov. Simultaneous Wald-based conﬁ-\\ndence intervals307and Wald intervals substituting the bootstrap covariance\\nmatrix estimator are added to the plot when Predictcalls the multcomp pack-\\nage (Figure 9.5).\\nx1s←seq(0, 5, length=100)\\npwald ←Predict(f, x1=x1s)\\npsand ←Predict( robcov(f), x1=x1s)\\npbootcov ←Predict(b, x1=x1s, usebootcoef= FALSE)\\npbootnp ←Predict(b, x1=x1s)\\npbootbca ←Predict(b, x1=x1s, boot.type= 'bca')\\npbootbas ←Predict(b, x1=x1s, boot.type= 'basic ')\\npsimult ←Predict(b, x1=x1s, conf.type= 'simultaneous ')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a5f052cc-d821-4c9d-8203-33044aba6707', embedding=None, metadata={'page_label': '203', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"9.8 Further Use of the Log Likelihood 203\\nz←rbind( 'Bootpercentile '= pbootnp,\\n'Robust sandwich '= psand,\\n'Boot BCa ' = pbootbca,\\n'Bootcovariance+ Wald '= pbootcov,\\nWald = pwald,\\n'Boot basic ' = pbootbas,\\nSimultaneous = psimult)\\nz$class ←ifelse(z$.set. %in% c( 'Bootpercentile ','Boot bca ',\\n'Boot basic '),'Other ','Wald ')\\nggplot(z, groups=c( '.set. ','class '),\\nconf= 'line ', ylim=c(-1, 9), legend.label= FALSE)\\nSee Problemsat chapter’send for a worrisomeinvestigationof bootstrapcon-\\nﬁdence interval coverage using simulation. It appears that when the model’s\\nlog odds distribution is not symmetric and includes very high or very low\\nprobabilities, neither the bootstrap percentile nor the bootstrap BCa inter-\\nvals have good coverage, while the basic bootstrap and ordinary Wald in-\\ntervals are fairly accuratef. It is diﬃcult in general to know when to trust\\nthe bootstrap for logistic and perhaps other models when computing conﬁ-\\ndence intervals, and the simulation problem suggeststhat the basic bootstrap\\nshould be used more frequently. Similarly, the distribution of bootstrap eﬀect\\nestimates can be suspect. Asymmetry in this distribution does not imply that\\nthe true sampling distribution is asymmetric or that the percentile intervals\\nare preferred.\\n9.8 Further Use of the Log Likelihood\\n9.8.1 Rating Two Models, Penalizing for Complexity\\nSuppose that froma singlesampletwocompeting modelsweredeveloped.Let\\nthe respective −2log likelihoods for these models be denoted by L1andL2,\\nand letp1andp2denote the number of parameters estimated in each model.\\nSuppose that L1<L2. It may be tempting to rate model one as the “best”\\nﬁtting or “best” predicting model. That model may provide a better ﬁt for\\nthe data at hand, but if it required many more parameters to be estimated,\\nit may not be better“for the money.”If both models were applied to a new\\nsample, model one’s overﬁtting of the original dataset may actually result in\\na worse ﬁt on the new dataset.\\nfLimited simulations using the conditional bootstrap and Firth’s penalized likeli-\\nhood281did not show signiﬁcant improvement in conﬁdence interval coverage.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9fc1e467-0777-4b4e-840e-da8c179ba815', embedding=None, metadata={'page_label': '204', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='204 9 Overview of Maximum Likelihood Estimation\\n0.02.55.07.5\\n012345\\nx1log odds.set.\\nBoot percentile\\nRobust sandwich\\nBoot BCa\\nBoot covariance+Wald\\nWald\\nBoot basic\\nSimultaneous\\nOther\\nWald\\nFig. 9.5 Predicted log odds and conﬁdence bands for seven types of conﬁdence in-\\ntervals. Seven categories are ordered top to bottom corresponding to order of lower\\nconﬁdence bands at x1=5. Dotted lines are for Wald–type methods that yield sym-\\nmetric conﬁdence intervals and assume normality of point estimators.\\nAkaike’s information criterion (AIC33,359,633)p r o v i d e sam e t h o df o rp e -\\nnalizing the log likelihood achieved by a given model for its complexity to\\nobtain a more unbiased assessment of the model’s worth. The penalty is\\nto subtract the number of parameters estimated from the log likelihood, or\\nequivalently to add twice the number of parameters to the −2log likelihood.\\nThe penalized log likelihood is analogous to Mallows’ Cpin ordinary multiple\\nregression. AIC would choose the model by comparing L1+2p1toL2+2p2\\nand picking the model with the lower value. We often use AIC in“adjusted 10\\nχ2”form:\\nAIC = LR χ2−2p. (9.53)\\nBreiman [ 66, Section 1.3] and Chatﬁeld [ 100, Section 4] discuss the fallacy of\\nAIC and Cpfor selecting from a series of non-prespeciﬁed models. 11\\n9.8.2 Testing Whether One Model Is Better\\nthan Another\\nOne way to test whether one model ( A) is better than another ( B)i st o\\ne m b e db o t hm o d e l si nam o r eg e n e r a lm o d e l( A+B). Then a LR χ2test', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9a506f2-8e1a-4e90-a854-f8a141303a04', embedding=None, metadata={'page_label': '205', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.8 Further Use of the Log Likelihood 205\\ncan be done to test whether Ais better than Bby changing the hypothesis\\nto test whether Aadds predictive information to B(H0:A+B>B)a n d\\nwhether Badds information to A(H0:A+B>A). The approach of testing\\nA>Bvia testing A+B>BandA+B>Ais especially useful for selecting\\nfrom competing predictors such as a multivariable model and a subjective\\nassessor.131,264,395,669\\nNote that LR χ2forH0:A+B>BminusLR χ2forH0:A+B>A\\nequalsLR χ2forH0:Ahas no predictive information minus LR χ2for\\nH0:Bhas no predictive information,665the diﬀerence in LR χ2for testing\\neach model (set of variables) separately. This gives further support to the use\\nof two separately computed Akaike’s information criteria for rating the two\\nsets of variables. 12\\nSee Section 9.8.4for an example.\\n9.8.3 Unitless Index of Predictive Ability\\nThegloballikelihoodratiotestforregressionisusefulfordeterminingwhether\\nany predictor is associated with the response. If the sample is large enough,\\neven weak associations can be“statistically signiﬁcant.”Even though a like-\\nlihood ratio test does not shed light on a model’s predictive strength, the log\\nlikelihood (L.L.) can still be useful here. Consider the following L.L.s:\\nBest (lowest) possible −2 L.L.:\\nL∗=−2 L.L. for a hypothetical model that perfectly predicts the outcome.\\n−2 L.L. achieved:\\nL=−2 L.L. for the ﬁtted model.\\nWorst−2 L.L.:\\nL0=−2 L.L. for a model that has no predictive information.\\nThe last −2 L.L., for a“no information”model, is the −2 L.L. under the null\\nhypothesis that all regressioncoeﬃcients except for intercepts are zero. A“no\\ninformation”model often contains only an intercept and some distributional\\nparameters (a variance, for example). 13\\nThe quantity L0−LisLR, the log likelihood ratio statistic for testing\\nthe global null hypothesis that no predictors are related to the response. It\\nis also the −2 log likelihood“explained”by the model. The best (lowest) −2\\nL.L. isL∗, so the amount of L.L. that is capable of being explained by the\\nmodel is L0−L∗. The fraction of −2 L.L. explained that was capable of being\\nexplained is\\n(L0−L)/(L0−L∗)=LR/(L0−L∗). (9.54)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='91bfa583-d2ac-4a27-9b52-2347c4c9ccd9', embedding=None, metadata={'page_label': '206', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='206 9 Overview of Maximum Likelihood Estimation\\nThe fraction of log likelihood explained is analogous to R2in an ordinary\\nlinear model, although Korn and Simon365,366provide a much more precise\\nnotion.\\nAkaike’s information criterion can be used to penalize this measure of\\nassociation for the number of parameters estimated ( p, say) to transform\\nthis unitless measure of association into a quantity that is analogous to the\\nadjusted R2or Mallows’ Cpin ordinary linear regression. We let Rdenote\\nthe square root of such a penalized fraction of log likelihood explained. Ris\\ndeﬁned by\\nR2=(LR−2p)/(L0−L∗). (9.55)\\nTheRindex can be used to assess how well the model compares with a\\n“perfect”model, as well as to judge whether a more complex model has pre-\\ndictive strength that justiﬁes its additional parameters. Had pbeen used in\\nEquation 9.55rather than 2 p,R2is negative if the log likelihood explained\\nis less than what one would expect by chance. Rwill be the square root of\\n1−2p/(L0−L∗) if the model perfectly predicts the response. This upper\\nlimit will be near one if the sample size is large.\\nPartialRindexescanalsobe deﬁnedbysubstituting the −2L.L.explained\\nfor a given factor in place of that for the entire model, LR. The “penalty\\nfactor”pbecomes one. This index Rpartialis deﬁned by\\nR2\\npartial=(LRpartial−2)/(L0−L∗), (9.56)\\nwhich is the (penalized) fraction of −2 log likelihood explained by the pre-\\ndictor. Here LRpartialis the log likelihood ratio statistic for testing whether\\nthe predictor is associated with the response, after adjustment for the other\\npredictors. Since such likelihood ratio statistics are tedious to compute, the\\n1 d.f. Wald χ2can be substituted for the LRstatistic (keeping in mind that\\ndiﬃculties with the Wald statistic can arise).\\nLiu and Dyer424and Cox and Wermuth136point out diﬃculties with the\\nR2measure for binary logistic models. Cox and Snell135and Magee432used\\nother analogies to derive other R2measures that may have better properties.\\nFor a sample of size nand a Wald statistic for testing overall association,\\nthey deﬁned\\nR2\\nW=W\\nn+W\\nR2\\nLR=1−exp(−LR/n) (9.57)\\n=1−λ2/n,\\nwhereλis the null model likelihood divided by the ﬁtted model likelihood. In\\nthe case of ordinary least squares with normality both of the above indexes\\nare equal to the traditional R2.R2\\nLRis equivalent to Maddala’s index [ 431,\\nEq.2.44]. Cragg and Uhler137and Nagelkerke471suggested dividing R2\\nLRby', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d2d1239-103e-44db-a50e-8756f048a0c2', embedding=None, metadata={'page_label': '207', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.8 Further Use of the Log Likelihood 207\\nits maximum attainable value\\nR2\\nmax=1−exp(−L0/n) (9.58)\\nto derive R2\\nNwhich ranges from 0 to 1. This is the form of the R2index we\\nuse throughout.\\nFor penalizing for overﬁtting, see Verweij and van Houwelingen640for an\\noverﬁtting-corrected R2that uses a cross-validated likelihood. 14\\n9.8.4 Unitless Index of Adequacy of a Subset\\nof Predictors\\nLog likelihoods are also useful for quantifying the predictive information con-\\ntained in a subset of the predictors compared with the information contained\\nin the entire set of predictors.264LetLRagain denote the −2 log likelihood\\nratio statistic for testing the joint signiﬁcance of the full set of predictors. Let\\nLRsdenote the −2 log likelihood ratio statistic for testing the importance of\\nthe subset of predictors of interest, excluding the other predictors from the\\nmodel. A measure of adequacy of the subset for predicting the response is\\ngiven by\\nA=LRs/LR. (9.59)\\nAis then the proportion of log likelihood explained by the subset with refer-\\nence to the log likelihood explained by the entire set. When A= 1, the subset\\ncontains all the predictive information found in the whole set of predictors;\\nthat is, the subset is adequate by itself and the additional predictors contain\\nno independent information. When A= 0, the subset contains no predictive\\ninformation by itself.\\nCaliﬀ et al.89used the Aindex to quantify the adequacy (with respect to\\nprognosis)oftwocompetingsetsofpredictorsthateachdescribethe extentof\\ncoronary artery disease. The response variable was time until cardiovascular\\ndeath and the statistical model used was the Cox132proportional hazards\\nmodel. Some of their results are reproduced in Table 9.3. A chance-corrected 15\\nadequacy measure could be derived by squaring the ratio of the R-index for\\nthe subset to the R-index for the whole set. A formal test of superiority of\\nX1= maximum % stenosis over X2= jeopardy score can be obtained by\\ntesting whether X1adds toX2(LR χ2=5 7.5−42.6=1 4.9) and whether\\nX2adds toX1(LR χ2=5 7.5−51.8=5.7).X1adds more to X2(14.9) than\\nX2adds toX1(5.7). The diﬀerence 14 .9−5.7=9.2 equals the diﬀerence in\\nsingle factor χ2(51.8−42.6)665.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e3f2451-1052-47c7-9b1c-c59e180408b9', embedding=None, metadata={'page_label': '208', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='208 9 Overview of Maximum Likelihood Estimation\\nTable 9.3 Completing prognostic markers\\nPredictors Used LR χ2Adequacy\\nCoronary jeopardy score 42.6 0.74\\nMaximum % stenosis in each artery 51.8 0.90\\nCombined 57.5 1.00\\n9.9 Weighted Maximum Likelihood Estimation\\nIt is commonly the case that data elements represent combinations of values\\nthat pertain to a set of individuals. This occurs, for example, when unique\\ncombinations of XandYare determined from a massive dataset, along with\\nthe frequency of occurrence of each combination, for the purpose of reducing\\nthe size of the dataset to analyze. For the ith combination we have a case\\nweightwithat is a positive integer representing a frequency. Assuming that\\nobservations represented by combination iare independent, the likelihood\\nneeded to represent all wiobservations is computed simply by multiplying\\nall of the likelihood elements (each having value Li), yielding a total likeli-\\nhood contribution for combination iofLwi\\nior a log likelihood contribution\\nofwilogLi. To obtain a likelihood for the entire dataset one computes the\\nproduct over all combinations. The total log likelihood is∑wilogLi.A sa n\\nexample, the weighted likelihood that would be used to ﬁt a weighted logistic\\nregression model is given by\\nL=n∏\\ni=1PwiYi\\ni(1−Pi)wi(1−Yi), (9.60)\\nwhere there are ncombinations,∑n\\ni=1wi>n,a n dPiis Prob[Yi=1|Xi]a s\\ndictated by the model. Note that in general the correct likelihood function\\ncannotbeobtainedbyweightingthedataandusinganunweightedlikelihood.\\nBy a small leap one can obtain weighted maximum likelihood estimates\\nfrom the above method even if the weights do not represent frequencies or\\neven integers, as long as the weights are non-negative. Non-frequency weights\\nare commonly used in sample surveys to adjust estimates back to better\\nrepresent a target population when some types of subjects have been over-\\nsampled from that population. Analysts should beware of possible losses in\\neﬃciency when obtaining weighted estimates in sample surveys.363,364Mak-\\ningtheregressionestimatesconditionalonsamplingstratabyincludingstrata\\nas covariables may be preferable to re-weighting the strata. If weighted esti-\\nmates must be obtained, the weighted likelihood function is generally valid\\nfor obtaining properly weighted parameter estimates. However, the variance–\\ncovariance matrix obtained by inverting the information matrix from the\\nweighted likelihood will not be correct in general. For one thing, the sum of\\nthe weights may be far from the number of subjects in the sample. A rough', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='886e1276-a552-427b-9ed9-6e9fcad7db5e', embedding=None, metadata={'page_label': '209', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.10 Penalized Maximum Likelihood Estimation 209\\napproximation to the variance–covariance matrix may be obtained by ﬁrst\\nmultiplying each weight by n/∑wiand then computing the weighted in-\\nformation matrix, where nis the number of actual subjects in the sample.\\n16\\n9.10 Penalized Maximum Likelihood Estimation\\nMaximizing the log likelihood provides the best ﬁt to the dataset at hand,\\nbut this can also result in ﬁtting noise in the data. For example, a categor-\\nical predictor with 20 levels can produce extreme estimates for some of the\\n19 regression parameters, especi ally for the sma ll cells (see Section 4.5). A\\nshrinkage approach will often result in regression coeﬃcient estimates that\\nwhile biased are lower in mean squared error and hence are more likely to be\\nclose to the true unknown parametervalues. Ridge regressionis one approach\\nto shrinkage, but a more general and bette r developed approach is penalized\\nmaximum likelihood estimation,237,388,639,641which is really a special case 17\\nof Bayesian modeling with a Gaussian prior. Letting Ldenote the usual like-\\nlihood function and λbe a penalty factor, we maximize the penalized log\\nlikelihood given by\\nlogL−1\\n2λp∑\\ni=1(siβi)2, (9.61)\\nwheres1,s2,...,s pare scale factors chosen to make siβiunitless. Most au-\\nthors standardize the data ﬁrst and do not have scale factors in the equation,\\nbut Equation 9.61has the advantage of allowing estimation of βon the orig-\\ninal scale of the data. The usual methods (e.g., Newton–Raphson) are used\\nto maximize 9.61.\\nThe choice of the scaling constants has received far too little attention in\\nthe ridge regression and penalized MLE literature. It is common to use the 18\\nstandard deviation of each column of the design matrix to scale the corre-\\nsponding parameter. For models containing nothing but continuous variables\\nthat enter the regression linearly, this is usually a reasonable approach. For\\ncontinuous variables represented with multiple terms (one of which is lin-\\near), it is not always reasonable to scale each nonlinear term with its own\\nstandard deviation. For dummy variables, scaling using the standard devia-\\ntion (√\\nd(1−d), where dis the mean of the dummy variable, i.e., the frac-\\ntion of observations in that cell) is problematic since this will result in high\\nprevalance cells getting more shrinkage than low prevalence ones because the\\nhigh prevalence cells will dominate the penalty function.\\nAn advantage of the formulation in Equation 9.61is that one can assign\\nscale constantsof zerofor parametersfor whichno shrinkageis desired.237,639\\nFor example, one may have prior beliefs that a linear additive model will ﬁt\\nthe data. In that case, nonlinear and non-additive terms may be penalized.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f89feba-89eb-4657-8a9d-59b5b9dba4be', embedding=None, metadata={'page_label': '210', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='210 9 Overview of Maximum Likelihood Estimation\\nForacategoricalpredictorhaving clevels,usersofridgeregressionoftendo\\nnot recognizethat the amount of shrinkageand the predicted values from the\\nﬁtted model depend on how the design matrix is coded. For example, one will\\nget diﬀerent predictions depending on which cell is chosen as the reference\\ncell when constructing dummy variables. The setup in Equation 9.61has the\\nsame problem. For example, if for a three-category factor we use category 1\\nas the reference cell and have parameters β2andβ3, the unscaled penalty\\nfunction is β2\\n2+β2\\n3. If category 3 were used as the reference cell instead, the\\npenalty would be β2\\n3+(β2−β3)2. To get around this problem, Verweij and\\nvan Houwelingen639proposed using the penalty function∑c\\ni(βi−β)2,w h e r e\\nβis the mean of all cβs. This causes shrinkage of all parameters toward\\nthe mean parameter value. Letting the ﬁrst category be the reference cell,\\nwe usec−1 dummy variables and deﬁne β1≡0. For the case c=3t h e\\nsum of squares is 2[ β2\\n2+β2\\n3−β2β3]/3. Forc=2t h ep e n a l t yi s β2\\n2/2. If no\\nscale constant is used, this is the same as scaling β2with√\\n2×the standard\\ndeviation of a binary dummy variable with prevalance of 0.5.\\nThe sum of squares can be written in matrix form as [ β2,...,β c]′\\n(A−B)[β2,...,β c], where Ais ac−1×c−1i d e n t i t ym a t r i xa n d Bis\\nac−1×c−1 matrix all of whose elements are1\\nc. 19\\nFor general penalty functions such as that just described, the penalized\\nlog likelihood can be generalized to\\nlogL−1\\n2λβ′Pβ. (9.62)\\nFor purposes of using the Newton–Raphson procedure, the ﬁrst derivative\\nof the penalty function with respect to βis−λPβ, and the negative of the\\nsecond derivative is λP.\\nAnother problem in penalized estimation is how the choice of λis made. 20\\nMany authors use cross-validation. A limited number of simulation stud-\\nies in binary logistic regression modeling has shown that for each λbeing\\nconsidered, at least 10-fold cross-validation must be done so as to obtain a\\nreasonable estimate of predictive accuracy. Even then, a smoother207(“su-\\nper smoother”) must be used on the ( λ, accuracy) pairs to allow location of\\nthe optimum value unless one is careful in choosing the initial sub-samples\\nand uses these same splits throughout. Simulation studies have shown that a\\nmodiﬁed AIC is not only much quicker to compute (since it requires no cross-\\nvalidation) but performs better at ﬁnding a good value of λ(see below). 21\\nFor a given λ, the eﬀective number of parameters being estimated is re-\\nduced because of shrinkage. Gray [ 237, Eq.2.9] and others estimate the ef-\\nfective degrees of freedom by computing the expected value of a global Wald\\nstatistic for testing association, when the null hypothesis of no association is\\ntrue. The d.f. is equal to\\ntrace[I(ˆβP)V(ˆβP)], (9.63)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c330b7dd-5ccd-4a8e-811a-41615bb738df', embedding=None, metadata={'page_label': '211', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.10 Penalized Maximum Likelihood Estimation 211\\nwhereˆβPis the penalized MLE (the parameters that maximize Equa-\\ntion9.61),Iis the information matrix computed from ignoring the penalty\\nfunction, and Vis the covariance matrix computed by inverting the infor-\\nmation matrix that included the second derivatives with respect to βin the\\npenalty function. 22\\nGray [237, Eq.2.6]states that a better estimate of the variance–covariance\\nmatrix for ˆβPthanV(ˆβP)i s\\nV∗=V(ˆβP)I(ˆβP)V(ˆβP). (9.64)\\nTherneau (personal communication, 2000) has found in a limited number\\nof simulation studies that V∗underestimates the true variances, and that a\\nbetter estimate of the variance–covariancematrix is simply V(ˆβP), assuming\\nthat the model is correctly speciﬁed. This is the covariance matrix used by\\ndefault in the rmspackage (the user can request that the sandwich estimator\\nbe used instead) and is in fact the one Gray used for Wald tests.\\nPenalization will bias estimates of β, so hypothesis tests and conﬁdence\\nintervals using ˆβPmay not have a simple interpretation. The same prob-\\nlem arises in score and likelihood ratio tests. So far, penalization is better\\nunderstood in pure prediction mode unless Bayesian methods are used.\\nEquation 9.63can be used to derive a modiﬁed AIC (see [ 639,E q . 6 ]\\nand [641, Eq.7]) on the model χ2scale:\\nLRχ2−2×eﬀective d .f., (9.65)\\nwhere LR χ2is the likelihood ratio χ2for the penalized model, but ignoring\\nthe penalty function. If a variety of λare tried and one plots the ( λ,A I C )\\npairs, the λthat maximizes AIC will often be a good choice, that is, it is\\nlikely to be near the value of λthat maximizes predictive accuracy on a\\nfuture datasetg.\\nNotethat ifonedoespenalizedmaximumlikelihoodestimationwhereaset\\nof variablesbeing penalized has a negativevalue for the unpenalized χ2−2×\\nd.f., the value of λthat will optimize the overall model AIC will be ∞.\\nAs an example, considersomesimulated data ( n= 100)with one predictor\\nin which the true model is Y=X1+ǫ,w h e r e ǫhas a standard normal\\ndistribution and so does X1. We use a series of penalties (found by trial and\\nerror)that give rise to sensible eﬀective d.f., and ﬁt penalized restricted cubic\\nspline functions with ﬁve knots. We penalize two ways: all terms in the model\\nincluding the coeﬃcient of X1, which in reality needs no penalty; and only\\nthe nonlinear terms. The following Rprogram, in conjunction with the rms\\npackage, does the job.\\ngSeveral examples from simulated datasets have shown that using BIC to choose a\\npenalty results in far too much shrinkage.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e8724da-f5b3-4fda-abf8-3dc4466e9acc', embedding=None, metadata={'page_label': '212', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"212 9 Overview of Maximum Likelihood Estimation\\nset.seed (191)\\nx1←rnorm(100)\\ny←x1 + rnorm(100)\\npens←df←aic←c(0,.07,.5,2,6,15,60)\\nall←nl←list()\\nfor(penalize in 1:2) {\\nfor(i in 1:length(pens)) {\\nf←ols(y∼rcs(x1,5), penalty=\\nlist(simple=if( penalize ==1)pens[i] else 0,\\nnonlinear= pens[i]))\\ndf[i] ←f$stats[ 'd.f. ']\\naic[i] ←AIC(f)\\nnam←paste(if( penalize == 1) 'all'else 'nl',\\n'penalty: ', pens[i], sep= '')\\nnam←as.character( pens[i])\\np←Predict(f, x1=seq( -2.5, 2.5, length=100),\\nconf.int= FALSE)\\nif(penalize == 1) all[[nam]] ←p else nl[[nam]] ←p\\n}\\nprint(rbind(df=df, aic=aic))\\n}\\n[,1] [,2] [,3] [,4] [,5] [,6]\\ndf 4.0000 3.213591 2.706069 2.30273 2.029282 1.822758\\naic 270.6653 269.154045 268.222855 267.56594 267.288988 267.552915\\n[,7]\\ndf 1.513609\\naic 270.805033\\n[,1] [,2] [,3] [,4] [,5] [,6]\\ndf 4.0000 3.219149 2.728126 2.344807 2.109741 1.960863\\naic 270.6653 269.167108 268.287933 267.718681 267.441197 267.347475\\n[,7]\\ndf 1.684421\\naic 267.892073\\nall←do.call( 'rbind ', all); all$type ←'Penalize All '\\nnl←do.call( 'rbind ', nl) ; nl$type ←'Penalize Nonlinear '\\nboth←as.data.frame(rbind.data.frame(all, nl))\\nboth$Penalty ←both$.set.\\nggplot(both, aes(x=x1, y=yhat, color=Penalty)) + geom_line() +\\ngeom_abline(col=gray(.7)) + facet_grid( ∼type)\\n# Figure 9.6\\nThe left panel in Figure 9.6corresponds to penalty = list(simple=a, nonlin-\\near=a)in theRprogram,meaningthatallparametersexceptthe interceptare\\nshrunkby the sameamount a(this wouldbe moreappropriatehadtherebeen\\nmultiple predictors). As eﬀective d.f. get smaller (penalty factor gets larger),\\nthe regressionﬁts get ﬂatter (too ﬂat for the largestpenalties) and conﬁdence\\nbands get narrower. The right graph corresponds to penalty=list(simple=0,\\nnonlinear=a) , causing only the cubic spline terms that are nonlinear in X1\\nto be shrunk. As the amount of shrinkage increases (d.f. lowered), the ﬁts\\nbecome more linear and closer to the true regression line (longer dotted line).\\nAgain, conﬁdence intervals become smaller. 23\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='588c8ab6-c4e8-417e-b049-eaeb368e4ab7', embedding=None, metadata={'page_label': '213', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.11 Further Reading 213\\nPenalize All Penalize Nonlinear\\n−2024\\n−2 −1 0 1 2 −2 −1 0 1 2\\nx1yhatPenalty\\n0\\n0.07\\n0.5\\n2\\n6\\n15\\n60\\nFig. 9.6 Penalized least squares estimates for a n unnecessary ﬁve-k not restricted\\ncubic spline function. In the left graph all parameters (except the intercept) are\\npenalized. The eﬀective d.f. are 4 ,3.21,2.71,2.30,2.03,1.82, and 1 .51. In the right\\ngraph, only parameters associated with nonlinear functions of X1are penalized. The\\neﬀective d.f. are 4 ,3.22,2.73,2.34,2.11,1.96, and 1 .68.\\n9.11 Further Reading\\n1Boos60has some nice generalizations of the score test. Morgan et al.464show\\nhow score test χ2statistics may negative unless the expected information matrix\\nis used.\\n2See Marubini and Valsecchi [ 444, pp. 164–169] for an excellent description of\\nthe relationship between the three types of test statistics.\\n3References [ 115,507] have good descriptions of methods used to maximize log L.\\n4As Long and Ervin426argue, for small sample sizes, the usual Huber–White co-\\nvariance estimator should not be used because there the residuals do not have\\nconstant variance even under homoscedasticity. They showed that a simple co r-\\nrection due to Efron and others can result in substantially better estimates.\\nLin and Wei,410Binder,55and Lin407have applied the Huber estimator to the\\nCox132survival model. Freedman206questioned the use of sandwich estima-\\ntors because they are often used to obtain the right variances on the wrong\\nparameters when the model doesn’t ﬁt. He also has some excellent background\\ninformation.\\n5Feng et al.188showed that in the case of cluster correlations arising from re-\\npeated measurement data with Gaussian errors, the cluster bootstrap performs\\nexcellently even when the number of observations per cluster is large and the\\nnumber of subjects is small. Xiao and Abrahamowicz676compared the cluster\\nbootstrap with a two-stage cluster bootstrap in the context of the Cox model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff928875-73b7-4876-83b0-91f5bc530c17', embedding=None, metadata={'page_label': '214', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='214 9 Overview of Maximum Likelihood Estimation\\n6Graubard and Korn235and Fitzmaurice195describe the kinds of situations in\\nwhich the working independence model can be trusted.\\n7Minkin,460Alho,11Doganaksoy and Schmee,160and Meeker and Escobar452\\ndiscuss the need for LR and score-based conﬁdence intervals. Alho found that\\nscore-based intervals are usually more tedious to compute, and provided use-\\nful algorithms for the computation of either type of interval (see also [ 452]\\nand [444, p. 167]). Score and LR intervals require iterative computations and\\nhave to deal with the fact that when one parameter is changed (e.g., biis re-\\nstricted tobezero), allotherparameter estimates change. DiCiccio andEfron157\\nprovide a method for very accurate conﬁdence intervals for exponential fa milies\\nthat requires a modest amount of additional computation. Venzon and Mool-\\ngavkarprovidean eﬃcient general method forcomputing LR-basedintervals.636\\nBrazzale and Davison65developed some promising and feasible ways to make\\nunconditional likelihood-based inferences more accurate in small samples.\\n8Carpenter and Bithell92have an excellent overview of several variations on the\\nbootstrap for obtaining conﬁdence limits.\\n9Tibshirani and Knight610developed an easy to program approach for deriving\\nsimultaneous conﬁdence sets that is likely to be useful for getting simultaneous\\nconﬁdence regions for the entire vector of model parameters, for population val-\\nues for an entire sequence of predictor values, and for a set of regression eﬀects\\n(e.g., interquartile-range odds ratios for age for both sexes). The basic idea is\\nthat during the, say, 1000 bootstrap repetitions one stores the −2 log likelihood\\nfor each model ﬁt, being careful to compute the likelihood at the current boot -\\nstrap parameter estimates but with respect to the original data matrix, not\\nthe bootstrap sample of the data matrix. To obtain an approximate simultane-\\nous 0.95 conﬁdence set one computes the 0.95 quantile of the −2 log likelihood\\nvalues and determines which vectors of parameter estimates correspond to −2\\nlog likelihoods that are at least as small as the 0.95 quantile of all −2 log like-\\nlihoods. Once the qualifying parameter estimates are found, the quantities of\\ninterest are computed from those parameter estimates and an outer envelope\\nof those quantities is found. Computations are facilitated with the rmspackage\\nconfplot function.\\n10van Houwelingen and le Cessie [ 633, Eq.52] showed, consistent with AIC, that\\nthe average optimism in a mean logarithmic (minus log likelihood) quality score\\nfor logistic models is p/n.\\n11Schwarz560derived a diﬀerent penalty using large-sample Bayesian properties\\nof competing models. His Bayesian Information Criterion (BIC) chooses the\\nmodel having the lowest value of L+1/2plognor the highest value of LR\\nχ2−plogn. Kass and Raftery have done several studies of BIC.337Smith\\nand Spiegelhalter576and Laud and Ibrahim377discussed other useful gener-\\nalizations of likelihood penalties. Zheng and Loh685studied several penalty\\nmeasures, and found that AIC does not penalize enough for overﬁtting in the\\nordinary regression case. Kass and Raftery [ 337, p. 790] provide a nice review\\nof this topic, stating that “AIC picks the correct model asymptotically if the\\ncomplexity of the true model grows with sample size” and that “AIC selects\\nmodels that are too big even when the sample size is large.”But they also cite\\nother papers that show the existence of cases where AIC can work better than\\nBIC. According to Buckland et al.,80BIC “assumes that a true model exists\\nand is low-dimensional.”\\nHurvich and Tsai314,315made an improvement in AIC that resulted in much\\nbetter model selection for small n. They deﬁned the corrected AIC as\\nAICC=L Rχ2−2p[1+p+1\\nn−p−1]. (9.66)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eab56f67-b210-4153-8ce9-c601f770c7de', embedding=None, metadata={'page_label': '215', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.11 Further Reading 215\\nIn [314] they contrast asymptotically eﬃcient model selection with AIC when\\nthe true model has inﬁnitely many parameters with improvements using ot her\\nindexes such as AIC Cwhen the model is ﬁnite.\\nOne diﬃculty in applying the Schwarz, AIC C, and related criteria is that with\\ncensored or binary responses it is not clear that the actual sample size nshould\\nbe used in the formula.\\n12Goldstein,222Willan et al.,669and Royston and Thompson534have nice dis-\\ncussions on comparing non-nested regression models. Schemper’s method549is\\nuseful for testing whether a set of variables provides signiﬁcantly greater infor-\\nmation (using an R2measure) than another set of variables.\\n13van Houwelingen and le Cessie [ 633, Eq.22] recommended using L/2 (also called\\nthe Kullback–Leibler error rate) as a quality index.\\n14Schemper549provides a bootstrap technique for testing for signiﬁcant diﬀer-\\nences between correlated R2measures. Mittlb ¨ock and Schemper,461Schemper\\nand Stare,554Korn and Simon,365,366Menard,454and Zheng and Agresti684\\nhave excellent discussions about the pros and cons of various indexes of the\\npredictive value of a model.\\n15Al-Radi et al.10presented another analysis comparing competing predictors\\nusing the adequacy index and a receiver operating characteristic curve area\\napproach based on a test for whether one predictor has a higher probability of\\nbeing“more concordant” than another.\\n16[55,97,409] provide good variance–covariance estimators from a weighted max-\\nimum likelihood analysis.\\n17Huang and Harrington310developed penalized partial likelihood estimates for\\nCox models and provided useful background information and theoretical results\\nabout improvements in mean squared errors of regression estimates. They used\\na bootstrap error estimate for selection of the penalty parameter.\\n18Sardy538proposes that the square roots of the diagonals of the inverse of the\\ncovariance matrix for the predictors be used for scaling rather than thes tandard\\ndeviations.\\n19Park and Hastie483and articles referenced therein describe how quadratic pe-\\nnalized logistic regression automatically sets coeﬃcient estimates for empty cells\\nto zero and forces the sum of kcoeﬃcients for a k-level categorical predictor to\\nequal zero.\\n20Greenland241has a nice discussion of the relationship between penalized max-\\nimum likelihood estimation and mixed eﬀects models. He cautions against esti-\\nmating the shrinkage parameter.\\n21See310f o rab o o t s t r a pa p p r o a c ht os e l e c t i o no f λ.\\n22Verweij and van Houwelingen [ 639, Eq.4] derived another expressionfor d.f.,but\\nitrequiresmore computation anddidnotperform anybetterthanEquation 9.63\\nin choosing λin several examples tested.\\n23See van Houwelingen and Thorogood631for an approximate empirical Bayes\\napproach to shrinkage. See Tibshirani608for the use of a non-smooth penalty\\nfunction that results in variable selection as well as shrinkage (see Section 4.3).\\nVerweij and van Houwelingen640used a “cross-validated likelihood” based on\\nleave-out-one estimates to penalize for overﬁtting. Wang and Taylor652pre-\\nsented some methods for carrying out hypothesis tests and computing con-\\nﬁdence limits under penalization. Moons et al.462presented a case study of\\npenalized estimation and discussed the advantages of penalization.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ac96688-8a11-4f6a-a4b2-2e276c96f1a7', embedding=None, metadata={'page_label': '216', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='216 9 Overview of Maximum Likelihood Estimation\\nTable 9.4 Likelihood ratio global test statistics\\nVariables in Model LR χ2\\nage 100\\nsex 108\\nage, sex 111\\nage260\\nage, age2102\\nage, age2, sex 115\\n9.12 Problems\\n1. A sample of size 100 from a normal distribution with unknown mean and\\nstandard deviation ( μandσ) yielded the following log likelihood values\\nwhen computed at two values of μ.\\nlogL(μ=1 0,σ=5 )=−800\\nlogL(μ=2 0,σ=5 )=−820.\\nWhat do you know about μ? What do you know about Y?\\n2. Severalregressionmodelswereconsideredforpredicting aresponse. LRχ2\\n(corrected for the intercept) for models containing various combinations of\\nvariables are found in Table 9.4. Compute all possible meaningful LR χ2.\\nFor each, state the d.f. and an approximate P-value. State which LR χ2\\ninvolving only one variable is not very meaningful.\\n3. For each problem below, rank Wald, score, and LRstatistics by overall\\nstatistical properties and then by computational convenience.\\na. A forwardstepwise variableselection (to be lateraccounted for with the\\nbootstrap) is desired to determine a concise model that contains most\\nof the independent information in all potential predictors.\\nb. Atestofindependentassociationofeachvariableinagivenmodel(each\\nvariableadjusted forthe eﬀectsofall othervariablesin the givenmodel)\\nis to be obtained.\\nc. A model that contains only additive eﬀects is ﬁtted. A large number\\nof potential interaction terms are to be tested using a global (multiple\\nd.f.) test.\\n4. Consider a univariate saturated model in 3 treatments (A, B, C) that is\\nquadratic in age. Write out the model with all the βs, and write in detail\\nthe contrast for comparing treatment B with treatment C for 30 year olds.\\nSketch out the same contrastusing the“diﬀerencein predictions”approach\\nwithout simpliﬁcation.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='238077fb-bbb9-4f9f-b73c-ca59e699743e', embedding=None, metadata={'page_label': '217', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='9.12 Problems 217\\n5. Simulate a binary logistic model for n= 300 with an average fraction of\\nevents somewhere between 0.15 and 0.3 . Use 5 continuous covariates and\\nassume the model is everywhere linear. Fit an unpenalized model, then\\nsolve for the optimum quadratic penalty λ. Relate the resulting eﬀective\\nd.f. to the 15:1 rule of thumb, and compute the heuristic shrinkage coeﬃ-\\ncient ˆγfor the unpenalized model and for the optimally penalized model,\\ninserting the eﬀective d.f. for the number of non-intercept parameters in\\nthe model.\\n6. For a similar setup as the binary logistic model simulation in Section 9.7,\\ndo a Monte Carlo simulation to determine the coverage probabilities for\\nordinaryWaldand forthreetypes ofbootstrap conﬁdenceintervalsforthe\\ntrue x=5 to x=1 log odds ratio. In addition, consider the Wald-type con-\\nﬁdence interval arising from the sandwich covariance estimator. Estimate\\nthe non-coverage probabilities in both tails. Use a sample size n= 200\\nwith the single predictor x1having a standard log-normal distribution,\\nand the true model being logit( Y=1 )=1+ x1/2. Determine whether\\nincreasing the sample size relieves any problem you observed.Some Rcode\\nfor this simulation is on the web site.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='808c1d86-78f2-46b9-ac3f-8ddd9703680d', embedding=None, metadata={'page_label': '219', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 10\\nBinary Logistic Regression\\n10.1 Model\\nBinary responses are commonly studied in many ﬁelds. Examples include 1\\nthe presence or absence of a particular disease, death during surgery, or a\\nconsumer purchasing a product. Often one wishes to study how a set of\\npredictor variables Xis related to a dichotomous response variable Y.T h e\\npredictorsmay describe such quantities astreatment assignment, dosage,risk\\nfactors, and calendar time.\\nFor convenience we deﬁne the response to be Y= 0 or 1, with Y=1\\ndenotingtheoccurrenceoftheeventofinterest.Oftenadichotomousoutcome\\ncanbe studiedbycalculatingcertainproportions,forexample,theproportion\\nof deaths among females and the proportion among males. However,in many\\nsituations, there are multiple descriptors, or one or more of the descriptors\\nare continuous. Without a statistical model, studying patterns such as the\\nrelationship between age and occurren ce of a disease, for example, would\\nrequire the creation of arbitrary age groups to allow estimation of disease\\nprevalence as a function of age.\\nLettingXdenotethevectorofpredictors {X1,X2,...,X k},aﬁrstattempt\\nat modeling the response might use the ordinary linear regression model\\nE{Y|X}=Xβ, (10.1)\\nsince the expectation of a binary variable Yis Prob{Y=1}. However, such\\na model by deﬁnition cannot ﬁt the data over the whole range of the pre-\\ndictors since a purely linear model E {Y|X}=P r o b{Y=1|X}=Xβcan\\nallow Prob {Y=1}to exceed 1 or fall below 0. The statistical model that is\\ngenerally preferred for the analysis of binary responses is instead the binary\\nlogistic regression model, stated in terms of the probability that Y=1g i v e n\\nX, the values of the predictors:\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 10219', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5417f695-5e9a-4291-89e2-89fc777d34da', embedding=None, metadata={'page_label': '220', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='220 10 Binary Logistic Regression\\nProb{Y=1|X}=[1+exp( −Xβ)]−1. (10.2)\\nAs before, Xβstands for β0+β1X1+β2X2+...+βkXk. The binary lo-\\ngistic regression model was developed primarily by Cox129and Walker and\\nDuncan.647The regression parameters βare estimated by the method of 2\\nmaximum likelihood (see below).\\nThe function\\nP=[1+exp( −x)]−1(10.3)\\nis called the logistic function. This function is plotted in Figure 10.1forx\\nvarying from −4 to +4. This function has an unlimited range for xwhileP\\nis restricted to range from 0 to 1.\\n−4 −2 0 2 40.00.20.40.60.81.0\\nXP\\nFig. 10.1 Logistic function\\nFor future derivations it is useful to express xin terms of P.S o l v i n gt h e\\nequation above for xby using\\n1−P=e x p (−x)/[1+exp( −x)] (10.4)\\nyields the inverse of the logistic function:\\nx= log[P/(1−P)] = log[odds that Y= 1 occurs] = logit {Y=1}.(10.5)\\nOther methods that have been used to analyze binary response data in-\\nclude the probit model, which writes Pin terms of the cumulative normal\\ndistribution, and discriminant analysis. Probit regression,although assuming\\na similar shape to the logistic function for the regression relationship be-\\ntweenXβand Prob {Y=1}, involves more cumbersome calculations, and\\nthere is no natural interpretation of its regression parameters. In the past,\\ndiscriminant analysis has been the predominant method since it is the sim-\\nplest computationally. However, it makes more assumptions than logistic re-\\ngression. The model used in discriminant analysis is stated in terms of the 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e6d12940-0c16-4727-81ce-af6908dfa6e6', embedding=None, metadata={'page_label': '221', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.1 Model 221\\ndistribution of Xgiven the outcome group Y, even though one is seldom in-\\nterested in the distribution of the predictors per se. The discriminant model\\nhas to be inverted using Bayes’ rule to derive the quantity of primary in-\\nterest, Prob {Y=1}. By contrast, the logistic model is a direct probability\\nmodelsince it is stated in terms of Prob {Y=1|X}. Since the distribution\\nof a binary random variable Yis completely deﬁned by the true probability\\nthatY= 1 and since the model makes no assumption about the distribu-\\ntion of the predictors, the logistic model makes no distributional assumptions\\nwhatsoever.\\n10.1.1 Model Assumptions and Interpretation\\nof Parameters\\nSince the logistic model is a direct probability model, its only assumptions\\nrelate to the form of the regression equation. Regression assumptions are\\nveriﬁable, unlike the assumption of multivariate normality made by discrimi-\\nnant analysis. The logistic model assumptions are most easily understood by\\ntransforming Prob {Y=1}to make a model that is linear in Xβ:\\nlogit{Y=1|X}= logit(P) = log[P/(1−P)]\\n=Xβ, (10.6)\\nwhereP=P r o b{Y=1|X}. Thus the model is a linear regression model in\\nthe log odds that Y= 1 since logit( P)i saw e i g h t e ds u mo ft h e Xs. If all\\neﬀects are additive (i.e., no interactions are present), the model assumes that\\nfor every predictor Xj,\\nlogit{Y=1|X}=β0+β1X1+...+βjXj+...+βkXk\\n=βjXj+C, (10.7)\\nwhere if all other factors are held constant, Cis a constant given by\\nC=β0+β1X1+...+βj−1Xj−1+βj+1Xj+1+...+βkXk.(10.8)\\nThe parameter βjis then the change in the log odds per unit change in\\nXjifXjrepresents a single factor that is linear and does not interact with\\nother factors and if all other factors are held constant. Instead of writing this\\nrelationship in terms of log odds, it could just as easily be written in terms\\nof the odds that Y=1 :\\nodds{Y=1|X}=e x p (Xβ), (10.9)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='efe29e64-8913-40cf-8489-44887a2fa1ae', embedding=None, metadata={'page_label': '222', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='222 10 Binary Logistic Regression\\nand if all factors other than Xjare held constant,\\nodds{Y=1|X}=e x p (βjXj+C)=e x p (βjXj)exp(C).(10.10)\\nThe regressionparameters can also be written in terms of odds ratios .T h e\\nodds that Y=1w h e n Xjis increased by d, divided by the odds at Xjis\\nodds{Y=1|X1,X2,...,X j+d,...,X k}\\nodds{Y=1|X1,X2,...,X j,...,X k}\\n=exp[βj(Xj+d)]exp(C)\\n[exp(βjXj)exp(C)](10.11)\\n=e x p [βjXj+βjd−βjXj]=e x p (βjd).\\nThus the eﬀect of increasing Xjbydis to increase the odds that Y=1b y\\naf a c t o ro fe x p ( βjd), or to increase the log odds that Y= 1 by an increment\\nofβjd. In general, the ratio of the odds of response for an individual with\\npredictor variable values X∗compared with an individual with predictors\\nXis\\nX∗:Xodds ratio = exp( X∗β)/exp(Xβ)\\n=e x p [ (X∗−X)β]. (10.12)\\nNow consider some special cases of the logistic multiple regression model.\\nIf there is only one predictor Xand that predictor is binary, the model can\\nbe written\\nlogit{Y=1|X=0}=β0\\nlogit{Y=1|X=1}=β0+β1. (10.13)\\nHereβ0is the log odds of Y=1w h e n X= 0. By subtracting the two\\nequations above, it can be seen that β1is the diﬀerence in the log odds\\nwhenX= 1 as compared with X= 0, which is equivalent to the log of the\\nratio of the odds when X= 1 compared with the odds when X=0 .T h e\\nquantity exp( β1) is the odds ratio for X= 1 compared with X= 0. Letting\\nP0=P r o b{Y=1|X=0}andP1=P r o b{Y=1|X=1}, the regression\\nparameters are interpreted by\\nβ0= logit(P0) = log[P0/(1−P0)]\\nβ1= logit(P1)−logit(P0) (10.14)\\n=l o g [P1/(1−P1)]−log[P0/(1−P0)]\\n=l o g{[P1/(1−P1)]/[P0/(1−P0)]}.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='31520657-d649-4ce4-9cfa-e10fae8818bc', embedding=None, metadata={'page_label': '223', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.1 Model 223\\nSince there are only two quantities to model and two free parameters,\\nthere is no way that this two-sample model can’t ﬁt; the model in this case\\nis essentially ﬁtting two cell proportions. Similarly, if there are g−1 dummy\\nindicator Xs representing ggroups, the ANOVA-type logistic model must\\nalways ﬁt.\\nIf there is one continuous predictor X, the model is\\nlogit{Y=1|X}=β0+β1X, (10.15)\\nand without further modiﬁcation (e.g., taking log transformation of the pre-\\ndictor), the model assumes a straight line in the log odds, or that an increase\\ninXby one unit increases the odds by a factor of exp( β1).\\nNow consider the simplest analysis of covariance model in which there are\\ntwo treatments (indicated by X1= 0 or 1) and one continuous covariable\\n(X2). The simplest logistic model for this setup is\\nlogit{Y=1|X}=β0+β1X1+β2X2, (10.16)\\nwhich can be written also as\\nlogit{Y=1|X1=0,X2}=β0+β2X2\\nlogit{Y=1|X1=1,X2}=β0+β1+β2X2. (10.17)\\nTheX1=1:X1= 0 odds ratio is exp( β1), independent of X2. The odds\\nratio for a one-unit increase in X2is exp(β2), independent of X1.\\nThis model, with no term for a possible interaction between treatment\\nand covariable, assumes that for each treatment the relationship between X2\\nand log odds is linear, and that the lines have equal slope; that is, they are\\nparallel. Assuming linearity in X2, the only way that this model can fail is\\nfor the two slopes to diﬀer. Thus, the only assumptions that need veriﬁcation\\nare linearity and lack of interaction between X1andX2.\\nTo adapt the model to allow or test for interaction, we write\\nlogit{Y=1|X}=β0+β1X1+β2X2+β3X3, (10.18)\\nwhere the derived variable X3is deﬁned to be X1X2.T h et e s tf o rl a c ko f\\ninteraction (equal slopes) is H0:β3= 0. The model can be ampliﬁed as\\nlogit{Y=1|X1=0,X2}=β0+β2X2\\nlogit{Y=1|X1=1,X2}=β0+β1+β2X2+β3X2(10.19)\\n=β′\\n0+β′\\n2X2,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59811b71-2a96-47dc-9e3f-4a2a02ab0a08', embedding=None, metadata={'page_label': '224', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='224 10 Binary Logistic Regression\\nTable 10.1 Eﬀect of an odds ratio of two on various risks\\nWithout Risk Factor With Risk Factor\\nProbability Odds Odds Probability\\n.2 .25 .5 .33\\n.5 1 2.67\\n.8 4 8.89\\n.9 9 18 .95\\n.98 49 98 .99\\nwhereβ′\\n0=β0+β1andβ′\\n2=β2+β3. The model with interaction is therefore\\nequivalent to ﬁtting two separate logistic models with X2as the only predic-\\ntor, one model for each treatment group. Here the X1=1:X1= 0 odds\\nratio is exp( β1+β3X2).\\n10.1.2 Odds Ratio, Risk Ratio, and Risk Diﬀerence\\nAs discussed above, the logistic model quantiﬁes the eﬀect of a predictor in\\nterms of an odds ratio or log odds ratio. An odds ratio is a natural descrip-\\ntion of an eﬀect in a probability model since an odds ratio canbe constant.\\nFor example, suppose that a given risk factor doubles the odds of disease.\\nTable10.1shows the eﬀect of the risk factor fo r various levels of initial risk.\\nSince odds have an unlimited range, any positive odds ratio will still yield\\na valid probability. If one attempted to describe an eﬀect by a risk ratio, the\\neﬀect can only occur over a limited range of risk (probability). For example, a\\nrisk ratio of 2 can only apply to risks below .5; above that point the risk ratio\\nmust diminish. (Risk ratios are similar to odds ratios if the risk is small.)\\nRisk diﬀerences have the same diﬃculty; the risk diﬀerence cannot be con-\\nstant and must depend on the initial risk. Odds ratios,on the other hand, can\\ndescribeaneﬀectoverthe entirerangeofrisk.Anoddsratiocan,forexample,\\ndescribe the eﬀect of a treatment independently of covariables aﬀecting risk.\\nFigure10.2depicts the relationship between risk of a subject without the risk\\nfactor and the increase in risk for a variety of relative increases (odds ratios).\\nIt demonstrates how absolute risk increase is a function of the baseline risk.\\nRisk increase will also be a function of factors that interact with the risk fac-\\ntor, that is, factors that modify its relative eﬀect. Once a model is developed\\nfor estimating Prob {Y=1|X}, this model can easily be used to estimate the\\nabsolute risk increase as a function of baseline risk factors as well as inter-\\nacting factors. Let X1be a binary risk factor and let A={X2,...,X p}be\\nthe other factors (which for convenience we assume do not interact with X1).\\nThen the estimate of Prob {Y=1|X1=1,A}−Prob{Y=1|X1=0,A}is', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b357f32c-fb93-4bf8-a6e2-e6ca60cf948c', embedding=None, metadata={'page_label': '225', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.1 Model 225\\n0.0 0.2 0.4 0.6 0.8 1.00.00.10.20.30.40.50.6\\nRisk for Subject Without Risk FactorIncrease in Risk\\n1.11.251.51.75234510\\nFig. 10.2 Absolute beneﬁt as a function of risk of the event in a control subject and\\nthe relative eﬀect (odds ratio) of the risk factor. The odds ratios are given for each\\ncurve.\\nTable 10.2 Example binary response data\\nFemales Age :37 39 39 42 47 48 48 52 53 55 56 57 58 58 60 64 65 68 68 70\\nResponse :00000010000001001111\\nMales Age :34 38 40 40 41 43 43 43 44 46 47 48 48 50 50 52 55 60 61 61\\nResponse :11000111001110111111\\n1\\n1+exp−[ˆβ0+ˆβ1+ˆβ2X2+...+ˆβpXp]\\n−1\\n1+exp−[ˆβ0+ˆβ2X2+...+ˆβpXp](10.20)\\n=1\\n1+(1−ˆR\\nˆR)exp(−ˆβ1)−ˆR,\\nwhereˆRis the estimate of the baseline risk, Prob {Y=1|X1=0}.T h er i s k\\ndiﬀerence estimate can be plotted against ˆRor againstlevels of variablesin A\\nto display absolute risk increase against overall risk (Figure 10.2) or against\\nspeciﬁc subject characteristics. 4\\n10.1.3 Detailed Example\\nConsider the data in Table 10.2. A graph of the data, along with a ﬁtted\\nlogistic model (described later), appears in Figure 10.3. The graph also dis-\\nplays proportions of responses obtained by stratifying the data by sex and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='107c0dba-84c2-45c8-a30f-df9207964de8', embedding=None, metadata={'page_label': '226', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"226 10 Binary Logistic Regression\\nage group ( <45,45−54,≥55). The age points on the abscissa for these\\ngroups are the overall mean ages in the three age intervals (40.2, 49.1, and\\n61.1, respectively).\\nrequire(rms)\\ngetHdata( sex.age.response)\\nd←sex.age.response\\ndd←datadist(d); options(datadist= 'dd')\\nf←lrm(response ∼sex + age, data=d)\\nfasr←f# Save for later\\nw←function(...)\\nwith(d, {\\nm←sex== 'male '\\nf←sex== 'female '\\nlpoints(age [f],response[f], pch=1)\\nlpoints(age [m],response[m], pch=2)\\naf←cut2(age, c(45,55), levels.mean=TRUE)\\nprop←tapply(response, list(af, sex), mean,\\nna.rm=TRUE)\\nagem←as.numeric(row.names(prop))\\nlpoints(agem, prop[, 'female '],\\npch=4, cex=1.3, col= 'green ')\\nlpoints(agem, prop[, 'male '],\\npch=5, cex=1.3, col= 'green ')\\nx←rep(62, 4); y ←seq(.25, .1, length=4)\\nlpoints(x, y, pch=c(1, 2, 4, 5),\\ncol=rep(c( 'blue ','green '),each=2))\\nltext(x+5, y,\\nc('F Observed ','M Observed ',\\n'F Proportion ','M Proportion '), cex=.8)\\n})# Figure 10.3\\nplot(Predict(f, age=seq(34, 70, length=200), sex, fun=plogis),\\nylab= 'Pr[response] ', ylim=c(-.02, 1.02), addpanel=w)\\nltx←function(fit) latex(fit, inline= TRUE,columns=54,\\nfile= '', after= '$.', digits=3,\\nsize= 'Ssize ', before= '$X\\\\\\\\hat{\\\\\\\\beta}= ')\\nltx(f)\\nXˆβ=−9.84+3.49[male]+0 .158age.\\nDescriptive statistics for assessing the association between sex and re-\\nsponse, age group and response, and age group and response stratiﬁed by\\nsex are found below. Corresponding ﬁtted logistic models, with sex coded as\\n0 = female, 1 = male are also given. Models were ﬁtted ﬁrst with sex as the\\nonly predictor,thenwith ageasthe (continuous)predictor,then withsex and\\nage simultaneously. First consider the relationship between sex and response,\\nignoring the eﬀect of age.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2273661e-b5b8-43fd-a5d4-c2ad07e2416e', embedding=None, metadata={'page_label': '227', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.1 Model 227\\nagePr[response]\\n0.20.40.60.8\\n40 50 60 70F Observed\\nM Observed\\nF Proportion\\nM Proportionfemalemale\\nFig. 10.3 Data, subgroup proportions, and ﬁtted logistic model, with 0.95 pointwise\\nconﬁdence bands\\nsex response\\nFrequency\\nRow Pct 0 1 Total Odds/Log\\nF 14 6 20 6/14=.429\\n70.00 30.00 -.847\\nM 6 14 20 14/6=2.33\\n30.00 70.00 .847\\nTotal 20 20 40\\nM:F odds ratio = (14/6)/(6/14) = 5.44, log=1.695\\nStatistics for sex ×response\\nStatistic d.f. Value P\\nχ21 6.400 0.011\\nLikelihood Ratio χ21 6.583 0.010\\nParameter Estimate Std Err Wald χ2P\\nβ0 −0.8473 0.4880 3 .0152\\nβ1 1.6946 0.6901 6 .0305 0.0141', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bdc8fc32-6260-46a4-bd88-2b0a0ce57058', embedding=None, metadata={'page_label': '228', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='228 10 Binary Logistic Regression\\nNote that the estimate of β0,ˆβ0is the log odds for females and that ˆβ1is the\\nlog odds (M:F) ratio. ˆβ0+ˆβ1=.847, the log odds for males. The likelihood\\nratio test for H0: no eﬀect of sex on probability of response is obtained as\\nfollows.\\nLog likelihood ( β1=0 ):−27.727\\nLog likelihood (max) : −24.435\\nLRχ2(H0:β1=0 ) : −2(−27.727−−24.435) = 6 .584.\\n(Note the agreement of the LR χ2with the contingency table likelihood ratio\\nχ2, and compare 6.584 with the Wald statistic 6.03.)\\nNext, consider the relationship between age and response, ignoring sex.\\nage response\\nFrequency\\nRow Pct 0 1 Total Odds/Log\\n<45 8 5 13 5/8=.625\\n61.5 38.4 -.47\\n45-54 6 6 12 6/6=1\\n50.0 50.0 0\\n55+ 6 9 15 9/6=1.5\\n40.0 60.0 .405\\nTotal 20 20 40\\n55+ : <45 odds ratio = (9/6)/(5/8) = 2.4, log=.875\\nParameter Estimate Std Err Wald χ2P\\nβ0 −2.7338 1.8375 2 .2134 0.1368\\nβ1 0.0540 0.0358 2 .2763 0.1314\\nThe estimate of β1is in rough agreement with that obtained from the\\nfrequencytable.The55+: <45logoddsratiois.875,andsincetherespective\\nmean ages in the 55+ and <45 age groups are 61.1 and 40.2, an estimate of\\nthe log odds ratio increase per year is .875/(61.1−40.2) =.875/20.9=.042.\\nThe likelihood ratio test for H0: no association between age and response\\nis obtained as follows.\\nLog likelihood ( β1=0 ):−27.727\\nLog likelihood (max) : −26.511\\nLRχ2(H0:β1=0 ) : −2(−27.727−−26.511) = 2 .432.\\n(Compare 2.432 with the Wald statistic 2.28.)\\nNext we consider the simultaneous association of age and sex with\\nresponse.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea9e0c0c-a026-4304-95a3-846edf0e03e8', embedding=None, metadata={'page_label': '229', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.1 Model 229\\nsex=F\\nage response\\nFrequency\\nRow Pct 0 1 Total\\n<45 4 0 4\\n100.0 0.0\\n45-54 4 1 5\\n80.0 20.0\\n55+ 6 5 11\\n54.6 45.4\\nTotal 14 6 20\\nsex=M\\nage response\\nFrequency\\nRow Pct 0 1 Total\\n<45 4 5 9\\n44.4 55.6\\n45-54 2 5 7\\n28.6 71.4\\n55+ 0 4 4\\n0.0 100.0\\nTotal 6 14 20\\nA logistic model for relating sex and age simultaneously to response is\\ngiven below.\\nParameter Estimate Std Err Wald χ2P\\nβ0 −9.8429 3.6758 7 .1706 0.0074\\nβ1(sex) 3 .4898 1.1992 8 .4693 0.0036\\nβ2(age) 0 .1581 0.0616 6 .5756 0.0103\\nLikelihood ratio tests are obtained from the information below.\\nLog likelihood ( β1=0,β2=0 ):−27.727\\nLog likelihood (max) : −19.458\\nLog likelihood ( β1=0 ) : −26.511\\nLog likelihood ( β2=0 ) : −24.435\\nLRχ2(H0:β1=β2=0 ) : −2(−27.727−−19.458) = 16 .538\\nLRχ2(H0:β1=0 )s e x |age :−2(−26.511−−19.458) = 14 .106\\nLRχ2(H0:β2= 0) age |sex :−2(−24.435−−19.458) = 9 .954.\\nThe 14.1 should be compared with the Wald statistic of 8.47, and 9.954\\nshould be compared with 6.58. The ﬁtted logistic model is plotted separately', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fefeefc-8452-4552-b022-e3060a8bfdff', embedding=None, metadata={'page_label': '230', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='230 10 Binary Logistic Regression\\nfor females and males in Figure 10.3. The ﬁtted model is\\nlogit{Response = 1 |sex,age}=−9.84+3.49×sex+.158×age,(10.21)\\nwhere as before sex = 0 for females, 1 for males. For example, for a 40-year-\\nold female, the predicted logit is −9.84 +.158(40) = −3.52. The predicted\\nprobability of a response is 1 /[1+exp(3 .52)] =.029. For a 40-year-old male,\\nthe predicted logit is −9.84 + 3.49 +.158(40) = −.03, with a probability\\nof.492.\\n10.1.4 Design Formulations\\nThe logistic multiple regression model can incorporate the same designs as\\ncan ordinary linear regression. An analysis of variance (ANOVA) model for\\nat r e a t m e n tw i t h klevels can be formulated with k−1 dummy variables.\\nThis logistic model is equivalent to a 2 ×kcontingency table. An analysis\\nof covariance logistic model is simply an ANOVA model augmented with\\ncovariables used for adjustment.\\nOne unique design that is interesting to consider in the context of logistic\\nmodels is a simultaneous comparison of multiple factors between two groups.\\nSuppose, for example, that in a randomized trial with two treatments one\\nwished to test whether any of 10 baseline characteristics are mal-distributed\\nbetween the two groups. If the 10 factors are continuous, one could perform a\\ntwo-sample Wilcoxon–Mann–Whitney test or a t-test for each factor (if each\\nis normally distributed). However, this procedure would result in multiple\\ncomparison problems and would also not be able to detect the combined ef-\\nfect of small diﬀerences across all the factors. A better procedure would be a\\nmultivariate test. The Hotelling T2test is designed for just this situation. It\\nis ak-variable extension of the one-variable unpaired t-test. The T2test, like\\ndiscriminant analysis, assumes multivariate normality of the kfactors. This\\nassumption is especially tenuous when some of the factors are polytomous. A\\nbetter alternative is the global test of no regression from the logistic model.\\nThis test is valid because it can be shown that H0:m e a nXis the same for\\nboth groups (= H0:m e a nXdoes not depend on group = H0:m e a nX|\\ngroup=constant)istrueifandonlyif H0:P r o b{group|X}=constant.Thus\\nkfactors can be tested simultaneously for diﬀerences between the two groups\\nusingthebinarylogisticmodel,whichhasfarfewerassumptionsthandoesthe\\nHotelling T2test. The logistic global test of no regression(with kd.f.) would\\nbe expected to have greater power if there is non-normality. Since the logistic\\nmodel makes no assumption regarding the distribution of the descriptor vari-\\nables, it can easily test for simultaneous group diﬀerences involvinga mixture\\nof continuous, binary, and nominal variables. In observational studies, such', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ccafde3-6768-4839-aedd-4079cf44a7f3', embedding=None, metadata={'page_label': '231', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.2 Estimation 231\\nmodels for treatment received or exposure (propensity score models) hold\\ngreat promise for adjusting for confounding.117,380,526,530,5315\\nO’Brien479has developed a general test for comparing group 1 with\\ngroup 2 for a single measurement. His test detects location and scale dif-\\nferences by ﬁtting a logistic model for Prob {Group 2}usingXandX2as\\npredictors.\\nForarandomizedstudywhereadjustmentforconfoundingisseldomneces-\\nsary,adjustingforcovariablesusingabinarylogisticmodelresultsin increases\\nin standard errors of regression coeﬃcients.527This is the opposite of what\\nhappens in linear regression where there is an unknown variance parameter\\nthat is estimated using the residual squared error. Fortunately, adjusting for\\ncovariables using logistic regression, b y accounting for subject heterogeneity,\\nwill result in larger regression coeﬃcients even for a randomized treatment\\nvariable. The increase in estimated regression coeﬃcients more than oﬀsets\\nthe increase in standard error284,285,527,588.\\n10.2 Estimation\\n10.2.1 Maximum Likelihood Estimates\\nThe parametersinthe logisticregressionmodelareestimatedusing the maxi-\\nmum likelihood(ML) method. The method is based on the same principles as\\nthe one-sample proportion example described in Section 9.1. The diﬀerence\\nis that the general logistic model is not a single sample or a two-sampleprob-\\nlem. The probability of response for the ith subject depends on a particular\\nset of predictors Xi, and in fact the list of predictors may not be the same\\nfor any two subjects. Denoting the response and probability of response of\\ntheith subject by YiandPi, respectively, the model states that\\nPi=P r o b{Yi=1|Xi}=[1+exp( −Xiβ)]−1. (10.22)\\nThe likelihood of an observed response Yigiven predictors Xiand the un-\\nknown parameters βis\\nPYi\\ni[1−Pi]1−Yi. (10.23)\\nThe joint likelihood of all responses Y1,Y2,...,Y nis the product of these\\nlikelihoods for i=1,...,n. The likelihood and log likelihood functions are\\nrewritten by using the deﬁnition of Piabove to allow them to be recognized\\nas a function of the unknown parameters β. Except in simple special cases\\n(such as the k-sample problem in which all Xs are dummy variables), the\\nML estimates(MLE) of βcannot be written explicitly. The Newton–Raphson\\nmethod described in Section 9.4is usually used to solve iteratively for the\\nlist of values βthat maximize the log likelihood. The MLEs are denoted by', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4b122259-393d-4d5e-a12a-8dd902ce1503', embedding=None, metadata={'page_label': '232', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='232 10 Binary Logistic Regression\\nˆβ. The inverse of the estimated observed information matrix is taken as the\\nestimate of the variance–covariance matrix of ˆβ.\\nUnderH0:β1=β2=...=βk= 0, the intercept parameter β0can be\\nestimated explicitly and the log likelihood under this global null hypothesis\\ncan be computed explicitly. Under the global null hypothesis, Pi=P=\\n[ 1+e x p ( −β0)]−1and the MLE of PisˆP=s/nwheresis the number of\\nresponses and ni st h es a m p l es i z e .T h eM L Eo f β0isˆβ0= logit(ˆP). The log\\nlikelihood under this null hypothesis is 6\\nslog(ˆP)+(n−s)log(1−ˆP)\\n=slog(s/n)+(n−s)log[(n−s)/n] (10.24)\\n=slogs+(n−s)log(n−s)−nlog(n).\\n10.2.2 Estimation of Odds Ratios and Probabilities\\nOnceβis estimated, one can estimate any log odds, odds, or odds ratios.\\nThe MLE of the Xj+1:Xjl o go d d sr a t i oi s ˆβj,a n dt h ee s t i m a t eo ft h e\\nXj+d:Xjlog odds ratio is ˆβjd, all other predictors remaining constant\\n(assuming the absence of interactions and nonlinearities involving Xj). For\\nlarge enough samples, the MLEs are normally distributed with variancesthat\\nare consistently estimated from the estimated variance–covariance matrix.\\nLettingzdenotethe1 −α/2criticalvalueofthestandardnormaldistribution,\\na two-sided 1 −αconﬁdence interval for the log odds ratio for a one-unit\\nincrease in Xjis [ˆβj−zs,ˆβj+zs], where sis the estimated standard error\\nofˆβj. (Note that for α=.05, i.e., for a 95% conﬁdence interval, z=1.96.)\\nA theorem in statistics states that the MLE of a function of a parameter\\nis that same function of the MLE of the parameter. Thus the MLE of the\\nXj+1:Xjodds ratio is exp( ˆβj). Also, if a 1 −αconﬁdence interval of a\\nparameter βis [c,d]a n df(u) is a one-to-one function, a 1 −αconﬁdence\\nintervalof f(β)i s[f(c),f(d)]. Thus a 1 −αconﬁdence interval for the Xj+1:\\nXjodds ratio is exp[ ˆβj±zs]. Note that while the conﬁdence interval for βjis\\nsymmetric about ˆβj, the conﬁdence interval for exp( βj)i sn o t .B yt h es a m e\\ntheorem just used, the MLE of Pi=P r o b{Yi=1|Xi}is\\nˆPi=[1+exp( −Xiˆβ)]−1. (10.25)\\nA conﬁdence interval for Picould be derived by computing the standard\\nerror of ˆPi, yielding a symmetric conﬁdence interval. However, such an in-\\nterval would have the disadvantage that its endpoints could fall below zero\\nor exceed one. A better approach uses the fact that for large samples Xˆβ\\nis approximately normally distributed. An estimate of the variance of Xˆβ\\nin matrix notation is XVX′whereVis the estimated variance–covariance', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='081a6f0f-2d7f-49bc-947f-0387c9f641ca', embedding=None, metadata={'page_label': '233', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.2 Estimation 233\\nmatrix of ˆβ(see Equation 9.51). This variance is the sum of all variances and\\ncovariances of ˆβweighted by squares and products of the predictors. The es-\\ntimated standard error of Xˆβ,s, is the square root of this variance estimate.\\nA1−αconﬁdence interval for Piis then 7\\n{1+exp[−(Xiˆβ±zs)]}−1. (10.26)\\n10.2.3 Minimum Sample Size Requirement\\nSuppose there were no covariates, so that the only parameter in the model is\\nthe intercept. What is the sample size required to allow the estimate of the\\nintercept to be precise enough so that the predicted probability is within 0.1\\nof the true probability with 0.95 conﬁdence, when the true intercept is in the\\nneighborhood of zero? The answer is n=96. What if there were one covariate,\\nand it was binary with a prevalence of1\\n2? One would need 96 subjects with\\nX= 0 and 96 with X= 1 to have an upper bound on the margin of error\\nfor estimating Prob {Y=1|X=x}not exceed 0.1 for either value of xa.\\nNow consider a very simple single continuous predictor case in which X\\nhas a normal distribution with mean zero and standard deviation σ,w i t ht h e\\ntrue Prob {Y=1|X=x}=[1+exp( −x)]−1. The expected number of events\\nisn\\n2b. The following simulation answers the question“What should nbe so\\nthat the expected maximum absolute error (over x∈[−1.5,1.5]) inˆPis less\\nthanǫ?”\\nsigmas ←c(.5, .75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 4)\\nns ←seq(25, 300, by=25)\\nnsim ←1000\\nxs ←seq(-1.5, 1.5, length=200)\\npactual ←plogis(xs)\\ndn←list(sigma=format(sigmas), n=format(ns))\\nmaxerr ←N1←array(NA, c(length(sigmas), length(ns)), dn)\\nrequire(rms)\\ni←0\\nfor(s in sigmas) {\\ni←i+1\\nj←0\\nfor(n in ns) {\\naThe general formula for the sample size required to achieve a margin of error of δin\\nestimating a true probability of θat the 0.95 conﬁdence level is n=(1.96\\nδ)2×θ(1−θ).\\nSetθ=1\\n2(intercept=0) for the worst case.\\nbTheRcode can easily be modiﬁed for other event frequencies, or the minimum of\\nthe number of events and non-events for a dataset at hand can be compared withn\\n2in this simulation. An average maximum absolute error of 0.05 corresponds roughly\\nto a half-width of the 0.95 conﬁdence interval of 0.1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='faf913e5-5f05-420e-a513-323b50d6b516', embedding=None, metadata={'page_label': '234', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"234 10 Binary Logistic Regression\\nj←j+1\\nn1←maxe←0\\nfor(k in 1:nsim) {\\nx←rnorm(n, 0, s)\\nP←plogis(x)\\ny←ifelse(runif(n) ≤P, 1, 0)\\nn1←n1 + sum(y)\\nbeta←lrm.fit(x, y)$coefficients\\nphat←plogis(beta[1] + beta[2] * xs)\\nmaxe←maxe + max(abs(phat - pactual))\\n}\\nn1←n1/nsim\\nmaxe←maxe/nsim\\nmaxerr[i,j] ←maxe\\nN1[i,j] ←n1\\n}\\n}\\nxrange ←range(xs)\\nsimerr ←llist(N1, maxerr, sigmas, ns, nsim,xrange)\\nmaxe←reShape( maxerr)\\n# Figure 10.4\\nxYplot(maxerr ∼n, groups= sigma, data=maxe,\\nylab=expression( paste( 'Average Maximum ',\\nabs(hat(P) - P))),\\ntype= 'l', lty=rep(1:2, 5), label.curve= FALSE,\\nabline=list(h=c(.15, .1, .05), col=gray(.85)))\\nKey(.8, .68, other=list(cex=.7,\\ntitle=expression( ∼∼∼∼∼∼∼∼∼∼∼ sigma)))\\n10.3 Test Statistics\\nThe likelihood ratio, score, and Wald statistics discussed earlier can be used\\nto test any hypothesis in the logistic model. The likelihood ratio test is gen-\\nerally preferred. When true parameters are near the null values all three\\nstatistics usually agree. The Wald test has a signiﬁcant drawback when the\\ntrue parameter value is very far from the null value. In such case the stan-\\ndard error estimate becomes too large. As ˆβjincreases from 0, the Wald test\\nstatistic for H0:βj= 0 becomes larger, but after a certain point it becomes\\nsmaller. The statistic will eventually drop to zero if ˆβjbecomes inﬁnite.278\\nInﬁnite estimates can occur in the logistic model especially when there is a\\nbinary predictor whose mean is near 0 or 1. Wald statistics are especially\\nproblematic in this case. For example, if 10 out of 20 males had a disease and\\n5 out of 5 females had the disease, the female : male odds ratio is inﬁnite and\\nso is the logistic regression coeﬃcient for sex. If such a situation occurs, the\\nlikelihood ratio or score statistic should be used instead of the Wald statistic.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b10c5a69-2318-4e7f-bbf9-38e7dbdf8d05', embedding=None, metadata={'page_label': '235', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.4 Residuals 235\\nnAverage Maximum P^−P\\n0.050.100.150.20\\n50 100 150 200 250 300σ\\n0.5\\n0.75\\n1\\n1.25\\n1.5\\n1.75\\n2\\n2.5\\n3\\n4\\nFig. 10.4 Simulated expected maximum error in estimating probabilities for x∈\\n[−1.5,1.5] with a single normally distributed Xwith mean zero\\nFork-sample (ANOVA-type) logistic models, logistic model statistics are\\nequivalent to contingency table χ2statistics. As exempliﬁed in the logistic\\nmodel relating sex to response described previously, the global likelihood\\nratio statistic for all dummy variables in a k-sample model is identical to the\\ncontingency table ( k-sample binomial) likelihood ratio χ2statistic. The score\\nstatistic for this same situation turns out to be identical to the k−1 degrees\\nof freedom Pearson χ2for ak×2 table.\\nAs mentioned in Section 2.6, it can be dangerous to interpret individual\\nparameters, make pairwise treatment comparisons, or test linearity if the\\noverall test of association for a factor represented by m ultiple parameters is\\ninsigniﬁcant.\\n10.4 Residuals\\nSeveraltypesofresidualscanbecomputedforbinarylogisticmodelﬁts.Many\\nof these residuals are used to examine the inﬂuence of individual observations\\non the ﬁt. The partial residual can be used for directly assessing how each 8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='42705355-c116-49f7-9adf-e688032d2255', embedding=None, metadata={'page_label': '236', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='236 10 Binary Logistic Regression\\npredictor should be transformed. For the ith observation, the partial residual\\nfor thejth element of Xis deﬁned by\\nrij=ˆβjXij+Yi−ˆPi\\nˆPi(1−ˆPi), (10.27)\\nwhereXijis the value of the jth variable in the ith observation, Yiis the\\ncorresponding value of the response, and ˆPiis the predicted probability that\\nYi= 1. A smooth plot (using, e.g., loess) of Xijagainstrijwill provide an\\nestimate of how Xjshould be transformed, adjusting for the other Xs (using\\ntheir current transformations). Typically one tentatively models Xjlinearly\\nand checks the smoothed plot for linearity. A U-shaped relationship in this\\nplot, for example, indicates that a squared term or spline function needs to\\nbe added for Xj. This approach does assume additivity of predictors. 9\\n10.5 Assessment of Model Fit\\nAs the logistic regression model makes no distributional assumptions, only\\nthe assumptions of linearity and additivity need to be veriﬁed (in addition\\nto the usual assumptions about independence of observations and inclusion\\nof important covariables). In ordinary linear regression there is no global\\ntest for lack of model ﬁt unless there are replicate observations at various\\nsettings of X. This is because ordinary regression entails estimation of a\\nseparate variance parameter σ2. In logistic regression there are global tests\\nfor goodness of ﬁt. Unfortunately, some of the most frequently used ones are\\ninappropriate. For example, it is common to see a deviance test of goodness\\nof ﬁt based on the“residual”log likelihood, with P-values obtained from a χ2\\ndistribution with n−pd.f. This P-value is inappropriate since the deviance\\ndoes nothaveanasymptotic χ2distribution,dueto the factsthat the number\\nof parameters estimated is increasing at the same rate as nand the expected\\ncell frequencies are far below ﬁve (by deﬁnition).\\nHosmer and Lemeshow304have developed a commonly used test for good-\\nness of ﬁt for binary logistic models based on grouping into deciles of pre-\\ndicted probability and performing an ordinary χ2test for the mean predicted\\nprobability against the observed fraction of events (using 8 d.f. to account\\nfor evaluating ﬁt on the model development sample). The Hosmer–Lemeshow\\ntest is dependent on the choice of how predictions are grouped303and it is\\nnot clear that the choice of the number of groups should be independent of n.\\nHosmer et al.303have compared a number of global goodness of ﬁt tests for\\nbinary logistic regression. They concluded that the simple unweighted sum of\\nsquarestestofCopas124asmodiﬁed byleCessieandvan Houwelingen387isas', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13ef248e-d323-4f61-9690-89cc9bbf58e4', embedding=None, metadata={'page_label': '237', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.5 Assessment of Model Fit 237\\ngood as any. They used a normal Z-test for the sum of squared errors ( n×B,\\nwhereBistheBrierindexinEquation 10.35).Thistesttakesintoaccountthe\\nfact that one cannot obtain a χ2distribution for the sum of squares. It also\\ntakes into account the estimation of β. It is not yet clear for which types of\\nlack of ﬁt this test has reasonablepower. Returning to the external validation\\ncase whereuncertaintyof βdoes notneed to be accountedfor, Stallard584has\\nfurther documented the lack of power of the original Hosmer-Lemeshow test\\nand found more power with a logarithmic scoring rule (deviance test) and a\\nχ2test that, unlike the simple unweighted sum of squares test, weights each\\nsquared error by dividing it by ˆPi(1−ˆPi). A scaled χ2distribution seemed to\\nprovide the best approximation to the null distribution of the test statistics.\\nMore powerfor detecting lack of ﬁt is expected to be obtained from testing\\nspeciﬁc alternatives to the model. In the model\\nlogit{Y=1|X}=β0+β1X1+β2X2, (10.28)\\nwhereX1is binary and X2is continuous, one needs to verify that the log\\nodds is related to X1andX2according to Figure 10.5.\\nX2logit{Y=1}X1= 0X1= 1\\nFig. 10.5 Logistic regression assumptions for one binary and one continuous predic-\\ntor\\nThe simplest method for validating that the data are consistent with the\\nno-interaction linear model involves stratifying the sample by X1and quan-\\ntile groups (e.g., deciles) of X2.265Within each stratum the proportion of\\nresponses ˆPis computed and the log odds calculated from log[ ˆP/(1−ˆP)].\\nThe number of quantile groups should be such that there are at least 20 (and\\nperhaps many more)subjects in each X1×X2group.Otherwise,probabilities\\ncannot be estimated precisely enough to allow trends to be seen above“noise”\\nin the data. Since at least 3 X2groups must be formed to allow assessment\\nof linearity, the total sample size must be at least 2 ×3×20 = 120 for this\\nmethod to work at all.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b751f8e-9c70-4ec5-8036-13ee525da088', embedding=None, metadata={'page_label': '238', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"238 10 Binary Logistic Regression\\nFigure10.6demonstrates this method for a large sample size of 3504 sub-\\njects stratiﬁed by sex and deciles of age. Linearity is apparent for males while\\nthere isevidenceforslightinteractionbetweenageandsexsincethe agetrend\\nfor females appears curved.\\ngetHdata( acath)\\nacath$sex ←factor(acath$sex, 0:1, c( 'male ','female '))\\ndd←datadist( acath); options(datadist= 'dd')\\nf←lrm(sigdz ∼rcs(age, 4) * sex, data=acath)\\nw←function(...)\\nwith(acath, {\\nplsmo(age, sigdz, group=sex, fun=qlogis, lty= 'dotted ',\\nadd=TRUE, grid=TRUE)\\naf←cut2(age, g=10, levels.mean= TRUE)\\nprop←qlogis(tapply( sigdz, list(af, sex), mean,\\nna.rm=TRUE))\\nagem←as.numeric(row.names( prop))\\nlpoints( agem,prop[, 'female '], pch=4, col= 'green ')\\nlpoints( agem,prop[, 'male '], pch=2, col= 'green ')\\n})# Figure 10.6\\nplot(Predict(f, age, sex), ylim=c(-2,4), addpanel=w,\\nlabel.curve= list(offset=unit(0.5, 'cm')))\\nThe subgrouping method requires relatively large sample sizes and does\\nnot use continuous factors eﬀectively. The orderingof values is not used at all\\nbetween intervals, and the estimate of the relationship for a continuous vari-\\nable has little resolution. Also, the method of grouping chosen (e.g., deciles\\nvs. quintiles vs. rounding) can alter the shape of the plot.\\nIn this dataset with only two variables, it is eﬃcient to use a nonpara-\\nmetric smoother for age, separately for males and females. Nonparametric\\nsmoothers, such as loess111used here, work well for binary response vari-\\nables (see Section 2.4.7); the logit transformation is made on the smoothed\\nprobability estimates. The smoothed estimates are shown in Figure 10.6. 10\\nWhen there are several predictors, the restricted cubic spline function is\\nbetter for estimating the true relationship between X2and logit {Y=1}for\\ncontinuousvariableswithoutassuminglinearity.Byﬁttingamodelcontaining\\nX2expanded into k−1t e r m s ,w h e r e kis the number of knots, one can obtain\\nan estimate of the transformation of X2as discussed in Section 2.4:\\nlogit{Y=1|X}=ˆβ0+ˆβ1X1+ˆβ2X2+ˆβ3X′\\n2+ˆβ4X′′\\n2\\n=ˆβ0+ˆβ1X1+f(X2), (10.29)\\nwhereX′\\n2andX′′\\n2are constructed spline variables (when k= 4). Plotting\\nthe estimated spline function f(X2)v e r s u sX2will estimate how the eﬀect of\\nX2should be modeled. If the sample is suﬃciently large, the spline function\\ncan be ﬁtted separately for X1=0a n d X1= 1, allowing detection of even\\nunusual interaction patterns. A formal test of linearity in X2is obtained by\\ntestingH0:β3=β4=0 .\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a028dd39-eb39-4c20-8a0f-ad41785ea5fc', embedding=None, metadata={'page_label': '239', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.5 Assessment of Model Fit 239\\nAge, Yearlog odds\\n−10123\\n30 40 50 60 70 80male\\nfemale\\nFig. 10.6 Logit proportions of signiﬁcant coronary artery disease by sex and deciles\\nof age for n=3504 patients, with spline ﬁts (smooth curves). Spline ﬁts are for k=4\\nknots at age= 36, 48, 56, and 68 years, and interaction between age and sex is allowed.\\nShaded bands are pointwise 0.95 conﬁdence limits for predicted log odds. Smooth\\nnonparametric estimates are shown as dotted curves. Data courtesy of the Duke\\nCardiovascular Disease Databank.\\nFor testing interaction between X1andX2, a product term (e.g., X1X2)\\ncan be added to the model and its coeﬃcient tested. A more general simul-\\ntaneous test of linearity and lack of interaction for a two-variable model in\\nwhich one variable is binary (or is assumed linear) is obtained by ﬁtting the\\nmodel\\nlogit{Y=1|X}=β0+β1X1+β2X2+β3X′\\n2+β4X′′\\n2\\n+β5X1X2+β6X1X′\\n2+β7X1X′′\\n2 (10.30)\\nand testing H0:β3=...=β7= 0. This formulation allows the shape of the\\nX2eﬀect to be completely diﬀerent for each level of X1. There is virtually\\nno departure from linearity and additivity that cannot be detected from this\\nexpanded model formulation.The most computationallyeﬃcient test for lack\\nof ﬁt is the score test (e.g., X1andX2are forced into a tentative model\\nand the remaining variables are candidates). Figure 10.6also depicts a ﬁtted\\nspline logistic model with k= 4, allowing for general interaction between\\nage and sex as parameterized above. The ﬁtted function, after expanding the\\nrestricted cubic spline function for simplicity (see Equation 2.27), is given\\nabove. Note the good agreement between the empirical estimates of log odds\\nand the spline ﬁts and nonparametric estimates in this large dataset.\\nAn analysisofloglikelihoodforthismodelandvarioussub-modelsisfound\\nin Table 10.3.T h eχ2for global tests is corrected for the intercept and the\\ndegrees of freedom does not include the intercept.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='32b99483-f205-4627-8313-85d2fd535fde', embedding=None, metadata={'page_label': '240', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='240 10 Binary Logistic Regression\\nTable 10.3 LRχ2tests for coronary artery disease risk\\nModel / Hypothesis Likelihood d.f. PFormula\\nRatioχ2\\na: sex, age (linear, no interaction) 766 .02\\nb: sex, age, age ×sex 768 .23\\nc: sex, spline in age 769 .44\\nd: sex, spline in age, interaction 782 .57\\nH0: no age ×sex interaction 2 .21 . 14 (b−a)\\ngiven linearity\\nH0: age linear |no interaction 3 .42 . 18 (c−a)\\nH0: age linear, no interaction 16 .6 5 .005 ( d−a)\\nH0: age linear, product form 14 .4 4 .006 ( d−b)\\ninteraction\\nH0: no interaction, allowing for 13 .1 3 .004 ( d−c)\\nnonlinearity in age\\nTable 10.4 AIC on χ2scale by number of knots\\nkModelχ2AIC\\n0 99.23 97.23\\n3 112.69 108.69\\n4 121.30 115.30\\n5 123.51 115.51\\n6 124.41 114.51\\nThis analysis conﬁrms the ﬁrst impression from the graph, namely, that\\nage×sex interaction is present but it is not of the form of a simple product\\nbetween age and sex (change in slope). In the context of a linear age eﬀect,\\nthere is no signiﬁcant product interaction eﬀect ( P=.14). Without allowing\\nfor interaction, there is no signiﬁcant nonlinear eﬀect of age ( P=.18). How-\\never, the general test of lack of ﬁt with 5 d.f. indicates a signiﬁcant departure\\nfrom the linear additive model ( P=.005).\\nIn Figure 10.7, data from 2332 patients who underwent cardiac catheteri-\\nzation at Duke University Medical Center and were found to have signiﬁcant\\n(≥75%) diameter narrowing of at least one major coronary artery were ana-\\nlyzed (the dataset is available from the Web site). The relationship between\\nthe time from the onset of symptoms of coronaryartery disease (e.g., angina,\\nmyocardial infarction) to the probability that the patient has severe (three-\\nvessel disease or left main disease— tvdlm) coronary disease was of interest.\\nThere were 1129 patients with tvdlm. A logistic model was used with the\\nduration of symptoms appearing as a restricted cubic spline function with\\nk=3,4,5, and 6 equally spaced knots in terms of quantiles between .05 and\\n.95. The best ﬁt for the number of parameters was chosen using Akaike’s\\ninformation criterion (AIC), computed in Table 10.4as the model likelihood', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a4188aa-5928-4eb3-9170-896c708bc915', embedding=None, metadata={'page_label': '241', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.5 Assessment of Model Fit 241\\nratioχ2minus twice the number of parameters in the model aside from the\\nintercept. The linear model is denoted k=0 .\\ndz←subset( acath, sigdz==1)\\ndd←datadist(dz)\\nf←lrm(tvdlm ∼rcs(cad.dur, 5), data=dz)\\nw←function(...)\\nwith(dz, {\\nplsmo(cad.dur, tvdlm, fun= qlogis, add= TRUE,\\ngrid=TRUE, lty= 'dotted ')\\nx←cut2(cad.dur, g=15, levels.mean= TRUE)\\nprop←qlogis(tapply( tvdlm, x, mean, na.rm=TRUE))\\nxm←as.numeric( names(prop))\\nlpoints(xm, prop, pch=2, col= 'green ')\\n})# Figure 10.7\\nplot(Predict(f, cad.dur), addpanel=w)\\nDuration of Symptoms of Coronary Artery Diseaselog odds\\n−1012\\n0 100 200 300\\nFig. 10.7 Estimated relationship between duration of symptoms and the log odds\\nof severe coronary artery disease for k= 5. Knots are marked with arrows. Solid line\\nis spline ﬁt; dotted line is a nonparametric loess estimate.\\nFigure10.7displays the spline ﬁt for k= 5. The triangles represent sub-\\ngroup estimates obtained by dividing the sample into groups of 150 patients.\\nFor example, the leftmost triangle represents the logit of the proportion\\noftvdlmin the 150 patients with the shortest duration of symptoms, ver-\\nsus the mean duration in that group. A Wald test of linearity, with 3 d.f.,\\nshowed highly signiﬁcantnonlinearity ( χ2= 23.92with 3 d.f.). The plotof the\\nspline transformation suggests a log transformation, and when log (duration\\nof symptoms in months + 1) was ﬁtted in a logistic model, the log likelihood\\nof the model (119.33 with 1 d.f.) was virtually as good as the spline model\\n(123.51 with 4 d.f.); the corresponding Akaike information criteria (on the χ2\\nscale)are117.33and115.51.To checkfor adequacyinthe log transformation,\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eda8acff-a96b-4228-a154-72e8f21ed505', embedding=None, metadata={'page_label': '242', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"242 10 Binary Logistic Regression\\na ﬁve-knot restricted cubic spline function was ﬁtted to log10(months + 1),\\nas displayed in Figure 10.8. There is some evidence for lack of ﬁt on the right,\\nbut the Wald χ2for testing linearity yields P=.27.\\nf←lrm(tvdlm ∼log10(cad.dur + 1), data=dz)\\nw←function(...)\\nwith(dz, {\\nx←cut2(cad.dur, m =150,levels.mean= TRUE)\\nprop←tapply( tvdlm, x, mean, na.rm=TRUE)\\nxm←as.numeric( names(prop))\\nlpoints(xm, prop, pch=2, col= 'green ')\\n})\\n# Figure 10.8\\nplot(Predict(f, cad.dur, fun=plogis), ylab= 'P',\\nylim=c(.2, .8), addpanel=w)\\nDuration of Symptoms of Coronary Artery DiseaseP\\n0.30.40.50.60.7\\n0 100 200 300\\nFig. 10.8 Fitted linear logistic model in log10(duration + 1), with subgroup es-\\ntimates using groups of 150 patients. Fitted equation is logit( tvdlm)=−.9809 +\\n.7122log10(months +1).\\nIf the model contains two continuous predictors, they may both be ex-\\npanded with spline functions in orderto test linearityor to describe nonlinear\\nrelationships. Testing interaction is more diﬃcult here. If X1is continuous,\\none might temporarily group X1into quantile groups. Consider the subset\\nof 2258 (1490 with disease) of the 3504 patients used in Figure 10.6who\\nhave serum cholesterol measured. A logistic model for predicting signiﬁcant\\ncoronary disease was ﬁtted with age in tertiles (modeled with two dummy\\nvariables), sex, age ×sex interaction, four-knot restricted cubic spline in\\ncholesterol, and age tertile ×cholesterol interaction. Except for the sex ad-\\njustment this model is equivalent to ﬁtting three separate spline functions in\\ncholesterol, one for each age tertile. The ﬁtted model is shown in Figure 10.9\\nfor cholesterol and age tertile against logit of signiﬁcant disease. Signiﬁcant\\nage×cholesterol interaction is apparent from the ﬁgure and is suggested by\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d767eaac-e0a8-490b-890b-41571742e3d3', embedding=None, metadata={'page_label': '243', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.5 Assessment of Model Fit 243\\nthe Wald χ2statistic (10.03) that follows. Note that the test for linearity of\\nthe interaction with respect to cholesterol is very insigniﬁcant ( χ2=2 . 4 0o n\\n4 d.f.), but we retain it for now. The ﬁtted function is\\nacath ←transform( acath,\\ncholesterol = choleste,\\nage.tertile = cut2(age,g=3),\\nsx = as.integer( acath$sex) - 1)\\n# sx for loess, need to code as numeric\\ndd←datadist( acath); options(datadist= 'dd')\\n# First model stratifies age intotertiles to get more\\n# empirical estimates of age x cholesterol interaction\\nf←lrm(sigdz ∼age.tertile*(sex + rcs( cholest erol,4)),\\ndata=acath)\\nprint(f, latex=TRUE)\\nLogistic Regression Model\\nlrm(formula = sigdz ~ age.tertile * (sex + rcs(cholesterol, 4)),\\ndata = acath)\\nFrequencies of Missing Values Due to Each Variable\\nsigdz age.tertile sex cholesterol\\n0 0 0 1246\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 2258 LRχ2533.52R20.291C 0.780\\n0 768 d.f. 14 g 1.316Dxy0.560\\n1 1490 Pr(>χ2)<0.0001gr3.729γ 0.562\\nmax|∂logL\\n∂β|2×10−8gp0.252τa0.251\\nBrier 0.173\\nCoef S.E. Wald ZPr(>|Z|)\\nIntercept -0.4155 1.0987 -0.38 0.7053\\nage.tertile=[49,58) 0.8781 1.7337 0.51 0.6125\\nage.tertile=[58,82] 4.7861 1.8143 2.64 0.0083\\nsex=female -1.6123 0.1751 -9.21 <0.0001\\ncholesterol 0.0029 0.0060 0.48 0.6347\\ncholesterol’ 0.0384 0.0242 1.59 0.1126\\ncholesterol” -0.1148 0.0768 -1.49 0.1350\\nage.tertile=[49,58) * sex=female -0.7900 0.2537 -3.11 0.0018\\nage.tertile=[58,82] * sex=female -0.4530 0.2978 -1.52 0.1283\\nage.tertile=[49,58) * cholesterol 0.0011 0.0095 0.11 0.9093\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ef74f70-92d7-4a04-89a2-5fbf09411aee', embedding=None, metadata={'page_label': '244', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"244 10 Binary Logistic Regression\\nCoef S.E. Wald ZPr(>|Z|)\\nage.tertile=[58,82] * cholesterol -0.0158 0.0099 -1.59 0.1111\\nage.tertile=[49,58) * cholesterol’ -0.0183 0.0365 -0.50 0.6162\\nage.tertile=[58,82] * cholesterol’ 0.0127 0.0406 0.31 0.7550\\nage.tertile=[49,58) * cholesterol” 0.0582 0.1140 0.51 0.6095\\nage.tertile=[58,82] * cholesterol” -0.0092 0.1301 -0.07 0.9436\\nltx(f)\\nXˆβ=−0.415 + 0.878[age.tertile∈[49,58)] + 4.79[age.tertile∈[58,82]]−\\n1.61[female] + 0 .00287cholesterol+ 1 .52×10−6(cholesterol −160)3\\n+−4.53×\\n10−6(cholesterol −208)3\\n++3.44×10−6(cholesterol −243)3\\n+−4.28×10−7\\n(cholesterol −319)3\\n++[female][ −0.79[age.tertile∈[49,58)]−0.453[age.tertile∈\\n[58,82]]]+[age .tertile∈[49,58)][0.00108cholesterol −7.23×10−7(cholesterol −\\n160)3\\n++2.3×10−6(cholesterol −208)3\\n+−1.84×10−6(cholesterol −243)3\\n++\\n2.69×10−7(cholesterol −319)3\\n+]+[age.tertile∈[58,82]][−0.0158cholesterol+\\n5×10−7(cholesterol −160)3\\n+−3.64×10−7(cholesterol −208)3\\n+−5.15×10−7\\n(cholesterol −243)3\\n++3.78×10−7(cholesterol −319)3\\n+].\\n# Table 10.5:\\nlatex(anova(f), file= '', size= 'smaller ',\\ncaption= 'Crudely categorizing age intotertiles ',\\nlabel= 'tab:anova-tertiles ')\\nyl←c(-1,5)\\nplot(Predict(f, cholesterol, age.tertile),\\nadj.subtitle= FALSE, ylim=yl) # Figure 10.9\\nTable 10.5 Crudely categorizing age into tertiles\\nχ2d.f.P\\nage.tertile (Factor+Higher Order Factors) 120 .74 10<0.0001\\nAll Interactions 21.87 8 0.0052\\nsex (Factor+Higher Order Factors) 329 .54 3<0.0001\\nAll Interactions 9.78 2 0.0075\\ncholesterol (Factor+Higher Order Factors) 93 .75 9<0.0001\\nAll Interactions 10.03 6 0.1235\\nNonlinear (Factor+Higher Order Factors) 9.96 6 0.1263\\nage.tertile ×sex (Factor+Higher Order Factors) 9 .78 2 0.0075\\nage.tertile ×cholesterol (Factor+Higher Order Factors) 10 .03 6 0.1235\\nNonlinear 2.62 4 0.6237\\nNonlinear Interaction : f(A,B) vs. AB 2.62 4 0.6237\\nTOTAL NONLINEAR 9.96 6 0.1263\\nTOTAL INTERACTION 21 .87 8 0.0052\\nTOTAL NONLINEAR + INTERACTION 29 .67 10 0.0010\\nTOTAL 410.75 14<0.0001\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b53ec69f-8fe8-4372-bbf9-b87cd5e1c9cb', embedding=None, metadata={'page_label': '245', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.5 Assessment of Model Fit 245\\nCholesterol, mg %log odds\\n01234\\n100 200 300 400[17,49)[49,58)[58,82]\\nFig. 10.9 Log odds of signiﬁcant coronary artery disease modeling age with two\\ndummy variables\\nBefore ﬁtting a parametric model that allows interaction between age and\\ncholesterol, let us use the local regression model of Cleveland et al.96dis-\\ncussed in Section 2.4.7. This nonparametric smoothing method is not meant\\nto handle binary Y, but it can still provide useful graphical displays in the\\nbinary case.Figure 10.10depicts the ﬁt froma localregressionmodel predict-\\ningY= 1 = signiﬁcant coronary artery disease. Predictors are sex (modeled\\nparametrically with a dummy variable), age, and cholesterol, the last two\\nﬁtted nonparametrically. The eﬀect of not explicitly modeling a probability\\nis seen in the ﬁgure, as the predicted probabilities exceeded 1. Because of this\\nwe do not take the logit transformation but leave the predicted values in raw\\nform. However, the overall shape is in agreement with Figure 10.10.\\n# Re-do model withcontinuous age\\nf←loess(sigdz ∼age * (sx + cholesterol), data=acath,\\nparametric=\"sx\", drop.square=\"sx\")\\nages ←seq(25, 75, length=40)\\nchols ←seq(100, 400, length=40)\\ng←expand.grid(cholesterol= chols, age=ages, sx=0)\\n# drop sex dimension of gridsinceheld to 1 value\\np←drop(predict(f, g))\\np[p < 0.001] ←0.001\\np[p > 0.999] ←0.999\\nzl←c(-3, 6) # Figure 10.10\\nwireframe( qlogis(p) ∼cholesterol*age,\\nxlab=list(rot =30),ylab=list(rot=-40),\\nzlab=list(label= \\'log odds \\', rot=90), zlim=zl,\\nscales = list(arrows = FALSE), data=g)\\nChapter 2discussed linear splines, which can be used to construct linear\\nspline surfaces by adding all cross-products of the linear variables and spline\\nterms in the model. With a suﬃcient number of knots for each predictor, the\\nlinear spline surface can ﬁt a wide variety of patterns. However, it requires', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fb4dac66-78b5-4f28-9f88-d95e0d727254', embedding=None, metadata={'page_label': '246', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='246 10 Binary Logistic Regression\\na large number of parameters to be estimated. For the age–sex–cholesterol\\nexample, a linear spline surface is ﬁtted for age and cholesterol, and a sex\\n×age spline interaction is also allowed. Figure 10.11shows a ﬁt that placed\\nknots at quartiles of the two continuous variablesc. The algebraic form of the\\nﬁtted model is shown below.\\nf←lrm(sigdz ∼lsp(age,c(46,52,59)) *\\n(sex + lsp(cholesterol,c(196,224,259))),\\ndata=acath)\\nltx(f)\\nXˆβ=−1.83 + 0.0232 age + 0 .0759(age −46)+−0.0025(age −52)++\\n2.27(age−59)++3.02[female] −0.0177cholesterol+0 .114(cholesterol −196)+−\\n0.131(cholesterol −224)++0.0651(cholesterol −259)++[female][ −0.112age+\\n0.0852 (age −46)+−0.0302 (age −52)++0.176 (age −59)+] + age\\n[0.000577cholesterol −0.00286(cholesterol −196)++0.00382(cholesterol −\\n224)+−0.00205(cholesterol −259)+]+(age−46)+[−0.000936cholesterol+\\n0.00643(cholesterol −196)+−0.0115(cholesterol −224)++0.00756(cholesterol −\\n259)+] + (age −52)+[0.000433 cholesterol −0.0037 (cholesterol −196)++\\n0.00815 (cholesterol −224)+−0.00715 (cholesterol −259)+] + (age −59)+\\n[−0.0124cholesterol+0 .015(cholesterol −196)+−0.0067(cholesterol −224)++\\n0.00752(cholesterol −259)+].\\n100150200250300350400\\n3040506070−20246\\ncholesterolagelog odds\\nFig. 10.10 Local regression ﬁt for the logit of the probability of signiﬁcant coronary\\ndisease vs. age and cholesterol for males, based on the loessfunction.\\ncIn the wireframe plots that follow, predictions for cholesterol–age combinations for\\nwhich fewer than 5 exterior points exist are not shown, so as to not extrapolate to\\nregions not supported by at least ﬁve points beyond the data perimeter.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9afa96c2-d16f-4b16-a002-9491422ca9b4', embedding=None, metadata={'page_label': '247', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.5 Assessment of Model Fit 247\\nlatex(anova(f), caption= 'Linear spline surface ', file= '',\\nsize= 'smaller ', label= 'tab:anova-lsp ')# Table 10.6\\nperim ←with(acath,\\nperimeter( cholesterol, age, xinc=20, n=5))\\nzl←c(-2, 4) # Figure 10.11\\nbplot(Predict(f, cholesterol, age, np =40),perim=perim,\\nlfun=wireframe, zlim=zl, adj.subtitle= FALSE)\\nTable 10.6 Linear spline surface\\nχ2d.f.P\\nage (Factor+Higher Order Factors) 164 .17 24<0.0001\\nAll Interactions 42.28 20 0.0025\\nNonlinear (Factor+Higher Order Factors) 25.21 18 0.1192\\nsex (Factor+Higher Order Factors) 343 .80 5<0.0001\\nAll Interactions 23.90 4 0.0001\\ncholesterol (Factor+Higher Order Factors) 100 .13 20<0.0001\\nAll Interactions 16.27 16 0.4341\\nNonlinear (Factor+Higher Order Factors) 16.35 15 0.3595\\nage×sex (Factor+Higher Order Factors) 23 .90 4 0.0001\\nNonlinear 12.97 3 0.0047\\nNonlinear Interaction : f(A,B) vs. AB 12.97 3 0.0047\\nage×cholesterol (Factor+Higher Order Factors) 16 .27 16 0.4341\\nNonlinear 11.45 15 0.7204\\nNonlinear Interaction : f(A,B) vs. AB 11.45 15 0.7204\\nf(A,B) vs. Af(B) + Bg(A) 9.38 9 0.4033\\nNonlinear Interaction in age vs. Af(B) 9.99 12 0.6167\\nNonlinear Interaction in cholesterol vs. Bg(A) 10.75 12 0.5503\\nTOTAL NONLINEAR 33 .22 24 0.0995\\nTOTAL INTERACTION 42 .28 20 0.0025\\nTOTAL NONLINEAR + INTERACTION 49 .03 26 0.0041\\nTOTAL 449.26 29<0.0001\\nChapter 2also discussed a tensor spline extension of the restricted cubic\\nspline model to ﬁt a smooth function of two predictors, f(X1,X2). Since\\nthis function allows for general interaction between X1andX2,t h et w o -\\nvariable cubic spline is a powerful tool for displaying and testing interaction,\\nassuming the sample size warrants estimating 2( k−1)+(k−1)2parameters\\nfor a rectangular grid of k×kknots. Unlike the linear spline surface, the\\ncubic surface is smooth. It also requires fewer parameters in most situations.\\nThe general cubic model with k= 4 (ignoring the sex eﬀect here) is\\nβ0+β1X1+β2X′\\n1+β3X′′\\n1+β4X2+β5X′\\n2+β6X′′\\n2+β7X1X2\\n+ β8X1X′\\n2+β9X1X′′\\n2+β10X′\\n1X2+β11X′\\n1X′\\n2 (10.31)\\n++ β12X′\\n1X′′\\n2+β13X′′\\n1X2+β14X′′\\n1X′\\n2+β15X′′\\n1X′′\\n2,\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89f1a2d4-a315-4cde-a9dc-5a6cb7ce59f4', embedding=None, metadata={'page_label': '248', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='248 10 Binary Logistic Regression\\nwhereX′\\n1,X′′\\n1,X′\\n2,a n dX′′\\n2are restricted cubic spline component variables\\nforX1andX2fork= 4. A general test of interaction with 9 d.f. is H0:β7=\\n...=β15= 0. A test of adequacy of a simple product form interaction is\\nH0:β8=...=β15= 0 with 8 d.f. A 13 d.f. test of linearity and additivity\\nisH0:β2=β3=β5=β6=β7=β8=β9=β10=β11=β12=β13=β14=\\nβ15=0.\\nFigure10.12depictstheﬁtofthismodel.Thereisexcellentagreementwith\\nFigures10.9and10.11, including an increased (but probably insigniﬁcant)\\nrisk with low cholesterol for age ≥57.\\nf←lrm(sigdz ∼rcs(age,4)*(sex + rcs(cholest erol,4)),\\ndata=acath, tol=1 e-11)\\nltx(f)\\nXˆβ=−6.41 + 0.166age−0.00067(age −36)3\\n++0.00543(age −48)3\\n+−\\n0.00727(age −56)3\\n++0.00251(age −68)3\\n++2.87[female]+0 .00979cholesterol+\\n1.96×10−6(cholesterol −160)3\\n+−7.16×10−6(cholesterol −208)3\\n++6.35×\\n10−6(cholesterol −243)3\\n+−1.16×10−6(cholesterol −319)3\\n++[female][ −0.109age+\\n7.52×10−5(age−36)3\\n++0.00015(age −48)3\\n+−0.00045(age −56)3\\n++0.000225(age −\\n68)3\\n+] + age[−0.00028cholesterol+ 2 .68×10−9(cholesterol −160)3\\n++3.03×\\n10−8(cholesterol −208)3\\n+−4.99×10−8(cholesterol −243)3\\n++1.69×10−8\\n(cholesterol −319)3\\n+] + age′[0.00341cholesterol −4.02×10−7(cholesterol −\\n160)3\\n++9.71×10−7(cholesterol −208)3\\n+−5.79×10−7(cholesterol −243)3\\n++8.79×\\n10−9(cholesterol −319)3\\n+]+age′′[−0.029cholesterol+3 .04×10−6(cholesterol −\\n100150200250300350400\\n3040506070−2−101234\\nAge\\n,Yearlog odds\\nCholesterol, mg %\\nFig. 10.11 Linear spline surface for males, with knots for age at 46, 52, 59 and knots\\nfor cholesterol at 196, 224, and 259 (quartiles).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='403e6061-2779-43a3-9adc-14098a14acd8', embedding=None, metadata={'page_label': '249', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.5 Assessment of Model Fit 249\\n160)3\\n+−7.34×10−6(cholesterol −208)3\\n++4.36×10−6(cholesterol −243)3\\n+−\\n5.82×10−8(cholesterol −319)3\\n+].\\nlatex(anova(f), caption= 'Cubic spline surface ', file= '',\\nsize= 'smaller ', label= 'tab:anova-rcs ')#Table 10.7\\n# Figure 10.12:\\nbplot(Predict(f, cholesterol, age, np =40),perim=perim,\\nlfun=wireframe, zlim=zl, adj.subtitle= FALSE)\\nTable 10.7 Cubic spline surface\\nχ2d.f.P\\nage (Factor+Higher Order Factors) 165 .23 15<0.0001\\nAll Interactions 37.32 12 0.0002\\nNonlinear (Factor+Higher Order Factors) 21.01 10 0.0210\\nsex (Factor+Higher Order Factors) 343 .67 4<0.0001\\nAll Interactions 23.31 3<0.0001\\ncholesterol (Factor+Higher Order Factors) 97 .50 12<0.0001\\nAll Interactions 12.95 9 0.1649\\nNonlinear (Factor+Higher Order Factors) 13.62 8 0.0923\\nage×sex (Factor+Higher Order Factors) 23 .31 3<0.0001\\nNonlinear 13.37 2 0.0013\\nNonlinear Interaction : f(A,B) vs. AB 13.37 2 0.0013\\nage×cholesterol (Factor+Higher Order Factors) 12 .95 9 0.1649\\nNonlinear 7.27 8 0.5078\\nNonlinear Interaction : f(A,B) vs. AB 7.27 8 0.5078\\nf(A,B) vs. Af(B) + Bg(A) 5.41 4 0.2480\\nNonlinear Interaction in age vs. Af(B) 6.44 6 0.3753\\nNonlinear Interaction in cholesterol vs. Bg(A) 6.27 6 0.3931\\nTOTAL NONLINEAR 29 .22 14 0.0097\\nTOTAL INTERACTION 37 .32 12 0.0002\\nTOTAL NONLINEAR + INTERACTION 45 .41 16 0.0001\\nTOTAL 450.88 19<0.0001\\nStatistics for testing age ×cholesterol components of this ﬁt are above.\\nNone of the nonlinear interaction components is signiﬁcant, but we again\\nretain them.\\nThe general interaction model can be restricted to be of the form\\nf(X1,X2)=f1(X1)+f2(X2)+X1g2(X2)+X2g1(X1) (10.32)\\nby removingthe parameters β11,β12,β14,a n dβ15from the model. The previ-\\nous table of Wald statistics included a test of adequacy of this reduced form\\n(χ2=5.41 on 4 d.f., P=.248). The resulting ﬁt is in Figure 10.13.\\nf←lrm(sigdz ∼sex*rcs(age,4) + rcs(cholest erol,4) +\\nrcs(age,4) %ia% rcs(cholesterol,4), data=acath)\\nlatex(anova(f), file= '', size= 'smaller ',\\ncaption= 'Singly nonlinear cubic spline surface ',\\nlabel= 'tab:anova-ria ')#Table 10.8\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f95710b1-6269-4d02-8b9d-bc1a1da810f8', embedding=None, metadata={'page_label': '250', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='250 10 Binary Logistic Regression\\n100150200250300350400\\n3040506070−2−101234\\nCholesterol, mg %Age\\n,Yearlog odds\\nFig. 10.12 Restricted cubic spline surface in two variables, each with k=4k n o t s\\nTable 10.8 Singly nonlinear cubic spline surface\\nχ2d.f.P\\nsex (Factor+Higher Order Factors) 343 .42 4<0.0001\\nAll Interactions 24.05 3<0.0001\\nage (Factor+Higher Order Factors) 169 .35 11<0.0001\\nAll Interactions 34.80 8<0.0001\\nNonlinear (Factor+Higher Order Factors) 16.55 6 0.0111\\ncholesterol (Factor+Higher Order Factors) 93 .62 8<0.0001\\nAll Interactions 10.83 5 0.0548\\nNonlinear (Factor+Higher Order Factors) 10.87 4 0.0281\\nage×cholesterol (Factor+Higher Order Factors) 10 .83 5 0.0548\\nNonlinear 3.12 4 0.5372\\nNonlinear Interaction : f(A,B) vs. AB 3.12 4 0.5372\\nNonlinear Interaction in age vs. Af(B) 1.60 2 0.4496\\nNonlinear Interaction in cholesterol vs. Bg(A) 1.64 2 0.4400\\nsex×age (Factor+Higher Order Factors) 24 .05 3<0.0001\\nNonlinear 13.58 2 0.0011\\nNonlinear Interaction : f(A,B) vs. AB 13.58 2 0.0011\\nTOTAL NONLINEAR 27 .89 10 0.0019\\nTOTAL INTERACTION 34 .80 8<0.0001\\nTOTAL NONLINEAR + INTERACTION 45 .45 12<0.0001\\nTOTAL 453.10 15<0.0001\\n# Figure 10.13:\\nbplot(Predict(f, cholesterol, age, np =40),perim=perim,\\nlfun=wireframe, zlim=zl, adj.subtitle= FALSE)\\nltx(f)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e7a8b468-4d76-4111-a0f8-6c4041a285cf', embedding=None, metadata={'page_label': '251', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.5 Assessment of Model Fit 251\\nTable 10.9 Linear interaction surface\\nχ2d.f.P\\nage (Factor+Higher Order Factors) 167 .83 7<0.0001\\nAll Interactions 31.03 4<0.0001\\nNonlinear (Factor+Higher Order Factors) 14.58 4 0.0057\\nsex (Factor+Higher Order Factors) 345 .88 4<0.0001\\nAll Interactions 22.30 3 0.0001\\ncholesterol (Factor+Higher Order Factors) 89 .37 4<0.0001\\nAll Interactions 7.99 1 0.0047\\nNonlinear 10.65 2 0.0049\\nage×cholesterol (Factor+Higher Order Factors) 7 .99 1 0.0047\\nage×sex (Factor+Higher Order Factors) 22 .30 3 0.0001\\nNonlinear 12.06 2 0.0024\\nNonlinear Interaction : f(A,B) vs. AB 12.06 2 0.0024\\nTOTAL NONLINEAR 25 .72 6 0.0003\\nTOTAL INTERACTION 31 .03 4<0.0001\\nTOTAL NONLINEAR + INTERACTION 43 .59 8<0.0001\\nTOTAL 452.75 11<0.0001\\nXˆβ=−7.2+2.96[female]+0 .164age+7 .23×10−5(age−36)3\\n+−0.000106(age −\\n48)3\\n+−1.63×10−5(age−56)3\\n++4.99×10−5(age−68)3\\n++0.0148cholesterol+\\n1.21×10−6(cholesterol −160)3\\n+−5.5×10−6(cholesterol −208)3\\n++5.5×\\n10−6(cholesterol −243)3\\n+−1.21×10−6(cholesterol −319)3\\n++ age[−0.00029\\ncholesterol+9 .28×10−9(cholesterol −160)3\\n++1.7×10−8(cholesterol −208)3\\n+−\\n4.43×10−8(cholesterol −243)3\\n++1.79×10−8(cholesterol −319)3\\n+]+cholesterol[2 .3×\\n10−7(age−36)3\\n++4.21×10−7(age−48)3\\n+−1.31×10−6(age−56)3\\n++6.64×\\n10−7(age−68)3\\n+]+[female][ −0.111age+8 .03×10−5(age−36)3\\n++0.000135(age −\\n48)3\\n+−0.00044(age −56)3\\n++0.000224(age −68)3\\n+].\\nThe ﬁt is similar to the former one except that the climb in risk for low-\\ncholesterol older subjects is less pronounced. The test for nonlinear interac-\\ntion is now more concentrated( P=.54 with 4 d.f.). Figure 10.14accordingly\\ndepicts a ﬁt that allows age and cholesterol to have nonlinear main eﬀects,\\nbut restricts the interaction to be a product between (untransformed) age\\nand cholesterol. The function agrees substantially with the previous ﬁt.\\nf←lrm(sigdz ∼rcs(age,4)*sex + rcs(cholesterol,4) +\\nage %ia% cholesterol, data=acath)\\nlatex(anova (f),caption= 'Linear interaction surface ', file= '',\\nsize= 'smaller ', label= 'tab:anova-lia ')#Table 10.9\\n# Figure 10.14:\\nbplot(Predict(f, cholesterol, age, np =40),perim=perim,\\nlfun=wireframe, zlim=zl, adj.subtitle= FALSE)\\nf.linia ←f# save linear interaction fit for later\\nltx(f)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68e9c886-4314-4039-821c-19adb41b69ac', embedding=None, metadata={'page_label': '252', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='252 10 Binary Logistic Regression\\n100150200250300350400\\n3040506070−2−101234\\nCholesterol, mg %Age\\n,Yearlog odds\\nFig. 10.13 Restricted cubic spline ﬁt with age ×spline(cholesterol) and cholesterol\\n×spline(age)\\nXˆβ=−7.36+0.182age−5.18×10−5(age−36)3\\n++8.45×10−5(age−48)3\\n+−2.91×\\n10−6(age−56)3\\n+−2.99×10−5(age−68)3\\n++2.8[female]+0 .0139cholesterol+\\n1.76×10−6(cholesterol −160)3\\n+−4.88×10−6(cholesterol −208)3\\n++3.45×\\n10−6(cholesterol −243)3\\n+−3.26×10−7(cholesterol −319)3\\n+−0.00034age ×\\ncholesterol+ [female][ −0.107age+ 7 .71×10−5(age−36)3\\n++0.000115(age −\\n48)3\\n+−0.000398(age −56)3\\n++0.000205(age −68)3\\n+].\\nThe Wald test for age ×cholesterol interaction yields χ2=7.99 with 1\\nd.f.,P=.005. These analyses favor the nonlinear model with simple prod-\\nuct interaction in Figure 10.14as best representing the relationships among\\ncholesterol, age, and probability of prognostically severe coronary artery dis-\\nease. A nomogram depicting this model is shown in Figure 10.21.\\nUsing this simple product interaction model, Figure 10.15displays pre-\\ndicted cholesterol eﬀects at the mean age within each age tertile. Substantial\\nagreement with Figure 10.9is apparent.\\n# Make estimates of cholesterol effects for mean age in\\n# tertiles corresponding to initial analysis\\nmean.age ←\\nwith(acath,\\nas.vector( tapply(age, age.tertile, mean,na.rm=TRUE)))\\nplot(Predict(f, cholesterol, age=round(mean.age,2),\\nsex=\"male\"),\\nadj.subtitle= FALSE, ylim=yl) #3 curves, Figure 10.15', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f93c3f4b-df25-4af1-99c4-2e1d79143df1', embedding=None, metadata={'page_label': '253', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.5 Assessment of Model Fit 253\\n100150200250300350400\\n3040506070−2−101234\\nCholesterol, mg %Age\\n, Yearlog odds\\nFig. 10.14 Spline ﬁt with nonlinear eﬀects of cholesterol and age and a simple\\nproduct interaction\\nCholesterol,  mg %log odds\\n01234\\n100 200 300 40041.7453.0663.73\\nFig. 10.15 Predictions from linear interaction model with mean age in tertiles indi-\\ncated.\\nThe partial residuals discussed in Section 10.4c a nb eu s e dt oc h e c kl o -\\ngistic model ﬁt (although it may be diﬃcult to deal with interactions). As\\nan example, reconsider the “duration of symptoms” ﬁt in Figure 10.7. Fig-\\nure10.16displays“loess smoothed”and raw partial residuals for the original\\nand log-transformed variable. The latter provides a more linear relationship,\\nespecially where the data are most dense.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1dc836ee-cf51-4420-a5be-23ef9f58f12f', embedding=None, metadata={'page_label': '254', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='254 10 Binary Logistic Regression\\nTable 10.10 Merits of Methods for Checking Logistic Model Assumptions\\nMethod Choice Assumes Uses Ordering Low Good\\nRequired Additivity of X Variance Resolution\\nonX\\nStratiﬁcation Intervals\\nSmoother on X1Bandwidth x x x\\nstratifying on X2 (not on X2)( i f m i n . s t r a t . ) ( X1)\\nSmooth partial Bandwidth x x x x\\nresidual plot\\nSpline model Knots x x x x\\nfor allXs\\nf←lrm(tvdlm ∼cad.dur, data=dz, x= TRUE, y= TRUE)\\nresid(f, \" partial\", pl=\" loess\", xlim=c(0 ,250), ylim=c(-3,3))\\nscat1d(dz$ cad.dur)\\nlog.cad.dur ←log10(dz$ cad.dur + 1)\\nf←lrm(tvdlm ∼log.cad.dur, data=dz, x= TRUE, y= TRUE)\\nresid(f, \" partial\", pl=\" loess\", ylim =c(-3,3))\\nscat1d( log.cad.dur) # Figure 10.16\\n0 50 150 250−3−2−10123\\ncad.durri\\n0.0 1.0 2.0−3−2−10123\\nlog.cad.durri\\nFig. 10.16 Partial residualsfor durationand log10(duration+1). Data densityshown\\nat top of each plot.\\nTable10.10summarizesthe relativemeritsofstratiﬁcation,nonparametric\\nsmoothers, and regression splines for determining or checking binary logistic\\nmodel ﬁts.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d0f795e0-836b-46fe-9a18-bd2bb8ddbbc5', embedding=None, metadata={'page_label': '255', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.7 Overly Inﬂuential Observations 255\\n10.6 Collinearity\\nThe variance inﬂation factors (VIFs) discussed in Section 4.6can apply to\\nany regression ﬁt.147,654These VIFs allow the analyst to isolate which vari-\\nable(s) are responsible for highly correlatedparameterestimates. Recallthat,\\nin general, collinearityis not a large problem compared with nonlinearity and\\noverﬁtting.\\n10.7 Overly Inﬂuential Observations\\nPregibon511developed a number of regression diagnostics that apply to the\\nfamily of regressionmodels ofwhich logistic regressionis a member. Inﬂuence\\nstatistics based on the“leave-out-one”method use an approximation to avoid\\nhaving to reﬁt the model ntimes for nobservations. This approximation\\nuses the ﬁt and covariance matrix at the last iteration and assumes that\\nthe“weights”in the weighted least squares ﬁt can be kept constant, yielding\\na computationally feasible one-step estimate of the leave-out-one regression\\ncoeﬃcients.\\nHosmer and Lemeshow [ 305, pp. 149–170] discuss many diagnostics for\\nlogistic regression and show how the ﬁnal ﬁt can be used in any least squares\\nprogram that provides diagnostics. A new dependent variable to be used in\\nthat way is\\nZi=Xˆβ+Yi−ˆPi\\nVi, (10.33)\\nwhereVi=ˆPi(1−ˆPi), andˆPi=[ 1+e x p −Xˆβ]−1is the predicted probability\\nthatYi=1 .T h e Vi,i=1,2,...,nareusedasweightsinanordinaryweighted\\nleast squares ﬁt of XagainstZ. This least squares ﬁt will provide regression\\ncoeﬃcients identical to b. The new standard errors will be oﬀ from the actual\\nlogistic model ones by a constant.\\nAs discussed in Section 4.9, the standardized change in the regression co-\\neﬃcients upon leaving out each observation in turn (DFBETAS) is one of the\\nmost useful diagnostics, as these can pinpoint which observations are inﬂu-\\nential on each part of the model. After carefully modeling predictor trans-\\nformations, there should be no lack of ﬁt due to improper transformations.\\nHowever, as the white blood count example in Section 4.9indicates, it is\\ncommonly the case that extreme predictor values can still have too much\\ninﬂuence on the estimates of coeﬃcients involving that predictor.\\nIn the age–sex–response example of Section 10.1.3, both DFBETAS and\\nDFFITS identiﬁed the same inﬂuential observations. The observation given\\nby age = 48 sex = female response = 1 was inﬂuential for both age and sex,\\nwhile the observation age = 34 sex = male response = 1 was inﬂuential for\\nage and the observation age = 50 sex = male response = 0 was inﬂuential\\nfor sex. It can readily be seen from Figure 10.3that these points do not ﬁt\\nthe overall trends in the data. However, as these data were simulated from a', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fbc9e0f0-aebf-4e2e-a2e4-92a0cfe6dfa0', embedding=None, metadata={'page_label': '256', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='256 10 Binary Logistic Regression\\nTable 10.11 Example inﬂuence statistics\\nFemales Males\\nDFBETAS DFFITS DFBETAS DFFITS\\nIntercept Age Sex Intercept Age Sex\\n0.0 0.0 0.0 0 0.5 -0.5 -0.2 2\\n0.0 0.0 0.0 0 0.2 -0.3 0.0 1\\n0.0 0.0 0.0 0 -0.1 0.1 0.0 -1\\n0.0 0.0 0.0 0 -0.1 0.1 0.0 -1\\n-0.1 0.1 0.1 0 -0.1 0.1 -0.1 -1\\n-0.1 0.1 0.1 0 0.0 0.0 0.1 0\\n0.7 -0.7 -0.8 3 0.0 0.0 0.1 0\\n-0.1 0.1 0.1 0 0.0 0.0 0.1 0\\n-0.1 0.1 0.1 0 0.0 0.0 -0.2 -1\\n-0.1 0.1 0.1 0 0.1 -0.1 -0.2 -1\\n-0.1 0.1 0.1 0 0.0 0.0 0.1 0\\n-0.1 0.0 0.1 0 -0.1 0.1 0.1 0\\n-0.1 0.0 0.1 0 -0.1 0.1 0.1 0\\n0.1 0.0 -0.2 1 0.3 -0.3 -0.4 -2\\n0.0 0.0 0.1 -1 -0.1 0.1 0.1 0\\n0.1 -0.2 0.0 -1 -0.1 0.1 0.1 0\\n-0.1 0.2 0.0 1 -0.1 0.1 0.1 0\\n-0.2 0.2 0.0 1 0.0 0.0 0.0 0\\n-0.2 0.2 0.0 1 0.0 0.0 0.0 0\\n-0.2 0.2 0.1 1 0.0 0.0 0.0 0\\npopulation model that is truly linear in age and additive in age and sex, the\\napparent inﬂuential observations are just random occurrences. It is unwise\\nto assume that in real data all points will agree with overall trends. Removal\\nof such points would bias the results, making the model apparently more\\npredictive than it will be prospectively. See Table 10.11. 11\\nf←update( fasr, x=TRUE, y= TRUE)\\nwhich.influence(f, .4) # Table 10.11\\n10.8 Quantifying Predictive Ability\\nThe test statistics discussed above allo w one to test whether a factor or set of\\nfactors is related to the response. If the s ample is suﬃciently large, a factor\\nthat gradesriskfrom.01 to .02 may be a signiﬁcantriskfactor.However,that\\nfactor is not very useful in predicting the response for an individual subject.\\nThere is controversy regarding the appropriateness of R2from ordinary least\\nsquares in this setting.136,424The generalized R2\\nNindex of Nagelkerke471and 12\\nCragg and Uhler137, Maddala431, and Magee432described in Section 9.8.3\\ncan be useful for quantifying the predictive strength of a model:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9e8326b0-584a-490a-bfe4-71c721f17e18', embedding=None, metadata={'page_label': '257', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.8 Quantifying Predictive Ability 257\\nR2\\nN=1−exp(−LR/n)\\n1−exp(−L0/n), (10.34)\\nwhere LR is the global log likelihood ratio statistic for testing the importance\\nof allppredictors in the model and L0is the−2 log likelihood for the null\\nmodel. 13\\nTjur613coined the term “coeﬃcient of discrimination” D, deﬁned as the\\naverage ˆPwhenY= 1 minus the average ˆPwhenY= 0, and showed how it\\nties in with sum of squares–based R2measures. Dhas many advantages as\\nan index of predictive powerd.\\nLinnet416advocates quadratic and logarithmic probability scoring rules\\nfor measuring predictive performance for probability models. Linnet shows\\nhow to bootstrap such measures to get bias-corrected estimates and how to\\nuse bootstrapping to compare two correlated scores. The quadratic scoring\\nrule is Brier’s score, frequently used in judging meteorologic forecasts30,73:\\nB=1\\nnn∑\\ni=1(ˆPi−Yi)2, (10.35)\\nwhereˆPiis the predicted probability and Yithe corresponding observed re-\\nsponse for the ith observation. 14\\nA unitless index of the strength of the rank correlation between predicted\\nprobabilityofresponseand actualresponseisamoreinterpretablemeasureof\\nthe ﬁtted model’s predictive discrimination.One such index is the probability\\nof concordance, c, between predicted probability and response. The cindex,\\nwhich is derived from the Wilcoxon–Mann–Whitney two-sample rank test,\\nis computed by taking all possible pairs of subjects such that one subject\\nresponded and the other did not. The index is the proportion of such pairs\\nwith the responder having a higher predicted probability of response than\\nthe nonresponder.\\nBamber39and Hanley and McNeil255have shown that cis identical to a\\nwidely used measure of diagnostic discrimination, the area under a“receiver\\noperatingcharacteristic”(ROC)curve.Avalueof cof.5indicatesrandompre-\\ndictions, and a value of 1 indicates perfect prediction (i.e., perfect separation\\nof responders and nonresponders). A model having cgreater than roughly\\n.8 has some utility in predicting the responses of individual subjects. The\\nconcordance index is also related to another widely used index, Somers’ Dxy\\nrank correlation579between predicted probabilities and observed responses,\\nby the identity\\nDxy=2 (c−.5). (10.36)\\nDxyis the diﬀerence between concordance and discordance probabilities.\\nWhenDxy= 0, the model is making random predictions. When Dxy=1 ,\\ndNote that DandB(below) and other indexes not related to c(below) do not work\\nwell in case-control studies because of their reliance on absolute probability estimates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='527ca455-dd5d-4922-9f02-0088d1f070de', embedding=None, metadata={'page_label': '258', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='258 10 Binary Logistic Regression\\nthe predictions are perfectly discriminating. These rank-based indexes have\\nthe advantage of being insensitive to the prevalence of positive responses. 15\\nAcommonlyusedmeasureofpredictiveabilityforbinarylogisticmodels is\\nthe fraction of correctly classiﬁed responses. Here one chooses a cutoﬀ on the\\npredicted probability of a positive response and then predicts that a response\\nwill be positive if the predicted probability exceeds this cutoﬀ. There are a\\nnumber of reasons why this measure should be avoided.\\n1. It’s highly dependent on the cutpoint chosen for a“positive”prediction.\\n2. You can add a highly signiﬁcant variable to the model and have the per-\\ncentage classiﬁed correctly actually decrease. Classiﬁcation error is a very\\ninsensitive and statistically ineﬃcient measure264,633since if the threshold\\nfor “positive” is, say 0 .75, a prediction of 0 .99 rates the same as one of\\n0.751.\\n3. It gets away from the purpose of ﬁtting a logistic model. A logistic model\\nis a model for the probability of an event, not a model for the occurrence\\nof the event. For example, suppose that the event we are predicting is\\nthe probability of being struck by lightning. Without having any data,\\nwe would predict that you won’t get struck by lightning. However, you\\nmight develop an interesting model that discovers real risk factors that\\nyield probabilities of being struck that range from 0 .000000001 to 0 .001.\\n4. If you make a classiﬁcation rule from a probability model, you are being\\npresumptuous. Suppose that a model is developed to assist physicians\\nin diagnosing a disease. Physicians sometimes profess to desiring a binary\\ndecisionmodel,butifgivenaprobabilitytheywillrightfullyapplydiﬀerent\\nthresholds for treating diﬀerent patients or for ordering other diagnostic\\ntests. Even though the age of the patient may be a strong predictor of\\nthe probability of disease, the physician will often use a lower threshold\\nof disease likelihood for treating a young patient. This usage is above and\\nbeyond how age aﬀects the likelihood.\\n5. If a disease were present in only 0.02 of the population, one could be 0.98\\naccurate in diagnosing the disease by ruling that everyone is disease–free,\\ni.e., by avoiding predictors. The proportion classiﬁed correctly fails to take\\nthe diﬃculty of the task into account.\\n6. van Houwelingen and le Cessie633demonstrated a peculiar property that\\noccurs when you try to obtain an honest estimate of classiﬁcation error\\nusing cross-validation.The cross-validatederrorrate correctsthe apparent\\nerrorrate only if the predicted probabilityis exactly 1 /2oris1/2±1/(2n).\\nThecross-validationestimateofoptimism is“zerofor nevenandnegligibly\\nsmall for nodd.”Better measures of error rate such as the Brier score and\\nlogarithmic scoring rule do not have this problem. They also have the\\nnice property of being maximized when the predicted probabilities are the\\npopulation probabilities.416. 16', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='92acef47-c489-4859-b709-24b20e45cfc7', embedding=None, metadata={'page_label': '259', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.9 Validating the Fitted Model 259\\n10.9 Validating the Fitted Model\\nThe major cause of unreliable models is overﬁtting the data. The methods\\nd e s c r i b e di nS e c t i o n 5.3can be used to assess the accuracy of models fairly.\\nIf a sample has been held out and never used to study associations with the\\nresponse, indexes of predictive accuracy can now be estimated using that\\nsample. More eﬃcient is cross-validation, and bootstrapping is the most ef-\\nﬁcient validation procedure. As discussed earlier, bootstrapping does not re-\\nquire holding out any data, since all aspects of model development (stepwise\\nvariable selection, tests of linearity, estimation of coeﬃcients, etc.) are re-\\nvalidated on samples taken with replacement from the whole sample.\\nCox130proposed and Harrell and Lee267and Miller et al.457further de-\\nveloped the idea of ﬁtting a new binary logistic model to a new sample to\\nestimate the relationship between the predicted probability and the observed\\noutcome in that sample. This ﬁt provides a simple calibration equation that\\ncan be used to quantify unreliability (lack of calibration) and to calibrate\\nthe predictions for future use. This logistic calibration also leads to indexes\\nof unreliability ( U), discrimination ( D), and overall quality ( Q=D−U)\\nwhich are derived from likelihood ratio tests267.Qis a logarithmic scoring\\nrule, which can be compared with Brier’s index (Equation 10.35). See[633]\\nfor many more ideas.\\nWith bootstrapping we do not have a separate validation sample for as-\\nsessing calibration, but we can estimate the overoptimism in assuming that\\nthe ﬁnal model needs no calibration, that is, it has overall intercept=0 and\\nslope=1. As discussed in Section 5.3, reﬁtting the model\\nPc=P r o b{Y=1|Xˆβ}=[1+exp −(γ0+γ1Xˆβ)]−1(10.37)\\n(wherePcdenotes the calibrated probability and the original predicted prob-\\nability is ˆP=[ 1+e x p ( −Xˆβ)]−1) in the original sample will always result in\\nγ=(γ0,γ1)=( 0,1), since a logistic model will always“ﬁt”the training sam-\\nple when assessed overall. We thus estimate γby using Efron’s172method to\\nestimate the overoptimism in (0 ,1) to obtain bias-corrected estimates of the\\ntrue calibration. Simulations have shown this method produces an eﬃcient\\nestimate of γ.259\\nMorestringentcalibrationcheckscanbemadebyrunningseparatecalibra-\\ntions for diﬀerent covariate levels. Smooth nonparametric curves described in\\nSection10.11are more ﬂexible than the linear-logit calibration method just\\ndescribed.\\nA goodset ofindexestoestimateforsummarizingamodelvalidationisthe\\ncorDxyindexes and measures of calibration. In addition, the overoptimism\\nin the indexes may be reported to quantify the amount of overﬁtting present.\\nT h ee s t i m a t eo f γcan be used to draw a calibration curve by plotting ˆP\\non thex-axis and ˆPc=[ 1+e x p −(γ0+γ1L)]−1on they-axis, where L=\\nlogit(ˆP).130,267An easily interpreted index of unreliability, Emax, follows\\nimmediately from this calibration model:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8d1d3ee6-98c2-46e6-a8da-c76e5ea83b60', embedding=None, metadata={'page_label': '260', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"260 10 Binary Logistic Regression\\nEmax(a,b)= m a x\\na≤ˆP≤b|ˆP−ˆPc|, (10.38)\\nthe maximum error in predicted probabilities over the range a≤ˆP≤b.I n\\nsome cases, we would compute the maximum absolute diﬀerence in predicted\\nand calibrated probabilities over the entire interval, that is, use Emax(0,1).\\nThe null hypothesis H0:Emax(0,1) = 0 can easily be tested by testing\\nH0:γ0=0,γ1= 1 as above. Since Emaxdoes not weight the discrepancies\\nby the actual distribution of predictions, it may be preferable to compute the\\naverage absolute discrepancy over the actual distribution of predictions (or\\nto use a mean squared error, incorporating the same calibration function).\\nIfstepwisevariableselectionisbeingdone,amatrixdepictingwhichfactors\\nare selected at each bootstrap sample will shed light on how arbitrary is the\\nselection of“signiﬁcant”factors. See Section 5.3for reasons to compare full\\nand stepwise model ﬁts.\\nAsanexampleusingbootstrappingto validatethe calibrationanddiscrim-\\ninationofamodel,considerthedatainSection 10.1.3.Using150sampleswith\\nreplacement, we ﬁrst validate the additive model with age and sex forced into\\neverymodel.Theoptimism-correcteddiscriminati onandcalibrationstatistics\\nproduced by validate (see Section 10.11) are in the table below.\\nd←sex.age.response\\ndd←datadist(d); options(datadist= 'dd')\\nf←lrm(response ∼sex + age, data=d, x= TRUE, y= TRUE)\\nset.seed(3) # for reproducibility\\nv1←validate(f, B =150)\\nlatex(v1,\\ncaption= 'Bootstrap Validation, 2 Predictors Without\\nStepdown ', digits=2, size= 'Ssize ', file= '')\\nBootstrap Validation, 2 Predictors Without Stepdown\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.70 0.70 0.67 0 .04 0 .66 150\\nR20.45 0.48 0.43 0 .05 0 .40 150\\nIntercept 0 .00 0.00 0.01−0.01 0 .01 150\\nSlope 1 .00 1.00 0.91 0 .09 0 .91 150\\nEmax 0.00 0.00 0.02 0 .02 0 .02 150\\nD 0.39 0.44 0.36 0 .07 0 .32 150\\nU −0.05−0.05 0.04−0.09 0 .04 150\\nQ 0.44 0.49 0.32 0 .16 0 .28 150\\nB 0.16 0.15 0.18−0.03 0 .19 150\\ng 2.10 2.49 1.97 0 .52 1 .58 150\\ngp 0.35 0.35 0.34 0 .01 0 .34 150\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='af2d2b33-c07e-4e13-b3c8-2e7c3fceef89', embedding=None, metadata={'page_label': '261', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.9 Validating the Fitted Model 261\\nNow we incorporate variable selection. The variables selected in the ﬁrst\\n10 bootstrap replications are shown below. The apparent Somers’ Dxyis 0.7,\\nand the bias-corrected Dxyis 0.66. The slope shrinkage factor is 0.91. The\\nmaximum absolute error in predicted probability is estimated to be 0.02.\\nWe next allow for step-down variable selection at each resample. For illus-\\ntrationpurposesonly,weuseasuboptimalstoppingrulebasedonsigniﬁcance\\nofindividual variables at the α=0.10 level. Of the 150 repetitions, both age\\nand sex were selected in 137, and neither variable was selected in 3 samples.\\nThe validation statistics are in the table below.\\nv2←validate(f, B =150, bw=TRUE,\\nrule= 'p', sls=.1, type= 'individual ')\\nlatex(v2,\\ncaption= 'Bootstrap Validation, 2 Predictors with Stepdown ',\\ndigits=2, B=15, file= '', size= 'Ssize ')\\nBootstrap Validation, 2 Predictors with Stepdown\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.70 0.70 0.64 0 .07 0 .63 150\\nR20.45 0.49 0.41 0 .09 0 .37 150\\nIntercept 0 .00 0.00−0.04 0 .04−0.04 150\\nSlope 1 .00 1.00 0.84 0 .16 0 .84 150\\nEmax 0.00 0.00 0.05 0 .05 0 .05 150\\nD 0.39 0.45 0.34 0 .11 0 .28 150\\nU −0.05−0.05 0.06−0.11 0 .06 150\\nQ 0.44 0.50 0.28 0 .22 0 .22 150\\nB 0.16 0.14 0.18−0.04 0 .20 150\\ng 2.10 2.60 1.88 0 .72 1 .38 150\\ngp 0.35 0.35 0.33 0 .02 0 .33 150\\nFactors Retained in Backwards Elimination\\nFirst 15 Resamples\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a34c9e19-ee35-4b2b-bca8-4fb574a48351', embedding=None, metadata={'page_label': '262', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"262 10 Binary Logistic Regression\\nsex age\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n••\\n•\\nFrequencies of Numbers of Factors Retained\\n01 2\\n3 10 137\\nThe apparentSomers’ Dxyis 0.7 for the originalstepwise model (whichac-\\ntually retained both age and sex), and the bias-corrected Dxyis 0.63, slightly\\nworse than the more correct model which forced in both variables. The cal-\\nibration was also slightly worse as reﬂected in the slope correction factor\\nestimate of 0.84 versus 0.91.\\nNext, ﬁve additional candidate variables are considered. These variables\\nare random uniform variables, x1,...,x5o nt h e[ 0 ,1] interval, and have no\\nassociation with the response.\\nset.seed (133)\\nn←nrow(d)\\nx1←runif(n)\\nx2←runif(n)\\nx3←runif(n)\\nx4←runif(n)\\nx5←runif(n)\\nf←lrm(response ∼a g e+s e x+x 1+x 2+x 3+x 4+x 5 ,\\ndata=d, x= TRUE, y= TRUE)\\nv3←validate(f, B =150, bw=TRUE,\\nrule= 'p', sls=.1, type= 'individual ')\\nk←attr(v3, 'kept ')\\n# Compute number of x1-x5 selected\\nnx←apply(k[ ,3:7], 1, sum)\\n# Get selections of age and sex\\nv←colnames(k)\\nas←apply(k[ ,1:2], 1,\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a93620e-e903-4b1b-b864-74f1e1d33ba1', embedding=None, metadata={'page_label': '263', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.9 Validating the Fitted Model 263\\nfunction(x) paste(v[1:2][x], collapse= ','))\\ntable(paste(as, '',n x , 'Xs'))\\n0 Xs 1 Xs age 2 Xs age, sex 0 Xs\\n5031 3 4\\nage, sex 1 Xs age, sex 2 Xs age, sex 3 Xs age, sex 4 Xs\\n17 11 7 1\\nsex 0 Xs sex 1 Xs\\n12 3\\nlatex(v3,\\ncaption= 'Bootstrap Validation with 5 Noise Variables and\\nStepdown ', digits=2, B=15, size= 'Ssize ', file= '')\\nBootstrap Validation with 5 Noise Variables and Stepdown\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.70 0.47 0.38 0 .09 0 .60 139\\nR20.45 0.34 0.23 0 .11 0 .34 139\\nIntercept 0 .00 0.00 0.03−0.03 0 .03 139\\nSlope 1 .00 1.00 0.78 0 .22 0 .78 139\\nEmax 0.00 0.00 0.06 0 .06 0 .06 139\\nD 0.39 0.31 0.18 0 .13 0 .26 139\\nU −0.05−0.05 0.07−0.12 0 .07 139\\nQ 0.44 0.36 0.11 0 .25 0 .19 139\\nB 0.16 0.17 0.22−0.04 0 .20 139\\ng 2.10 1.81 1.06 0 .75 1 .36 139\\ngp 0.35 0.23 0.19 0 .04 0 .31 139\\nFactors Retained in Backwards Elimination\\nFirst 15 Resamples\\na g es e xx 1x 2x 3x 4x 5\\n• • ••••\\n••• •\\n••\\n•• • •\\n•••\\n••\\n••\\n•• •\\n•• •\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='51bb1e46-74ee-42a7-b80e-1b5dc7b14ae4', embedding=None, metadata={'page_label': '264', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='264 10 Binary Logistic Regression\\nFrequencies of Numbers of Factors Retained\\n01234 5 6\\n50 15 37 18 11 7 1\\nUsing step-down variable selection with the same stopping rule as before,\\nthe“ﬁnal”model on the original sample correctly deleted x1,...,x5. Of the\\n150bootstraprepetitions,11samplesy ieldedasingulari tyornon-convergence\\neither in the full-model ﬁt or after step-down variable selection. Of the 139\\nsuccessful repetitions, the frequencies of the number of factors selected, as\\nwell as the frequency of variable combinations selected, are shown above.\\nValidation statistics are also shown above.\\nFigure10.17depicts the calibration(reliability) curvesforthe three strate-\\ngies using the corrected intercept and slope estimates in the above tables as\\nγ0andγ1, and the logistic calibration model Pc=[ 1+e x p −(γ0+γ1L)]−1,\\nwherePcis the“actual”or calibrated probability, Lis logit(ˆP), andˆPis the\\npredicted probability. The shape of the calibration curves (driven by slopes\\n<1) is typical ofoverﬁtting—lowpredictedprobabilitiesaretoo low and high\\npredicted probabilities are too high. Predictions near the overall prevalence\\nof the outcome tend to be calibrated even when overﬁtting is present.\\ng←function(v) v[c( \\'Intercept \\',\\'Slope \\'),\\'index.corrected \\']\\nk←rbind(g(v1), g(v2), g(v3))\\nco←c(2,5,4,1)\\nplot(0, 0, ylim=c(0,1), xlim=c(0,1),\\nxlab=\"Predicted Probability\",\\nylab=\"Estimated Actual Probability\", type=\"n\")\\nlegend(.45,.35 ,c(\"age, sex\", \"age, sex stepdown\",\\n\"age, sex, x1-x5\", \"ideal\"),\\nlty=1, col=co, cex=.8, bty=\"n\")\\nprobs ←seq(0, 1, length=200); L ←qlogis(probs)\\nfor(i in 1:3) {\\nP←plogis(k[i, \\'Intercept \\']+k [ i , \\'Slope \\']*L )\\nlines(probs, P, col=co[i], lwd=1)\\n}\\nabline(a=0, b=1, col=co[4], lwd=1) # Figure 10.17\\n“Honest” calibration curves may also be estimated using nonparametric\\nsmoothers in conjunction with bootstrapping and cross-validation (see\\nSection10.11).\\n10.10 Describing the Fitted Model\\nOnce the propervariableshavebeen modeledand allmodel assumptionshave\\nbeen met, the analyst needs to present and interpret the ﬁtted model. There\\nare at least three ways to proceed. The coeﬃcients in the model may be\\ninterpreted. For each variable, the change in log odds for a sensible change in\\nthe variable value (e.g., interquartilerange)may be computed. Also, the odds', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='970753b8-6396-4b91-8272-32881df8d601', embedding=None, metadata={'page_label': '265', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.10 Describing the Fitted Model 265\\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nPredicted ProbabilityEstimated Actual Probabilityage, sex\\nage, sex stepdown\\nage, sex, x1−x5\\nideal\\nFig. 10.17 Estimated logistic calibration (reliability) curves obtained by bootstrap-\\nping three modeling strategies.\\nTable 10.12 Eﬀects Response : sigdz\\nLow High ΔEﬀect S.E. Lower 0.95 Upper 0.95\\nage 46 59 13 0.90629 0.18381 0.546030 1.26650\\nOdds Ratio 46 59 13 2.47510 1.726400 3.54860\\ncholesterol 196 259 63 0.75479 0.13642 0.487410 1.02220\\nOdds Ratio 196 259 63 2.12720 1.628100 2.77920\\nsex — female:male 1 2 -2.42970 0.14839 -2.720600 -2.13890\\nOdds Ratio 1 2 0.08806 0.065837 0.11778\\nratio or factor by which the odds increasesfor a certain change in a predictor,\\nholding all other predictors constant, may be displayed. Table 10.12contains\\nsuch summary statistics for the linear age ×cholesterol interaction surface\\nﬁt described in Section 10.5.\\ns←summary(f.linia) # Table 10.12\\nlatex(s, file= '', size= 'Ssize ',\\nlabel= 'tab:lrm-cholxage-confbar ')\\nplot(s) # Figure 10.18\\nThe outer quartiles of age are 46 and 59 years, so the“half-sample”odds\\nratio for ageis 2 .47,with 0 .95 conﬁdence interval [1 .63,3.74]when sex is male\\nand cholesterol is set to its median. The eﬀect of increasing cholesterol from\\n196 (its lower quartile) to 259 (its upper quartile) is to increase the log odds\\nby 0.79or to increasethe odds by a factor of 2 .21.Since thereareinteractions\\nallowedbetweenageandsex andbetweenageandcholesterol,eachodds ratio\\nin the above table depends on the setting of at least one other factor. The\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ce72d57-108f-49a4-a53f-d1a171383752', embedding=None, metadata={'page_label': '266', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='266 10 Binary Logistic Regression\\nOdds Ratio\\n0.10 0.75 1.50 2.50 3.50\\nage − 59:46\\ncholesterol − 259:196\\nsex − female:male\\nAdjusted to:age=52 sex=male cholesterol=224.5 \\nFig. 10.18 Odds ratios and conﬁdence bars, using quartiles of age and cholesterol\\nfor assessing their eﬀects on the odds of coronary disease\\nresults are shown graphically in Figure 10.18. The shaded conﬁdence bars\\nshow various levels of conﬁdence and do not pin the analyst down to, say, the\\n0.95 level.\\nFor those used to thinking in terms of odds or log odds, the preceding\\ndescription may be suﬃcient. Many prefer instead to interpret the model in\\nterms of predicted probabilities instead of odds. If the model contains only\\na single predictor (even if several spline terms are required to represent that\\npredictor), one may simply plot the pr edictor against the predicted response.\\nSuch a plot is shown in Figure 10.19which depicts the ﬁtted relationship\\nbetween age of diagnosis and the probability of acute bacterial meningitis\\n(ABM) as opposed to acute viral meningitis (AVM), based on an analysis of\\n422 cases from Duke University Medical Center.580The data may be found\\non the web site. A linear spline function with knots at 1, 2, and 22 years was\\nused to model this relationship.\\nWhen the model containsmorethan one predictor,one maygraphthe pre-\\ndictoragainstlogodds,andbarringinteractions,theshapeofthisrelationship\\nwill be independent of the level of the other predictors. When displaying the\\nmodel on what is usually a more interpretable scale, the probability scale, a\\ndiﬃculty arises in that unlike log odds the relationship between one predictor\\nand the probability of response depends on the levels of all other factors. For\\nexample, in the model\\nProb{Y=1|X}={1+exp[−(β0+β1X1+β2X2)]}−1(10.39)\\nthere is no way to factor out X1when examining the relationship between\\nX2and the probability of a response. For the two-predictor case one can plot\\nX2versus predicted probability for each level of X1. When it is uncertain\\nwhether to include an interaction in this model, consider presenting graphs\\nfor two models (with and without interaction terms included) as was done\\nin[658].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2286940d-19e2-4860-96ca-9bb7fbd785db', embedding=None, metadata={'page_label': '267', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.10 Describing the Fitted Model 267\\n0.000.250.500.751.00\\n02 0 4 0 6 0\\nAge in YearsProbabiity ABM vs AVM\\nFig. 10.19 Linear spline ﬁt for probability of bacterial versus viral meningitis as a\\nfunction of age at onset580. Points are simple proportions by age quantile groups.\\nWhen three factors are present, one could draw a separate graph for each\\nlevel ofX3, a separate curve on each graph for each level of X1,a n dv a r y X2\\non thex-axis.Instead ofthis, or if morethan three factors arepresent,a good\\nway to display the results may be to plot“adjusted probability estimates”as\\na function of one predictor, adjusting all other factors to constants such as\\nthe mean. For example, one could display a graph relating serum cholesterol\\nto probability of myocardial infarction or death, holding age constant at 55,\\nsex at 1 (male), and systolic blood pressure at 120 mmHg.\\nThe ﬁnal method fordisplayingthe relationshipbetween severalpredictors\\nand probability of response is to construct a nomogram.40,254A nomogram\\nnot only sheds light on how the eﬀect of one predictor on the probability of\\nresponse depends on the levels of other factors, but it allows one to quickly\\nestimate the probability of response for individual subjects. The nomogram\\nin Figure 10.20allows one to predict the probability of acute bacterialmenin-\\ngitis (giventhe patienthaseither viralorbacterialmeningitis)usingthe same\\nsample as in Figure 10.19. Here there are four continuous predictor values,\\nnone of which are linearly related to log odds of bacterial meningitis: age\\nat admission (expressed as a linear spline function), month of admission (ex-\\npressedas |month−8|),cerebrospinalﬂuid glucose/bloodglucoseratio(linear\\neﬀect truncated at .6; that is, the eﬀect is the glucose ratio if it is ≤.6, and .6\\nifitexceeded.6),andthecube rootofthetotalnumberofpolymorphonuclear\\nleukocytes in the cerebrospinal ﬂuid. 17\\nThe model associatedwith Figure 10.14is depicted in what couldbe called\\na “precision nomogram”in Figure 10.21. Discrete cholesterol levels were re-\\nquired because of the interaction between two continuous variables.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='45132b0f-7670-47b8-98ee-7e30d9edcc16', embedding=None, metadata={'page_label': '268', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='268 10 Binary Logistic Regression\\nAge Month Probability\\nABM vs AVMGlucose\\nRatioTotal\\nPMN\\nReading\\nLine\\nAAReading\\nLine\\nBB\\n22253035404550556065707580\\n22y20151052y18m12m\\n0m6m12m\\n1 Aug1 Jul1 Jun1 May1 Apr1 Mar1 Feb\\n1 Aug1 Sep1 Oct1 Nov1 Dec1 Jan1 Feb\\n0.010.050.100.200.300.400.500.600.700.800.900.950.99\\n≥.60.55.50.45.40.35.30.25.20.15.10.05\\n01050100200300400500100015002000250030004000500060007000800090001000011000\\nFig. 10.20 Nomogram for estimating probability of bacterial (ABM) versus viral\\n(AVM) meningitis. Step 1, place ruler on reading lines for patient’s age and month\\nof presentation and mark intersection with line A; step 2, place ruler on values for\\nglucose ratio and total polymorphonuclear leukocyte (PMN) count in cerebrospinal\\nﬂuid and mark intersection with line B; step 3, use ruler to join marks on lines A and\\nB, then read oﬀ the probability of ABM versus AVM.580\\n# Draw a nomogram thatshows examples of confidence intervals\\nnom←nomogram( f.linia, cholesterol=seq (150, 400, by=50),\\ninteract= list(age=seq(30, 70, by=10)),\\nlp.at=seq(-2, 3.5, by=.5),\\nconf.int= TRUE,conf.lp=\"all\",\\nfun=function(x)1/(1+exp(-x)), # or plogis\\nfunlabel=\"Probability of CAD\",\\nfun.at=c(seq(.1, .9, by=.1), .95, .99)\\n)# Figure 10.21\\nplot(nom, col.grid = gray(c(0.8, 0.95)),\\nvarname.label= FALSE, ia.space=1, xfrac=.46, lmgp=.2)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='15c4a869-10bb-4daa-88fc-4e844f1f0974', embedding=None, metadata={'page_label': '269', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.11RFunctions 269\\n10.11RFunctions\\nThe general Rstatistical modeling functions96d e s c r i b e di nS e c t i o n 6.2work\\nwith the author’s lrmfunction for ﬁtting binary and ordinal logistic regres-\\nsion models. lrmhas several options for doing penalized maximum likelihood\\nestimation, with special treatment of categorical predictors so as to shrink\\nall estimates (including the reference cell) to the mean. The following exam- 18\\nple ﬁts a logistic model containing predictors age,blood.pressure ,a n dsex,\\nwithageﬁtted with a smooth ﬁve-knot restricted cubic spline function and a\\ndiﬀerent shape of the age relationship for males and females.\\nfit←lrm(death ∼blood.pressure + sex * rcs(age ,5))\\nanova(fit)\\nplot(Predict(fit, age, sex))\\nThepentrace function makes it easy to check the eﬀects of a sequence of\\npenalties. The following code ﬁts an unpenalized model and plots the AIC\\nand Schwarz BIC for a variety of penalties so that approximately the best\\ncross-validating model can be chosen (and so we can learn how the penalty\\nrelates to the eﬀective degrees of freedom). Here we elect to only penalize the\\nnonlinear or non-additive parts of the model.\\nf←lrm(death ∼rcs(age,5)*treatment + lsp(sbp,c (120,140)),\\nx=TRUE, y= TRUE)\\nplot(pentrace(f,\\npenalty= list(nonlinear=seq(.25,10,by=.25))) )\\nSee Sections 9.8.1and9.10for more information. 19\\nTheresiduals function for lrmand the which.influence function can be\\nused tocheckpredictortransformationsaswellasto analyzeoverlyinﬂuential\\nobservationsinbinarylogisticregression.SeeFigure 10.16foroneapplication.\\nTheresiduals function will also perform the unweighted sum of squares test\\nfor global goodness of ﬁt described in Section 10.5.\\nThevalidate function when used on an object created by lrmdoes resam-\\npling validation of a logistic regression model, with or without backward\\nstep-down variable deletion. It provides bias-corrected Somers’ Dxyrank\\ncorrelation, R2\\nNindex, the intercept and slope of an overall logistic calibra-\\ntion equation, the maximum absolute diﬀerence in predicted and calibrated\\nprobabilities Emax, the discrimination index D[(model L.R. χ2−1)/n], the\\nunreliability index U= (diﬀerence in −2 log likelihood between uncalibrated\\nXβandXβwith overall intercept and slope calibrated to test sample) /n,\\nand the overall quality index Q=D−U.267The “corrected” slope can\\nbe thought of as a shrinkage factor that takes overﬁtting into account. See\\npredab.resample in Section 6.2for the list of resampling methods.\\nThecalibrate function produces bootstrapped or cross-validated calibra-\\ntioncurvesforlogisticandlinearmodels.The“apparent”calibrationaccuracy\\nis estimated using a nonparametric smoother relating predicted probabilities', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8a53d438-fa51-4bb7-bd22-2202ad652d97', embedding=None, metadata={'page_label': '270', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='270 10 Binary Logistic Regression\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 0 0\\ncholesterol (age=30\\nsex=male)150 250 300 350 400\\ncholesterol (age=40\\nsex=male)150 250 300 350 400\\ncholesterol (age=50\\nsex=male)200250 300 350 400\\ncholesterol (age=60\\nsex=male)200250 350\\ncholesterol (age=70\\nsex=male)200250 400\\ncholesterol (age=30\\nsex=female)150 250 300 350 400\\ncholesterol (age=40\\nsex=female)150 250 300 350 400\\ncholesterol (age=50\\nsex=female)200250 300 350 400\\ncholesterol (age=60\\nsex=female)200250 350\\ncholesterol (age=70\\nsex=female)200250 400\\nTotal Points\\n0 1 02 03 04 05 06 07 08 09 0 1 0 0\\nLinear Predictor\\n−2 −1.5 −1 −0.5 0 0.5 1 1.5 2 2.5 3 3.5\\nProbability of CAD\\n0.2 0.3 0.40.50.6 0.7 0.8 0.9 0.95\\nFig. 10.21 Nomogram relating age, sex, and cholesterol to the log odds and to\\nthe probability of signiﬁcant coronary artery disease. Select one axis correspo nding\\nto sex and to age ∈{30,40,50,60,70}. There is linear interaction between age and\\nsex and between age and cholesterol. 0 .70 and 0 .90 conﬁdence intervals are shown\\n(0.90 in gray). Note that for the “Linear Predictor” scale there are various lengths\\nof conﬁdence intervals near the same value of Xˆβ, demonstrating that the standard\\nerror of Xˆβdepends on the individual Xvalues. Also note that conﬁdence intervals\\ncorresponding to smaller patient groups (e.g., females) are wider.\\nto observed binary outcomes. The nonparametric estimate is evaluated at a\\nsequence of predicted probability levels. Then the distances from the 45◦line\\nare compared with the diﬀerences when the current model is evaluated back\\non the whole sample (or omitted sample for cross-validation). The diﬀerences\\nin the diﬀerences are estimates of overoptimism. After averaging over many\\nreplications, the predicted-value-speciﬁc diﬀerences are then subtracted from', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85074413-02b8-4fa5-9baf-f4724d9c7075', embedding=None, metadata={'page_label': '271', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"10.12 Further Reading 271\\nthe apparent diﬀerences and an adjusted calibration curve is obtained. Un-\\nlikevalidate,calibrate does not assume a linear logistic calibration. For an\\nexample, see the end of Chapter 11.calibrate will print the mean absolute\\ncalibrationerror,the 0.9 quantileof the absoluteerror,and the mean squared\\nerror, all over the observed distribution of predicted values.\\nTheval.prob function is used to compute measures of discrimination and\\ncalibration of predicted probabilities for a separate sample from the one\\nused to derive the probability estimates. Thus val.prob is used in exter-\\nnal validation and data-splitting. The function computes similar indexes as\\nvalidate plus the Brier score and a statistic for testing for unreliability or\\nH0:γ0=0,γ1=1 .\\nIn the following example, a logistic model is ﬁtted on 100 observations\\nsimulated from the actual model given by\\nProb{Y=1|X1,X2,X3}=[ 1+e x p [ −(−1+2X1)]]−1,(10.40)\\nwhereX1is a randomuniform[0 ,1]variable.Hence X2andX3areirrelevant.\\nAfter ﬁtting a linear additive model in X1,X2,andX3, the coeﬃcients are\\nused to predict Prob {Y=1}on a separate sample of 100 observations.\\nset.seed (13)\\nn←200\\nx1←runif(n)\\nx2←runif(n)\\nx3←runif(n)\\nlogit ←2*(x1-.5)\\nP←1/(1+exp(-logit))\\ny←ifelse(runif(n) ≤P, 1, 0)\\nd←data.frame(x1, x2, x3, y)\\nf←lrm(y∼x1 + x2 + x3, subset =1:100)\\nphat←predict(f, d [101:200,], type= 'fitted ')\\n# Figure 10.22\\nv←val.prob( phat, y [101:200], m=20, cex=.5)\\nThe output is shown in Figure 10.22.\\nTheRbuilt-infunction glm,averygeneralmodelingfunction,canﬁtbinary\\nlogistic models. The response variable mustbe coded 0/1 for glmto work. Glm\\nis a slight modiﬁcation of the built-in glmfunction in the rmspackage that\\nallows ﬁts to use rmsmethods. This facilitates Poissonand severalother types\\nof regression analysis.\\n10.12 Further Reading\\n1See [590] for modeling strategies speciﬁc to binary logistic regression.\\n2See [632] for a nice review of logistic modeling. Agresti6is an excellent source\\nfor categorical Yin general.\\n3Not only does discriminant analysis assume the same regression model as lo-\\ngistic regression, but it also assumes that the predictors are each normally\\ndistributed and that jointly the predictors have a multivariate normal distr i-\\nbution. These assumptions are unlikely to be met in practice, especially when\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a0fcb663-78ed-4df0-949f-479a769b26ae', embedding=None, metadata={'page_label': '272', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='272 10 Binary Logistic Regression\\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nPredicted ProbabilityActual Probability Ideal\\nLogistic calibration\\nNonparametric\\nGrouped observationsDxy\\nC (ROC)  \\nR2\\nD\\nU\\nQ\\nBrier    \\nIntercept\\nSlope\\nEmax\\nS:z\\nS:p 0.339\\n 0.670\\n 0.010\\n−0.003\\n−0.020\\n 0.017\\n 0.235\\n−0.371\\n 0.544\\n 0.211\\n 2.351\\n 0.019\\nFig. 10.22 Validation of a logistic model in a test sample of size n= 100. The\\ncalibrated risk distribution (histogram of logistic-calibrated probabilities) is shown.\\none of the predictors is a discrete variable such as sex group. When discrimi-\\nnant analysis assumptions are violated, logistic regression yields more accurate\\nestimates.251,514Even when discriminant analysis is optimal (i.e., when all\\nits assumptions are satisﬁed) logistic regression is virtually as accurate as the\\ndiscriminant model.264\\n4See [573] for a review of measures of eﬀect for binary outcomes.\\n5Cepedaet al.95found that propensity adjustment is better than covariate ad-\\njustment with logistic models when the number of events per variable is less\\nthan 8.\\n6Pregibon512developed a modiﬁcation of the log likelihood function that when\\nmaximized results in a ﬁt that is resistant to overly inﬂuential and outlying\\nobservations.\\n7See Hosmer and Lemeshow306for methods of testing for a diﬀerence in the\\nobserved event proportion and the predicted event probability (average of pre-\\ndicted probabilities) for a group of heterogeneous subjects.\\n8See Hosmer and Lemeshow,305Kay and Little,341and Collett [ 115, Chap.5].\\nLandwehr et al.373proposed the partial residual (see also Fowlkes199).\\n9See Berk and Booth51for other partial-like residuals.\\n10See [341] for an example comparing a smoothing method with a parametric\\nlogistic model ﬁt.\\n11See Collett [ 115, Chap.5] and Pregibon512for more information about inﬂuence\\nstatistics. Pregibon’s resistant estimator of βhandles overly inﬂuential groups\\nof observations and allows one to estimate the weight that an observation con-\\ntributed to the ﬁt after making the ﬁt robust. Observations receiving low weight\\nare partially ignored but are not deleted.\\n12Buyse86showed that in the case of a single categorical predictor, the ordi-\\nnaryR2has a ready interpretation in terms of variance explained for binary\\nresponses. Menard454studied various indexes for binary logistic regression. He\\ncriticized R2\\nNfor being too dependent on the proportion of observations with\\nY=1 .H ue ta l .309further studied the properties of variance-based R2mea-\\nsures for binary responses. Tjur613has a nice discussiondiscrimination graphics', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c78475a9-0ebf-47b0-b4e9-79fad8eb8ddc', embedding=None, metadata={'page_label': '273', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='10.13 Problems 273\\nand sum of squares–based R2measures for binary logistic regression, as well\\nas a good discussion of“separation”and inﬁnite regression coeﬃcients. Sums of\\nsquares are approximated various ways.\\n13Very little work has been done on developing adjusted R2measures in logistic\\nregression and other non-linear model setups. Liao and McGee406developed\\none adjusted R2measure for binary logistic regression, but it uses simulation to\\nadjust for the bias of overﬁtting. One might as well use the bootstrap to adjust\\nany of the indexes discussed in this section.\\n14[123,633] have more pertinent discussion of probability accuracy scores.\\n15Copas121demonstrated how ROC areas can be misleading when applied to\\ndiﬀerent responses having greatly diﬀerent prevalences. He proposed another\\napproach, the logit rank plot. Newsom473is an excellent reference on Dxy.\\nNewson474developed several generalizations to Dxyincluding a stratiﬁed ver-\\nsion, and discussed the jackknife variance estimator for them. ROC areas are\\nnot very useful for comparing two models118,493(but see490).\\n16Gneiting and Raftery219have an excellent review of proper scoring rules.\\nHand253contains much information about assessing classiﬁcation accuracy.\\nMittlb¨ock and Schemper461have an excellent review of indexes of explained\\nvariation for binary logistic models. See also Korn and Simon366and Zheng\\nand Agresti.684.\\n17Pryor et al.515presented nomograms for a 10-variable logistic model. One of the\\nvariables was sex, which interacted with some of the other variables. Evaluation\\nof predicted probabilities was simpliﬁed by the construction of separate nomo-\\ngrams for females and males. Seven terms for discrete predictors were collapsed\\ninto one weighted point score axis in the nomograms, and age by risk factor\\ninteractions were captured by having four age scales.\\n18Moons et al.462presents a case study in penalized binary logistic regression\\nmodeling.\\n19Thercspline.plot function in the HmiscRpackage does not allow for in-\\nteractions as does lrm, but it can provide detailed output for checking spline\\nﬁts. This function plots the estimated spline regression and conﬁdence limits,\\nplacing summary statistics on the graph. If there are no adjustment variables,\\nrcspline.plot can also plot two alternative estimates of the regression func-\\ntion: proportions or logit proportions on grouped data, and a nonparametric\\nestimate. The nonparametric regression estimate is based on smoothing the b i-\\nnary responses and taking the logit transformation of the smoothed estimat es, if\\ndesired. The smoothing uses the“super smoother”of Friedman207implemented\\nin theRfunction supsmu.\\n10.13 Problems\\n1. Consider the age–sex–response example in Section 10.1.3. This dataset is\\navailable from the text’s web site in the Datasets area.\\na. Duplicate the analyses done in Section 10.1.3.\\nb. For the model containing both age and sex, test H0: logit response is\\nlinear in age versus Ha: logit response is quadratic in age. Use the best\\ntest statistic.\\nc. Using a Wald test, test H0: no age ×sex interaction. Interpret all\\nparameters in the model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f16bd19b-5e2c-437e-a975-f5fb3a543ec3', embedding=None, metadata={'page_label': '274', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='274 10 Binary Logistic Regression\\nd. Plot the estimated logit response as a function of age and sex, with and\\nwithout ﬁtting an interaction term.\\ne. Perform a likelihood ratio test of H0: the model containing only age\\nand sex is adequate versus Ha: model is inadequate. Here,“inadequate”\\nmay mean nonlinearity (quadratic) in age or presence of an interaction.\\nf. Assumingnointeractionispresent,test H0:modelislinearinageversus\\nHa: model is nonlinear in age. Allow “nonlinear” to be more general\\nthan quadratic. (Hint: use a restricted cubic spline function with knots\\nat age=39, 45, 55, 64 years.)\\ng. Plot age against the estimated spline transformation of age (the trans-\\nformation that would make age ﬁt linearly). You can set the sex and\\nintercept terms to anything you choose. Also plot Prob {response = 1 |\\nage, sex}from this ﬁtted restricted cubic spline logistic model.\\n2. Consider a binary logistic regression model using the following predictors:\\nage(years),sex,race(white, African-American,Hispanic,Oriental,other),\\nblood pressure (mmHg). The ﬁtted model is given by\\nlogit Prob[ Y=1|X]=Xˆβ=−1.36+.03(race = African-American)\\n−.04(race = hispanic)+ .05(race = oriental) −.06(race = other)\\n+.07|blood pressure −110|+.3(sex = male) −.1age+.002age2+\\n(sex = male)[ .05age−.003age2].\\na. Compute the predicted logit (log odds) that Y= 1 for a 50-year-old\\nfemale Hispanic with a blood pressure of 90 mmHg. Also compute the\\nodds that Y=1( P r o b [ Y=1 ]/Prob[Y= 0]) and the estimated proba-\\nbility that Y=1 .\\nb. Estimate odds ratios for each nonwhite race compared with the ref-\\nerence group (white), holding all other predictors constant. Why can\\nyou estimate the relative eﬀect of race for all types of subjects without\\nspecifying their characteristics?\\nc. Compute the odds ratio for a blood pressure of 120 mmHg compared\\nwith a blood pressure of 105, holding age ﬁrst to 30 years and then to\\n40 years.\\nd. Compute the odds ratio for a blood pressure of 120 mmHg compared\\nwith a blood pressure of 105, all other variables held to unspeciﬁed\\nconstants. Why is this relative eﬀect meaningful without knowing the\\nsubject’s age, race, or sex?\\ne. Compute the estimated risk diﬀerence in changing blood pressure from\\n105 mmHg to 120 mmHg, ﬁrst for age = 30 then for age = 40, for a\\nwhite female. Why does the risk diﬀerence depend on age?\\nf. Computetherelativeoddsformalescomparedwithfemales,forage=50\\nand other variables held constant.\\ng. Same as the previous question but for females : males instead of males\\n: females.\\nh. Compute the odds ratio resulting from increasing age from 50 to 55\\nfor males, and then for females, other var iables held constant. What is\\nwrong with the following question: What is the relative eﬀect of chang-\\ning age by one year?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='34824dab-fab6-4917-b3d0-2caaba5f0efa', embedding=None, metadata={'page_label': '275', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 11\\nCase Study in Binary Logistic Regression,\\nModel Selection and Approximation:\\nPredicting Cause of Death\\n11.1 Overview\\nThis chapter contains a case study on developing, describing, and validating\\na binary logistic regression model. In addition, the following methods are\\nexempliﬁed:\\n1. Data reduction using incomplete linear and nonlinear principal compo-\\nnents\\n2. Use of AIC to choose from ﬁve modeling variations, deciding which is best\\nfor the number of parameters\\n3. Model simpliﬁcation using stepwise variable selection and approximation\\nof the full model\\n4. The relationship between the degree of approximation and the degree of\\npredictive discrimination loss\\n5. Bootstrap validation that includes penalization for model uncertainty\\n(variable selection) and that demonstrates a loss of predictive discrimi-\\nnation over the full model even when compensating for overﬁtting the full\\nmodel.\\nThe datareductionandpre-transformationmethodsused herewerediscussed\\nin more detail in Chapter 8. Single imputation will be used because of the\\nlimited quantity of missing data.\\n11.2 Background\\nConsider the randomized tr ial of estrogen for treat ment of prostate cancer87\\ndescribedinChapter 8.Inthistrial,largerdosesofestrogenreducedtheeﬀect\\nof prostate cancer but at the cost of increased risk of cardiovascular death.\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 11275', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d34c257-dd16-4145-9b12-4c68e8a1138e', embedding=None, metadata={'page_label': '276', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='276 11 Binary Logistic Regression Case Study 1\\nKay340didaformalanalysisofthe competingrisksforcancer,cardiovascular,\\nand other deaths. It can also be quite informative to study how treatment\\nand baseline variables relate to the cause of death for those patients who\\ndied.376We subset the original dataset of those patients dying from prostate\\ncancer (n= 130), heart or vascular disease ( n= 96), or cerebrovascular\\ndisease (n= 31). Our goal is to predict cardiovascular–cerebrovasculardeath\\n(cvd,n= 127) given the patient died from either cvdor prostate cancer. Of\\ninterest is whether the time to death has an eﬀect on the cause of death, and\\nwhether the importance of certain variables depends on the time of death.\\n11.3 Data Transformations and Single Imputation\\nInR, ﬁrst obtain the desired subset of the data and do some preliminary\\ncalculationssuchas combining aninfrequentcategorywith the nextcategory,\\nand dichotomizing ekgfor use in ordinary principal components (PCs).\\nrequire(rms)\\ngetHdata(prostate)\\nprostate ←\\nwithin(prostate, {\\nlevels(ekg)[levels(ekg) %in%\\nc(\\'old MI \\',\\'recent MI \\')]←\\'MI\\'\\nekg.norm ←1*(ekg %in% c( \\'normal \\',\\'benign \\'))\\nlevels(ekg) ←abbreviate( levels(ekg))\\npfn←as.numeric(pf)\\nlevels(pf) ←levels(pf)[c(1,2,3 ,3)]\\ncvd←status %in% c(\"dead - heart or vascular\",\\n\"dead - cerebrovascular\")\\nrxn = as.numeric(rx) })\\n# Use transcan to compute optimal pre-transformations\\nptrans ←# See Figure 8.3\\ntranscan( ∼sz + sg + ap + sbp + dbp +\\nage + wt + hg + ekg + pf + bm + hx + dtime + rx,\\nimputed= TRUE,transformed= TRUE,\\ndata=prostate, pl= FALSE, pr= FALSE)\\n# Use transcan single imputations\\nimp←impute(ptrans, data=prostate, list.out= TRUE)\\nImputed missing values with the following frequencies\\nand stored them in variables with their original names:\\nsz sg age wt ekg\\n5 1 1128\\nNAvars ←all.vars( ∼sz + sg + age + wt + ekg)\\nfor(x in NAvars) prostate[[x]] ←imp[[x]]\\nsubset ←prostate$ status %in% c(\"dead - heart or vascular\",', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dd7712a0-65d4-494e-ad4a-c63d4f1b1fd9', embedding=None, metadata={'page_label': '277', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.4 Principal Components, Pretransformations 277\\n\"dead - cerebrovascular\",\" dead - prostatic ca\")\\ntrans ←ptrans$ transformed[ subset,]\\npsub ←prostate[ subset,]\\n11.4 Regression on Original Variables, Principal\\nComponents and Pretransformations\\nWe ﬁrst examine the performance of data reduction in predicting the cause\\nof death, similar to what we did for survival time in Section 8.6. The ﬁrst\\nanalyses assess how well PCs (on raw and transformed variables) predict the\\ncause of death.\\nThere are 127 cvds. We use the 15:1 rule of thumb discussed on P. 72to\\njustify using the ﬁrst 8 PCs. apis log-transformed because of its extreme\\ndistribution.\\n# Function to compute the first k PCs\\nipc←function(x, k=1, ...)\\nprincomp(x, ..., cor=TRUE)$scores[,1:k]\\n# Compute the first 8 PCs on raw variables then on\\n# transformed ones\\npc8←ipc(∼sz + sg + log(ap) + sbp + dbp + age +\\nwt + hg + ekg.norm + pfn + bm + hx + rxn + dtime,\\ndata=psub, k=8)\\nf8←lrm(cvd ∼pc8, data=psub)\\npc8t←ipc(trans, k=8)\\nf8t←lrm(cvd ∼pc8t, data=psub)\\n# Fit binary logistic model on original variables\\nf←lrm(cvd ∼sz + sg + log(ap) + sbp + dbp + age +\\nwt + hg + ekg + pf + bm + hx + rx + dtime, data=psub)\\n# Expand continuous variables usingsplines\\ng←lrm(cvd ∼rcs(sz,4) + rcs(sg,4) + rcs(log(ap),4) +\\nrcs(sbp,4) + rcs(dbp,4) + rcs(age,4) + rcs(wt,4) +\\nrcs(hg,4) + ekg + pf + bm + hx + rx + rcs(dtime,4),\\ndata=psub)\\n# Fit binary logistic model on individual transformed var.\\nh←lrm(cvd ∼trans, data=psub)\\nThe ﬁveapproachesto modelingtheoutcomearecomparedusingAIC (where\\nsmaller is better).\\nc(f8=AIC(f8), f8t=AIC(f8t), f=AIC(f), g=AIC(g), h=AIC(h))\\nf8 f8t f g h\\n257.6573 254.5172 255.8545 263.8413 254.5317\\nBased on AIC, the more traditional model ﬁtted to the raw data and as-\\nsuming linearity for all the continuous predictors has only a slight chance\\nof producing worse cross-validated predictive accuracy than other methods.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ec5233d5-7430-48c1-a970-16783de86f48', embedding=None, metadata={'page_label': '278', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='278 11 Binary Logistic Regression Case Study 1\\nThe chances are also good that eﬀect estimates from this simple model will\\nhave competitive mean squared errors.\\n11.5 Description of Fitted Model\\nHere we describe the simple all-linear full model. Summary statistics and a\\nWald-ANOVA tablearebelow,followedbypartialeﬀects plotswith pointwise\\nconﬁdence bands, and odds ratios over default ranges of predictors.\\nprint(f, latex=TRUE)\\nLogistic Regression Model\\nlrm(formula = cvd ~ sz + sg + log(ap) + sbp + dbp + age + wt +\\nhg + ekg + pf + bm + hx + rx + dtime, data = psub)\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 257 LRχ2144.39R20.573C 0.893\\nFALSE 130 d.f. 21 g 2.688Dxy0.786\\nTRUE 127 Pr(>χ2)<0.0001gr14.701γ 0.787\\nmax|∂logL\\n∂β|6×10−11gp0.394τa0.395\\nBrier 0.133\\nCoef S.E. Wald ZPr(>|Z|)\\nIntercept -4.5130 3.2210 -1.40 0.1612\\nsz -0.0640 0.0168 -3.80 0.0001\\nsg -0.2967 0.1149 -2.58 0.0098\\nap -0.3927 0.1411 -2.78 0.0054\\nsbp -0.0572 0.0890 -0.64 0.5201\\ndbp 0.3917 0.1629 2.40 0.0162\\nage 0.0926 0.0286 3.23 0.0012\\nwt -0.0177 0.0140 -1.26 0.2069\\nhg 0.0860 0.0925 0.93 0.3524\\nekg=bngn 1.0781 0.8793 1.23 0.2202\\nekg=rd&ec -0.1929 0.6318 -0.31 0.7601\\nekg=hbocd -1.3679 0.8279 -1.65 0.0985\\nekg=hrts 0.4365 0.4582 0.95 0.3407\\nekg=MI 0.3039 0.5618 0.54 0.5886\\npf=in bed <50% daytime 0.9604 0.6956 1.38 0.1673\\npf=in bed >50% daytime -2.3232 1.2464 -1.86 0.0623\\nbm 0.1456 0.5067 0.29 0.7738\\nhx 1.0913 0.3782 2.89 0.0039', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ac64a49d-d8a4-43f8-8d9b-89d297827716', embedding=None, metadata={'page_label': '279', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"11.5 Description of Fitted Model 279\\nCoef S.E. Wald ZPr(>|Z|)\\nrx=0.2 mg estrogen -0.3022 0.4908 -0.62 0.5381\\nrx=1.0 mg estrogen 0.7526 0.5272 1.43 0.1534\\nrx=5.0 mg estrogen 0.6868 0.5043 1.36 0.1733\\ndtime -0.0136 0.0107 -1.27 0.2040\\nan←anova(f)\\nlatex(an, file= '', table.env= FALSE)\\nχ2d.f.P\\nsz 14 .42 1 0.0001\\nsg 6 .67 1 0.0098\\nap 7 .74 1 0.0054\\nsbp 0 .41 1 0.5201\\ndbp 5 .78 1 0.0162\\nage 10 .45 1 0.0012\\nwt 1 .59 1 0.2069\\nhg 0 .86 1 0.3524\\nekg 6 .76 5 0.2391\\npf 5 .52 2 0.0632\\nbm 0 .08 1 0.7738\\nhx 8 .33 1 0.0039\\nrx 5 .72 3 0.1260\\ndtime 1 .61 1 0.2040\\nTOTAL 66 .87 21<0.0001\\nplot(an) # Figure 11.1\\ns←f$stats\\ngamma.hat ←(s['Model L.R. ']-s [ 'd.f. '])/s[ 'Model L.R. ']\\ndd←datadist( psub); options(datadist= 'dd')\\nggplot( Predict(f), sepdiscrete= 'vertical ', vnames= 'names ',\\nrdata=psub,\\nhistSpike.opts= list(frac= function(f) .1*f/max(f) ))\\n# Figure 11.2\\nplot(summary(f), log= TRUE) # Figure 11.3\\nThe van Houwelingen–Le Cessie heuri stic shrinkage estimate (Equation 4.3)\\nis ˆγ=0.85, indicating that this model will validate on new data about 15%\\nworse than on this dataset.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3f5de37-f461-4e52-9dce-4ef901f6922a', embedding=None, metadata={'page_label': '280', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='280 11 Binary Logistic Regression Case Study 1\\nbm\\nsbp\\nhg\\nwt\\ndtime\\nekg\\nrx\\npf\\ndbp\\nsg\\nap\\nhx\\nage\\nsz\\n02468 1 0 1 2\\nχ2− df\\nFig. 11.1 Ranking of apparent importance of predictors of cause of death\\n11.6 Backwards Step-Down\\nNow use fast backward step-down (with total residual AIC as the stopping\\nrule) to identify the variables that explain the bulk of the cause of death.\\nLater validation will take this screening of variables into account.The greatly\\nreduced model results in a simple nomogram.\\nfastbw(f)\\nDeleted Chi-Sq d.f. P Residual d.f. P AIC\\nekg 6.76 5 0.2391 6.76 5 0.2391 -3.24\\nbm 0.09 1 0.7639 6.85 6 0.3349 -5.15\\nhg 0.38 1 0.5378 7.23 7 0.4053 -6.77\\nsbp 0.48 1 0.4881 7.71 8 0.4622 -8.29\\nwt 1.11 1 0.2932 8.82 9 0.4544 -9.18\\ndtime 1.47 1 0.2253 10.29 10 0.4158 -9.71\\nrx 5.65 3 0.1302 15.93 13 0.2528 -10.07\\npf 4.78 2 0.0915 20.71 15 0.1462 -9.29\\nsg 4.28 1 0.0385 25.00 16 0.0698 -7.00\\ndbp 5.84 1 0.0157 30.83 17 0.0209 -3.17\\nApproximate Estimates after Deleting Factors\\nCoef S.E.Wald Z P\\nIntercept -3.74986 1.82887 -2.050 0.0403286\\nsz -0.04862 0.01532 -3.174 0.0015013\\nap -0.40694 0.11117 -3.660 0.0002518\\nage 0.06000 0.02562 2.342 0.0191701\\nhx 0.86969 0.34339 2.533 0.0113198\\nFactors in Final Model\\n[1] sz ap age hx', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e6741e7-89df-444a-adf6-4e4f16443421', embedding=None, metadata={'page_label': '281', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"11.6 Backwards Step-Down 281\\nage ap dbp\\ndtime hg sbp\\nsg sz wt−6−4−2024\\n−6−4−2024\\n−6−4−202455 60 65 70 75 80 0 20 40 5.0 7.5 10.0 12.5\\n0 20 40 60 10 12 14 16 12.5 15.0 17.5\\n8 10 12 14 0 10 20 30 40 80 90 100 110 120log odds\\nbm ekg hx\\npf rx01\\nnrmlbngnrd&echbocdhrtsMI\\nnormal activityin bed < 50% daytimein bed > 50% daytime\\nplacebo0.2 mg estrogen1.0 mg estrogen5.0 mg estrogen\\n−6 −4 −2 0 2 4 −6 −4 −2 0 2 41\\n0\\nlog odds\\nFig. 11.2 Partial eﬀects (log odds scale) in full model for cause of death, along with\\nvertical line segments showing the raw data distribution of predictors\\nfred←lrm(cvd ∼sz + log(ap) + age + hx, data=psub)\\nlatex(fred,file= '')\\nProb{cvd}=1\\n1+exp(−Xβ),where\\nXˆβ=\\n−5.009276−0.05510121sz −0.509185log(ap)+0 .0788052age+1 .070601hx\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5c868384-59f5-48c8-8716-956029fc2dd1', embedding=None, metadata={'page_label': '282', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='282 11 Binary Logistic Regression Case Study 1\\nOdds Ratio\\n 0.10  0.50  2.00  8.00\\nsz − 25:6\\nsg − 12:9\\nap − 7:0.5999756\\nsbp − 16:13\\ndbp − 9:7\\nage − 76:70\\nwt − 106:89\\nhg − 14.59961:12\\nbm − 1:0\\nhx − 1:0\\ndtime − 37:11\\nekg − nrml:hrts\\nekg − bngn:hrts\\nekg − rd&ec:hrts\\nekg − hbocd:hrts\\nekg − MI:hrts\\npf − in bed < 50% daytime:normal activity\\npf − in bed > 50% daytime:normal activity\\nrx − 0.2 mg estrogen:placebo\\nrx − 1.0 mg estrogen:placebo\\nrx − 5.0 mg estrogen:placebo\\nFig. 11.3 Interquartile-range odds ratios for continuous predictors and simple odds\\nratios for categorical predictors. Numbers at left are upper quartile : lower quartile or\\ncurrent group : reference group. The bars represent 0 .9,0.95,0.99 conﬁdence limits.\\nThe intervals are drawn on the log odds ratio scale and labeled on the odds rati o\\nscale. Ranges are on the original scale.\\nnom←nomogram( fred, ap=c(.1, .5, 1, 5, 10, 50),\\nfun=plogis, funlabel=\"Probability\",\\nfun.at=c(.01,.05,.1,.25,.5,.75,.9,.95,.99))\\nplot(nom, xfrac=.45) # Figure 11.4\\nIt is readily seen from this model that patients with a history of heart\\ndisease, and patients with less extensive prostate cancer are those more likely\\nto die from cvdrather than from cancer. But beware that it is easy to over-\\ninterpret ﬁndings when using unpenalized estimation, and conﬁdence inter-\\nvals are too narrow. Let us use the bootstrap to study the uncertainty in\\nthe selection of variables and to penalize for this uncertainty when estimat-\\ning predictive performance of the model. The variables selected in the ﬁrst 20\\nbootstrap resamplesare shown,making it obvious that the set of“signiﬁcant”\\nvariables, i.e., the ﬁnal model, is somewhat arbitrary.\\nf←update(f, x= TRUE, y= TRUE)\\nv←validate(f, B =200, bw= TRUE)\\nlatex(v, B=20, digits=3)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='532c04b8-221a-4140-b401-64fd698614bd', embedding=None, metadata={'page_label': '283', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.6 Backwards Step-Down 283\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 0 0\\nSize of Primary Tumor\\n(cm^2)70 65 60 55 50 45 40 35 30 25 20 15 10 5 0\\nSerum Prostatic Acid\\nPhosphatase50 10 5 1 0.5 0.1\\nAge in Years\\n45 50 55 60 65 70 75 80 85 90\\nHistory of Cardiovascular\\nDisease01\\nTotal Points\\n0 50 100 150 200 250 300\\nLinear Predictor\\n−5 −4 −3 −2 −1 0 1 2 3 4\\nProbability\\n0.01 0.05 0.1 0.25 0.5 0.75 0.90.95\\nFig. 11.4 Nomogram calculating XˆβandˆPforcvdas the cause of death, using\\nthe step-down model. For each predictor, read the points assigned on the 0–100 scale\\nand add these points. Read the result on the Total Points scale and then read the\\ncorresponding predictions below it.\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.682 0.713 0.643 0 .071 0 .611 200\\nR20.439 0.481 0.393 0 .088 0 .351 200\\nIntercept 0 .000 0.000−0.006 0 .006−0.006 200\\nSlope 1 .000 1.000 0.811 0 .189 0 .811 200\\nEmax 0.000 0.000 0.048 0 .048 0 .048 200\\nD 0.395 0.449 0.346 0 .102 0 .293 200\\nU −0.008−0.008 0.018−0.026 0 .018 200\\nQ 0.403 0.456 0.329 0 .128 0 .275 200\\nB 0.162 0.151 0.174−0.022 0 .184 200\\ng 1.932 2.213 1.756 0 .457 1 .475 200\\ngp 0.341 0.355 0.320 0 .035 0 .306 200', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='585951f1-31f7-482d-b1fc-47648e5492ae', embedding=None, metadata={'page_label': '284', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='284 11 Binary Logistic Regression Case Study 1\\nFactors Retained in Backwards Elimination\\nFirst 20 Resamples\\nsz sg ap sbp dbp age wt hg ekg pf bm hx rx dtime\\n•• •\\n•• • • • • • •\\n•• • •\\n••\\n•• • •\\n••\\n•• • • •\\n•• • •\\n•• •\\n•• •\\n•• • •\\n•• • •\\n•• • •\\n•• • •\\n•• • • • • ••\\n•• • •\\n••• • • •\\n••\\n•• • •\\n••\\nFrequencies of Numbers of Factors Retained\\n1 23456 7 8 9 1 1 1 2\\n63 94 76 11 91 0842 3 1\\nThe slope shrinkage (ˆ γ) is a bit lower than was estimated above. There is\\ndrop-oﬀ in all indexes. The estimated likely future predictive discrimination\\nof the model as measured by Somers’ Dxyfell from 0.682 to 0.611. The\\nlatter estimate is the one that should be claimed when describing model\\nperformance.\\nA nearly unbiased estimate of future calibration of the stepwise-derived\\nmodel is given below.\\ncal←calibrate(f, B =200, bw= TRUE)\\nplot(cal) # Figure 11.5\\nThe amount of overﬁtting seen in Figure 11.5is consistent with the indexes\\nproduced by the validate function.\\nFor comparison, consider a bootstrap validation of the full model without\\nusing variable selection.\\nvfull ←validate(f, B =200)\\nlatex(vfull, digits=3)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='765fc552-ce5d-4940-9853-5545d87aaa10', embedding=None, metadata={'page_label': '285', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.6 Backwards Step-Down 285\\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nPredicted Pr{cvd}Actual Probability\\nMean absolute error=0.028 n=257 B= 200 repetitions, bootApparent\\nBias−corrected\\nIdeal\\nFig. 11.5 Bootstrap overﬁtting–corrected calibration curve estimate for the back-\\nwards step-down cause of death logistic model, along with a rug plot showing t he dis-\\ntribution of predic ted risks. The smooth nonpa rametric calibration estimator ( loess)\\nis used.\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.786 0.833 0.738 0 .095 0 .691 200\\nR20.573 0.641 0.501 0 .140 0 .433 200\\nIntercept 0 .000 0.000−0.013 0 .013−0.013 200\\nSlope 1 .000 1.000 0.690 0 .310 0 .690 200\\nEmax 0.000 0.000 0.085 0 .085 0 .085 200\\nD 0.558 0.653 0.468 0 .185 0 .373 200\\nU −0.008−0.008 0.051−0.058 0 .051 200\\nQ 0.566 0.661 0.417 0 .244 0 .322 200\\nB 0.133 0.115 0.150−0.035 0 .168 200\\ng 2.688 3.464 2.355 1 .108 1 .579 200\\ngp 0.394 0.416 0.366 0 .050 0 .344 200\\nComparedto the validationof the full model, the step-downmodel has less\\noptimism, but it started with a smaller Dxydue to loss of information from\\nremovingmoderatelyimportant variables.The improvementin optimism was\\nnot enough to oﬀset the eﬀect of eliminating variables. If shrinkage were used\\nwith the full model, it would have better calibration and discrimination than\\nthe reduced model, since shrinkage does not diminish Dxy. Thus stepwise\\nvariable selection failed at delivering excellent predictive discrimination.\\nFinally, compare previous results with a bootstrap validation of a step-\\ndown model using a better signiﬁcance level for a variable to stay in the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e1cc115-f6b0-4e83-b935-65fa7de433a4', embedding=None, metadata={'page_label': '286', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"286 11 Binary Logistic Regression Case Study 1\\nmodel (α=0.5,589) and using individual approximate Wald tests rather\\nthan tests combining all deleted variables.\\nv5←validate(f, bw= TRUE, sls=0.5, type= 'individual ', B=200)\\nBackwards Step-down - Original Model\\nDeleted Chi-Sq d.f. P Residual d.f. P AIC\\nekg 6.76 5 0.2391 6.76 5 0.2391 -3.24\\nbm 0.09 1 0.7639 6.85 6 0.3349 -5.15\\nhg 0.38 1 0.5378 7.23 7 0.4053 -6.77\\nsbp 0.48 1 0.4881 7.71 8 0.4622 -8.29\\nwt 1.11 1 0.2932 8.82 9 0.4544 -9.18\\ndtime 1.47 1 0.2253 10.29 10 0.4158 -9.71\\nrx 5.65 3 0.1302 15.93 13 0.2528 -10.07\\nApproximate Estimates after Deleting Factors\\nCoef S.E.Wald Z P\\nIntercept -4.86308 2.67292 -1.819 0.068852\\nsz -0.05063 0.01581 -3.202 0.001366\\nsg -0.28038 0.11014 -2.546 0.010903\\nap -0.24838 0.12369 -2.008 0.044629\\ndbp 0.28288 0.13036 2.170 0.030008\\nage 0.08502 0.02690 3.161 0.001572\\npf=in bed < 50% daytime 0.81151 0.66376 1.223 0.221485\\npf=in bed > 50% daytime -2.19885 1.21212 -1.814 0.069670\\nhx 0.87834 0.35203 2.495 0.012592\\nFactors in Final Model\\n[1] sz sg ap dbp age pf hx\\nlatex(v5, digits=3, B=0)\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.739 0.801 0.716 0 .085 0 .654 200\\nR20.517 0.598 0.481 0 .117 0 .400 200\\nIntercept 0 .000 0.000−0.008 0 .008−0.008 200\\nSlope 1 .000 1.000 0.745 0 .255 0 .745 200\\nEmax 0.000 0.000 0.067 0 .067 0 .067 200\\nD 0.486 0.593 0.444 0 .149 0 .337 200\\nU −0.008−0.008 0.033−0.040 0 .033 200\\nQ 0.494 0.601 0.411 0 .190 0 .304 200\\nB 0.147 0.125 0.156−0.030 0 .177 200\\ng 2.351 2.958 2.175 0 .784 1 .567 200\\ngp 0.372 0.401 0.358 0 .043 0 .330 200\\nThe performance statistics are midway between the full model and the\\nsmaller stepwise model.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8bb545e1-a28f-487e-89f6-b5ec25e4ae49', embedding=None, metadata={'page_label': '287', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"11.7 Model Approximation 287\\n11.7 Model Approximation\\nFrequently a better approach than stepwise variable selection is to approx-\\nimate the full model, using its estimates of precision, as discussed in Sec-\\ntion5.5. Stepwise variable selection as well as regression trees are useful for\\nmaking the approximations, and the sacriﬁce in predictive accuracy is always\\napparent.\\nWe begin by computing the“gold standard”linear predictor from the full\\nmodel ﬁt ( R2=1.0), then running backwards step-down OLS regression to\\napproximate it.\\nlp←predict(f) # Compute linear predictor from full model\\n# Insert sigma=1 as otherwise sigma=0 willcause problems\\na←ols(lp ∼sz + sg + log(ap) + sbp + dbp + age + wt +\\nhg + ekg + pf + bm + hx + rx + dtime, sigma=1,\\ndata=psub)\\n# Specify silly stopping criterion to remove all variables\\ns←fastbw(a, aics =10000)\\nbetas ←s$Coefficients # matrix, rows= iterations\\nX ←cbind(1, f$x) # design matrix\\n# Compute the series of approximations to lp\\nap←X %*% t(betas)\\n# For each approx. compute approximation R∧2 and ratio of\\n# likelihood ratio chi-square for approximate model to that\\n# of original model\\nm←ncol(ap) - 1 # all but intercept-only model\\nr2←frac←numeric(m)\\nfullchisq ←f$stats[ 'Model L.R. ']\\nfor(i in 1:m) {\\nlpa←ap[,i]\\nr2[i] ←cor(lpa, lp)∧2\\nfapprox ←lrm(cvd ∼lpa, data=psub)\\nfrac[i] ←fapprox$ stats[ 'Model L.R. '] / fullchisq\\n}# Figure 11.6:\\nplot(r2, frac,type= 'b',\\nxlab=expression( paste( 'Approximation ',R∧2)),\\nylab=expression( paste( 'Fraction of ',\\nchi∧2,'Preserved ')))\\nabline(h=.95, col=gray(.83)); abline(v=.95, col=gray(.83))\\nabline(a=0, b=1, col=gray(.83))\\nAfter 6 deletions, slightly more than 0.05 of both the LR χ2and the approx-\\nimationR2are lost (see Figure 11.6). Therefore we take as our approximate\\nmodel the one that removed 6 predictors. The equation for this model is\\nbelow, and its nomogram is in Figure 11.7.\\nfapprox ←ols(lp ∼sz + sg + log(ap) + age + ekg + pf + hx +\\nrx, data=psub)\\nfapprox$ stats[ 'R2']# as a check\\nR2 0.9453396\\nlatex(fapprox, file= '')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8cee7f24-4d94-4e18-9e76-fa5fc700e6a0', embedding=None, metadata={'page_label': '288', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='288 11 Binary Logistic Regression Case Study 1\\n0.5 0.6 0.7 0.8 0.9 1.00.40.50.60.70.80.91.0\\nApproximation R2Fraction of χ2 Preserved\\nFig. 11.6 Fraction of explainable variation (full model LR χ2)i ncvdthat was\\nexplained by approximate models, along with approximation accuracy ( x–axis)\\nE(lp) =Xβ,where\\nXˆβ=\\n−2.868303−0.06233241 sz −0.3157901 sg −0.3834479 log(ap)+0 .09089393 age\\n+1.396922[bngn]+0 .06275034[rd&ec] −1.24892[hbocd]+0 .6511938[hrts]\\n+0.3236771[MI]\\n+1.116028[in bed <50% daytime] −2.436734[in bed >50% daytime]\\n+1.05316 hx\\n−0.3888534[0 .2 mg estrogen]+0 .6920495[1 .0 mg estrogen]\\n+0.7834498[5 .0 mg estrogen]\\nand [c] = 1 if subject is in group c,0o t h e r w i s e .\\nnom←nomogram( fapprox, ap =c(.1, .5, 1, 5, 10, 20, 30, 40),\\nfun=plogis, funlabel=\"Probability\",\\nlp.at=(-5):4,\\nfun.lp.at= qlogis(c(.01,.05,.25,.5,.75,.95,.99)))\\nplot(nom, xfrac=.45) # Figure 11.7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='038c9a92-c84f-48cf-bffb-913d6e2eeab8', embedding=None, metadata={'page_label': '289', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='11.7 Model Approximation 289\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 0 0\\nSize of Primary Tumor\\n(cm^2)70 65 60 55 50 45 40 35 30 25 20 15 10 5 0\\nCombined Index of Stage\\nand Hist. Grade15 14 13 12 11 10 9 8 7 6 5\\nSerum Prostatic Acid\\nPhosphatase40 10 5 1 0.5 0.1\\nAge in Years\\n45 50 55 60 65 70 75 80 85 90\\nekg\\nhbocd rd&ec hrtsnrml bngn\\npf\\nin bed > 50% daytime in bed < 50% daytimenormal activity\\nHistory of Cardiovascular\\nDisease01\\nrx\\n0.2 mg estrogenplacebo\\nTotal Points\\n0 50 100 150 200 250 300 350 400\\nLinear Predictor\\n−5 −3 −1 0 1 2 3 4\\nProbability\\n0.01 0.05 0.250.50.75 0.95 0.99\\nFig. 11.7 Nomogram for predicting the probability of cvdbased on the approximate\\nmodel', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4d7b196c-4dff-40f4-beff-cb298d95e1a6', embedding=None, metadata={'page_label': '291', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 12\\nLogistic Model Case Study 2: Survival\\nof Titanic Passengers\\nThis case study demonstrates the development of a binary logistic regression\\nmodel to describe patterns of survival in passengers on the Titanic, based on\\npassenger age, sex, ticket class, and the number of family members accom-\\npanying each passenger. Nonparametric regression is also used. Since many\\nof the passengers had missing ages, multiple imputation is used so that the\\ncompleteinformationonthe othervariablescanbeeﬃcientlyutilized. Titanic\\npassenger data were gathered by many researchers. Primary references are\\ntheEncyclopedia Titanica atwww.encyclopedia-titanica.org and Eaton and\\nHaas.169Titanic survival patterns have been analyzed previously151,296,571\\nbut without incorporation of individual passenger ages. Thomas Cason while\\na University of Virginia student compiled and interpreted the data from the\\nWorld Wide Web. One thousand three hundred nine of the passengers are\\nrepresented in the dataset, which is available from this text’s Web site under\\nthe name titanic3. An early analysisof Titanic data may be found in Bron75.\\n12.1 Descriptive Statistics\\nFirst we obtain basic descriptive statistics on key variables.\\nrequire(rms)\\ngetHdata(titanic3) # get dataset from web site\\n# List of names of variables to analyze\\nv←c('pclass ','survived ','age','sex','sibsp ','parch ')\\nt3←titanic3[, v]\\nunits(t3$age) ←'years '\\nlatex(describe(t3), file= '')\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 12291\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c276ee4-e1ab-457b-a164-bd55ec32f36a', embedding=None, metadata={'page_label': '292', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"292 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\nt3\\n6 Variables 1309 Observations\\npclass\\nn missing unique\\n1309 0 3\\n1st (323, 25%), 2nd (277, 21%), 3rd (709, 54%)\\nsurvived : Survived\\nn missing unique Info Sum Mean\\n1309 0 2 0.71 500 0.382\\nage : Age [years]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n1046 263 98 1 29.88 5 14 21 28 39 50 57\\nlowest : 0.1667 0.3333 0.4167 0.6667 0.7500\\nhighest: 70.5000 71.0000 74.0000 76.0000 80.0000\\nsexn missing unique\\n1309 0 2\\nfemale (466, 36%), male (843, 64%)\\nsibsp : Number of Siblings/Spouses Aboard\\nn missing unique Info Mean\\n1309 0 7 0.67 0.4989\\n0 1234 5 8\\nFrequency 891 319 42 20 22 6 9\\n% 6 82 4322 0 1\\nparch : Number of Parents/Children Aboard\\nn missing unique Info Mean\\n1309 0 8 0.55 0.385\\n0 1 234569\\nFrequency 1002 170 113 86622\\n% 7 7 1 3 910000\\nNext,weobtainaccesstotheneededvariablesandobservations,andsavedata\\ndistribution characteristics for plotting and for computing predictor eﬀects.\\nThere are not many passengers having more than 3 siblings or spouses or\\nmore than 3 children, so we truncate two variables at 3 for the purpose of\\nestimating stratiﬁed survival probabilities.\\ndd←datadist(t3)\\n# describe distributions of variables to rms\\noptions(datadist= 'dd')\\ns←summary(survived ∼age + sex + pclass +\\ncut2(sibsp,0:3) + cut2( parch,0:3), data=t3)\\nplot(s, main= '', subtitles= FALSE) # Figure 12.1\\nNote the large number of missing ages. Also note the strong eﬀects of sex and\\npassengerclassonthe probabilityofsurviving.Theageeﬀect doesnot appear\\nto be very strong, because as we show later, much of the eﬀect is restricted to\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='89ca3da0-40e8-4f35-989e-4b91ed088a0a', embedding=None, metadata={'page_label': '293', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12.1 Descriptive Statistics 293\\nSurvived0.2 0.3 0.4 0.5 0.6 0.713092411317010025742319891709277323843466263245265246290N\\nMissing\\nfemale\\nmale\\n1st\\n2nd\\n3rd\\n0\\n1\\n2\\n0\\n1\\n2[ 0.167,22.0)\\n[22.000,28.5)\\n[28.500,40.0)\\n[40.000,80.0]\\n[3,8]\\n[3,9]Age [years]\\nsex\\npclass\\nNumber of Siblings/Spouses Aboard\\nNumber of Parents/Children Aboard\\nOverall\\nFig. 12.1 Univariable summaries of Titanic survival\\nage<21 years for one of the sexes. The eﬀects of the last two variables are\\nunclear as the estimated proportions are not monotonic in the values of these\\ndescriptors. Although some of the cell sizes are small, we can show four-way\\nempirical relationships with the fraction of surviving passengers by creating\\nfour cells for sibsp×parchcombinations and by creating two age groups. We\\nsuppress proportions based on fewer than 25 passengers in a cell. Results are\\nshown in Figure 12.2.\\ntn←transform(t3,\\nagec = ifelse(age < 21, 'child ','adult '),\\nsibsp= ifelse(sibsp == 0, 'no sib/sp ','sib/sp '),\\nparch= ifelse(parch == 0, 'no par/child ','par/child '))\\ng←function(y) if( length(y) < 25) NA else mean(y)\\ns←with(tn, summarize( survived,\\nllist(agec, sex, pclass, sibsp, parch), g))\\n# llist, summarize in Hmisc package\\n# Figure 12.2:\\nggplot(subset(s, agec != 'NA'),\\naes(x=survived, y=pclass, shape=sex)) +\\ngeom_point() + facet_grid( agec∼sibsp * parch) +\\nxlab( 'Proportion Surviving ') + ylab( 'Passenger Class ')+\\nscale_x_continuous( breaks=c(0, .5, 1))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9bc8eca1-53ad-4070-93d9-204e73ba3f1b', embedding=None, metadata={'page_label': '294', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='294 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\n1st2nd3rd\\n1st2nd3rdadult child\\n0.0 0.5 1.00.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0\\nProportion SurvivingPassenger Classsex\\nfemale\\nmaleno sib/sp\\nno par/childno sib/sp\\npar/childsib/sp\\nno par/childsib/sp\\npar/child\\nFig. 12.2 Multi-way summary of Titanic survival\\nNote that none of the eﬀects of sibsporparchfor common passenger groups\\nappear strong on an absolute risk scale.\\n12.2 Exploring Trends with Nonparametric Regression\\nAs described in Section 2.4.7,t h eloesssmoother has excellent performance\\nwhen the response is binary, as l ong as outlier detection is turned oﬀ. Here\\nwe use a ggplot2add-on function histSpikeg in theHmiscpackage to obtain\\nand plot the loessﬁt and age distribution. histSpikeg uses the“no iteration”\\noption for the Rlowessfunction when the response is binary.\\n# Figure 12.3\\nb←scale_size_discrete( range=c(.1, .85))\\nyl←ylab(NULL)\\np1←ggplot(t3, aes(x=age, y= survived)) +\\nhistSpikeg(survived ∼age, lowess= TRUE,data=t3) +\\nylim(0,1) + yl\\np2←ggplot(t3, aes(x=age, y=survived, color=sex)) +\\nhistSpikeg(survived ∼age + sex, lowess= TRUE,\\ndata=t3) + ylim(0,1) + yl\\np3←ggplot(t3, aes(x=age, y=survived, size=pclass)) +\\nhistSpikeg(survived ∼age + pclass, lowess= TRUE,\\ndata=t3) + b + ylim(0,1) + yl\\np4←ggplot(t3, aes(x=age, y=survived, color=sex,\\nsize=pclass)) +\\nhistSpikeg(survived ∼age + sex + pclass,\\nlowess= TRUE,data=t3) +\\nb + ylim(0,1) + yl\\ngridExtra::grid.arrange(p1, p2, p3, p4, ncol=2) # combine 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b5c08dda-f4ec-4406-bcd6-8ee73013eb5f', embedding=None, metadata={'page_label': '295', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12.2 Exploring Trends with Nonparametric Regression 295\\n0.000.250.500.751.00\\n02 0 4 0 6 0 8 0\\nage0.000.250.500.751.00\\n02 0 4 0 6 0 8 0\\nagesex\\nfemale\\nmale\\n0.000.250.500.751.00\\n02 0 4 0 6 0 8 0\\nagepclass\\n1st\\n2nd\\n3rd\\n0.000.250.500.751.00\\n02 0 4 0 6 0 8 0\\nagepclass\\n1st\\n2nd\\n3rd\\nsex\\nfemale\\nmale\\nFig. 12.3 Nonparametric regression ( loess) estimates of the relationship between\\nage and the probability of surviving the Titanic, with tick marks depicting the age\\ndistribution. The top left panel shows unstratiﬁed estimates of the probability of\\nsurvival. Other panels show nonparametric estimates by various stratiﬁcations.\\nFigure12.3shows much of the story of passenger survival patterns.“Women\\nand children ﬁrst” seems to be true e xcept for women in third class. It is\\ninteresting that there is no real cutoﬀ for who is considered a child. For men,\\nthe younger the greater chance of surviving. The interpretation of the eﬀects\\nof the “number of relatives”-type variables will be more diﬃcult, as their\\ndeﬁnitions are a function of age. Figure 12.4shows these relationships.\\n# Figure 12.4\\ntop←theme(legend.position= 'top')\\np1←ggplot(t3, aes(x=age, y=survived, color=cut2( sibsp,\\n0:2))) + stat_plsmo() + b + ylim(0,1) + yl + top +\\nscale_color_discrete( name= 'siblings/spouses ')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='09dd7507-1d0d-4bf3-a438-b5457bd2586a', embedding=None, metadata={'page_label': '296', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"296 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\np2←ggplot(t3, aes(x=age, y=survived, color=cut2( parch,\\n0:2))) + stat_plsmo() + b + ylim(0,1) + yl + top +\\nscale_color_discrete( name= 'parents/children ')\\ngridExtra::grid.arrange(p1, p2, ncol=2)\\n0.000.250.500.751.00\\n0 2 04 06 08 0\\nagesiblings/spouses 0 1 [2,8]\\n0.000.250.500.751.00\\n0 2 04 06 08 0\\nageparents/children 0 1 [2,9]\\nFig. 12.4 Relationship between age and survival stratiﬁed by the number of siblings\\nor spouses on board (left panel) or by the number of parents or children of the\\npassenger on board (right panel).\\n12.3 Binary Logistic Model With Casewise Deletion\\nof Missing Values\\nWhat follows is the standard analysis based on eliminating observations hav-\\ning any missing data. We develop an initial somewhat saturated logistic\\nmodel, allowing for a ﬂexible nonlinear age eﬀect that can diﬀer in shape\\nfor all six sex ×class strata. The sibspandparchvariables do not have suf-\\nﬁciently dispersed distributions to allow for us to model them nonlinearly.\\nAlso, there are too few passengers with nonzero values of these two variables\\nin sex×pclass×age strata to allow us to model complex interactions in-\\nvolving them. The meaning of these variables does depend on the passenger’s\\nage, so we consider only age interactions involving sibspandparch.\\nf1←lrm(survived ∼sex*pclass*rcs(age,5) +\\nrcs(age,5)*(sibsp + parch), data=t3) # Table 12.1\\nlatex(anova(f1), file= '', label= 'titanic-anova3 ',\\nsize= 'small ')\\nThree-wayinteractions are clearly insigniﬁcant( P=0.4) in Table 12.1.S o\\nisparch(P=0.6 for testing the combined main eﬀect + interaction eﬀects\\nforparch, i.e., whether parchis important for any age). These eﬀects would\\nbe deleted in almost all bootstrap resamples had we bootstrapped a variable\\nselection procedure using α=0.1f o rr e t e n t i o no ft e r m s ,s ow ec a ns a f e l y\\nignore these terms for future steps. The model not containing those terms\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='288f92a6-d461-4ec2-8204-67e346678d69', embedding=None, metadata={'page_label': '297', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Binary Logistic Model With Casewise Deletion of Missing Values 297\\nTable 12.1 Wald Statistics for survived\\nχ2d.f.P\\nsex (Factor+Higher Order Factors) 187 .15 15<0.0001\\nAll Interactions 59.74 14<0.0001\\npclass (Factor+Higher Order Factors) 100 .10 20<0.0001\\nAll Interactions 46.51 18 0.0003\\nage (Factor+Higher Order Factors) 56 .20 32 0.0052\\nAll Interactions 34.57 28 0.1826\\nNonlinear (Factor+Higher Order Factors) 28.66 24 0.2331\\nsibsp (Factor+Higher Order Factors) 19 .67 5 0.0014\\nAll Interactions 12.13 4 0.0164\\nparch (Factor+Higher Order Factors) 3 .51 5 0.6217\\nAll Interactions 3.51 4 0.4761\\nsex×pclass (Factor+Higher Order Factors) 42 .43 10<0.0001\\nsex×age (Factor+Higher Order Factors) 15 .89 12 0.1962\\nNonlinear (Factor+Higher Order Factors) 14.47 9 0.1066\\nNonlinear Interaction : f(A,B) vs. AB 4.17 3 0.2441\\npclass×age (Factor+Higher Order Factors) 13 .47 16 0.6385\\nNonlinear (Factor+Higher Order Factors) 12.92 12 0.3749\\nNonlinear Interaction : f(A,B) vs. AB 6.88 6 0.3324\\nage×sibsp (Factor+Higher Order Factors) 12 .13 4 0.0164\\nNonlinear 1.76 3 0.6235\\nNonlinear Interaction : f(A,B) vs. AB 1.76 3 0.6235\\nage×parch (Factor+Higher Order Factors) 3 .51 4 0.4761\\nNonlinear 1.80 3 0.6147\\nNonlinear Interaction : f(A,B) vs. AB 1.80 3 0.6147\\nsex×pclass×age (Factor+Higher Order Factors) 8 .34 8 0.4006\\nNonlinear 7.74 6 0.2581\\nTOTAL NONLINEAR 28 .66 24 0.2331\\nTOTAL INTERACTION 75 .61 30<0.0001\\nTOTAL NONLINEAR + INTERACTION 79 .49 33<0.0001\\nTOTAL 241.93 39<0.0001\\nis ﬁtted below. The ^2in the model formula means to expand the terms in\\nparentheses to include all main eﬀects and second-order interactions.\\nf←lrm(survived ∼(sex + pclass + rcs(age,5))∧2+\\nrcs(age,5)*sibsp, data=t3)\\nprint(f, latex=TRUE)\\nLogistic Regression Model\\nlrm(formula = survived ~ (sex + pclass + rcs(age, 5))^2\\n+ rcs(age, 5) * sibsp, data = t3)\\nFrequencies of Missing Values Due to Each Variable\\nsurvived sex pclass age sibsp\\n0 0 0 263 0', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='835d6473-33f3-48b5-a668-c4be54719bb2', embedding=None, metadata={'page_label': '298', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"298 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 1046 LRχ2553.87R20.555C 0.878\\n0 619 d.f. 26 g 2.427Dxy0.756\\n1 427 Pr(>χ2)<0.0001gr11.325γ 0.758\\nmax|∂logL\\n∂β|6×10−6gp0.365τa0.366\\nBrier 0.130\\nCoef S.E. Wald ZPr(>|Z|)\\nIntercept 3.3075 1.8427 1.79 0.0727\\nsex=male -1.1478 1.0878 -1.06 0.2914\\npclass=2nd 6.7309 3.9617 1.70 0.0893\\npclass=3rd -1.6437 1.8299 -0.90 0.3691\\nage 0.0886 0.1346 0.66 0.5102\\nage’ -0.7410 0.6513 -1.14 0.2552\\nage” 4.9264 4.0047 1.23 0.2186\\nage”’ -6.6129 5.4100 -1.22 0.2216\\nsibsp -1.0446 0.3441 -3.04 0.0024\\nsex=male * pclass=2nd -0.7682 0.7083 -1.08 0.2781\\nsex=male * pclass=3rd 2.1520 0.6214 3.46 0.0005\\nsex=male * age -0.2191 0.0722 -3.04 0.0024\\nsex=male * age’ 1.0842 0.3886 2.79 0.0053\\nsex=male * age” -6.5578 2.6511 -2.47 0.0134\\nsex=male * age”’ 8.3716 3.8532 2.17 0.0298\\npclass=2nd * age -0.5446 0.2653 -2.05 0.0401\\npclass=3rd * age -0.1634 0.1308 -1.25 0.2118\\npclass=2nd * age’ 1.9156 1.0189 1.88 0.0601\\npclass=3rd * age’ 0.8205 0.6091 1.35 0.1780\\npclass=2nd * age” -8.9545 5.5027 -1.63 0.1037\\npclass=3rd * age” -5.4276 3.6475 -1.49 0.1367\\npclass=2nd * age”’ 9.3926 6.9559 1.35 0.1769\\npclass=3rd * age”’ 7.5403 4.8519 1.55 0.1202\\nage * sibsp 0.0357 0.0340 1.05 0.2933\\nage’ * sibsp -0.0467 0.2213 -0.21 0.8330\\nage”* sibsp 0.5574 1.6680 0.33 0.7382\\nage”’ * sibsp -1.1937 2.5711 -0.46 0.6425\\nlatex(anova(f),file= '',label= 'titanic-anova2 ',size= 'small ')\\n#12.2\\nThisisaverypowerfulmodel(ROCarea= c=0.88);thesurvivalpatterns\\nareeasytodetect.TheWaldANOVAinTable 12.2indicatesespeciallystrong\\nsex and pclasseﬀects (χ2= 199and 109,respectively).There is a very strong\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7814be4b-8288-4a19-8b0f-4d93e017e289', embedding=None, metadata={'page_label': '299', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Binary Logistic Model With Casewise Deletion of Missing Values 299\\nTable 12.2 Wald Statistics for survived\\nχ2d.f.P\\nsex (Factor+Higher Order Factors) 199 .42 7<0.0001\\nAll Interactions 56.14 6<0.0001\\npclass (Factor+Higher Order Factors) 108 .73 12<0.0001\\nAll Interactions 42.83 10<0.0001\\nage (Factor+Higher Order Factors) 47 .04 20 0.0006\\nAll Interactions 24.51 16 0.0789\\nNonlinear (Factor+Higher Order Factors) 22.72 15 0.0902\\nsibsp (Factor+Higher Order Factors) 19 .95 5 0.0013\\nAll Interactions 10.99 4 0.0267\\nsex×pclass (Factor+Higher Order Factors) 35 .40 2<0.0001\\nsex×age (Factor+Higher Order Factors) 10 .08 4 0.0391\\nNonlinear 8.17 3 0.0426\\nNonlinear Interaction : f(A,B) vs. AB 8.17 3 0.0426\\npclass×age (Factor+Higher Order Factors) 6 .86 8 0.5516\\nNonlinear 6.11 6 0.4113\\nNonlinear Interaction : f(A,B) vs. AB 6.11 6 0.4113\\nage×sibsp (Factor+Higher Order Factors) 10 .99 4 0.0267\\nNonlinear 1.81 3 0.6134\\nNonlinear Interaction : f(A,B) vs. AB 1.81 3 0.6134\\nTOTAL NONLINEAR 22 .72 15 0.0902\\nTOTAL INTERACTION 67 .58 18<0.0001\\nTOTAL NONLINEAR + INTERACTION 70 .68 21<0.0001\\nTOTAL 253.18 26<0.0001\\nsex×pclassinteraction and a strong age ×sibspinteraction, considering\\nthe strength of sibspoverall.\\nLet us examine the shapes of predictor eﬀects. With so many interactions\\nin the model we need to obtain predicted values at least for all combinations\\nof sex and pclass.F o rsibspwe consider only two of its possible values.\\np←Predict(f, age, sex, pclass, sibsp=0, fun=plogis)\\nggplot(p) # Fig. 12.5\\nNote the agreement between the lower right-hand panel of Figure 12.3with\\nFigure12.5. This results from our use of similar ﬂexibility in the parametric\\nandnonparametricapproaches(andsimilareﬀectivedegreesoffreedom).The\\nestimated eﬀect of sibspas a function of age is shown in Figure 12.6.\\nggplot( Predict(f, sibsp, age=c(10,15,20 ,50),conf.int= FALSE))\\n## Figure 12.6\\nNote that children having many siblings apparently had lower survival. Mar-\\nried adults had slightly higher survival than unmarried ones.\\nThere will never be another Titanic, so we do not need to validate the\\nmodel for prospective use. But we use the bootstrap to validate the model\\nanyway, in an eﬀort to detect whether it is overﬁtting the data. We do not\\npenalizethe calculationsthatfollowforhavingexaminedtheeﬀect of parchor', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='db425975-254d-4121-acd1-1df17ce90e59', embedding=None, metadata={'page_label': '300', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"300 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\n1st 2nd 3rd\\n0.000.250.500.751.00\\n02 0 4 0 6 002 0 4 0 6 00 2 04 06 0\\nAge, yearssex\\nfemale\\nmale\\nFig. 12.5 Eﬀects of predictors on probability of survival of Titanic passengers, esti-\\nmated for zero siblings or spouses\\n−6−5−4−3−2−10\\n02468\\nNumber of Siblings/Spouses Aboardlog oddsAge, years\\n10\\n15\\n20\\n50\\nFig. 12.6 Eﬀect of number of siblings and spouses on the log odds of surviving, for\\nthird class males\\nfortestingthree-wayinteractions,inthebeliefthatthesetestswouldreplicate\\nwell.\\nf←update(f, x= TRUE, y= TRUE)\\n# x=TRUE, y= TRUE adds raw data to fit object so can bootstrap\\nset.seed (131) # so can replicate re-samples\\nlatex(validate(f, B =200), digits=2, size= 'Ssize ')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f603970a-965d-40de-8b3f-4c462c4cda76', embedding=None, metadata={'page_label': '301', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.3 Binary Logistic Model With Casewise Deletion of Missing Values 301\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.76 0.77 0.74 0 .03 0 .72 200\\nR20.55 0.58 0.53 0 .05 0 .50 200\\nIntercept 0 .00 0.00−0.08 0 .08−0.08 200\\nSlope 1 .00 1.00 0.87 0 .13 0 .87 200\\nEmax 0.00 0.00 0.05 0 .05 0 .05 200\\nD 0.53 0.56 0.50 0 .06 0 .46 200\\nU 0.00 0.00 0.01−0.01 0 .01 200\\nQ 0.53 0.56 0.49 0 .07 0 .46 200\\nB 0.13 0.13 0.13−0.01 0 .14 200\\ng 2.43 2.75 2.37 0 .37 2 .05 200\\ngp 0.37 0.37 0.35 0 .02 0 .35 200\\ncal←calibrate(f, B =200) # Figure 12.7\\nplot(cal, subtitles= FALSE)\\nn=1046 Mean absolute error=0.009 Mean squared error=0.00012\\n0.9 Quantile of absolute error=0.017\\nThe output of validate indicates minor overﬁtting. Overﬁtting would have\\nbeen worse had the risk factors not been so strong. The closeness of the cali-\\nbration curve to the 45◦line in Figure 12.7demonstrates excellent validation\\non an absolute probability scale. But the extent of missing data casts some\\ndoubt on the validity of this model, and on the eﬃciency of its parameter\\nestimates.\\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nPredicted Pr{survived=1}Actual Probability\\nApparent\\nBias−corrected\\nIdeal\\nFig. 12.7 Bootstrap overﬁtting-corrected loess nonparametric calibration curve for\\ncasewise deletion model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9bdffd6f-3709-489d-8159-cc722fa5a031', embedding=None, metadata={'page_label': '302', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"302 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\n12.4 Examining Missing Data Patterns\\nThe ﬁrst step to dealing with missing data is understanding the patterns\\nof missing values. To do this we use the Hmisclibrary’s naclusandnaplot\\nfunctions, and the recursive partitioning library of Atkinson and Therneau.\\nBelownaclustells us which variables tend to be missing on the same persons,\\nand it computes the proportionof missing values for each variable. The rpart\\nfunction derives a tree to predict which types of passengers tended to have\\nage missing.\\nna.patterns ←naclus( titanic3)\\nrequire( rpart) # Recursive partitioning package\\nwho.na ←rpart(is.na(age) ∼sex + pclass + survived +\\nsibsp + parch, data=titanic3, minbucket =15)\\nnaplot(na.patterns, 'na per var ')\\nplot(who.na, margin=.1); text(who.na) # Figure 12.8\\nplot(na.patterns)\\nWe see in Figure 12.8that age tends to be missing on the same passengers\\nas the body bag identiﬁer, and that it is missing in only 0.09 of ﬁrst or sec-\\nond class passengers. The category of pa ssengers having the highest fraction\\nof missing ages is third class passengers having no parents or children on\\nboard. Below we use Hmisc’ssummary.formula function to plot simple descrip-\\ntive statistics on the fractionof missing ages, stratiﬁedby other variables.We\\nsee that without adjusting for other variables, age is slightly more missing on\\nnonsurviving passengers.\\nplot(summary( is.na(age) ∼sex + pclass + survived +\\nsibsp + parch, data=t3)) # Figure 12.9\\nLet us derive a logistic model to predict missingness of age, to see if the\\nsurvival bias maintains after adjustment for the other variables.\\nm←lrm(is.na(age) ∼sex * pclass + survived + sibsp + parch,\\ndata=t3)\\nprint(m, latex=TRUE, needspace= '2in')\\nLogistic Regression Model\\nlrm(formula = is.na(age) ~ sex * pclass + survived + sibsp +\\nparch, data = t3)\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 1309 LRχ2114.99R20.133C 0.703\\nFALSE 1046 d.f. 8 g 1.015Dxy0.406\\nTRUE 263 Pr(>χ2)<0.0001gr2.759γ 0.452\\nmax|∂logL\\n∂β|5×10−6gp0.126τa0.131\\nBrier 0.148\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='daaaf6a2-16bb-4e3b-b790-a6b156a0c1fe', embedding=None, metadata={'page_label': '303', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12.4 Examining Missing Data Patterns 303\\nFraction of NAs in each Variable\\nFraction of NAs0.0 0.2 0.4 0.6 0.8pclass\\nsurvived\\nname\\nsex\\nsibsp\\nparch\\nticket\\ncabin\\nboat\\nfare\\nembarked\\nage\\nhome.dest\\nbody|pclass=ab\\nparch>=0.5\\n0.09167\\n0.1806 0.3249boat\\nembarked\\ncabin\\nfare\\nticket\\nparch\\nsibsp\\nage\\nbody\\nhome.dest\\nsex\\nname\\npclass\\nsurvived\\n0.40.30.20.10.0Fraction Missing\\nFig. 12.8 Patterns of missing data. Upper left panel shows the fraction of observa-\\ntions missing on each predictor. Lower panel depicts a hierarchical cluster analysis o f\\nmissingness combinations. The similarity measure shown on the Y- a x i si st h ef r a c -\\ntion of observations for which both variables are missing. Right panel shows the r esult\\nof recursive partitioning for predicting is.na(age) .T h erpartfunction found only\\nstrong patterns according to passenger class.\\nCoef S.E. Wald ZPr(>|Z|)\\nIntercept -2.2030 0.3641 -6.05 <0.0001\\nsex=male 0.6440 0.3953 1.63 0.1033\\npclass=2nd -1.0079 0.6658 -1.51 0.1300\\npclass=3rd 1.6124 0.3596 4.48 <0.0001\\nsurvived -0.1806 0.1828 -0.99 0.3232\\nsibsp 0.0435 0.0737 0.59 0.5548\\nparch -0.3526 0.1253 -2.81 0.0049\\nsex=male * pclass=2nd 0.1347 0.7545 0.18 0.8583\\nsex=male * pclass=3rd -0.8563 0.4214 -2.03 0.0422\\nlatex(anova(m), file= '', label= 'titanic-anova.na ')\\n# Table 12.3\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d5f13c0e-5081-448b-aed2-cdbcbfc09538', embedding=None, metadata={'page_label': '304', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='304 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\nis.na(age)0.0 0.2 0.4 0.6 0.8 1.0130922668113170100296222042319891500809709277323843466N\\nfemale\\nmale\\n1st\\n2nd\\n3rd\\nNo\\nYes\\n0\\n1\\n2\\n3\\n4\\n5\\n8\\n0\\n1\\n2\\n3\\n4\\n5\\n6\\n9sex\\npclass\\nSurvived\\nNumber of Siblings/Spouses Aboard\\nNumber of Parents/Children Aboard\\nOverallmean\\nFig. 12.9 Univariable descriptions of proportion of passengers with missing age\\nFortunately, after controlling for other variables, Table 12.3provides evi-\\ndence that nonsurviving passengers are no more likely to have age missing.\\nThe only important predictors of missingness are pclassandparch(the more\\nparents or children the passenger has on board, the less likely age was to be\\nmissing).\\n12.5 Multiple Imputation\\nMultiple imputation is expected to reduce bias in estimates as well as to\\nprovide an estimate of the variance–covariance matrix of ˆβpenalized for im-\\nputation. With multiple imputation, survival status can be used to impute\\nmissing ages, so the age relationship will not be as attenuated as with single\\nconditional mean imputation. aregImpute The following uses the Hmisc pack-\\nagearegImpute function to do predictive mean matching, using van Buuren’s\\n“Type 1” matching [ 85, Section 3.4.2] in conjunction with bootstrapping to\\nincorporate all uncertainties, in the context of smooth additive imputation', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='73e6950b-3169-4139-9ca7-df6061650455', embedding=None, metadata={'page_label': '305', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='12.5 Multiple Imputation 305\\nTable 12.3 Wald Statistics for is.na(age)\\nχ2d.f.P\\nsex (Factor+Higher Order Factors) 5 .61 3 0.1324\\nAll Interactions 5.58 2 0.0614\\npclass (Factor+Higher Order Factors) 68 .43 4<0.0001\\nAll Interactions 5.58 2 0.0614\\nsurvived 0.98 1 0.3232\\nsibsp 0.35 1 0.5548\\nparch 7.92 1 0.0049\\nsex×pclass (Factor+Higher Order Factors) 5 .58 2 0.0614\\nTOTAL 82 .90 8<0.0001\\nmodels. Sampling of donors is handled by distance weighting to yield better\\ndistributions of imputed values. By default, aregImpute does not transform\\nagewhen it is being predicted from the other v ariables. Four knots are used\\nto transform agewhen used to impute other variables (not needed here as no\\nother missings were present in the variables of interest). Since the fraction of\\nobservations with missing age is263\\n1309=0.2 we use 20 imputations.\\nset.seed (17) # so can reproduce random aspects\\nmi←aregImpute( ∼age + sex + pclass +\\nsibsp + parch + survived,\\ndata=t3, n.impute=20, nk=4, pr= FALSE)\\nmi\\nMultiple Imputation usingBootstrap and PMM\\naregImpute( formula = ∼age + sex + pclass + sibsp + parch + survived,\\ndata = t3, n.impute = 20, nk = 4, pr = FALSE)\\nn: 1309 p: 6 Imputations: 20 nk: 4\\nNumber of NAs:\\nage sex pclass sibsp parch survived\\n26300000\\ntyped.f.\\nage s 1\\nsex c 1\\npclass c 2\\nsibsp s 2\\nparch s 2\\nsurvived l 1\\nTransformation of Target Variables Forced to be Linear\\nR-squares for Predicting Non- Missing Values for Each Variable\\nUsing Last Imputations of Predictors\\nage\\n0.295', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eaa9f985-5fd4-4625-90b3-2ec140b7f192', embedding=None, metadata={'page_label': '306', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"306 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\n# Print the first 10 imputations for the first 10 passengers\\n# having missing age\\nmi$imputed$age [1:10, 1:10]\\n[,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\\n16 40 49 24 29 60.0 58 64 36 50 61\\n38 33 45 40 49 80.0 2 38 38 36 53\\n41 29 24 19 31 40.0 60 64 42 30 65\\n47 40 42 29 48 36.0 46 64 30 38 42\\n60 52 40 22 31 38.0 22 19 24 40 33\\n70 16 14 23 23 18.0 24 19 27 59 23\\n71 30 62 57 30 42.0 31 64 40 40 63\\n75 43 23 36 61 45.5 58 64 27 24 50\\n81 44 57 47 31 45.0 30 64 62 39 67\\n107 52 18 24 62 32.5 38 64 47 19 23\\nplot(mi)\\nEcdf(t3$age, add= TRUE, col= 'gray ', lwd=2,\\nsubtitles= FALSE)#Fig.12.10\\n0 2 04 06 08 00.00.20.40.60.81.0\\nImputed ageProportion <= x\\nFig. 12.10 Distributionsof imputedandactual ages fortheTitanicdataset.Imputed\\nvalues are in black and actual ages in gray.\\nWe now ﬁt logistic models for ﬁve completed datasets. The fit.mult.impute\\nfunction ﬁts ﬁve models and examines the within– and between–imputation\\nvariances to compute an imputation-corrected variance–covariance matrix\\nthat is stored in the ﬁt object f.mi.fit.mult.impute will also average the ﬁve\\nˆβvectors, storing the result in f.mi$coefficients . The function also prints\\nthe ratio of imputation-corrected variances to average ordinary variances.\\nf.mi←fit.mult.impute(\\nsurvived ∼(sex + pclass + rcs(age,5))∧2+\\nrcs(age,5)*sibsp,\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c0160491-3512-4017-8d47-3fbd639721b0', embedding=None, metadata={'page_label': '307', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12.6 Summarizing the Fitted Model 307\\nTable 12.4 Wald Statistics for survived\\nχ2d.f.P\\nsex (Factor+Higher Order Factors) 240 .42 7<0.0001\\nAll Interactions 54.56 6<0.0001\\npclass (Factor+Higher Order Factors) 114 .21 12<0.0001\\nAll Interactions 36.43 10 0.0001\\nage (Factor+Higher Order Factors) 50 .37 20 0.0002\\nAll Interactions 25.88 16 0.0557\\nNonlinear (Factor+Higher Order Factors) 24.21 15 0.0616\\nsibsp (Factor+Higher Order Factors) 24 .22 5 0.0002\\nAll Interactions 12.86 4 0.0120\\nsex×pclass (Factor+Higher Order Factors) 30 .99 2<0.0001\\nsex×age (Factor+Higher Order Factors) 11 .38 4 0.0226\\nNonlinear 8.15 3 0.0430\\nNonlinear Interaction : f(A,B) vs. AB 8.15 3 0.0430\\npclass×age (Factor+Higher Order Factors) 5 .30 8 0.7246\\nNonlinear 4.63 6 0.5918\\nNonlinear Interaction : f(A,B) vs. AB 4.63 6 0.5918\\nage×sibsp (Factor+Higher Order Factors) 12 .86 4 0.0120\\nNonlinear 1.84 3 0.6058\\nNonlinear Interaction : f(A,B) vs. AB 1.84 3 0.6058\\nTOTAL NONLINEAR 24 .21 15 0.0616\\nTOTAL INTERACTION 67 .12 18<0.0001\\nTOTAL NONLINEAR + INTERACTION 70 .99 21<0.0001\\nTOTAL 298.78 26<0.0001\\nlrm, mi, data=t3, pr=FALSE)\\nlatex(anova(f.mi), file= '', label= 'titanic-anova.mi ',\\nsize= 'small ')# Table 12.4\\nThe Wald χ2for age is reduced by accounting for imputation but is in-\\ncreased (by a lesser amount) by using patterns of association with survival\\nstatus to impute missing age. The Wald tests are all adjusted for multiple im-\\nputation. Now examine the ﬁtted age relationship using multiple imputation\\nvs. casewise deletion.\\np1←Predict(f, age, pclass, sex, sibsp=0, fun=plogis)\\np2←Predict( f.mi, age, pclass, sex, sibsp=0, fun=plogis)\\np←rbind( 'Casewise Deletion '=p1, 'Multiple Imputation '=p2)\\nggplot(p, groups= 'sex', ylab= 'Probability of Surviving ')\\n# Figure 12.11\\n12.6 Summarizing the Fitted Model\\nIn this section we depict the model ﬁtted using multiple imputation, by com-\\nputing odds ratiosand by showingvarious predicted values. For age,the odds\\nratio for an increase from 1 year old to 30 years old is computed, instead of\\nthe default odds ratio based on outer quartiles of age. The estimated odds\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1059c3bc-816d-487c-b128-f869386b0a5b', embedding=None, metadata={'page_label': '308', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"308 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\nCasewise Deletion Multiple Imputation\\n0.000.250.500.751.00\\n0.000.250.500.751.00\\n0.000.250.500.751.001st 2nd 3rd\\n02 0 4 0 6 0 02 0 4 0 6 0\\nAge, yearsProbability of Survivingsex\\nfemale\\nmale\\nFig. 12.11 Predicted probabilityofsurvivalformales fromﬁtusingcasewisedeletion\\nagain (top) and multiple random draw imputation (bottom). Both sets of predictions\\nare forsibsp=0.\\nratios are very dependent on the levels of interacting factors, so Figure 12.12\\ndepicts only one of many patterns.\\n# Get predicted values for certain types of passengers\\ns←summary( f.mi, age=c(1,30), sibsp=0:1)\\n# override default ranges for 3 variables\\nplot(s, log= TRUE,main= '') # Figure 12.12\\nNow compute estimated probabilities of survival for a variety of settings of\\nthe predictors.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b057a62d-077a-46a6-86f7-7385f605cdf6', embedding=None, metadata={'page_label': '309', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"12.6 Summarizing the Fitted Model 309\\n0.10 0.50 2.00 5.00\\nage − 30:1\\nsibsp − 1:0\\nsex − female:male\\npclass − 1st:3rd\\npclass − 2nd:3rd\\nAdjusted to:sex=male pclass=3rd age=28 sibsp=0 \\nFig. 12.12 Odds ratios for some predictor settings\\nphat←predict( f.mi,\\ncombos ←\\nexpand.grid(age=c(2,21 ,50),sex= levels(t3$sex),\\npclass=levels(t3$pclass),\\nsibsp=0), type= 'fitted ')\\n# Can also use Predict( f.mi, age=c(2,21 ,50), sex, pclass,\\n# sibsp=0, fun=plogis) $yhat\\noptions( digits=1)\\ndata.frame( combos, phat)\\nage sex pclass sibsp phat\\n1 2 female 1st 0 0.97\\n2 21 female 1st 0 0.98\\n3 50 female 1st 0 0.97\\n4 2 male 1st 0 0.88\\n5 21 male 1st 0 0.48\\n6 50 male 1st 0 0.27\\n7 2 female 2nd 0 1.00\\n8 21 female 2nd 0 0.90\\n9 50 female 2nd 0 0.82\\n10 2 male 2nd 0 1.00\\n11 21 male 2nd 0 0.08\\n12 50 male 2nd 0 0.04\\n13 2 female 3rd 0 0.85\\n14 21 female 3rd 0 0.57\\n15 50 female 3rd 0 0.37\\n16 2 male 3rd 0 0.91\\n17 21 male 3rd 0 0.13\\n18 50 male 3rd 0 0.06\\noptions( digits=5)\\nWe can also get predicted values by creating an Rfunction that will evaluate\\nthe model on demand.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='108a40ce-18ac-4f85-9125-89f6845964ab', embedding=None, metadata={'page_label': '310', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"310 12 Logistic Model Case Study 2: Survival of Titanic Passengers\\npred.logit ←Function( f.mi)\\n# Note: if don 't define sibsp to pred.logit, defaults to 0\\n# normally just type the function name to see its body\\nlatex(pred.logit, file= '', type= 'Sinput ', size= 'small ',\\nwidth.cutoff =49)\\npred.logit ←f u n c t i o n ( s e x=” m a l e ” ,p c l a s s=” 3 r d ” ,\\nage = 28, sibsp = 0)\\n{\\n3.2427671 −0.95431809 ∗(sex = = ”male”) + 5.4086505 ∗\\n(pclass = = ”2nd”) −1.3378623 ∗(pclass = =\\n”3rd”) + 0.091162649 ∗age−0.00031204327 ∗\\npmax(age −6, 0)∧3 + 0.0021750413 ∗pmax(age −\\n21, 0)∧3−0.0027627032 ∗pmax(age −27, 0)∧3+\\n0.0009805137 ∗pmax(age −36, 0)∧3−8.0808484e −05∗\\npmax(age −55.8 , 0)∧3−1.1567976 ∗sibsp +\\n(sex == ”male”) ∗(−0.46061284 ∗(pclass = =\\n”2nd”) + 2.0406523 ∗(pclass = = ”3rd”)) +\\n(sex == ”male”) ∗(−0.22469066 ∗age + 0.00043708296 ∗\\npmax(age −6, 0)∧3−0.0026505136 ∗pmax(age −\\n21, 0)∧3 + 0.0031201404 ∗pmax(age −27,\\n0)∧3−0.00097923749 ∗pmax(age −36,\\n0)∧3 + 7.2527708e −05∗pmax(age −55.8 ,\\n0)∧3) + (pclass == ”2nd”) ∗(−0.46144083 ∗\\nage + 0.00070194849 ∗pmax(age −6, 0)∧3−\\n0.0034726662 ∗pmax(age −21, 0)∧3 + 0.0035255387 ∗\\npmax(age −27, 0)∧3−0.0007900891 ∗pmax(age −\\n36, 0)∧3 + 3.5268151e −05∗pmax(age −55.8 ,\\n0)∧3) + (pclass == ”3rd”) ∗(−0.17513289 ∗\\nage + 0.00035283358 ∗pmax(age −6, 0)∧3−\\n0.0023049372 ∗pmax(age −21, 0)∧3 + 0.0028978962 ∗\\npmax(age −27, 0)∧3−0.00105145 ∗pmax(age −\\n36, 0)∧3 + 0.00010565735 ∗pmax(age −55.8 ,\\n0)∧3) + sibsp ∗(0.040830773 ∗age−1.5627772e −05∗\\npmax(age −6, 0)∧3 + 0.00012790256 ∗pmax(age −\\n21, 0)∧3−0.00025039385 ∗pmax(age −27,\\n0)∧3 + 0.00017871701 ∗pmax(age −36, 0)∧3−\\n4.0597949e −05∗pmax(age −55.8 , 0)∧3)\\n}\\n# Run the newly created function\\nplogis( pred.logit(age=c(2,21 ,50), sex= 'male ', pclass= '3rd'))\\n[1] 0.914817 0.132640 0.056248\\nA nomogram could be used to obtain predicted values manually, but this is\\nnot feasible when so many interaction terms are present.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0e51add-45da-43af-a268-b27e1b65678f', embedding=None, metadata={'page_label': '311', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 13\\nOrdinal Logistic Regression\\n13.1 Background\\nMany medical and epidemiologic studies incorporate an ordinal response\\nvariable. In some cases an ordinal response Yrepresents levels of a standard\\nmeasurement scale such as severity of pain (none, mild, moderate, severe).\\nIn other cases, ordinal responses are constructed by specifying a hierarchy\\nof separate endpoints. For example, clinicians may specify an ordering of\\nthe severity of several component events and assign patients to the worst\\nevent present from among none, heart attack, disabling stroke, and death.\\nStill another use of ordinal response methods is the application of rank-based\\nmethods to continuous responses so as to obtain robust inferences. For ex-\\nample, the proportional odds model described later allows for a continuous\\nYand is really a generalization of the Wilcoxon–Mann–Whitney rank test.\\nThus the semiparametric proportional odds model is a direct competitor of\\nordinary linear models.\\nThere are many variations of logistic models used for predicting an ordinal\\nresponse variable Y. All of them have the advantage that they do not assume\\na spacing between levels of Y. In other words, the same regressioncoeﬃcients\\nandP-valuesresultfromananalysisofaresponsevariablehavinglevels0 ,1,2\\nwhen the levels are recoded 0 ,1,20. Thus ordinal models use only the rank-\\nordering of values of Y.\\nInthischapterweconsidertwoofthemostpopularordinallogisticmodels,\\nthe proportional odds (PO) form of an ordinal logistic model647and the for-\\nwardcontinuationratio(CR) ordinallogisticmodel.190Chapter 15dealswith\\na wider variety of ordinal models with emphasis on analysis of continuous Y. 1\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 13311', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4a33a907-e6f3-4c48-be37-3fdf3601c92c', embedding=None, metadata={'page_label': '312', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='312 13 Ordinal Logistic Regression\\n13.2 Ordinality Assumption\\nAbasicassumptionofallcommonlyusedordinalregressionmodelsisthatthe\\nresponsevariablebehavesin anordinalfashionwith respecttoeachpredictor.\\nAssuming that a predictor Xis linearly related to the log odds of some\\nappropriate event, a simple way to check for ordinality is to plot the mean\\nofXstratiﬁed by levels of Y. These means should be in a consistent order.\\nIf for many of the Xs, two adjacent categories of Ydo not distinguish the\\nmeans, that is evidence that those levels of Yshould be pooled.\\nOne can also estimate the mean or expected value of X|Y=j(E(X|Y=\\nj)) given that the ordinal model assumptions hold. This is a useful tool for\\nchecking those assumptions, at least in an unadjusted fashion. For simplicity,\\nassume that Xis discrete, and let Pjx=P r (Y=j|X=x) be the probability\\nthatY=jgivenX=xthat is dictated from the model being ﬁtted, with\\nXbeing the only predictor in the model. Then\\nPr(X=x|Y=j)=P r (Y=j|X=x)Pr(X=x)/Pr(Y=j)\\nE(X|Y=j)=∑\\nxxPjxPr(X=x)/Pr(Y=j), (13.1)\\nand the expectation can be estimated by\\nˆE(X|Y=j)=∑\\nxxˆPjxfx/gj, (13.2)\\nwhereˆPjxdenotes the estimate of Pjxfrom the ﬁtted one-predictor model\\n(for inner values of Yin the PO models, these probabilities are diﬀerences\\nbetween terms given by Equation 13.4below),fxis the frequency of X=x\\nin the sample of size n,a n dgjis the frequency of Y=jin the sample. This\\nestimate can be computed conveniently without grouping the data by X.F o r\\nnsubjects let the nvalues of Xbex1,x2,...,x n.T h e n\\nˆE(X|Y=j)=n∑\\ni=1xiˆPjxi/gj. (13.3)\\nNote that if one were to compute diﬀerences between conditional means of X\\nand the conditionalmeans of Xgiven PO,and if furthermorethe means were\\nconditioned on Y≥jinstead of Y=j, the result would be proportional to\\nmeans of score residuals deﬁned later in Equation 13.6.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='99092198-6b54-4e46-b5cb-3ecc4c5a07cd', embedding=None, metadata={'page_label': '313', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3 Proportional Odds Model 313\\n13.3 Proportional Odds Model\\n13.3.1 Model\\nThe most commonly used ordinal logistic model was described in Walker\\nand Duncan647and later called the proportional odds (PO) model by Mc-\\nCullagh.449The PO model is best stated as follows, for a response variable\\nhaving levels 0 ,1,2,...,k:\\nPr[Y≥j|X]=1\\n1+exp[−(αj+Xβ)], (13.4)\\nwherej=1,2,...,k. Some authors write the model in terms of Y≤j.O u r\\nformulation makes the model coeﬃcients consistent with the binary logistic\\nmodel. There are kintercepts ( αs). For ﬁxed j, the model is an ordinary\\nlogistic model for the event Y≥j. By using a common vector of regression\\ncoeﬃcients βconnecting probabilities for varying j, the PO model allows for\\nparsimonious modeling of the distribution of Y. 2\\nThere is a nice connection between the PO model and the Wilcoxon–\\nMann–Whitney two-sample test: when there is a single predictor X1that is\\nbinary, the numerator of the score test for testing H0:β1= 0 is proportional\\nto the two-sample test statistic [ 664, pp. 2258–2259].\\n13.3.2 Assumptions and Interpretation of Parameters\\nThere is an implicit assumption in the PO model that the regression coef-\\nﬁcients ( β) are independent of j, the cutoﬀ level for Y. One could say that\\nthere is no X×Yinteractionif PO holds. For a speciﬁc Y-cutoﬀj, the model\\nhas the same assumptions as the binary logistic model (Section 10.1.1). That\\nis, the model in its simplest form assumes the log odds that Y≥jis linearly\\nrelated to each Xand that there is no interaction between the Xs.\\nIn designing clinical studies, one sometimes hears the statement that an\\nordinal outcome should be avoided since statistical tests of patterns of those\\noutcomes arehard to interpret.In f act, one interprets eﬀects in the POmodel\\nusing ordinary odds ratios. The diﬀerence is that a single odds ratio is as-\\nsumed to apply equally to alleventsY≥j,j=1,2,...,k. If linearity and\\nadditivity hold, the Xm+1:Xmodds ratio for Y≥jis exp(βm), whatever\\nthe cutoﬀ j.\\nThe proportionalhazardsassumption is frequently violated,just as the as-\\nsumptions of normalityof residualswith equal variancein ordinaryregression\\nare frequently violated, but the PO model can still be useful and powerful in\\nthis situation. As stated by Senn and Julious564,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3e120ce-8357-4cbd-bc90-fa23d4a20d47', embedding=None, metadata={'page_label': '314', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='314 13 Ordinal Logistic Regression\\nClearly, the dependence of the proportional odds model on the assumption\\nof proportionality can be over-stressed. Suppose that two diﬀerent statisticians\\nwould cutthesame three-pointscale atdiﬀerentcutpoints.Itis hard toseeh ow\\nanybody who could accept either dichotomy could object to the compromise\\nanswer produced by the proportional odds model.\\nSometimes it helps in interpreting the model to estimate the mean Yas\\na function of one or more predictors, even though this assumes a spacing for\\ntheY-levels. 3\\n13.3.3 Estimation\\nThePOmodelisﬁttedusingMLEonasomewhatcomplexlikelihoodfunction\\nthatisdependentondiﬀerencesinlogisticmodelprobabilities.Theestimation\\nprocess forces the αs to be in descending order.\\n13.3.4 Residuals\\nSchoenfeld residuals557are very eﬀective233in checking the proportional haz-\\nards assumption in the Cox132survival model. For the PO model one could\\nanalogously compute each subject’s contribution to the ﬁrst derivative of\\nthe log likelihood function with respect to βm, average them separately by\\nlevels of Y, and examine trends in the residual plots as in Section 20.6.2.\\nA few examples have shown that such plots are usually hard to interpret.\\nEasily interpreted score residual plots for the PO model can be constructed,\\nhowever, by using the ﬁtted PO model to predict a series of binary events\\nY≥j,j=1,2,...,k, using the corresponding predicted probabilities\\nˆPij=1\\n1+exp[−(ˆαj+Xiˆβ)], (13.5)\\nwhereXistands for a vector of predictors for subject i. Then, after forming\\nan indicator variable for the event currently being predicted ([ Yi≥j]), one\\ncomputesthescore(ﬁrstderivative)components Uimfromanordinarybinary\\nlogistic model:\\nUim=Xim([Yi≥j]−ˆPij), (13.6)\\nfor the subject iand predictor m. Then, for each column of U,p l o tt h em e a n\\n¯U·mand conﬁdence limits, with Y(i.e.,j)o nt h e x-axis. For each predictor\\nthe trend against jshould be ﬂat if PO holds.aIn binary logistic regression,\\npartial residuals are very useful as they allow the analyst to ﬁt linear eﬀects\\naIfˆβwere derived from separate binary ﬁts, all ¯U·m≡0.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='491e00e9-131c-4d49-a89e-15929b32fe16', embedding=None, metadata={'page_label': '315', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.3 Proportional Odds Model 315\\nfor all the predictors but then to nonparametrically estimate the true trans-\\nformation that each predictor requires (Section 10.4). The partial residual is\\ndeﬁned as follows, for the ith subject and mth predictor variable.115,373\\nrim=ˆβmXim+Yi−ˆPi\\nˆPi(1−ˆPi), (13.7)\\nwhere\\nˆPi=1\\n1+exp[−(α+Xiˆβ)]. (13.8)\\nA smoothed plot (e.g., using the moving linear regression algorithm in\\nloess111)o fXimagainstrimprovides a nonparametric estimate of how Xm\\nrelates to the log relative odds that Y=1|Xm. For ordinal Y, we just need\\nto compute binary model partial residuals for all cutoﬀs j:\\nrim=ˆβmXim+[Yi≥j]−ˆPij\\nˆPij(1−ˆPij), (13.9)\\nthen to make a plot for each mshowing smoothed partial residual curves for\\nallj, looking for similar shapes and slopes for a given predictor for all j.E a c h\\ncurve provides an estimate of how Xmrelates to the relative log odds that\\nY≥j. Since partial residuals allow examination of predictor transformations\\n(linearity) while simultaneously allowing examination of PO (parallelism),\\npartial residual plots are g enerally preferred over score residual plots for or-\\ndinal models.\\nLi and Shepherd402have a residual for ordinal models that serves for the\\nentire range of Ywithout the need to consider cutoﬀs. Their residual is use-\\nful for checking functional form of predictors but not the proportional odds\\nassumption.\\n13.3.5 Assessment of Model Fit\\nPeterson and Harrell502developed score and likelihood ratio tests for testing\\nthe PO assumption. The score test is used in the SAS PROC LOGISTIC,540\\nbut its extreme anti-conservatism in many cases can make it unreliable.5024\\nFor determining whether the PO assumption is likely to be satisﬁed for\\neachpredictorseparately,thereareseveralgraphicsthatareuseful.Oneisthe\\ngraph comparing means of X|Ywith and without assuming PO, as described\\ninSection 13.2(seeFigure 14.2foranexample).Anotheristhesimplemethod\\nof stratifying on each predictor and computing the logits of all proportions of\\nthe form Y≥j,j=1,2,...,k. When proportionalodds holds, the diﬀerences\\nin logits between diﬀerent values of jshould be the same at all levels of X,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9edd200a-327b-4650-97b8-190f74d9dbef', embedding=None, metadata={'page_label': '316', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"316 13 Ordinal Logistic Regression\\nbecause the model dictates that logit( Y≥j|X)−logit(Y≥i|X)=αj−αi,\\nfor any constant X. An example of this is in Figure 13.1.\\nrequire( Hmisc)\\ngetHdata(support)\\nsfdm←as.integer(support$ sfdm2) - 1\\nsf←function(y)\\nc('Y≥1'=qlogis(mean(y ≥1)), 'Y≥2'=qlogis(mean(y ≥2)),\\n'Y≥3'=qlogis(mean(y ≥3)))\\ns←summary( sfdm∼adlsc + sex + age + meanbp, fun=sf,\\ndata=support)\\nplot(s, which =1:3, pch=1:3, xlab= 'logit ', vnames= 'names ',\\nmain= '', width.factor=1.5) # Figure 13.1\\nlogit−0.5 0.0 0.5 1.0 1.5841210204216211210210210211464377210199150282N\\n0.000\\nfemale\\nmale[0.495,1.167)\\n[1.167,3.024)\\n[3.024,7.000]\\n[19.8, 52.4)\\n[52.4, 65.3)\\n[65.3, 74.8)\\n[74.8,100.1]\\n[  0, 64)\\n[ 64, 78)\\n[ 78,108)\\n[108,180]adlsc\\nsex\\nage\\nmeanbp\\nOverall\\nFig. 13.1 Checking PO assumption separately for a series of predictors. The circle,\\ntriangle, and plus sign correspond to Y≥1,2,3, respectively. PO is checked by\\nexamining the vertical constancy of distances between any two of these three symbols.\\nResponse variable is the severe functional disability scale sfdm2from the 1000-patient\\nSUPPORT dataset, with the last two categories combined because of low frequenc y\\nof coma/intubation.\\nWhenYis continuous or almost continuous and Xis discrete, the PO model\\nassumes that the logit of the cumulative distribution function of Yis parallel\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e10bbc99-c5f1-478e-8373-d27e1c630cbf', embedding=None, metadata={'page_label': '317', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"13.3 Proportional Odds Model 317\\nacross categories of X. The corresponding, more rigid, assumptions of the\\nordinary linear model (here, parametric ANOVA) are parallelism and linear-\\nity if the normal inverse cumulative distribution function across categories\\nofX. As an example consider the web site’s diabetes dataset, where we con-\\nsider the distribution of log glycohemoglobin across subjects’ body frames.\\ngetHdata(diabetes)\\na←Ecdf(∼log(glyhb), group= frame, fun=qnorm,\\nxlab= 'log(HbA1c) ', label.curves= FALSE, data=diabetes,\\nylab=expression( paste(Phi∧-1, (F[n](x )))))# Fig. 13.2\\nb←Ecdf(∼log(glyhb), group= frame, fun= qlogis,\\nxlab= 'log(HbA1c) ', label.curves= list(keys= 'lines '),\\ndata=diabetes, ylab= expression( logit(F[n](x ))))\\nprint(a, more= TRUE,split=c(1,1,2 ,1))\\nprint(b, split=c(2,1,2 ,1))\\nlog(HbA1c)Φ−1(Fn(x))\\n−202\\n1.0 1.5 2.0 2.5\\nlog(HbA1c)logit(Fn(x))\\n−505\\n1.0 1.5 2.0 2.5small\\nmedium\\nlarge\\nFig. 13.2 Transformed empirical cumulativedistributionfunctionsstratiﬁed bybody\\nframe in the diabetes dataset. Left panel: checking all assumptions of the parametric\\nANOVA. Rightpanel:checkingallassumptionsof thePOmodel (here,Kruskal–Wallis\\ntest).\\nOne could conclude the right panel of Figure 13.2displays more parallelism\\nthan the left panel displays linearity, so the assumptions of the PO model are\\nbetter satisﬁed than the assumptions of the ordinary linear model.\\nChapter 14has many examples of graphics for assessing ﬁt of PO models.\\nRegarding assessmentof linearity and addi tivity assumptions, splines, partial\\nresidual plots, and interaction tests are among the best tools. Fagerland and\\nHosmer182have a good review of goodness-of-ﬁt tests for the PO model.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ab7cd4b3-8828-4b78-be0e-a38ac5feb1a9', embedding=None, metadata={'page_label': '318', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='318 13 Ordinal Logistic Regression\\n13.3.6 Quantifying Predictive Ability\\nTheR2\\nNcoeﬃcient is really computed from the model LR χ2(χ2added to\\na model containing only the kintercept parameters) to describe the model’s\\npredictive power. The Somers’ Dxyrank correlation between XˆβandYis\\nan easily interpreted measure of predictive discrimination. Since it is a rank\\nmeasure, it does not matter which intercept αis used in the calculation.\\nThe probability of concordance, c, is also a useful measure. Here one takes all\\npossible pairs of subjects having diﬀering Yvalues and computes the fraction\\nof such pairs for which the values of Xˆβare in the same direction as the two\\nYvalues.ccould be called a generalized ROC area in this setting. As before,\\nDxy=2 (c−0.5). Note that Dxy,c,a n dt h eB r i e rs c o r e Bcan easily be\\ncomputed for various dichotomizations of Y, to investigate predictive ability\\nin more detail.\\n13.3.7 Describing the Fitted Model\\nAsdiscussedinSection 5.1,modelsarebestdescribedbycomputingpredicted\\nvalues or diﬀerences in predicted values. For PO models there are four and\\nsometimes ﬁve types of relevant predictions:\\n1. logit[Y≥j|X], i.e., the linear predictor\\n2. Prob[Y≥j|X]\\n3. Prob[Y=j|X]\\n4. Quantiles of Y|X(e.g., the medianb)\\n5.E(Y|X)i fYis interval scaled.\\nFor the ﬁrst two quantities above a good default choice for jis the middle\\ncategory. Partial eﬀect plots are as simple to draw for PO models as they are\\nfor binary logistic models. Other useful graphics, as before, are odds ratio\\ncharts and nomograms. For the latter, an axis displaying the predicted mean\\nmakes the model more interpretable, under scaling assumptions on Y.\\n13.3.8 Validating the Fitted Model\\nThe PO model is validated much the same way as the binary logistic model\\n(see Section 10.9). For estimating an overﬁtting-corrected calibration curve\\n(Section 10.11) one estimates Pr( Y≥j|X)u s i n go n e jat a time.\\nbIfYdoes not have very many levels, the median will be a discontinuous function\\nofXand may not be satisfactory.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='79dc7b06-6ea6-4e8d-8d88-43a36aa82826', embedding=None, metadata={'page_label': '319', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.4 Continuation Ratio Model 319\\n13.3.9 RFunctions\\nThermspackage’s lrmandormfunctions ﬁt the PO model directly, assuming\\nthat the levels of the response variable (e.g., the levelsof afactorvariable)\\narelistedintheproperorder. lrmisintended to be used forthe casewherethe\\nnumber of unique values of Yare less than a few dozen whereas ormhandles\\nthe continuous Ycase eﬃciently, as well as allowing for links other than the\\nlogit. See Chapter 15for more information.\\nIf the response is numeric, lrmassumes the numeric codes properly order\\nthe responses. If it is a character vector and is not a factor,lrmassumes the\\ncorrectorderingisalphabetic.Ofcourse orderedvariablesin Rareappropriate\\nresponse variables for ordinal regression. The predictfunction ( predict.lrm )\\ncan compute all the quantities listed in Section 13.3.7except for quantiles.\\nTheRfunctions popowerandposamsize (in the Hmiscpackage) compute\\npower and sample size estimates for ordinal responses using the proportional\\nodds model.\\nThe function plot.xmean.ordinaly inrmscomputes and graphs the quanti-\\nties described in Section 13.2.It plots simple Y-stratiﬁedmeans overlaidwith\\nˆE(X|Y=j),withjonthex-axis.The ˆEsarecomputedforbothPOandcon-\\ntinuation ratio ordinal logistic models. The Hmiscpackage’s summary.formula\\nfunction is also useful for assessingthe PO assumption (Figure 13.1). Generic\\nrmsfunctions such as validate,calibrate ,a n dnomogram work with PO model\\nﬁts from lrmas long as the analyst speciﬁes which intercept(s) to use. rmshas\\na special function generator Meanfor constructing an easy-to-use function for\\ngetting the predicted mean Yfrom a PO model. This is handy with plotand\\nnomogram. If the ﬁt has been run through the bootcovfunction, it is easy to\\nuse thePredictfunction to estimate bootstrap conﬁdence limits for predicted\\nmeans.\\n13.4 Continuation Ratio Model\\n13.4.1 Model\\nUnlike the PO model, which is based on cumulative probabilities, the contin-\\nuation ratio (CR) model is based on conditional probabilities. The (forward)\\nCR model31,52,190is stated as follows for Y=0,...,k.\\nPr(Y=j|Y≥j,X)=1\\n1+exp[−(θj+Xγ)]\\nlogit(Y=0|Y≥0,X) = logit( Y=0|X)\\n=θ0+Xγ (13.10)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ba33d4e-9663-4de5-8452-8e3e620ac878', embedding=None, metadata={'page_label': '320', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='320 13 Ordinal Logistic Regression\\nlogit(Y=1|Y≥1,X)=θ1+Xγ\\n...\\nlogit(Y=k−1|Y≥k−1,X)=θk−1+Xγ.\\nTheCRmodelhasbeensaidtobelikelytoﬁtordinalresponseswhensubjects\\nhave to“pass through”one category to get to the next. The CR model is a\\ndiscrete version of the Cox proportional hazards model. The discrete hazard\\nfunction is deﬁned as Pr( Y=j|Y≥j).\\n13.4.2 Assumptions and Interpretation of Parameters\\nThe CR model assumes that the vector of regression coeﬃcients, γ,i st h e\\nsame regardless of which conditional probability is being computed.\\nOne could say that there is no X×condition interaction if the CR model\\nholds. For a speciﬁc condition Y≥j, the model has the same assumptions as\\nthe binary logistic model (Section 10.1.1). That is, the model in its simplest\\nform assumes that the log odds that Y=jconditional on Y≥jis linearly\\nrelated to each Xand that there is no interaction between the Xs.\\nA single odds ratiois assumedto apply equallyto allconditions Y≥j,j=\\n0,1,2,...,k−1. If linearity and additivity hold, the Xm+1:Xmodds ratio\\nforY=jis exp(βm), whatever the conditioning event Y≥j.\\nTo compute Pr( Y>0|X) from the CR model, one only needs to take\\none minus Pr( Y=0|X). To compute other unconditional probabilities from\\nthe CR model, one must multiply the conditional probabilities. For example,\\nPr(Y>1|X)=P r (Y>1|X,Y≥1)×Pr(Y≥1|X)=[ 1−Pr(Y=1|Y≥\\n1,X)][1−Pr(Y=0|X)] = [1−1/(1+exp[−(θ1+Xγ)])][1−1/(1+exp[−(θ0+\\nXγ)])].\\n13.4.3 Estimation\\nArmstrong and Sloan31and Berridge and Whitehead52showed how the CR\\nmodel can be ﬁtted using an ordinary binary logistic model likelihood func-\\ntion, after certain rows of the Xmatrix are duplicated and a new binary Y\\nvector is constructed. For each subject, one constructs separate records by\\nconsidering successive conditions Y≥0,Y≥1,...,Y≥k−1f o rar e s p o n s e\\nvariable with values 0 ,1,...,k. The binary response for each applicable con-\\ndition or “cohort”is set to 1 if the subject failed at the current“cohort”or\\n“risk set,”that is, if Y=jwhere the cohort being considered is Y≥j.T h e\\nconstructed cohortvariableis carriedalong with the new XandY.T h i sv a r i -\\nable is considered to be categorical and its coeﬃcients are ﬁtted by adding\\nk−1 dummy variables to the binary logistic model. For ease of computation,', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67cde554-0492-472f-a2d8-0cf190bbd622', embedding=None, metadata={'page_label': '321', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.4 Continuation Ratio Model 321\\nthe CR model is restated asfollows,with the ﬁrstcohortused asthe reference\\ncell.\\nPr(Y=j|Y≥j,X)=1\\n1+exp[−(α+θj+Xγ)]. (13.11)\\nHereαis an overallintercept, θ0≡0,andθ1,...,θ k−1areincrements from α.\\n13.4.4 Residuals\\nTo check CR model assumptions, binary logistic model partial residuals are\\nagain valuable. We separately ﬁt a sequence of binary logistic models using a\\nseries of binary events and the corresponding applicable (increasingly small)\\nsubsets of subjects, and plot smoothed partial residuals against Xfor all of\\nthe binary events. Parallelism in these plots indicates that the CR model’s\\nconstant γassumptions are satisﬁed.\\n13.4.5 Assessment of Model Fit\\nThe partial residual plots just described are very useful for checking the\\nconstant slope assumption of the CR model. The next section shows how to\\ntest this assumption formally. Linearity can be assessed visually using the\\nsmoothed partial r esidual plot, and interactions between predictors can be\\ntested as usual.\\n13.4.6 Extended CR Model\\nThe PO model has been extended by Peterson and Harrell502to allow for\\nunequalslopesforsomeorallofthe Xsforsomeoralllevelsof Y.Thispartial\\nPO model requiresspecialized software.The CR model can be extended more\\neasily. In Rnotation, the ordinary CR model is speciﬁed as 5\\ny∼cohort + X1 + X2 + X3 + ...\\nwithcohortdenoting a polytomous variable. The CR model can be extended\\nto allow for some or all of the βs to change with the cohort or Y-cutoﬀ.31\\nSuppose that non-constant slope is allowed for X1andX2.T h eRnotation for\\nthe extended model would be\\ny∼cohort*(X1 + X2) + X3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f32ab186-8a83-495c-bb01-3ccf495ed8c7', embedding=None, metadata={'page_label': '322', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='322 13 Ordinal Logistic Regression\\nThe extended CR model is a discrete version of the Cox survival model with\\ntime-dependent covariables.\\nThere is nothing about the CR model that makes it ﬁt a given dataset\\nbetter than other ordinal models such as the PO model. The real beneﬁt of\\nthe CR model is that using standard binary logistic model software one can\\nﬂexibly specify how the equal-slopes assumption can be relaxed.\\n13.4.7 Role of Penalization in Extended CR Model\\nAs demonstrated in the upcoming case study, penalized MLE is invaluable in\\nallowingthe modeltobeextendedintoanunequal-slopesmodelinsofarasthe\\ninformation content in the data will support. Faraway186has demonstrated\\nhow all data-drivensteps ofthe modeling processincrease the real variancein\\n“ﬁnal”parameter estimates, when one estimates variances without assuming\\nthat the ﬁnal model was prespeciﬁed. For ordinal regression modeling, the\\nmost important modeling steps are (1) choice of predictor variables, (2) se-\\nlecting or modeling predictor transformations, and (3) allowance for unequal\\nslopes across Y-cutoﬀs(i.e., non-POor non-CR).RegardingSteps (2) and (3)\\none is tempted to rely on graphical methods such as residual plots to make\\ndetours in the strategy, but it is very diﬃcult to estimate variances or to\\nproperly penalize assessments of predictive accuracy for subjective modeling\\ndecisions. Regarding (1), shrinkagehas been provento work better than step-\\nwise variable selection when one is attempting to build a main-eﬀects model.\\nChoosing a shrinkage factor is a well-deﬁned, smooth, and often a unique\\nprocess as opposed to binary decisions on whether variables are“in”or“out”\\nof the model. Likewise, instead of using arbitrary subjective (residual plots)\\nor objective ( χ2due tocohort×covariable interactions, i.e., non-constant\\ncovariable eﬀects), shrinkage can systematically allow model enhancements\\ninsofar as the informationcontent in the data will support, throughthe use of\\ndiﬀerential penalization. Shrinkage is a solution to the dilemma faced when\\nthe analyst attempts to choose between a parsimonious model and a more\\ncomplex one that ﬁts the data. Penalization does not require the analyst to\\nmake a binary decision, and it is a process that can be validated using the\\nbootstrap.\\n13.4.8 Validating the Fitted Model\\nValidation of statistical indexes such as Dxyand model calibration is done\\nusing techniques discussed previously, except that certain problems must be\\naddressed. First, when using the bootstrap, the resamplingmust take into ac-\\ncount the existence of multiple records per subject that were created to use', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d7f7e87b-8fec-4c26-9999-04a5321f37c5', embedding=None, metadata={'page_label': '323', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.4 Continuation Ratio Model 323\\nthe binary logistic likelihood trick. That is, sampling should be done with re-\\nplacement from subjectsrather than records. Second, the analyst must isolate\\nwhich event to predict. This is because when observations are expanded in\\norder to use a binary logistic likelihood function to ﬁt the CR model, several\\ndiﬀerent events are being predicted simultaneously. Somers’ Dxycould be\\ncomputed by relating Xˆγ(ignoring intercepts) to the ordinal Y, but other\\nindexes are not deﬁned so easily. The simplest approach here would be to\\nvalidate a single prediction for Pr( Y=j|Y≥j,X), for example. The sim-\\nplest event to predict is Pr( Y=0|X), as this would just require subsetting\\non all observations in the ﬁrst cohort level in the validation sample. It would\\nalso be easy to validate any one of the later conditional probabilities. The\\nvalidation functions described in the next section allow for such subsetting,\\nas well as handling the cluster sampling. Specialized calculations would be\\nneeded to validate an unconditional probability such as Pr( Y≥2|X).\\n13.4.9 RFunctions\\nThecr.setup function in rmsreturns a list of vectors useful in constructing\\na dataset used to trick a binary logistic function such as lrminto ﬁtting\\nCR models. The subsvector in this list contains observation numbers in the\\noriginal data, some of which are repeated. Here is an example.\\nu←cr.setup(Y) # Y=original ordinal response\\nattach(mydata[u$ subs,]) # mydata is the original dataset\\n# mydata[i,] subscripts input data,\\n# using duplicate values of i for\\n# repeats\\ny ←u$y # constructed binary responses\\ncohort ←u$cohort # cohort or risk set categories\\nf←lrm(y∼cohort*age + sex)\\nSince the lrmandpentrace functions have the capability to penalize dif-\\nferent parts of the model by diﬀerent amounts, they are valuable for ﬁtting\\nextended CR models in which the cohort×predictor interactions are allowed\\nto be only as important as the information content in the data will support.\\nSimple main eﬀects can be unpenalized or slightly penalized as desired.\\nThevalidate andcalibrate functions for lrmallow speciﬁcation of sub-\\nject identiﬁers when using the bootstrap, so the samples can be constructed\\nwith replacement from the original subjects. In other words, cluster sam-\\npling is done from the expanded records. This is handled internally by the\\npredab.resample function. These functions alsoallowone to specify a subsetof\\nthe recordsto use in the validation, which makes it especially easy to validate\\nthe part of the model used to predict Pr( Y=0|X).\\nTheplot.xmean.ordinaly function is useful for checkingthe CR assumption\\nfor single predictors, as described earlier. 6', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='913de061-88e3-422c-a149-8a37635ca09a', embedding=None, metadata={'page_label': '324', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='324 13 Ordinal Logistic Regression\\n13.5 Further Reading\\n1See5,25,26,31,32,52,63,64,113,126,240,245,276,354,449,502,561,664,679for some\\nexcellent background references, applications, and extensions to the ordinal\\nmodels.663and428demonstrate how to model ordinal outcomes with repeated\\nmeasurements within subject using random eﬀects in Bayesian models. The ﬁrst\\nto develop an ordinal regression model were Aitchison and Silvey8.\\n2Some analysts feel that combining categories improves the performance of test\\nstatistics when ﬁtting PO models when sample sizes are small and cells are\\nsparse. Murad et al.469demonstrated that this causes more problems, because\\nit results in overly conservative Wald tests.\\n3Anderson and Philips [ 26, p. 29] proposed methods for constructing properly\\nspaced response values given a ﬁtted PO model.\\n4The simplest demonstration of this is to consider a model in which there is a\\nsingle predictor that is totally independent of a nine-level response Y,s oP O\\nmusthold. A PO model is ﬁtted in SAS using:\\nDATA test;\\nDO i=1 to 50;\\ny=FLOOR(RANUNI(151)*9);\\nx=RANNOR(5);\\nOUTPUT;\\nEND;\\nPROC LOGISTIC; MODEL y=x;\\nThe score test for PO was χ2=5 6o n7d . f . , P<0.0001. This problem results\\nfrom some small cell sizes in the distribution of Y.502TheP-value for testing\\nthe regression eﬀect for Xwas 0.76.\\n5TheRglmnetcr package by Kellie Archer provides a diﬀerent way to ﬁt con-\\ntinuation ratio models.\\n6BenderandBenner48havesomeexamplesusingtheprecursorofthe rmspackage\\nfor ﬁtting and assessing the goodness of ﬁt of ordinal logistic regression models .\\n13.6 Problems\\nTest for the association between disease group and total hospital cost in\\nSUPPORT, without imputing any missing costs (exclude the one patient\\nhaving zero cost).\\n1. Use the Kruskal–Wallis rank test.\\n2. Use the proportional odds ordinal logistic model generalization of the\\nWilcoxon–Mann–WhitneyKruskal–WallisSpearmantest.Grouptotalcost\\ninto 20 quantile groups so that only 19 intercepts will need to be in the\\nmodel, not one less than the number of subjects (this would have taken\\nthe program too long to ﬁt the model). Use the likelihood ratio χ2for this\\nand later steps.\\n3. Use a binary logistic model to test for association between disease group\\nand whether total cost exceeds the median of total cost. In other words,\\ngroup total cost into two quantile groups and use this binary variable as\\nthe response. What is wrong with this approach?', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='01b2f1c5-5705-4240-b943-612d70b5168b', embedding=None, metadata={'page_label': '325', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='13.6 Problems 325\\n4. Instead of using only two cost groups, group cost into 3, 4, 5, 6, 8, 10,\\nand 12 quantile groups. Describe the relationship between the number of\\nintervals used to approximate the continuous response variable and the\\neﬃciency of the analysis. How many intervals of total cost, assuming that\\nthe ordering of the diﬀerent intervals is used in the analysis, are required\\nto avoid losing signiﬁcant information in this continuous variable?\\n5. If you were selecting one of the rank-based tests for testing the association\\nbetween disease and cost, which of any of the tests considered would you\\nchoose?\\n6. Whydoallofthe testsyoudidhavethe samenumberofdegreesoffreedom\\nfor the hypothesis of no association between dzgroup andtotcst?\\n7. What is the advantage of a rank-based test over a parametric test based\\non log(cost)?\\n8. Show that for a two-sample problem, the numerator of the score test for\\ncomparing the two groups using a proportional odds model is exactly the\\nnumerator of the Wilcoxon-Mann-Whitney two-sample rank-sum test.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e7d1c492-77f2-48aa-bd09-5a9af9723c2b', embedding=None, metadata={'page_label': '327', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 14\\nCase Study in Ordinal Regression,\\nData Reduction, and Penalization\\nThis casestudyistakenfromHarrelletal.272whichdescribedaWorldHealth\\nOrganization study439in which vital signs and a large number of clinical\\nsigns and symptoms were used to develop a predictive model for an ordinal\\nresponse. This response consists of laboratory assessments of diagnosis and\\nseverity of illness related to pneumonia, meningitis, and sepsis. Much of the\\nmodeling strategygivenin Chapter 4wasused to developthe model, with ad-\\nditionalemphasisonpenalizedmaximumlikelihoodestimation(Section 9.10).\\nThe following laboratory data are used in the response: cerebrospinal ﬂuid\\n(CSF) culture from a lumbar puncture (LP), blood culture (BC), arterial\\noxygen saturation ( SaO2, a measure of lung dysfunction), and chest X-ray\\n(CXR). The sample consisted of 4552 infants aged 90 days or less.\\nThis case study covers these topics:\\n1. deﬁnition of the ordinal response (Section 14.1);\\n2. scoring and clustering of clinical signs (Section 14.2);\\n3. testing adequacy of weights speciﬁed by subject-matter specialists and\\nassessing the utility of various scoring schemes using a tentative ordinal\\nlogistic model (Section 14.3);\\n4. assessing the basic ordinality assumptions and examining the propor-\\ntional odds and continuation ratio (PO and CR) assumptions separately\\nfor each predictor (Section 14.4);\\n5. deriving a tentative PO model using cluster scores and regression splines\\n(Section 14.5);\\n6. using residual plots to check PO, CR, and linearity assumptions (Sec-\\ntion14.6);\\n7. examining the ﬁt of a CR model (Section 14.7);\\n8. utilizing an extended CR model to allow some or all of the regression\\ncoeﬃcients to vary with cutoﬀs of the response level as well as to provide\\nformal tests of constant slopes (Section 14.8);\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 14327', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dbf1d5b5-bd23-450a-81d4-31c179f0562d', embedding=None, metadata={'page_label': '328', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='328 14 Ordinal Regression, Data Reduction, and Penalization\\nTable 14.1 Ordinal Outcome Scale\\nOutcome Deﬁnition n Fraction in Outcome Level\\nLevel BC, CXR Not Random\\nY Indicated Indicated Sample\\n(n= 2398) ( n= 1979) ( n= 175)\\n0 None of the below 3551 0 .63 0 .96 0 .91\\n1 90% ≤SaO2<95% 490 0 .17 0 .04a0.05\\nor CXR+\\n2 BC+ or CSF+ 511 0 .21 0 .00b0.03\\norSaO2<90%\\naSaO2was measured but CXR was not done\\nbAssumed zero since neither BC nor LP were done.\\n9. using penalized maximum likelihood estimation to improve accuracy\\n(Section 14.9);\\n10. approximating the full model by a sub-model and drawing a nomogram\\non the basis of the sub-model (Section 14.10); and\\n11. validating the ordinal model using the bootstrap (Section 14.11).\\n14.1 Response Variable\\nTo bea candidateforBC andCXR, aninfant hadtohaveaclinicalindication\\nfor one of the three diseases, according to prespeciﬁed criteria in the study\\nprotocol ( n= 2398). Blood work-up (but not necessarily LP) and CXR was\\nalso done on a random sample intended to be 10% of infants having no signs\\nor symptoms suggestive of infection ( n= 175). Infants with signs suggestive\\nof meningitis had LP done. All 4552 infants received a full physical exam and\\nstandardized pulse oximetry to measure SaO2. The vast majority of infants\\ngetting CXR had the X-rays interpreted by three independent radiologists.\\nThe analyses that follow are not corrected for veriﬁcation bias687with\\nrespect to BC, LP, and CXR, but Section 14.1has some data describing the\\nextent of the problem, and the problem is reduced by conditioning on a large\\nnumber of covariates.\\nPatientswereassignedtotheworstqualifyingoutcomecategory.Table 14.1\\nshows the deﬁnition of the ordinal outcome variable Yand shows the distri-\\nbution of Yby the lab work-up strategy.\\nThe eﬀect of veriﬁcation bias is a false negative fraction of 0.03 for Y=2 ,\\nfromcomparingthedetectionfractionofzerofor Y= 2inthe“NotIndicated”\\ngroup with the observed positive fraction of 0.03 in the random sample that\\nwas fully worked up. The extent of veriﬁcation bias in Y= 1 is 0.05−0.04 =\\n0.01. These biases are ignored in this analysis.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6d2050b4-5d54-4539-a43e-801d638f5218', embedding=None, metadata={'page_label': '329', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.2 Variable Clustering 329\\n14.2 Variable Clustering\\nForty-seven clinical signs were collected for each infant. Most questionnaire\\nitems were scored as a single variable using equally spaced codes, with 0 to\\n3 representing, for example, sign not present, mild, moderate, severe. The\\nresulting list of clinical signs with their abbreviations is given in Table 14.2.\\nThe signs are organized into clusters as discussed later.\\nTable 14.2 Clinical Signs\\nCluster Name Sign Name Values\\nAbbreviation of Sign\\nbul.conv abb bulging fontanel 0-1\\nconvul hx convulsion 0-1\\nhydration abk sunken fontanel 0-1\\nhdi hx diarrhoea 0-1\\ndeh dehydrated 0-2\\nstu skin turgor 0-2\\ndcp digital capillary reﬁll 0-2\\ndrowsy hcl less activity 0-1\\nqcr quality of crying 0-2\\ncsd drowsy state 0-2\\nslpm sleeping more 0-1\\nwake wakes less easily 0-1\\naro arousal 0-2\\nmvm amount of movement 0-2\\nagitated hcm crying more 0-1\\nslpl sleeping less 0-1\\ncon consolability 0-2\\ncsa agitated state 0-1\\ncrying hcm crying more 0-1\\nhcs crying less 0-1\\nqcr quality of crying 0-2\\nsmi2 smiling ability ×age>42 days 0-2\\nreﬀort nﬂ nasal ﬂaring 0-3\\nlcw lower chest in-drawing 0-3\\ngru grunting 0-2\\nccy central cyanosis 0-1\\nstop.breath hap hx stop breathing 0-1\\napn apnea 0-1\\nausc whz wheezing 0-1\\ncoh cough heard 0-1\\ncrs crepitation 0-2\\nhxprob hfb fast breathing 0-1\\nhdb diﬃculty breathing 0-1\\nhlt mother report resp. problems none, chest, other\\nfeeding hfa hx abnormal feeding 0-3\\nabsu sucking ability 0-2\\nafe drinking ability 0-2\\nlabor chi previous child died 0-1\\nfde fever at delivery 0-1\\nldy days in labor 1-9\\ntwb water broke 0-1\\nabdominal adb abdominal distension 0-4\\njau jaundice 0-1\\nomph omphalitis 0-1\\nfever.ill illd age-adjusted no. days ill\\nhfe hx fever 0-1\\npustular conj conjunctivitis 0-1\\noto otoscopy impression 0-2\\npuskin pustular skin rash 0-1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de34e1fc-4bc5-4df2-9d90-5217e0dff2e3', embedding=None, metadata={'page_label': '330', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='330 14 Ordinal Regression, Data Reduction, and Penalization\\ntwb\\nldy\\nfde\\nchi\\noto\\natt\\njau\\nomph\\nilld\\nsmi2\\nstrhltHy respir/chest\\nhfb\\ncrs\\nlcw\\nnfl\\ncoh\\nwhz\\nhltHy respir/no chest\\nhdb\\npuskin\\nconj\\nhfe\\nslpl\\nhcm\\nhap\\napn\\nccy\\ndcp\\naro\\ncsd\\nqcr\\nmvm\\nhcs\\nhcl\\nhfa\\nafe\\nabsu\\nslpm\\nwake\\nhdi\\nabk\\nstu\\ndeh\\nconvul\\nabb\\nabd\\ngru\\ncsa\\ncon\\n1.00.80.60.40.20.0Spearman ρ2\\nFig. 14.1 Hierarchical variable clustering using Spearman ρ2as a similarity measure\\nfor all pairs of variables.Note that sincethe hltvariable was nominal, it is represented\\nby two dummy variables here.\\nHere,hxstands for history, auscfor auscultation, and hxprobfor history of\\nproblems. Two signs ( qcr, hcm) were listed twice since they were later placed\\ninto two clusters each.\\nNext, hierarchical clustering was done using the matrix of squared Spear-\\nman rank correlation coeﬃcients as the similarity matrix. The varclus R\\nf u n c t i o nw a su s e da sf o l l o w s .\\nrequire(rms)\\ngetHdata(ari) # defines ari, Sc, Y, Y.death\\nvclust ←\\nvarclus( ∼illd + hlt + slpm + slpl + wake + convul + hfa +\\nhfb + hfe + hap + hcl + hcm + hcs + hdi +\\nfde + chi + twb + ldy + apn + lcw + nfl +\\nstr + gru + coh + ccy + jau + omph + csd +\\ncsa + aro + qcr + con + att + mvm + afe +\\nabsu + stu + deh + dcp + crs + abb + abk +\\nwhz + hdb + smi2 + abd + conj + oto + puskin,\\ndata=ari)\\nplot(vclust) # Figure 14.1\\nThe output appears in Figure 14.1. This output served as a starting point\\nfor clinicians to use in constructing more meaningful clinical clusters. The\\nclusters in Table 14.2were the consensus of the clinicians who were the in-\\nvestigators in the WHO study. Prior subject matter knowledge plays a key\\nrole at this stage in the analysis.\\n14.3 Developing Cluster Summary Scores\\nThe clusters listed in Table 14.2were ﬁrst scored by the ﬁrst principal com-\\nponent of transcan-transformed signs, denoted by PC1. Knowing that the\\nresulting weights may be too complex for clinical use, the primary reasons', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13da784c-a2d2-4cb6-b26b-c8cfb95af620', embedding=None, metadata={'page_label': '331', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.3 Developing Cluster Summary Scores 331\\nTable 14.3 Clinician Combinations, Rankings, and Scorings of Signs\\nCluster Combined/Ranked Signs in Order of Severity Weights\\nbul.conv abb ∪convul 0–1\\ndrowsy hcl, qcr >0, csd>0∪slpm∪wake, aro >0, mvm>0 0–5\\na g i t a t e d h c m ,s l p l ,c o n = 1 ,c s a ,c o n = 2 0 ,1 ,2 ,7 ,8 ,1 0\\nreﬀort nﬂ >0, lcw>1, gru=1, gru=2, ccy 0–5\\nausc whz, coh, crs >0 0–3\\nfeeding hfa=1, hfa=2, hfa=3, absu=1 ∪afe=1, 0–5\\nabsu=2 ∪afe=2\\nabdominal jau ∪abd>0∪omph 0–1\\nfor analyzing the principal components were to see if some of the clusters\\ncould be removed from consideration so that the clinicians would not spend\\ntime developing scoring rules for them. Let us“peek”at Yto assist in scoring\\nclusters at this point, but to do so in a very structured way that does not\\ninvolve the examination of a large number of individual coeﬃcients.\\nTo judge any cluster scoring scheme, we must pick a tentative outcome\\nmodel. For this purpose we chose the PO model. By using the 14 PC1sc o r -\\nresponding to the 14 clusters, the ﬁtted PO model had a likelihood ratio\\n(LR)χ2of 1155 with 14 d.f., and the predictive discrimination of the clus-\\nters was quantiﬁed by a Somers’ Dxyrank correlation between XˆβandY\\nof 0.596. The following clusters were not statistically important predictors\\nand we assumed that the lack of importance of the PC1s in predicting Y\\n(adjusted for the other PC1s) justiﬁed a conclusion that no sign within that\\nclusterwasclinicallyimportantinpredicting Y:hydration, hxprob, pustular,\\ncrying, fever.ill, stop.breath, labor . This list was identiﬁed using a back-\\nward step-down procedure on the full model. The total Wald χ2for these\\nsevenPC1s was 22.4 ( P=0.002). The reduced model had LR χ2= 1133\\nwith 7 d.f., Dxy=0.591. The bootstrap validation in Section 14.11penalizes\\nfor examining all candidate predictors.\\nThe clinicians were asked to rank the clinical severity of signs within each\\npotentially important cluster. During this step, the clinicians also ranked\\nseverity levels of some of the component signs, and some cluster scores were\\nsimpliﬁed, especially when the signs within a cluster occurred infrequently.\\nThe clinicians also assessed whether the severity points or weights should be\\nequally spaced, assigning unequally spaced weights for one cluster ( agitated).\\nThe resulting rankings and sign combinations are shown in Table 14.3.T h e\\nsigns or sign combinations separated by a comma are treated as separate\\ncategories, whereas some signs were unioned (“or”–ed) when the clinicians\\ndeemed them equally important. As an example, if an additive cluster score\\nwas to be used for drowsy, the scorings would be 0 = none present, 1 = hcl,\\n2=qcr>0,3=csd>0orslpmorwake,4=aro>0,5=mvm>0and the scores\\nwould be added.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0de95573-ebbe-43b1-8500-1032dc749477', embedding=None, metadata={'page_label': '332', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='332 14 Ordinal Regression, Data Reduction, and Penalization\\nTable 14.4 Predictive information of various cluster scoring strategies. AIC is on\\nthe likelihood ratio χ2scale.\\nScoring Method LR χ2d.f. AIC\\nPC1of each cluster 1133 7 1119\\nUnion of all signs 1045 7 1031\\nUnion of higher categories 1123 7 1109\\nHierarchical (worst sign) 1194 7 1180\\nAdditive, equal weights 1155 7 1141\\nAdditive using clinician weights 1183 7 1169\\nHierarchical, data-driven weights 1227 25 1177\\nThis table reﬂects some data reduction already (unioning some signs and\\nselection of levels of ordinal signs) but more reduction is needed. Even after\\nsigns are rankedwithin a cluster, there are various ways of assigning the clus-\\nter scores.We investigatedsixmethods.We startedwith the purelystatistical\\napproach of using PC1to summarize each cluster. Second, all sign combina-\\ntions within a cluster were unioned to represent a 0/1 cluster score. Third,\\nonly sign combinations thought by the clinicians to be severe were unioned,\\nresulting in drowsy=aro>0 ormvm>0,agitated=csa orcon=2,reffort=lcw>1 or\\ngru>0orccy,ausc=crs>0 ,a n dfeeding=absu>0 orafe>0. For clusters that are\\nnot scored 0/1 in Table 14.3, the fourth summarization method was a hi-\\nerarchical one that used the weight of the worst applicable category as the\\ncluster score. For example, if aro=1butmvm=0,drowsywould be scored as 4.\\nThe ﬁfth method counted the number of positive signs in the cluster. The\\nsixth method summed the weights of all signs or sign combinations present.\\nFinally, the worst sign combination present was again used as in the sec-\\nond method, but the points assigned to the category were data-driven ones\\nobtained by using extra dummy variables. This provided an assessment of\\nthe adequacy of the clinician-speciﬁed weights. By comparing rows 4 and 7\\nin Table 14.4we see that response data-driven sign weights have a slightly\\nworse AIC, indicating that the number of extra βparameters estimated was\\nnot justiﬁed by the improvement in χ2. The hierarchical method, using the\\nclinicians’ weights, performed quite well. The only cluster with inadequate\\nclinician weights was ausc—see below. The PC1method, without any guid-\\nance, performed well, as in268. The only reasons not to use it are that it\\nrequires a coeﬃcient for every sign in the cluster and the coeﬃcients are not\\ntranslatable into simple scores such as 0 ,1,....\\nRepresentationof clusters by a simple union of selected signs or of all signs\\nis inadequate, but otherwise the choice of methods is not very important in\\nterms of explaining variation in Y. We chose the fourth method, a hierar-\\nchical severity point assignment (using weights that were prespeciﬁed by the\\nclinicians), for its ease of use and of handling missing component variables\\n(in most cases) and potential for speed ing up the clinical exam (examining', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='abe22e78-57df-43e8-b516-51ea9d22a82d', embedding=None, metadata={'page_label': '333', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.5 A Tentative Full Proportional Odds Model 333\\nto detect more important signs ﬁrst). Because of what was learned regard-\\ning the relationship between auscandY, we modiﬁed the ausccluster score\\nby redeﬁning it as ausc=crs>0 (crepitations present). Note that neither the\\n“tweaking” of auscnor the examination of the seven scoring methods dis-\\nplayed in Table 14.4is taken into account in the model validation.\\n14.4 Assessing Ordinality of Yfor each X,\\nand Unadjusted Checking of PO and CR\\nAssumptions\\nSection13.2described a graphical method for assessing the ordinality as-\\nsumption for Yseparately with respect to each X, and for assessing PO and\\nCR assumptions individually. Figure 14.2is an example of such displays. For\\nthis dataset we expect strongly nonlinear eﬀects for temp, rr,a n dhrat,s of o r\\nthose predictors we plot the mean absolute diﬀerences from suitable“normal”\\nvalues as an approximate solution.\\nSc←transform(Sc,\\nausc = 1 * (ausc == 3),\\nbul.conv = 1 * (bul.conv == 'TRUE '),\\nabdominal = 1 * (abdominal == 'TRUE '))\\nplot.xmean.ordinaly(Y ∼age + abs(temp-37) + abs( rr-60) +\\nabs(hrat-125) + waz + bul.conv + drowsy +\\nagitated + reffort + ausc + feeding +\\nabdominal, data=Sc, cr= TRUE,\\nsubn=FALSE, cex.points=.65) # Figure 14.2\\nThe plot is shown in Figure 14.2.Ydoes not seem to operate in an ordinal\\nfashionwithrespectto age,|rr−60|,orausc.Fortheothervariables,ordinality\\nholds, and PO holds reasonably well for the other variables. For heart rate,\\nthe PO assumption appears to be satisﬁed perfectly. CR model assumptions\\nappear to be more tenuous than PO assumptions, when one variable at a\\ntime is ﬁtted.\\n14.5 A Tentative Full Proportional Odds Model\\nBased on what was determined in Section 14.3, the original list of 47 signs\\nwas reduced to seven predictors: two unions of signs ( bul.conv, abdominal ),\\none single sign ( ausc), and four“worst category”point assignments ( drowsy,\\nagitated, reffort, feeding ). Seven clusters were dropped for the time being\\nbecause of weak associations with Y. Such a limited use of variable selection\\nreduces the severe problems inherent with that technique.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9e08713e-a97e-4ce2-98da-1cabe1215240', embedding=None, metadata={'page_label': '334', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='334 14 Ordinal Regression, Data Reduction, and Penalization\\nl l\\nl\\nYage\\n01237 39 41C\\nC\\nC\\nlll\\nYabs(temp − 37)\\n0120.6 0.8 1.0CCCl\\nll\\nYabs(rr − 60)\\n01212.0 13.0 14.0C CC\\nlll\\nYabs(hrat − 125)\\n01232 36 40\\nCCC\\nl\\nl\\nl\\nYwaz\\n012−1.0 −0.6 −0.2C\\nC\\nC lll\\nYbul.conv\\n0120.04 0.10 0.16\\nCCC\\nlll\\nYdrowsy\\n0121.0 2.0CCC\\nlll\\nYagitated\\n0121.5 2.5 3.5CCC\\nlll\\nYreffort\\n0120.5 1.0 1.5 2.0\\nCCC\\nlll\\nYausc\\n0120.10 0.25 0.40\\nCCC\\nlll\\nYfeeding\\n0120.5 1.5 2.5\\nCCC\\nlll\\nYabdominal\\n0120.12 0.18 0.24\\nCCC\\nFig. 14.2 Examination of the ordinality of Yfor each predictor by assessing how\\nvarying Yrelate to the mean X, and whether the trend is monotonic. Solid lines\\nconnect the simple stratiﬁed means, and dashed lines connect the estimated expected\\nvalue of X|Y=jgiven that PO holds. Estimated expected values from the CR model\\nare marked with Cs.\\nAt this point in model development add to the model age and vital signs:\\ntemp(temperature), rr(respiratory rate), hrat(heart rate), and waz,w e i g h t -\\nfor-ageZ-score. Since agewas expected to modify the interpretation of temp,\\nrr,a n dhrat, and interactions between continuous variables would be diﬃcult\\nto use in the ﬁeld, we categorized ageinto three intervals: 0–6 days ( n= 302),\\n7–59 days ( n= 3042), and 60–90 days ( n= 1208).a\\nSc$ageg ←cut2(Sc$age, c(7, 60))\\nThe new variables temp, rr, hrat, waz were missing in, respectively, n=\\n13, 11, 147, and 20 infants. Since the three vital sign variables are somewhat\\ncorrelated with each other, customized single imputation models were de-\\nveloped to impute all the missing values without assuming linearity or even\\nmonotonicity of any of the regressions.\\naThese age intervals were also found to adequately capture most of the inter action\\neﬀects.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1c9f4dc-cd1e-481d-ac5a-6eec0af8418c', embedding=None, metadata={'page_label': '335', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.5 A Tentative Full Proportional Odds Model 335\\nvsign.trans ←transcan( ∼temp + hrat + rr, data=Sc,\\nimputed= TRUE, pl= FALSE)\\nConvergence criterion :2.222 0.643 0.191 0.056 0.016\\nConvergence in 6 iterations\\nR2achieved in predicting each variable:\\ntemp hrat rr\\n0.168 0.160 0.066\\nAdjusted R2:\\ntemp hrat rr\\n0.167 0.159 0.064\\nSc←transform(Sc,\\ntemp = impute(vsign.trans, temp),\\nhrat = impute(vsign.trans, hrat),\\nrr = impute(vsign.trans, rr))\\nAftertranscan estimatedoptimal restrictedcubic splinetransformations, temp\\ncould be predicted with adjusted R2=0.17 from hratandrr,hratcould be\\npredicted with adjusted R2=0.16 from tempandrr,a n drrcould be pre-\\ndicted with adjusted R2of only 0 .06. The ﬁrst two R2, while not large, mean\\nthat customizedimputations aremoreeﬃcientthanimputing with constants.\\nImputations on rrwere closer to the median rrof 48/minute as compared\\nwith the other two vital signs whose imputations have more variation. In a\\nsimilar manner, wazwas imputed using age, birth weight, head circumference,\\nbody length, and prematurity (adjusted R2for predicting wazfrom the oth-\\ners was 0.74). The continuous predictors temp, hrat, rr were not assumed to\\nlinearly relate to the log odds that Y≥j. Restricted cubic spline functions\\nwith ﬁve knots for temp,rrand four knots for hrat,waz were used to model\\nthe eﬀects of these variables:\\nf1←lrm(Y∼ageg*(rcs( temp,5)+rcs(rr,5)+rcs( hrat,4)) +\\nrcs(waz,4) + bul.conv + drowsy + agitated +\\nreffort + ausc + feeding + abdominal,\\ndata=Sc, x= TRUE, y= TRUE)\\n# x=TRUE, y= TRUE used by resid() below\\nprint(f1, latex= TRUE,coefs=5)\\nLogistic Regression Model\\nlrm(formula = Y ~ ageg * (rcs(temp, 5) + rcs(rr, 5) + rcs(hrat,\\n4)) + rcs(waz, 4) + bul.conv + drowsy + agitated + reffort +\\nausc + feeding + abdominal, data = Sc, x = TRUE, y = TRUE)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e05d07c-af6b-4dd7-af24-aa3a94f7db7b', embedding=None, metadata={'page_label': '336', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"336 14 Ordinal Regression, Data Reduction, and Penalization\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 4552 LRχ21393.18R20.355C 0.826\\n0 3551 d.f. 45 g 1.485Dxy0.653\\n1 490 Pr(>χ2)<0.0001gr4.414γ 0.654\\n2 511 gp0.225τa0.240\\nmax|∂logL\\n∂β|2×10−6Brier 0.120\\nCoef S.E. Wald ZPr(>|Z|)\\ny≥1 0.0653 7.6563 0.01 0.9932\\ny≥2 -1.0646 7.6563 -0.14 0.8894\\nageg=[ 7,60) 9.5590 9.9071 0.96 0.3346\\nageg=[60,90] 29.1376 15.8915 1.83 0.0667\\ntemp -0.0694 0.2160 -0.32 0.7480\\n...\\nWald tests of nonlinearity and interaction are shown in Table 14.5.\\nlatex(anova(f1), file= '', label= 'ordinal-anova.f1 ',\\ncaption= 'Waldstatistics from the proportional odds model ',\\nsize= 'smaller ')# Table 14.5\\nThe bottom four lines of the table are the most important. First, there is\\nstrong evidence that some associations with Yexist (45 d.f. test) and very\\nstrong evidence of nonlinearity in one of the vital signs or in waz(26 d.f. test).\\nThereismoderatelystrongevidenceforaninteractioneﬀectsomewhereinthe\\nmodel (22 d.f. test). We see that the grouped age variable agegis predictive\\nofY, but mainly as an eﬀect modiﬁer for rr,a n dhrat.tempis extremely\\nnonlinear,and rrismoderatelyso. hrat, adiﬃcultvariabletomeasurereliably\\nin young infants, is perhaps not important enough ( χ2=1 9,9 d.f.) to keep\\nin the ﬁnal model.\\n14.6 Residual Plots\\nSection13.3.4deﬁned binary logistic score residuals for isolating the PO\\nassumption in an ordinal model. For the tentative PO model, score residuals\\nf o rf o u ro ft h ev a r i a b l e sw e r ep l o t t e du s i n g\\nresid(f1, 'score.binary ', pl=TRUE, which=c(17,18,20,21))\\n## Figure 14.3\\nThe result is shown in Figure 14.3. We see strong evidence of non-PO for\\nauscand moderate evidence for drowsyandbul.conv, in agreement with\\nFigure14.2.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc749fea-aa42-465d-ae5c-e6b1b7848333', embedding=None, metadata={'page_label': '337', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.6 Residual Plots 337\\nTable 14.5 Wald statistics from the proportional odds model\\nχ2d.f.P\\nageg (Factor+Higher Order Factors) 41 .49 24 0.0147\\nAll Interactions 40.48 22 0.0095\\ntemp (Factor+Higher Order Factors) 37 .08 12 0.0002\\nAll Interactions 6.77 8 0.5617\\nNonlinear (Factor+Higher Order Factors) 31.08 9 0.0003\\nrr (Factor+Higher Order Factors) 81 .16 12<0.0001\\nAll Interactions 27.37 8 0.0006\\nNonlinear (Factor+Higher Order Factors) 27.36 9 0.0012\\nhrat (Factor+Higher Order Factors) 19 .00 9 0.0252\\nAll Interactions 8.83 6 0.1836\\nNonlinear (Factor+Higher Order Factors) 7.35 6 0.2901\\nwaz 35.82 3<0.0001\\nNonlinear 13.21 2 0.0014\\nbul.conv 12.16 1 0.0005\\ndrowsy 17.79 1<0.0001\\nagitated 8.25 1 0.0041\\nreﬀort 63.39 1<0.0001\\nausc 105.82 1<0.0001\\nfeeding 30.38 1<0.0001\\nabdominal 0.74 1 0.3895\\nageg×temp (Factor+Higher Order Factors) 6 .77 8 0.5617\\nNonlinear 6.40 6 0.3801\\nNonlinear Interaction : f(A,B) vs. AB 6.40 6 0.3801\\nageg×rr (Factor+Higher Order Factors) 27 .37 8 0.0006\\nNonlinear 14.85 6 0.0214\\nNonlinear Interaction : f(A,B) vs. AB 14.85 6 0.0214\\nageg×hrat (Factor+Higher Order Factors) 8 .83 6 0.1836\\nNonlinear 2.42 4 0.6587\\nNonlinear Interaction : f(A,B) vs. AB 2.42 4 0.6587\\nTOTAL NONLINEAR 78 .20 26<0.0001\\nTOTAL INTERACTION 40 .48 22 0.0095\\nTOTAL NONLINEAR + INTERACTION 96 .31 32<0.0001\\nTOTAL 1073.78 45<0.0001\\nPartialresidualscomputedseparatelyforeach Y-cutoﬀ(Section 13.3.4)are\\nthe most useful residuals for ordinal models as they simultaneously check lin-\\nearity, ﬁnd needed transformations, and check PO. In Figure 14.4,s m o o t h e d\\npartial residual plots were obtained for all predictors, after ﬁrst ﬁtting a sim-\\nple model in which every predictor was assumed to operate linearly. Inter-\\nactions were temporarily ignored and agewas used as a continuous variable.\\nf2←lrm(Y∼age + temp + rr + hrat + waz +\\nbul.conv + drowsy + agitated + reffort + ausc +\\nfeeding + abdominal, data=Sc, x=TRUE, y=TRUE)\\nresid(f2, 'partial ', pl=TRUE, label.curves= FALSE) # Figure 14.4\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='406ec12d-9047-4f53-b296-5f11e6561f66', embedding=None, metadata={'page_label': '338', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='338 14 Ordinal Regression, Data Reduction, and Penalization\\nYbul.conv\\n−0.004 0.00212\\nYbul.conv\\nYdrowsy\\n−0.02 0.02\\n12\\nYdrowsy\\nYreffort\\n−0.02 0.00 0.02\\n12\\nYreffort\\nYausc\\n−0.010 0.00512\\nYausc\\nFig. 14.3 Binary logistic model score residuals for binary events derived from two\\ncutoﬀs of the ordinal response Y. Note that the mean residuals, marked with closed\\ncircles, correspond closely to diﬀerences between solid and dashed lines at Y=1,2\\nin Figure 14.2. Score residual assessments for spline-expanded variables such as rr\\nwould have required one plot per d.f.\\nThe degreeofnon-parallelismgenerallyagreedwiththe degreeofnon-ﬂatness\\nin Figure 14.3and with the other score residual plots that were not shown.\\nThe partial residuals show that tempis highly nonlinear and that it is much\\nmore useful in predicting Y= 2. For the cluster scores, the linearity assump-\\ntion appears reasonable, except possibly for drowsy. Other nonlinear eﬀects\\nare taken into account using splines as before (except for age, which is cate-\\ngorized).\\nA model can have signiﬁcant lack of ﬁt with respect to some of the predic-\\ntors and still yield quite accuratepredictions.To see if that is the casefor this\\nPO model, we computed predicted probabilities of Y= 2 for all infants from\\nthe model and compared these with predictions from a customized binary\\nlogistic model derived to predict Pr( Y= 2). The mean absolute diﬀerence\\nin predicted probabilities between the two models is only 0.02, but the 0.90\\nquantile of that diﬀerence is 0.059. For high-risk infants, discrepancies of 0.2\\nwere common. Therefore we elected to consider a diﬀerent model.\\n14.7 Graphical Assessment of Fit of CR Model\\nIn order to take a ﬁrst look at the ﬁt of a CR model, let us consider the\\ntwo binary events that need to be predicted, and assess linearity and paral-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bfd41081-4f4c-41c3-818c-e548424b66aa', embedding=None, metadata={'page_label': '339', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.7 Graphical Assessment of Fit of CR Model 339\\nlelism over Y-cutoﬀs. Here we ﬁt a sequence of binary ﬁts and then use the\\nplot.lrm.partial function, which assembles partial residuals for a sequence\\nof ﬁts and constructs one graph per predictor.\\ncr0←lrm(Y==0 ∼age + temp + rr + hrat + waz +\\nbul.conv + drowsy + agitated + reffort + ausc +\\nfeeding + abdominal, data=Sc, x= TRUE, y= TRUE)\\n# Use the update function to saverepeating model right-\\n# hand side. An indicator variable for Y=1 is the\\n# response variable below\\ncr1←update(cr0, Y==1 ∼., subset=Y ≥1)\\nplot.lrm.partial(cr0, cr1, center=TRUE) # Figure 14.5\\nThe output is in Figure 14.5. There is not much more parallelism here than\\nin Figure 14.4. For the two most important predictors, auscandrr,t h e r ea r e\\nstrongly diﬀering eﬀects for the diﬀerent events being predicted (e.g., Y=0\\norY=1|Y≥1). As is often the case, there is no one constant βmodel that\\nsatisﬁes assumptions with respect to all predictors simultaneously, especially\\n02 0 4 0 6 0 8 0−0.6 0.0 0.4\\nage32 34 36 38 404.0 5.0 6.0 7.0\\ntemp0 40 80 1200.5 1.5 2.5\\nrr50 150 2500.0 1.0 2.0 3.0\\nhrat\\n−4 0 2 4−2.0 −0.5 1.0\\nwaz0.0 0.4 0.8−0.2 0.4 0.8 1.2\\nbul.conv012345−0.4 0.0 0.4 0.8\\ndrowsy02468 1 0−0.4 0.0 0.4 0.8\\nagitated\\n012345−0.5 0.5 1.5\\nreffort0.0 0.4 0.80.0 0.5 1.0\\nausc0123450.0 0.5 1.0\\nfeeding0.0 0.4 0.8−0.2 0.0 0.2 0.4\\nabdominal\\nFig. 14.4 Smoothed partial residualscorrespondingtotwocutoﬀsof Y,fro mamodel\\nin which all predictors were assumed to operate linearly and additively.The smoothed\\ncurves estimate the actual predictor transformations needed, and parallelism relates\\nto the PO assumption. Solid lines denote Y≥1 while dashed lines denote Y≥2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f8d7d6f4-2ffd-46f6-8121-3e876e59592b', embedding=None, metadata={'page_label': '340', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='340 14 Ordinal Regression, Data Reduction, and Penalization\\n0 2 04 06 08 0−0.4 −0.1 0.1\\nagecr0cr1\\n32 34 36 38 40−0.5 0.0 0.5\\ntempcr0cr1\\n04 0 8 0 1 2 0−0.8 −0.4 0.0\\nrrcr0cr1\\n50 150 250−2.0 −1.0 0.0\\nhratcr0\\ncr1\\n−4 0 2 4−0.5 0.5\\nwazcr0\\ncr1\\n0.0 0.4 0.8−0.4 −0.2 0.0\\nbul.convcr0cr1\\n012345−0.4 0.0 0.2\\ndrowsycr0cr1\\n02468 1 0−0.4 −0.1 0.2\\nagitatedcr0\\ncr1\\n012345−1.0 −0.4 0.0\\nreffortcr0cr1\\n0.0 0.4 0.8−1.0 −0.4 0.0\\nausccr0cr1\\n012345−0.8 −0.4 0.0\\nfeedingcr0cr1\\n0.0 0.4 0.8−0.15 −0.05\\nabdominalcr0cr1\\nFig. 14.5 loesssmoothed partial residual plots for binary models that are compo-\\nnents of an ordinal continuation ratio model. Solid lines correspond to a model for\\nY= 0, and dotted lines correspond to a model for Y=1|Y≥1.\\nwhen there is evidence for non-ordinality for auscin Figure 14.2.T h eC R\\nmodel will need to be generalized to adequately ﬁt this dataset.\\n14.8 Extended Continuation Ratio Model\\nThe CR model in its ordinary form has no advantage over the PO model for\\nthis dataset. But Section 13.4.6discussed how the CR model can easily be\\nextended to relax any of its assumptions. First we use the cr.setup function\\nto set up the data for ﬁtting a CR model using the binary logistic trick.\\nu←cr.setup(Y)\\nSc.expanded ←Sc[u$subs, ]\\ny ←u$y\\ncohort ←u$cohort', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dfbdde0c-c041-4108-83d1-43f1d633f338', embedding=None, metadata={'page_label': '341', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.8 Extended Continuation Ratio Model 341\\nHere the cohortvariablehas values ’all’, ’Y>=1’ correspondingto the condi-\\ntioning events in Equation 13.10. Once the data frame is expanded to include\\nthe diﬀerentriskcohorts,vectorssuchas agearelengthened(to5553records).\\nNow we ﬁt a fully extended CR model that makes no equal slopes assump-\\ntions; that is, the model hasto ﬁtYassuming the covariables are linear and\\nadditive. At this point, we omit hratbut add back all variables that were\\ndeleted by examining their association with Y. Recall that most of these\\nseven cluster scores were summarized using PC1. Adding back“insigniﬁcant”\\nvariables will allow us to validate the model fairly using the bootstrap, as\\nwell as to obtain conﬁdence intervals that are not falsely narrow.16\\nfull←\\nlrm(y∼cohort*(ageg*(rcs( temp,5) + rcs(rr ,5)) +\\nrcs(waz,4) + bul.conv + drowsy + agitated + reffort +\\nausc + feeding + abdominal + hydration + hxprob +\\npustular + crying + fever.ill + stop.breath + labor),\\ndata=Sc.expa nded, x= TRUE, y= TRUE)\\n# x=TRUE, y= TRUE are for pentrace, validate, calibrate below\\nperf←function(fit) { # model performance for Y=0\\npr←predict(fit, type= 'fitted ')[cohort == 'all']\\ns←round(somers2(pr, y[ cohort == 'all']), 3)\\npr←1-p r # Predict Prob[Y > 0] instead of Prob[Y = 0]\\nf←round(c(mean(pr < .05), mean(pr > .25),\\nmean(pr > .5)), 2)\\nf←paste(f[1], ',', f[2], ', and ', f[3], '.', sep= '')\\nlist(somers=s, fractions=f)\\n}\\nperf.unpen ←perf(full)\\nprint(full,latex=TRUE,coefs=5)\\nLogistic Regression Model\\nlrm(formula = y ~ cohort * (ageg * (rcs(temp, 5) +\\nrcs(rr, 5)) + rcs(waz, 4) + bul.conv + drowsy +\\nagitated + reffort + ausc + feeding + abdominal +\\nhydration + hxprob + pustular + crying + fever.ill +\\nstop.breath + labor), data = Sc.expanded, x = TRUE,\\ny = TRUE)\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 5553 LRχ21824.33R20.406C 0.843\\n0 1512 d.f. 87 g 1.677Dxy0.685\\n1 4041 Pr(>χ2)<0.0001gr5.350γ 0.687\\nmax|∂logL\\n∂β|8×10−7gp0.269τa0.272\\nBrier 0.135\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='86c698a8-ae80-463e-a473-f03238a675e8', embedding=None, metadata={'page_label': '342', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"342 14 Ordinal Regression, Data Reduction, and Penalization\\nTable 14.6 Wald statistics for cohortin the CR model\\nχ2d.f.P\\ncohort (Factor+Higher Order Factors) 199 .47 44<0.0001\\nAll Interactions 172.12 43<0.0001\\nTOTAL 199 .47 44<0.0001\\nCoef S.E. Wald ZPr(>|Z|)\\nIntercept 1.3966 9.0827 0.15 0.8778\\ncohort=Y ≥1 1.5077 14.6443 0.10 0.9180\\nageg=[ 7,60) -9.3715 11.4104 -0.82 0.4115\\nageg=[60,90] -26.4502 17.2188 -1.54 0.1245\\ntemp -0.0049 0.2551 -0.02 0.9846\\n...\\nlatex(anova( full,cohort), file= '',# Table 14.6\\ncaption= 'Waldstatistics for \\\\\\\\co{ cohort} in the CR model ',\\nsize= 'smaller[2] ', label= 'ordinal-anova.cohort ')\\nan←anova(full,india=FALSE, indnl=FALSE)\\nlatex(an, file= '', label= 'ordinal-anova.full ',\\ncaption= 'Waldstatistics for the continuation ratio model.\\nInteractions with \\\\\\\\co{cohort} assess non-proportional\\nhazards ', caption.lot= 'Waldstatistics for $Y$ in the\\ncontinuation ratio model ',\\nsize= 'smaller[2] ')# Table 14.7\\nThismodelhasLR χ2=1824with87d.f.WaldstatisticsareinTables 14.6\\nand14.7. The global test of the constant slopes assumption in the CR model\\n(test of all interactions involving cohort)h a sW a l d χ2= 172 with 43 d.f.,\\nP<0.0001. Consistent with Figure 14.5, the formal tests indicate that ausc\\nis the biggest violator, followed by wazandrr.\\n14.9 Penalized Estimation\\nWe know that the CR model must be extended to ﬁt these data adequately.If\\nthe model is fully extended to allow for all cohort×predictor interactions,we\\nhave not gained any precisionor power in using an ordinal model overusing a\\npolytomouslogisticmodel.Thereforeweseeksomerestrictionsonthemodel’s\\nparameters.The lrmandpentrace functions allow for diﬀering λfor shrinking\\ndiﬀerent types of terms in the model. Here we do a grid search to determine\\nthe optimum penalty for simple main eﬀect (non-interaction) terms and the\\npenalty forinteractionterms, most ofwhich are terms interactingwith cohort\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e7d1365-cf9f-4690-a429-a9f7a2ffe38d', embedding=None, metadata={'page_label': '343', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.9 Penalized Estimation 343\\nTable 14.7 Wald statistics for the continuation ratio model. Interactions with\\ncohortassess non-proportional hazards\\nχ2d.f.P\\ncohort 199.47 44<0.0001\\nageg 48.89 36 0.0742\\ntemp 59.37 24 0.0001\\nrr 93.77 24<0.0001\\nwaz 39.69 6<0.0001\\nbul.conv 10.80 2 0.0045\\ndrowsy 15.19 2 0.0005\\nagitated 13.55 2 0.0011\\nreﬀort 51.85 2<0.0001\\nausc 109.80 2<0.0001\\nfeeding 27.47 2<0.0001\\nabdominal 1.78 2 0.4106\\nhydration 4.47 2 0.1069\\nhxprob 6.62 2 0.0364\\npustular 3.03 2 0.2194\\ncrying 1.55 2 0.4604\\nfever.ill 3.63 2 0.1630\\nstop.breath 5.34 2 0.0693\\nlabor 5.35 2 0.0690\\nageg×temp 8 .18 16 0.9432\\nageg×rr 38.11 16 0.0015\\ncohort×ageg 14 .88 18 0.6701\\ncohort×temp 8 .77 12 0.7225\\ncohort×rr 19 .67 12 0.0736\\ncohort×waz 9 .04 3 0.0288\\ncohort×bul.conv 0 .33 1 0.5658\\ncohort×drowsy 0 .57 1 0.4489\\ncohort×agitated 0 .55 1 0.4593\\ncohort×reﬀort 2 .29 1 0.1298\\ncohort×ausc 38 .11 1<0.0001\\ncohort×feeding 2 .48 1 0.1152\\ncohort×abdominal 0 .09 1 0.7696\\ncohort×hydration 0 .53 1 0.4682\\ncohort×hxprob 2 .54 1 0.1109\\ncohort×pustular 2 .40 1 0.1210\\ncohort×crying 0 .39 1 0.5310\\ncohort×fever.ill 3 .17 1 0.0749\\ncohort×stop.breath 2 .99 1 0.0839\\ncohort×labor 0 .05 1 0.8309\\ncohort×ageg×temp 2 .22 8 0.9736\\ncohort×ageg×rr 10 .22 8 0.2500\\nTOTAL NONLINEAR 93 .36 40<0.0001\\nTOTAL INTERACTION 203 .10 59<0.0001\\nTOTAL NONLINEAR + INTERACTION 257 .70 67<0.0001\\nTOTAL 1211.73 87<0.0001', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d9bc8495-5fe2-4295-97cf-e40d410b2988', embedding=None, metadata={'page_label': '344', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='344 14 Ordinal Regression, Data Reduction, and Penalization\\nto allow for unequal slopes. The following code uses pentrace on the full\\nextended CR model ﬁt to ﬁnd the optimum penalty factors. All combinations\\nof thesimpleandinteraction λs for which the interaction penalty ≥the\\npenalty for the simple parameters are examined.\\nd←options( digits=4)\\npentrace( full,\\nlist(simple=c(0, .025,.05,.075,.1),\\ninteraction=c(0,10,50 ,100,125,150)))\\nBestpenalty:\\nsimple interaction df\\n0.05 125 49.75\\nsimple interaction df aic bic aic.c\\n0.000 0 87.00 1650 1074 1648\\n0.000 10 60.63 1671 1269 1669\\n0.025 10 60.11 1672 1274 1670\\n0.050 10 59.80 1672 1276 1670\\n0.075 10 59.58 1671 1277 1670\\n0.100 10 59.42 1671 1278 1670\\n0.000 50 54.64 1671 1309 1670\\n0.025 50 54.14 1672 1313 1671\\n0.050 50 53.83 1672 1316 1671\\n0.075 50 53.62 1672 1317 1671\\n0.100 50 53.46 1672 1318 1671\\n0.000 100 51.61 1672 1330 1671\\n0.025 100 51.11 1673 1334 1672\\n0.050 100 50.81 1673 1336 1672\\n0.075 100 50.60 1672 1337 1671\\n0.100 100 50.44 1672 1338 1671\\n0.000 125 50.55 1672 1337 1671\\n0.025 125 50.05 1673 1341 1672\\n0.050 125 49.75 1673 1343 1672\\n0.075 125 49.54 1672 1344 1672\\n0.100 125 49.39 1672 1345 1671\\n0.000 150 49.65 1672 1343 1671\\n0.025 150 49.15 1672 1347 1672\\n0.050 150 48.85 1673 1349 1672\\n0.075 150 48.64 1672 1350 1671\\n0.100 150 48.49 1672 1351 1671\\noptions(d)\\nWe see that shrinkage from 87 d.f. down to 49.75 eﬀective d.f. results in an\\nimprovement in χ2–scaled AIC of 23. The optimum penalty factors were 0.05\\nfor simple terms and 125 for interaction terms.\\nLet us now store a penalized version of the full ﬁt, ﬁnd where the eﬀective\\nd.f. were reduced, and compute χ2for each factor in the model. We take\\nthe eﬀective d.f. for a collection of model parameters to be the sum of the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='afc471b8-8fb2-4859-a457-391c27397510', embedding=None, metadata={'page_label': '345', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.9 Penalized Estimation 345\\ndiagonals of the matrix product deﬁned underneath Gray’s Equation 2.9237\\nthat correspond to those parameters.\\nfull.pen ←\\nupdate( full,\\npenalty= list(simple=.05, interaction =125))\\nprint(full.pen, latex= TRUE,coefs=FALSE)\\nLogistic Regression Model\\nlrm(formula = y ~ cohort * (ageg * (rcs(temp, 5) + rcs(rr, 5)) +\\nrcs(waz, 4) + bul.conv + drowsy + agitated + reffort + ausc +\\nfeeding + abdominal + hydration + hxprob + pustular + crying +\\nfever.ill + stop.breath + labor), data = Sc.expanded, x = TRUE,\\ny = TRUE, penalty = list(simple = 0.05, interaction = 125))\\nPenalty factors\\nsimple nonlinear interaction nonlinear.interaction\\n0.05 0.05 125 125\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 5553 LRχ21772.11R20.392C 0.840\\n0 1512 d.f. 49.75 g 1.594Dxy0.679\\n1 4041 Pr(>χ2)<0.0001gr4.924γ 0.681\\nmax|∂logL\\n∂β|1×10−7Penalty 21.48 gp0.263τa0.269\\nBrier 0.136\\neffective.df(full.pen)\\nOriginal and Effective Degrees of Freedom\\nOriginal Penalized\\nAll 87 49.75\\nSimple Terms 20 19.98\\nInteraction or Nonlinear 67 29.77\\nNonlinear 40 16.82\\nInteraction 59 22.57\\nNonlinear Interaction 32 9.62\\n## Compute discrimination for Y=0 vs. Y>0\\nperf.pen ←perf(full.pen) # Figure 14.6\\n# Exclude interactions and cohort effects from plot\\nplot(anova( full.pen), cex.labels=0.75, rm.ia=TRUE,\\nrm.other= 'cohort (Factor+Higher Order Factors) ')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98711512-8024-4508-a312-4122ef4d4c4f', embedding=None, metadata={'page_label': '346', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"346 14 Ordinal Regression, Data Reduction, and Penalization\\nageg\\nfever.ill\\ncrying\\npustular\\nabdominal\\nhydration\\nstop.breath\\nlabor\\nhxprob\\nbul.conv\\nagitated\\ndrowsy\\ntemp\\nfeeding\\nwaz\\nrr\\nreffort\\nausc\\n−20 0 20 40 60 80 100 120\\nχ2 − df\\nFig. 14.6 Importance of predictors in full penalized model, as judged by partial\\nWaldχ2minus the predictor d.f. The Wald χ2values for each line in the dot plot\\ninclude contributions from all higher-order eﬀects. Interaction eﬀects by thems elves\\nhave been removed as has the cohorteﬀect.\\nThis will be the ﬁnal model except for the model used in Section 14.10.\\nThe model has LR χ2= 1772. The output of effective.df shows that non-\\ninteraction terms have barely been penalized, and coeﬃcients of interaction\\nterms have been shrunken from 59 d.f. to eﬀectively 22.6 d.f. Predictive dis-\\ncrimination was assessed by computing the Somers’ Dxyrank correlation\\nbetween Xˆβand whether Y= 0, in the subset of records for which Y=0i s\\nwhat was being predicted. Here Dxy=0.672,and the ROC area is 0.838 (the\\nunpenalized model had an apparent Dxy=0.676). To summarize in another\\nway the eﬀectiveness of this model in screening infants for risks of any abnor-\\nmality, the fraction of infants with predicted probabilities that Y>0b e i n g\\n<0.05,>0.25,and>0.5 are,respectively,0.1,0.28,and 0.14. anovaoutput is\\np l o t t e di nF i g u r e 14.6to give a snapshotof the importance of the variouspre-\\ndictors. The Wald statistics used here are computed on a variance–covariance\\nmatrix which is adjusted for penalization (using Gray Equation 2.6237before\\nit was determined that the sandwich covariance estimator performs less well\\nthan the inverse of the penalized information matrix—see p. 211).\\nThe full equation for the ﬁtted model is below. Only the part of the equa-\\ntion used for predicting Pr( Y= 0) is shown, other than an intercept for\\nY≥1 that does not apply when Y=0 .\\nlatex(full.pen, which=1:21, file= '')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c6e7e98-49c7-499a-9092-5a6802a763ff', embedding=None, metadata={'page_label': '347', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.9 Penalized Estimation 347\\nXˆβ=\\n−1.337435[Y >=1 ]\\n+0.1074525[ageg ∈[7,60)] +0.1971287[ageg ∈[60,90]]\\n+0.1978706temp +0 .1091831(temp −36.19998)3\\n+−2.833442(temp −37)3\\n+\\n+5.07114(temp −37.29999)3\\n+−2.507527(temp −37.69998)3\\n+\\n+0.1606456(temp −39)3\\n+\\n+0.02090741rr −6.336873×10−5(rr−32)3\\n++8.405441×10−5(rr−42)3\\n+\\n+6.152416×10−5(rr−49)3\\n+−0.0001018105(rr −59)3\\n++1.960063×10−5(rr−76)3\\n+\\n−0.07589699waz +0 .02508918(waz +2 .9)3\\n+−0.1185068(waz +0 .75)3\\n+\\n+0.1225752(waz −0.28)3\\n+−0.02915754(waz −1.73)3\\n+−0.4418073 bul .conv\\n−0.08185088 drowsy −0.05327209 agitated −0.2304409 reﬀort\\n−1.158604 ausc −0.1599588 feeding −0.1608684 abdominal\\n−0.05409718 hydration +0 .08086387 hxprob+0 .007519746 pustular\\n+0.04712091 crying +0 .004298725 fever .ill−0.3519033 stop .breath\\n+0.06863879 labor\\n+[ageg∈[7,60)][6.499592×10−5temp−0.00279976(temp −36.19998)3\\n+\\n−0.008691166(temp −37)3\\n+−0.004987871(temp −37.29999)3\\n+\\n+0.0259236(temp −37.69998)3\\n+−0.009444801(temp −39)3\\n+]\\n+[ageg∈[60,90]][0.0001320368temp −0.00182639(temp −36.19998)3\\n+\\n−0.01640406(temp −37)3\\n+−0.0476041(temp −37.29999)3\\n+\\n+0.09142148(temp −37.69998)3\\n+−0.02558693(temp −39)3\\n+]\\n+[ageg∈[7,60)][−0.0009437598rr −1.044673×10−6(rr−32)3\\n+\\n−1.670499×10−6(rr−42)3\\n+−5.189082×10−6(rr−49)3\\n++1.428634×10−5(rr−59)3\\n+\\n−6.382087×10−6(rr−76)3\\n+]\\n+[ageg∈[60,90]][−0.001920811rr −5.52134×10−6(rr−32)3\\n+\\n−8.628392×10−6(rr−42)3\\n+−4.147347×10−6(rr−49)3\\n++3.813427×10−5(rr−59)3\\n+\\n−1.98372×10−5(rr−76)3\\n+]\\nwhere [c] = 1 if subject is in group c,0o t h e r w i s e ;( x)+=xifx>0,0o t h e r w i s e .\\nNow consider displays of the shapes of eﬀects of the predictors. For the\\ncontinuous variables tempandrrthat interact with age group, we show the\\neﬀects for all three age groups separately for each Ycutoﬀ. All eﬀects have\\nbeen centered so that the log odds at the median predictor value is zero\\nwhencohort=’all’ , so these plots actually show log odds relative to reference\\nvalues. The patterns in Figures 14.9and14.8are in agreement with those in\\nFigure14.5.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0c5050b7-9e30-4d9f-8d17-c09dee66a207', embedding=None, metadata={'page_label': '348', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"348 14 Ordinal Regression, Data Reduction, and Penalization\\nyl←c(-3, 1) # put all plots on common y-axis scale\\n# Plot predictors thatinteract withanother predictor\\n# Vary ageg over all age groups, then vary temp over its\\n# default range (10th smallest to 10th largest values in\\n# data). Make a separate plot for each 'cohort '\\n# ref.zero centers effects usingmedian x\\ndd←datadist(Sc.expanded); dd ←datadist(dd, cohort)\\noptions(datadist= 'dd')\\np1←Predict(full.pen, temp, ageg, cohort,\\nref.zero=TRUE, conf.int=FALSE)\\np2←Predict(full.pen, rr, ageg, cohort,\\nref.zero=TRUE, conf.int=FALSE)\\np←rbind(temp=p1, rr=p2) # Figure 14.7:\\nsource(paste( 'http:// biostat.mc.vanderbilt.edu/ wiki/pub/Main ',\\n'RConfiguration/graphicsSet.r ', sep= '/'))\\nggplot(p, ∼cohort, groups= 'ageg ', varypred= TRUE,\\nylim=yl, layout=c(2, 1), legend.position=c(.85,.8),\\naddlayer=ltheme(width=3, height=3, text=2.5, title=2.5),\\nadj.subtitle= FALSE) # ltheme defined withsource()\\n# For each predictor that only interacts with cohort, show\\n# the differing effects of the predictor for predicting\\n# Pr(Y=0) and Pr(Y=1 given Y exceeds 0) on the samegraph\\ndd$limits[ 'Adjust to ','cohort ']←'Y≥1'\\nv←Cs(waz, bul.conv, drowsy, agitated, reffort, ausc,\\nfeeding, abdominal, hydration, hxprob, pustular,\\ncrying)\\nyeq1←Predict( full.pen, name=v, ref.zero= TRUE)\\nyl←c(-1.5, 1.5)\\nggplot( yeq1,ylim=yl, sepdiscrete= 'vertical ')# Figure 14.8\\ndd$limits[ 'Adjust to ','cohort ']←'all'# original default\\nall←Predict( full.pen, name=v, ref.zero= TRUE)\\nggplot(all, ylim=yl, sepdiscrete= 'vertical ')# Figure 14.9\\n1\\n14.10 Using Approximations to Simplify the Model\\nParsimonious models can be developed by approximating predictions from\\nthe model to any desired level of accuracy. Let ˆL=Xˆβdenote the predicted\\nlog odds from the full penalized ordinal model, including multiple records for\\nsubjects with Y>0. Then we can use a variety of techniques to approximate\\nˆLfrom a subset of the predictors (in their raw form). With this approach\\none can immediately see what is lost over the full model by computing, for\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d6efb4ce-ed15-4e7f-a211-496da27e6e2b', embedding=None, metadata={'page_label': '349', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='14.10 Using Approximations to Simplify the Model 349\\nexample, the mean absolute error in predicting ˆL. Another advantage to full\\nmodel approximation is that shrinkage used in computing ˆLis inherited by\\nany model that predicts ˆL. In contrast, the usual stepwise methods result in\\nˆβthat are too large since the ﬁnal coeﬃcients are estimated as if the model\\nstructure were prespeciﬁed. 2\\nCART would be particularly useful as a model approximator as it would\\nresult in a prediction tree that would be easy for health workers to use.\\nall Y>=1\\n−3−2−101\\n34 36 38 40 34 36 38 40\\nTemperaturelog odds\\nall Y>=1\\n−3−2−101\\n30 60 90 30 60 90\\nAdjusted respiratory ratelog odds\\nFig. 14.7 Centered eﬀects of predictors on the log odds, showing the eﬀects of two\\npredictors with interaction eﬀects for the ageintervals noted. The title allrefers\\nto the prediction of Y=0|Y≥0, that is, Y=0 .Y>=1refers to predicting the\\nprobability of Y=1|Y≥1.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee11d8ea-9763-425c-8c79-cee577e56181', embedding=None, metadata={'page_label': '350', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='350 14 Ordinal Regression, Data Reduction, and Penalization\\nWeight−for−age zscore crying drowsy\\nfeeding hxprob hydration\\nreffort waz−101\\n−101\\n−1010.0 2.5 5.0 7.5 10.0 −5 −4 −3 −2 −1 0 1 012345\\n012345 −2 −1 0 1 2.5 5.0 7.5 10.0\\n012345 −4 −2 0 2log odds\\nAny Pustular Condition ausc\\nbul.conv pustular01\\n01\\n−1 0 11\\n1\\n00\\n1\\n−1 0 1\\nlog odds\\nFig. 14.8 Centered eﬀects of predictors on the log odds, for predicting Y=1|Y≥1\\nUnfortunately, a 50-node CART was required to predict ˆLwith anR2≥0.9,\\nand the mean absolute error in the predicted logit was still 0.4. This will\\nhappen when the model contains many important continuous variables.\\nLet’s approximatethe full model using its important components,by using\\na step-down technique predicting ˆLfrom all of the component variables using\\nordinary least squares.In using step-down with the least squares function ols\\ninrmsthere is a problem when the initial R2=1.0 as in that case the esti-\\nmate ofσ= 0. This can be circumvented by specifying an arbitrary nonzero\\nvalue of σtools(here 1.0), as we are not using the variance–covariance\\nmatrix from olsanyway. Since cohortinteracts with the predictors, separate\\napproximations can be developed for each level of Y. For this example we\\napproximate the log odds that Y= 0 using the cohort of patients used for\\ndetermining Y=0 ,t h a ti s , Y≥0o rcohort=’all’ .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='93c33272-c430-429c-aaea-f208081d177f', embedding=None, metadata={'page_label': '351', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.10 Using Approximations to Simplify the Model 351\\nWeight−for−age zscore crying drowsy\\nfeeding hxprob hydration\\nreffort waz−101\\n−101\\n−1010.0 2.5 5.0 7.5 10.0 −5 −4 −3 −2 −1 0 1 012345\\n012345 −2 −1 0 1 2.5 5.0 7.5 10.0\\n012345 −4 −2 0 2log odds\\nAny Pustular Condition ausc\\nbul.conv pustular01\\n01\\n01\\n01\\n−1 0 1 −1 0 1\\nlog odds\\nFig. 14.9 Centered eﬀects of predictors on the log odds, for predicting Y≥1. No\\nplot was made for the fever.ill, stop.breath .o rlaborcluster scores.\\nplogit ←predict(full.pen)\\nf←ols(plogit ∼ageg*(rcs( temp,5) + rcs(rr ,5)) +\\nrcs(waz,4) + bul.conv + drowsy + agitated +\\nreffort + ausc + feeding + abdominal + hydration +\\nhxprob + pustular + crying + fever.ill +\\nstop.breath + labor,\\nsubset=cohort== 'all', data=Sc.expanded, sigma=1)\\n# Do fast backward stepdown\\nw←options( width=120)\\nfastbw(f, aics=1e10)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='36505ad7-2b85-4643-b78d-5e75ec944883', embedding=None, metadata={'page_label': '352', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"352 14 Ordinal Regression, Data Reduction, and Penalization\\nDeleted Chi −Sq d.f. P Residual d.f. P AIC R2\\nageg ∗temp 1.87 8 0.9848 1.87 8 0.9848 −14.13 1.000\\nageg 0.05 2 0.9740 1.92 10 0.9969 −18.08 1.000\\npustular 0.02 1 0.8778 1.94 11 0.9987 −20.06 1.000\\nfever. ill 0.08 1 0.7828 2.02 12 0.9994 −21.98 1.000\\ncrying 9.47 1 0.0021 11.49 13 0.5698 −14.51 0.999\\nabdominal 12.66 1 0.0004 24.15 14 0.0440 −3.85 0.997\\nrr 17.90 4 0.0013 42.05 18 0.0011 6.05 0.995\\nhydration 13.21 1 0.0003 55.26 19 0.0000 17.26 0.993\\nlabor 23.48 1 0.0000 78.74 20 0.0000 38.74 0.990\\nstop.breath 33.40 1 0.0000 112.14 21 0.0000 70.14 0.986\\nbul.conv 51.53 1 0.0000 163.67 22 0.0000 119.67 0.980\\nagitated 63.66 1 0.0000 227.33 23 0.0000 181.33 0.972\\nhxprob 84.16 1 0.0000 311.49 24 0.0000 263.49 0.962\\ndrowsy 109.86 1 0.0000 421.35 25 0.0000 371.35 0.948\\ntemp 295.67 4 0.0000 717.01 29 0.0000 659.01 0.911\\nwaz 368.86 3 0.0000 1085.87 32 0.0000 1021.87 0.866\\nreffort 449.83 1 0.0000 1535.70 33 0.0000 1469.70 0.810\\nageg ∗rr 751.19 8 0.0000 2286.90 41 0.0000 2204.90 0.717\\nausc 1906.82 1 0.0000 4193.72 42 0.0000 4109.72 0.482\\nfeeding 3900.33 1 0.0000 8094.04 43 0.0000 8008.04 0.000\\nApproximate Estimates after Deleting Factors\\nCoef S.E. Wald Z P\\n[1 ,] 1.617 0.01482 109.1 0\\nFactors in Final Model\\nNone\\noptions(w)\\n# 1e10 causes all variables to eventually be\\n# deleted so can see mostimportant ones in order\\n# Fit an approximation to the fullpenalized model using\\n# most important variables\\nfull.approx ←\\nols(plogit ∼rcs(temp,5) + ageg*rcs(rr,5) +\\nrcs(waz,4) + bul.conv + drowsy + reffort +\\nausc + feeding,\\nsubset=cohort== 'all', data= Sc.expanded)\\np←predict(full.approx)\\nabserr ←mean(abs(p - plogit[cohort == 'all']))\\nDxy←somers2(p, y[ cohort == 'all'])['Dxy']\\nThe approximatemodel had R2againstthe fullpenalizedmodelof0.972,and\\nthe mean absolute error in predicting ˆLwas 0.17. The Dxyrank correlation\\nbetween the approximate model’s predicted logit and the binary event Y=0\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c8d1750f-d467-4aa5-82ea-803a7303db82', embedding=None, metadata={'page_label': '353', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.11 Validating the Model 353\\nis 0.665 as compared with the full model’s Dxy=0.672. See Section 19.5for\\nan example of computing correct estimates of variance of the parameters in\\nan approximate model.\\nNext turn to diagramming this model approximation so that all predicted\\nvalues can be computed without the use of a computer. We draw a type of\\nnomogram that converts each eﬀect in the model to a 0 to 100 scale which is\\njust proportional to the log odds. These points are added across predictors\\nto derive the“Total Points,”which are converted to ˆLa n dt h e nt op r e d i c t e d\\nprobabilities. For the interaction between rrandageg,rms’snomogram func-\\ntion automatically constructs three rraxes—only one is added into the total\\npoint score for a given subject. Here we draw a nomogram for predicting the\\nprobability that Y>0, which is 1 −Pr(Y= 0). This probability is derived\\nby negating ˆβandXˆβin the model derived to predict Pr( Y=0 ) .\\nf←full.approx\\nf$coefficients ←-f$coefficients\\nf$linear.predictors ←-f$linear.predictors\\nn←nomogram(f,\\ntemp=32:41, rr=seq(20 ,120,by=10),\\nwaz=seq(-1.5,2,by=.5),\\nfun=plogis, funlabel= 'Pr(Y>0) ',\\nfun.at=c(.02,.05,seq(.1,.9,by=.1),.95,.98))\\n# Print n to see point tables\\nplot(n, lmgp=.2, cex.axis=.6) # Figure 14.10\\nnewsubject ←\\ndata.frame( ageg= '[0 ,7 ) ', rr=30, temp=39, waz=0, drowsy=5,\\nreffort=2, bul.conv=0, ausc=0, feeding=0)\\nxb←predict(f, newsubject)\\nThe nomogram is shown in Figure 14.10. As an example in using the nomo-\\ngram,asix-day-oldinfantgetsapproximately9pointsforhavingarespiration\\nrate of 30/minute, 19 points for having a temperature of 39◦C, 11 points for\\nwaz=0, 14 points for drowsy=5, and 15 points for reffort=2 . Assuming that\\nbul.conv=ausc=feeding=0 , that infant gets 68 total points. This corresponds to\\nXˆβ=−0.68 and a probability of 0.34. 3\\n14.11 Validating the Model\\nFor the full CR model that was ﬁtted using penalized maximum likelihood\\nestimation (PMLE), we used 200 bootstrap replications to estimate and then\\nto correct for optimism in various statistical indexes: Dxy, generalized R2,\\nintercept and slope of a linear re-calibration equation for Xˆβ,t h em a x i m u m\\ncalibration error for Pr( Y= 0) based on the linear-logistic re-calibration\\n(Emax), and the Brier quadratic probability score B.P M L Ei su s e da te a c h\\nof the 200 resamples. During the bootstrap simulations, we sample with\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='449b464b-9add-438d-abc5-4015f85761d1', embedding=None, metadata={'page_label': '354', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"354 14 Ordinal Regression, Data Reduction, and Penalization\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 00\\nTemperature\\n37 36 35 34 33 3238 39 41\\nrr (ageg=[ 0, 7))\\n40 30 2050 60 70 90 110\\nrr (ageg=[ 7,60))\\n40 30 2050 60 70 80 90 100 110 120\\nrr (ageg=[60,90])\\n40 30 2050 60 70 80 90 100 110 120\\nWeight−for−age\\nzscore2 1 0 −0.5\\nbul.conv\\n01\\ndrowsy\\n024135\\nreffort\\n024135\\nausc\\n01\\nfeeding\\n024135\\nTotal Points\\n0 20 40 60 80 100 120 140 160 180 200 220 240 260\\nLinear Predictor\\n−4 −3 −2 −1 0 1 2 3 4 5 6\\nPr(Y>0)\\n0.02 0.05 0.1 0.2 0.3 0.40.50.6 0.7 0.8 0.9 0.95 0.98\\nFig. 14.10 Nomogram for predicting Pr( Y>0) from the penalized extended CR\\nmodel, using an approximate model ﬁtted using ordinary least squares ( R2=0.972\\nagainst the full model’s predicted logits).\\nreplacementfrom the patientsandnot from the 5553expanded records, hence\\nthe speciﬁcation cluster=u$subs ,w h e r e u$subsis the vector of sequential pa-\\ntient numbers computed from cr.setup above. To be able to assess predictive\\naccuracy of a single predicted probability, the subsetparameter is speciﬁed\\nso that Pr( Y= 0) is being assessed even though 5553 observations are used\\nto develop each of the 200 models.\\nset.seed(1) # so can reproduce results\\nv←validate( full.pen, B =200,cluster=u$ subs,\\nsubset=cohort== 'all')\\nlatex(v, file= '', digits=2, size= 'smaller ')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44e384f8-6ddc-488e-9089-0328a552fdf2', embedding=None, metadata={'page_label': '355', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.12 Summary 355\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.67 0 .68 0.67 0 .01 0 .66 200\\nR20.38 0 .38 0.37 0 .01 0 .36 200\\nIntercept −0.03−0.03 0.00−0.03 0 .00 200\\nSlope 1 .03 1 .03 1.00 0 .03 1 .00 200\\nEmax 0.00 0 .00 0.00 0 .00 0 .00 200\\nD 0.28 0 .29 0.28 0 .01 0 .27 200\\nU 0.00 0 .00 0.00 0 .00 0 .00 200\\nQ 0.28 0 .29 0.28 0 .01 0 .27 200\\nB 0.12 0 .12 0.12 0 .00 0 .12 200\\ng 1.47 1 .50 1.45 0 .04 1 .42 200\\ngp 0.22 0 .23 0.22 0 .00 0 .22 200\\nv←round(v, 3)\\nWe see that for the apparent Dxy=0.672 and that the optimism from\\noverﬁtting was estimated to be 0.011 for the PMLE model, so the bias-\\ncorrected estimate of predictive discrimination is 0.661. The intercept and\\nslope needed to re-calibrate Xˆβt oa4 5◦line are very near (0, 1). The es-\\ntimate of the maximum calibration error in predicting Pr( Y= 0) is 0.001,\\nwhich is quite satisfactory. The corrected Brier score is 0.122.\\nThe simple calibration statistics just listed do not address the issue of\\nwhether predicted values from the model are miscalibrated in a nonlinear\\nway, so now we estimate an overﬁtting-corrected calibration curve nonpara-\\nmetrically.\\ncal←calibrate( full.pen, B =200,cluster=u$ subs,\\nsubset=cohort== 'all')\\nerr←plot(cal) # Figure 14.11\\nn=5553 Mean absolute error=0.017 Mean squared error=0. 00043\\n0.9 Quantile of absolute error=0.038\\nThe results are shown in Figure 14.11. One can see a slightly nonlinear cali-\\nbrationfunctionestimate,buttheoverﬁtting-correctedcalibrationisexcellent\\neverywhere,being only slightly worsethan the apparent calibration.The esti-\\nmated maximum calibration error is 0.044. The excellent validation for both\\npredictive discrimination and calibrationare a result of the large sample size,\\nfrequency distribution of Y, initial data reduction, and PMLE.\\n14.12 Summary\\nClinically guided variable clustering and item weighting resulted in a great\\nreduction in the number of candidate predictor degrees of freedom and hence\\nincreased the true predictiveaccuracyof the model. Scoressummarizing clus-\\nters of clinical signs, along with temperature, respiration rate, and weight-\\nfor-age after suitable nonlinear transformationand allowance for interactions\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d412750b-dbcc-4f93-a411-75d14043c8ad', embedding=None, metadata={'page_label': '356', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='356 14 Ordinal Regression, Data Reduction, and Penalization\\n0.0 0.2 0.4 0.6 0.8 1.00.00.20.40.60.81.0\\nPredicted Pr{y=1}Actual Probability\\nApparent\\nBias−corrected\\nIdeal\\nFig. 14.11 Bootstrap calibration curve for the full penalized extended CR model.\\n200 bootstrap repetitions were used in conjunction with the loesssmoother.111Also\\nshown is a “rug plot” to demonstrate how eﬀective this model is in discriminating\\npatients into low- and high-risk groups for Pr( Y= 0) (which corresponds with the\\nderived variable value y=1w h e n cohort=’all’ ).\\nwith age, are powerful predictors of the ordinal response. Graphical methods\\nare eﬀective for detecting lack of ﬁt in the PO and CR models and for dia-\\ngramming the ﬁnal model. Model approximationalloweddevelopmentof par-\\nsimonious clinical prediction tools. Approximatemodels inherit the shrinkage\\nfrom the full model. For the ordinalmodel developedhere,substantial shrink-\\nage of the full model was needed.\\n14.13 Further Reading\\n1See Moons et al.462for another case study in penalized maximum likelihood\\nestimation.\\n2Thelassomethod of Tibshirani608,609also incorporates shrinkage into variable\\nselection.\\n3To see how this compares with predictions usingthe fullmodel, the extra clinical\\nsigns in that model that are not in the approximate model were predicted\\nindividually on the basis of Xˆβfrom the reduced model along with the signs\\nthat are in that model, using ordinary linear regression. The signs not speciﬁed\\nwhen evaluating the approximate model were then set to predicted values based\\non the values given for the 6-day-old infant above. The resulting Xˆβfor the full\\nmodel is −0.81 and the predicted probability is 0.31, as compared with -0.68\\nand 0.34 quoted above.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a08eff6d-afbd-4c26-bafe-b54211ba0ae7', embedding=None, metadata={'page_label': '357', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"14.14 Problems 357\\n14.14 Problems\\nDevelop a proportional odds ordinal logistic model predicting the severity\\nof functional disability ( sfdm2) in SUPPORT. The highest level of this vari-\\nablecorrespondstopatientsdyingbeforethetwo-monthfollow-upinterviews.\\nConsider this level as the most severe outcome. Consider the following pre-\\ndictors: age, sex, dzgroup, num.co, scoma, race (use all levels), meanbp, hrt,\\ntemp, pafi, alb, adlsc . The last variable is the baseline level of functional\\ndisability from the“activities of daily living scale.”\\n1. For the variables adlsc, sex, age, meanbp , and others if you like, make\\nplots of means of predictors stratiﬁed by levels of the response, to check\\nforordinality.Onthesameplot,showestimatesofmeansassumingthepro-\\nportional odds relationship between predictors and response holds. Com-\\nment on the evidence for ordinality and for proportional odds.\\n2. To allow for maximum adjustment of baseline functional status, treat\\nthis predictor as nominal (after rounding it to the nearest whole num-\\nber; fractional values are the result of imputation) in remaining steps, so\\nthat all dummy variables will be generated. Make a single chart showing\\nproportions of various outcomes stratiﬁed (individually) by adlsc, sex,\\nage, meanbp . For continuous predictors use quartiles. You can pass the fol-\\nlowing function to the summary(summary.formula ) function to obtain the\\nproportions of patients having sfdm2at or worse than each of its possi-\\nble levels (other than the ﬁrst level). An easy way to do this is to use\\nthecumcategory function with the Hmiscpackage’s summary.formula func-\\ntion.cumcategorysummary.formula Print estimates to only two signiﬁcant\\ndigits of precision. Manually check the calculations for the sexvariable\\nusingtable(sex, sfdm2) . Then plot all estimates on a single graph using\\nplot(object, which=1:4) ,w h e r e objectwas created by summary(actually\\nsummary.formula ). Note: for printing tables you may want to convert sfdm2\\nto a 0–4 variable so that column headers are short and so that later cal-\\nculations are simpler. You can use for example:\\nsfdm←as.integer( sfdm2) - 1\\n3. Use an Rfunction such as the following to compute the logitsof the cu-\\nmulative proportions.\\nsf←function(y)\\nc('Y≥1'=qlogis(mean(y ≥1)),\\n'Y≥2'=qlogis(mean(y ≥2)),\\n'Y≥3'=qlogis(mean(y ≥3)),\\n'Y≥4'=qlogis(mean(y ≥4)))\\nAs theY= 3 category is rare, it may be even better to omit the Y≥4\\ncolumn above, as was done in Section 13.3.9and Figure 13.1.F o re a c h\\npredictor pick two rows of the summarytable having reasonable sample\\nsizes, and take the diﬀerence between the two rows. Comment on the\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5a8322c6-877f-4e84-ad1e-080fc5d52110', embedding=None, metadata={'page_label': '358', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"358 14 Ordinal Regression, Data Reduction, and Penalization\\nvalidity of the proportional odds assumption by assessing how constant\\nthe row diﬀerences are across columns. Note: constant diﬀerences in log\\nodds (logits) mean constant ratios of odds or constant relative eﬀects of\\nthe predictor across outcome levels.\\n4. Make two plots nonparametrically relating ageto all of the cumulative\\nproportions or their logits. You can use commands such as the following\\n(to use the RHmiscpackage).\\nfor(i in 1:4)\\nplsmo(age, sfdm ≥i, add=i>1,\\nylim=c(.2,.8), ylab= 'Proportion Y ≥j')\\nfor(i in 1:4)\\nplsmo(age, sfdm ≥i, add=i>1, fun=qlogis,\\nylim=qlogis (c(.2,.8)), ylab= 'logit ')\\nComment on the linearity of the ageeﬀect (which of the two plots do\\nyou use?) and on the proportional odds assumption for age, by assessing\\nparallelism in the second plot.\\n5. Impute raceusing the most frequent category and pafiandalbusing\\n“normal”values.\\n6. Fit a model to predict the ordinal response using all predictors. For con-\\ntinuous ones assume a smooth relationship but allow it to be nonlinear.\\nQuantify the ability of the model to discriminate patients in the ﬁve out-\\ncomes. Do an overall likelihood ratio test for whether any variables are\\nassociated with the level of functional disability.\\n7. Compute partialtests ofassociationforeachpredictoranda testofnonlin-\\nearity for continuous ones. Compute a global test of nonlinearity. Graphi-\\ncally display the ranking of importance of the predictors.\\n8. Displaytheshapeofhoweachpredictorrelatestothelogoddsofexceeding\\nany level of sfdm2you choose, setting other predictors to typical values\\n(one value per predictor). By default, Predictwill make predictions for\\nthe second response category, which is a satisfactory choice here.\\n9. Use resampling to validate the Somers’ Dxyrank correlation between pre-\\ndicted logit and the ordinal outcome. Also validate the generalized R2,\\nand slope shrinkage coeﬃcient, all using a single Rcommand. Comment\\non the quality (potential“export-ability”) of the model.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3f112697-cc0c-4d14-8a9e-c530eb542915', embedding=None, metadata={'page_label': '359', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 15\\nRegression Models for Continuous Y\\nand Case Study in Ordinal Regression\\nThis chapterconcernsunivariatecontinuous Y. Therearemanymultivariable\\nmodels for predicting such response variables, such as\\n•linear models with assumed normal residuals, ﬁtted with ordinary least\\nsquares\\n•generalized linear models and other parametric models based on special\\ndistributions such as the gamma\\n•generalized additive models (GAMs)277\\n•generalization of GAMs to also nonparametrically transform Y(see\\nChapter 16)\\n•quantile regression (see Section 15.2)\\n•other robust regression models that, like quantile regression, use an objec-\\ntive diﬀerent from minimizing the sum of squared errors635\\n•semiparametric models based on the ranks of Y, such as the Cox pro-\\nportional hazards model (Chapter 20) and the proportional odds ordinal\\nlogistic model (Chapters 13and14)\\n•cumulative probability models (often called cumulative link models )w h i c h\\nare semiparametric models from a wider class of families than the logistic.\\nSemiparametricmodelsthattreat Yasordinalbutnotinterval-scaledhave\\nmany advantages including robustness and freedom from all distributional\\nassumptions for Yconditional on any given set of predictors. Advantages\\nare demonstrated in a case study of a cumulative probability ordinal model.\\nSome of the results are compared to quantile regression and OLS. Many of\\nthe methods used in the case study also apply to ordinary linear models.\\n15.1 The Linear Model\\nThe most popular multivariable model for analyzing a univariate continuous\\nYis the linear model\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 15359', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='55f59ce2-cdb2-4a52-8b73-2328174b911a', embedding=None, metadata={'page_label': '360', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='360 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nE(Y|X)=Xβ, (15.1)\\nwhereβis estimated using ordinary least squares, that is, by solving for ˆβto\\nminimize∑(Yi−Xˆβ)2.\\nTo compute P-values and conﬁdence limits using parametric methods we\\nwould have to assume that Y|Xis normal with mean Xβand constant vari-\\nanceσ2a. One could estimate conditional means of Ywithout any distribu-\\ntional assumptions, but least squares estimators are not robust to outliers or\\nhigh-leverage points, and the model would be inaccurate in estimating condi-\\ntional quantiles of Y|Xor Prob[Y≥c|X] unless normality of residuals holds.\\nTo be accurate in estimating all quantities, the linear model assumes that\\nthe Gaussian distribution of Y|X1is a simple shift from the distribution of\\nY|X2.\\n15.2 Quantile Regression\\nQuantile regression355,357is a diﬀerent approach to modeling Y.I tm a k e sn o\\ndistributional assumptions other than continuity of Y, while having all the\\nusual right hand side assumptions. Quantile regression provides essentially\\nthe same estimates as sample quantiles if there is only an intercept or a cate-\\ngoricalpredictorinthemodel.Quantileregressionis transformationinvariant\\n— pre-transforming Yis not important.\\nQuantile regression is a natural generalization of sample quantiles. Let\\nρτ(y)=y(τ−[y<0]). The τthsample quantile is the minimizer qof∑n\\ni−1ρτ(yi−q). For a conditional τthquantile of Y|Xthe corresponding\\nquantile regression estimator ˆβτminimizes∑n\\ni=1ρτ(Yi−Xβ).\\nIn non-large samples, quantile regression is not as eﬃcient at estimating\\nquantiles as is ordinary least squares at estimating the mean, if the latter’s\\nassumptions hold.\\nKoenker’s quantreg package in R356implements quantile regression, and\\nthermspackage’s Rqfunction provides a front-end that gives rise to various\\ngraphics and inference tools.\\nUsing quantile regression, we directly model the median as a function\\nof covariates so that only the Xβstructure need be correct. Other quantiles\\n(e.g., 90thpercentile) can be modeled but standard errorswill be much larger\\nas it is more diﬃcult to precisely estimate outer quantiles.\\naThe latter assumption may be dispensed with if we use a robust Huber–White or\\nbootstrap covariance matrix estimate. Normality may sometimes be dispensed with\\nby using bootstrap conﬁdence intervals.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4065ccbe-fa8b-4b4d-a083-837eb36a6143', embedding=None, metadata={'page_label': '361', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.3 Ordinal Regression Models for Continuous Y 361\\n15.3 Ordinal Regression Models for Continuous Y\\nA diﬀerent robust semiparametric regression approach than quantile regres-\\nsion is the cumulative probability ordinal model. Semiparametric models\\nhave several advantages over parametric models such as OLS. While quantile\\nregression has no restriction in the parameters when modeling one quantile\\nversus anotherb, ordinal cumulative probability models assume a connection\\nbetween distributions of Yfor diﬀerent X. Ordinal regression even makes\\none less assumption than quantile regression about the distribution of Yfor\\na speciﬁc X: the distribution need not be continuous.\\nApplying an increasing 1–1 transformation to Yresults in no change to\\nregressioncoeﬃcientestimateswithordinalregressionc.Regressioncoeﬃcient\\nestimates are completely robust to extreme Yvaluesd. Estimates of quantiles\\nofYfrom ordinal regression are exactly transformation-preserving, e.g., the\\nestimate of the median of log Yis exactly the log of the estimate of the\\nmedianY.\\nFor a general continuous distribution function F(y), an ordinal regression\\nmodel based on cumulative probabilities may be stated as followse.L e tt h e\\nordered unique values of Ybe denoted by y1,y2,...,y kand let the intercepts\\nassociatedwith y1,...,y kbeα1,α2,...,α k,whereα1=∞becauseProb[ Y≥\\ny1]=1 .L e t αy=αi,i:yi=y.T h e n\\nProb[Y≥yi|X]=F(αi+Xβ)=F(αyi+Xβ) (15.2)\\nFor the OLS fully parametric case, the model may be restated\\nProb[Y≥y|X]=P r o b [Y−Xβ\\nσ≥y−Xβ\\nσ] (15.3)\\n=1−Φ(y−Xβ\\nσ)=Φ(−y\\nσ+Xβ\\nσ) (15.4)\\nbQuantile regression allows the estimated value of the 0.5 quantile to be higher than\\nthe estimated value of the 0.6 quantile for some values of X. Composite quantile\\nregression690removes this possibility by forcing all the Xcoeﬃcients to be the same\\nacross multiple quantiles, a restriction not unlike what cumulative probability ordinal\\nmodels make.\\ncFor symmetric distributions applying a decreasing transformation will negate the\\ncoeﬃcients. For asymmetric distributions (e.g., Gumbel), reversing the order ofY\\nwill do more than change signs.\\ndOnly an estimate of mean Yfrom these ˆβs is non-robust.\\neIt is more traditional to state the model in terms of Prob[ Y≤y|X] but we use\\nProb[Y≥y|X] so that higher predicted values are associated with higher Y.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ad20c902-0912-40e8-9b1c-169cff19faf4', embedding=None, metadata={'page_label': '362', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='362 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nTable 15.1 Distribution families used in ordinal cumulative probability models. Φ\\ndenotes the Gaussian cumulative distribution function. For the Connection column,\\nP1=P r o b [ Y≥y|X1],P2=P r o b [ Y≥y|X2],Δ=(X2−X1)β. The connection\\nspeciﬁes the only distributional assumption if the model is ﬁtted semiparametrically,\\ni.e, contains an intercept for every unique Yvalue less one. For parametric models, P1\\nmust be speciﬁed absolutely instead of just requiring a relationship between P1and\\nP2. For example, the traditional Gaussian parametric model speciﬁes that Prob[ Y≥\\ny|X]=1−Φ(y−Xβ\\nσ)=Φ(−y+Xβ\\nσ).\\nDistribution F Inverse Link Name Connection\\n(Link Function)\\nLogistic [1 + exp( −y)]−1log(y\\n1−y) logitP2\\n1−P2=P1\\n1−P1exp(∆)\\nGaussian Φ(y) Φ−1(y)p robit P2=Φ(Φ−1(P1)+∆)\\nGumbel maximum exp( −exp(−y))log(−log(y)) log −log P2=Pexp(∆)\\n1\\nvalue\\nGumbel minimum 1 −exp(−exp(y))log(−log(1−y)) complementary 1−P2=( 1−P1)exp(∆)\\nvalue log−log\\nCauchy1\\nπtan−1(y)+1\\n2tan[π(y−1\\n2)] cauchit\\nso that to within an additive constantfαy=−y\\nσ(intercepts αare linear in\\nywhereas they are arbitrarily descending in the ordinal model), and σis\\nabsorbed in βto put the OLS model into the new notation.\\nThe general ordinal regression model assumes that for ﬁxed X1,X2,\\nF−1(Prob[Y≥y|X2])−F−1(Prob[Y≥y|X1]) (15.5)\\n=(X2−X1)β (15.6)\\nindependent of the αs (parallelism assumption). If F=[ 1+e x p ( −y)]−1,t h i s\\nis the proportional odds assumption.\\nCommon choices of F, implemented in the Rrms ormfunction, are shown\\nin Table 15.1. The Gumbel maximum value distribution is also called the\\nextreme value type I distribution. This distribution (log −log link) also rep-\\nresentsa continuoustime proportionalhazardsmodel. The hazardratiowhen\\nXchanges from X1toX2is exp(−(X2−X1)β).\\nThe mean of Y|Xis easily estimated from a ﬁtted cumulative probability\\nordinal model by computing\\nn∑\\ni=1yiˆProb[Y=yi|X] (15.7)\\nand theqthquantile of Y|Xisysuch that F−1(1−q)−Xˆβ=ˆαy.g\\nfˆαyare unchanged if a constant is added to all y.\\ngThe intercepts have to be shifted to the left one position in solving this equation\\nbecause the quantile is such that Prob[ Y≤y]=qwhereas the model is stated in\\nterms of Prob[ Y≥y].', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='961ba9d9-a348-4686-8804-e86ca185bca7', embedding=None, metadata={'page_label': '363', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.3 Ordinal Regression Models for Continuous Y 363\\nTheormfunction in the rmspackage takes advantage of the information\\nmatrix being of a sparse tri-band diagonal form for the intercept parameters.\\nThis makes the computations eﬃcient even for hundreds of intercepts (i.e.,\\nunique values of Y).ormis made to handle continuous Y.\\nOrdinal regression has nice properties in addition to those listed above,\\nallowing for\\n•estimation of quantiles as eﬃciently as quantile regression if the parallel\\nslopes assumptions hold\\n•eﬃcient estimation of mean Y\\n•direct estimation of Prob[ Y≥y|X]\\n•arbitrary clumping of values of Y, while still estimating βand mean Y\\neﬃcientlyh\\n•solutionsfor ˆβusing ordinaryNewton-Raphsonorother popularoptimiza-\\ntion techniques\\n•being based on a standard likelihood function, penalized estimation can\\nbe straightforward\\n•Wald, score,and likelihoodratio χ2tests thataremorepowerfulthan tests\\nfrom quantile regression.\\nOn the lastpoint, if thereis a singlepredictorin the model and itis binary,\\nthe score test from the proportional odds model is essentially the Wilcoxon\\ntest, and the score test from the Gumbel log-log cumulative probability\\nmodel is essentially the log-rank test.\\n15.3.1 Minimum Sample Size Requirement\\nWhenYis continuous and the purpose of an ordinal model includes semi-\\nparametric estimation of probabilities or quantiles, the accuracy of estimates\\nis limited even more by the accuracy of estimating the empirical cumulative\\ndistribution of Ythan by estimating β.W h e nβ= 0, intercept estimates are\\ntransformations of the empirical distribution step function. As described in\\nSection20.3, the sample size must be 184 to estimate the entire distribution\\nofYwith a global marginof errornot exceeding 0.1. For estimating the mean\\nofY, smaller sample sizes may be needed.\\nhBut it is not sensible to estimate quantiles of Ywhen there are heavy ties in Yin\\nthe area containing the quantile.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9b36cfd1-12d9-40f3-95d3-80c6f2362edc', embedding=None, metadata={'page_label': '364', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='364 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\n15.4 Comparison of Assumptions of Various Models\\nQuantileregressionmakesthefewestleft-hand-sidemodelassumptionsexcept\\nfortheassumptionthat Ybecontinuous,butcanhavelessestimatorprecision\\nthan other models and has lower power. To summarize how assumptions of\\nparametricmodelscomparetoassumptionsofsemiparametricordinalmodels,\\nconsider the ordinary linear model or its special case the equal variance two-\\nsamplet-test, vs. the probit or logit (proportional odds) ordinal model or\\ntheir specialcases the Van der Waerden(normal-scores)two-sampleranktest\\nor the Wilcoxon two-sample test. All the assumptions of the linear model\\nother than independence of residuals are captured in the following, using the\\nmore standard Y≤ynotation:\\nF(y|X)=P r o b [ Y≤y|X]=Φ(y−Xβ\\nσ) (15.8)\\nΦ−1(F(y|X)) =y−Xβ\\nσ(15.9)\\nOn the other hand, ordinal models assume the following:\\nyΦ−1(F(y|X))\\n−ΔXβσ\\nyΦ−1(F(y|X))\\nlogit(F(y|X))−ΔXβ\\nFig. 15.1 Assumptions of the linear model (left panel) and semiparametric ordi-\\nnal probit or logit (proportional odds) models (right panel). Ordinal models do not\\nassume any shape for the distribution of Yfor a given X; they only assume paral-\\nlelism. The linear model can relax the parallelism assumption if σis allowed to vary,\\nbut in practice it is diﬃcult to know how to vary it except for the unequal variance\\ntwo-sample t-test.\\nProb[Y≤y|X]=F(g(y)−Xβ), (15.10)\\nwheregis unknown and may be discontinuous. This translates to the paral-\\nlelism assumption in the right panel of Figure 15.1, whereas the linear model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a0057640-1653-4fc8-8a75-4f14d74de932', embedding=None, metadata={'page_label': '365', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.5 Dataset and Descriptive Statistics 365\\nmakes the additional strong assumption of linearity of normal inverse cu-\\nmulative distribution function, which arises from the Gaussian distribution\\nassumption.\\n15.5 Dataset and Descriptive Statistics\\nDiabetes Mellitus (DM) type II (adult onset diabetes) is strongly associ-\\nated with obesity. The currently best laboratory test for diabetes measures\\nglycosylated hemoglobin (HbA 1c), also called glycated hemoglobin, glycohe-\\nmoglobin, or hemoglobin A1c.H b A 1creﬂects average blood glucose for the\\npreceding 60 to 90 days. HbA 1c>7.0 is sometimes taken as a positive di-\\nagnosis of diabetes even though there are no data to support the use of a\\nthreshold.\\nThe goals of this analyses are to better understand eﬀects of body size\\nmeasurements on risk of DM and to enhance screening for DM. The best way\\nto develop a model for DM screening is notto ﬁt a binary logistic model\\nwith HbA 1c>7 as the response variable. There are at least two reasons for\\nthis. First, when the relationship between a measurement and its ultimate\\nclinical impact is smooth, all cutpoints are arbitrary. There is no justiﬁcation\\nfor any putative cut on HbA 1c. Second, such an analysis loses information by\\ntreating HbA 1c=2 the same as HbA 1c=6.9, and by treating HbA 1c=7.1 as\\nequal to HbA 1c=10. Failure to use all available information results in larger\\nstandard errors of ˆβ, lower power, and wider conﬁdence bands. It is better to\\npredict continuous HbA 1cusing a continuous response model, then use that\\nmodel to estimate the probability that HbA 1cexceeds any cutoﬀ, or estimate\\nthe 0.9 quantile of HbA 1c.\\nThe data used here are from the National Health and Nutrition Examina-\\ntion Survey (NHANES) 2009–2010 from the U.S. National Center for Health\\nStatistics/Centers for Disease Control. The original data may be obtained\\nfromhttp://www.cdc.gov/nchs/nhanes.htm94; the analysis ﬁle used here,\\ncallednhgh, may be obtained from the DataSets wiki page, along with Rcode\\nused to download and create the ﬁle. Note that CDC coded age ≥80 as 80.\\nWe use the subset of subjects with age ≥21 who have neither been diagnosed\\nnor treated for DM. Descriptive statistics are shown below.\\nrequire(rms)\\ngetHdata(nhgh)\\nw←subset(nhgh, age ≥21 & dx==0 & tx==0, select=-c(dx,tx))\\nlatex(describe(w), file= '')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b7aa880c-8b2f-4bd2-bb24-8831cc57816e', embedding=None, metadata={'page_label': '366', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='366 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nw\\n18 Variables 4629 Observations\\nseqn : Respondent sequence number\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4629 0 4629 1 56902 52136 52633 54284 56930 59495 61079 61641\\nlowest : 51624 51629 51630 51645 51647\\nhighest: 62152 62153 62155 62157 62158\\nsexn missing unique\\n4629 0 2\\nmale (2259, 49%), female (2370, 51%)\\nage : Age [years]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4629 0 703 1 48.57 23.33 26.08 33.92 46.83 61.83 74.83 80.00\\nlowest : 21.00 21.08 21.17 21.25 21.33\\nhighest: 79.67 79.75 79.83 79.92 80.00\\nre : Race/Ethnicity\\nn missing unique\\n4629 0 5\\nMexican American (832, 18%), Other Hispanic (474, 10%)\\nNon-Hispanic White (2318, 50%), Non-Hispanic Black (756, 16%)\\nOther Race Including Multi-Racial (249, 5%)\\nincome : Family Income\\nn missing unique\\n4389 240 14\\n[0,5000) (162, 4%), [5000,10000) (216, 5%), [10000,15000) (371, 8%)\\n[15000,20000) (300, 7%), [20000,25000) (374, 9%)\\n[25000,35000) (535, 12%), [35000,45000) (421, 10%)\\n[45000,55000) (346, 8%), [55000,65000) (257, 6%), [65000,75000) (188, 4%)\\n> 20000 (149, 3%), < 20000 (52, 1%), [75000,100000) (399, 9%)\\n>= 100000 (619, 14%)\\nwt : Weight [kg]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4629 0 890 1 80.49 52.44 57.18 66.10 77.70 91.40 106.52 118.00\\nlowest : 33.2 36.1 37.9 38.5 38.7\\nhighest: 184.3 186.9 195.3 196.6 203.0\\nht : Standing Height [cm]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4629 0 512 1 167.5 151.1 154.4 160.1 167.2 175.0 181.0 184.8\\nlowest : 123.3 135.4 137.5 139.4 139.8\\nhighest: 199.2 199.3 199.6 201.7 202.7', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4ac5e2ee-ae92-4534-b7d9-867a184dc242', embedding=None, metadata={'page_label': '367', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.5 Dataset and Descriptive Statistics 367\\nbmi : Body Mass Index [kg/m2]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4629 0 1994 1 28.59 20.02 21.35 24.12 27.60 31.88 36.75 40.68\\nlowest : 13.18 14.59 15.02 15.40 15.49\\nhighest: 61.20 62.81 65.62 71.30 84.87\\nleg : Upper Leg Length [cm]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4474 155 216 1 38.39 32.0 33.5 36.0 38.4 41.0 43.3 44.6\\nlowest : 20.4 24.9 25.0 25.1 26.4, highest: 49.0 49.5 49.8 50.0 50.3\\narml : Upper Arm Length [cm]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4502 127 156 1 37.01 32.6 33.5 35.0 37.0 39.0 40.6 41.7\\nlowest : 24.8 27.0 27.5 29.2 29.5, highest: 45.2 45.5 45.6 46.0 47.0\\narmc : Arm Circumference [cm]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4499 130 290 1 32.87 25.4 26.9 29.5 32.5 35.8 39.1 41.4\\nlowest : 17.9 19.0 19.3 19.5 19.9, highest: 54.2 54.9 55.3 56.0 61.0\\nwaist : Waist Circumference [cm]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4465 164 716 1 97.62 74.8 78.6 86.9 96.3 107.0 117.8 125.0\\nlowest : 59.7 60.0 61.5 62.0 62.4\\nhighest: 160.0 160.6 162.2 162.7 168.7\\ntri : Triceps Skinfold [mm]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4295 334 342 1 18.94 7.2 8.8 12.0 18.0 25.2 31.0 33.8\\nlowest : 2.6 3.1 3.2 3.3 3.4, highest: 39.6 39.8 40.0 40.2 40.6\\nsub : Subscapular Skinfold [mm]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n3974 655 329 1 20.8 8.60 10.30 14.40 20.30 26.58 32.00 35.00\\nlowest : 3.8 4.2 4.6 4.8 4.9, highest: 40.0 40.1 40.2 40.3 40.4\\ngh : Glycohemoglobin [%]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4629 0 63 0.99 5.533 4.8 5.0 5.2 5.5 5.8 6.0 6.3\\nlowest : 4.0 4.1 4.2 4.3 4.4, highest: 11.9 12.0 12.1 12.3 14.5\\nalbumin : Albumin [g/dL]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4576 53 26 0.99 4.261 3.7 3.9 4.1 4.3 4.5 4.7 4.8\\nlowest : 2.6 2.7 3.0 3.1 3.2, highest: 4.9 5.0 5.1 5.2 5.3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0bc50427-ac65-4d34-a217-db74524e12ed', embedding=None, metadata={'page_label': '368', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"368 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nbun : Blood urea nitrogen [mg/dL]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4576 53 50 0.99 13.03 7 8 10 12 15 19 22\\nlowest : 1 2 3 4 5, highest: 49 53 55 56 63\\nSCr : Creatinine [mg/dL]\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n4576 53 167 1 0.8887 0.58 0.62 0.72 0.84 0.99 1.14 1.25\\nlowest : 0.34 0.38 0.39 0.40 0.41\\nhighest: 5.98 6.34 9.13 10.98 15.66\\ndd←datadist(w); options(datadist= 'dd')\\n15.5.1 Checking Assumptions of OLS\\nand Other Models\\nFirst let’s see if ghwould make a Gaussian residuals model ﬁt. Use ordinary\\nregression on four key variables to collapse these into one variable (predicted\\nmean from the OLS model). Stratify the predicted means into six quantile\\ngroups.Apply the normal inversecumulative distributionfunction Φ−1to the\\nempirical cumulative distribution functions (ECDF) of ghusing these strata,\\nand check for normality and constant σ2. The ECDF estimates Prob[ Y≤\\ny|X] but for ordinal modeling we want to state models in terms of Prob[ Y≥\\ny|X] so take one minus the ECDF before inverse transforming.\\nf←ols(gh ∼rcs(age,5) + sex + re + rcs(bmi, 3), data=w)\\npgh←fitted(f)\\np←function(fun, row, col) {\\nf←substitute(fun); g ←function(F) eval(f)\\nz←Ecdf(∼gh, groups=cut2(pgh, g=6),\\nfun=function(F) g(1 - F),\\nylab=as.expression(f), xlim=c(4.5, 7.75), data=w,\\nlabel.curve=FALSE)\\nprint(z, split=c(col, row, 2, 2), more=row < 2 | col < 2)\\n}\\np(log(F/(1-F)), 1, 1)\\np(qnorm (F), 1, 2)\\np(-log(-log(F)), 2, 1)\\np(log(-log(1-F)), 2, 2)\\n# Get slopes of pgh for somecutoffs of Y\\n# Use glm complementary log-log link on Prob(Y < cutoff) to\\n# get log-log link on Prob(Y ≥cutoff)\\nr←NULL\\nfor(link in c( 'logit ','probit ','cloglog '))\\nf o r ( ki nc ( 5 ,5 . 5 ,6 ) ){\\nco←coef(glm(gh < k ∼pgh, data=w, family=binomial(link)))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a79cd72-e078-4251-ab2a-74ed78b030b8', embedding=None, metadata={'page_label': '369', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.5 Dataset and Descriptive Statistics 369\\nr←rbind(r, data.frame(link= link,cutoff=k,\\nslope=round(co[2],2)))\\n}\\nprint(r, row.names=FALSE)\\nlink cutoff slope\\nlogit 5.0 -3.39\\nlogit 5.5 -4.33\\nlogit 6.0 -5.62\\nprobit 5.0 -1.69\\nprobit 5.5 -2.61\\nprobit 6.0 -3.07\\ncloglog 5.0 -3.18\\ncloglog 5.5 -2.97\\ncloglog 6.0 -2.51\\nGlycohemoglobin, % Glycohemoglobin, %\\nGlycohemoglobin, % Glycohemoglobin, %log(F/(1 − F))\\n−505\\n5.0 5.5 6.0 6.5 7.0 7.5\\nqnorm (F)\\n−202\\n5.0 5.5 6.0 6.5 7.0 7.5− log(− log(F))\\n−20246\\n5.0 5.5 6.0 6.5 7.0 7.5\\nlog (− log(1 − F))\\n−6−4−202\\n5.0 5.5 6.0 6.5 7.0 7.5\\nFig. 15.2 Examination of normality and constant variance assumption, and assump-\\ntions for various ordinal models', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4b2bbe6-8a59-4104-aec1-421e811fb095', embedding=None, metadata={'page_label': '370', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"370 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nThe upper right curves in Figure 15.2are not linear, implying that a normal\\nconditional distribution cannot work for ghiThere is non-parallelism for the\\nlogit model. The other graphs will be used to guide selection of an ordinal\\nmodel below.\\n15.6 Ordinal Regression Applied to HbA 1c\\nIn the upper left panel of Figure 15.2, logit inverse curves are not parallel\\nso the proportional odds assumption does not hold when predicting HbA 1c.\\nThe log-log link yields the highest degree of parallelism and most constant\\nregression coeﬃcients across cutoﬀs of gh, so we use this link in an ordinal\\nregression model (linearity of the curves is not required).\\n15.6.1 Checking Fit for Various Models Using Age\\nAnother way to examine model ﬁt is to ﬂexibly ﬁt the single most important\\npredictor(age)usingavarietyofmethods,andcomparepredictionstosample\\nquantiles and means based on subsets on age. We use overlapping subsets\\nto gain resolution, with each subset composed of those subjects having age\\nwithin ﬁve years of the point being predicted by the models. Here we predict\\nthe 0.5, 0.75, and 0.9 quantiles and the mean. For quantiles we can compare\\nto quantile regression (discussed below) and for means we compare to OLS.\\nag←25:75\\nlag←length(ag)\\nq2←q3←p90←means ←numeric(lag)\\nfor(i in 1:lag) {\\ns←which(ab s(w$age - ag[i]) < 5)\\ny←w$gh[s]\\na←quantile(y, probs =c(.5, .75, .9))\\nq2[i] ←a[1]\\nq3[i] ←a[2]\\np90[i] ←a[3]\\nmeans[i] ←mean(y)\\n}\\nfams←c('logistic ','probit ','loglog ','cloglog ')\\nfe←function( pred,target) mean(abs(pred$yhat - target))\\nmod←gh∼rcs(age,6)\\nP←Er←list()\\nfor(est in c( 'q2','q3','p90','mean')) {\\nmeth←if(est == 'mean')'ols'else 'QR'\\np←list()\\ner←rep(NA, 5)\\nnames(er) ←c(fams, meth)\\nfor(family in fams) {\\nh←orm(mod, family=family, data=w)\\nfun←if(est == 'mean') Mean(h)\\nelse {\\nqu←Quantile(h)\\niThey are not parallel either.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3cfc6184-ad60-483b-b5b1-8c62c957eac1', embedding=None, metadata={'page_label': '371', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.6 Ordinal Regression Applied to HbA 1c 371\\nswitch(est, q2 = function(x) qu(.5, x),\\nq3 = function(x) qu(.75, x),\\np90 = function(x) qu(.9, x))\\n}\\np[[family]] ←z←Predict(h, age=ag, fun=fun, conf.int=FALSE)\\ner[family] ←fe(z, switch(est, mean= means, q2=q2, q3=q3, p90=p90))\\n}\\nh←switch(est,\\nmean= ols(mod, data=w),\\nq2 = Rq (mod, data=w),\\nq3 = Rq (mod, tau=0.75, data =w),\\np90 = Rq (mod, tau=0.90, data=w))\\np[[meth]] ←z←Predict(h, age=ag, conf.int=FALSE)\\ner[meth] ←fe(z, switch(est, mean= means, q2=q2, q3=q3, p90=p90))\\nEr[[est]] ←er\\npr←do.call( 'rbind ',p )\\npr$est ←est\\nP←rbind.data.frame(P, pr)\\n}\\nxyplot(yhat ∼age | est, groups= .set., data=P, type= 'l',# Figure 15.3\\nauto.key=list(x=.75, y=.2, points= FALSE, lines=TRUE),\\npanel=function(..., subscripts) {\\npanel.xyplot(..., subscripts=subscripts)\\nest←P$est[subscript s[1]]\\nlpoints(ag, switch(est, mean= means, q2=q2, q3=q3, p90=p90),\\ncol=gray(.7))\\ner←format(round(Er[[est]],3), nsmall=3)\\nltext(26, 6.15, paste(names(er), collapse= '\\\\n'),\\ncex=.7, adj=0)\\nltext(40, 6.15, paste(er, collapse= '\\\\n'),\\ncex=.7, adj=1)})\\nIt can be seen in Figure 15.3that models dedicated to a speciﬁc task\\n(quantile regression for quantiles and OLS for means) were best for those\\ntasks. Although the log-log ordinal cumulative probability model did not\\nestimate the median as accurately as some other methods, it does well for\\nthe 0.75 and 0.9 quantiles and is the best compromise overall because of\\nits ability to also directly predict the mean as well as quantities such as\\nProb[HbA 1c>7|X].\\nFrom here on we focus on the log-log ordinal model. Returning to the\\nbottom left of Figure 15.2, let’s look at quantile groups of predicted HbA 1c\\nby OLS and plot predicted distributions of actual HbA 1cagainst empirical\\ndistributions.\\nw$pghg ←cut2(pgh, g=6)\\nf←orm(gh ∼pghg,data=w)\\nlp←predict(f, newdata=data.frame(pghg=levels(w$pghg)))\\nep←ExProb(f) # Exceedance prob.functn. generator in rms\\nz←ep(lp)\\nj←order(w$pghg) # puts in order of lp ( levels of pghg)\\nplot(z, xlim=c(4, 7.5), data=w[j,c( 'pghg ','gh')])# Fig. 15.4\\nAgreement between predicted and observed exceedance probability distribu-\\ntions is excellent in Figure 15.4.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa3e7fa6-7b07-4ea4-933b-2b1e3d9eafb9', embedding=None, metadata={'page_label': '372', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"372 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nageyhat\\n5.25.45.65.86.06.2\\n30 40 50 60 70logistic\\nprobit\\nloglog\\ncloglog\\nols0.021\\n0.025\\n0.026\\n0.033\\n0.013mean\\nlogistic\\nprobit\\nloglog\\ncloglog\\nQR0.053\\n0.047\\n0.041\\n0.101\\n0.030p90logistic\\nprobit\\nloglog\\ncloglog\\nQR0.048\\n0.052\\n0.058\\n0.072\\n0.024q230 40 50 60 70\\n5.25.45.65.86.06.2logistic\\nprobit\\nloglog\\ncloglog\\nQR0.050\\n0.045\\n0.037\\n0.077\\n0.027q3\\nlogistic\\nprobit\\nloglog\\ncloglog\\nQR\\nols\\nFig. 15.3 Three estimated quantilesand estimated mean using6methods, compared\\nagainst caliper-matched sample quantiles/means (circles). Numbers are mean abso-\\nlute diﬀerences between predicted and sample quantities using overlapping intervals\\nof age and caliper matching. QR:quantile regression.\\nTo return to the initial look at a linear model with assumed Gaussian\\nresiduals, ﬁt a probit ordinal model and compare the estimated intercepts to\\nthe linear relationship with ghthat is assumed by the normal distribution.\\nf←orm(gh ∼rcs(age,6), family= probit, data=w)\\ng←ols(gh ∼rcs(age,6), data=w)\\ns←g$stats[ 'Sigma ']\\nyu←f$yunique[-1]\\nr←quantile(w$gh, c( .005,.995))\\nalphas ←coef(f)[1: num.intercepts(f)]\\nplot(-yu / s, alphas, type= 'l', xlim=rev(- r / s), # Fig. 15.5\\nxlab=expression(-y/hat(sigma)), ylab=expression(alpha[y]))\\nFigure15.5depicts a signiﬁcant departure from the linear form implied by\\nGaussian residuals (Eq. 15.4).\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='22fab300-867c-4d5e-93d8-82f4b3162944', embedding=None, metadata={'page_label': '373', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6 Ordinal Regression Applied to HbA 1c 373\\n4.0 4.5 5.0 5.5 6.0 6.5 7.0 7.50.00.20.40.60.81.0\\nghProb (Y ≥ y)[4.88,5.29)\\n[5.29,5.44)\\n[5.44,5.56)\\n[5.56,5.66)\\n[5.66,5.76)\\n[5.76,6.48]\\nFig. 15.4 Observed (dashed lines, open circles) and predicted (solid lines, closed cir-\\ncles) exceedance probability distributionsfrom a model using 6-tiles of OLS-predicted\\nHbA1c. Key shows quantile group intervals of predicted mean HbA 1c.\\n−14 −12 −10 −8−5−4−3−2−1012\\n−yσ^α y\\nFig. 15.5 Estimated intercepts from probit model. Linearity would have indicated\\nGaussian residuals.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7b03e279-6931-4df9-84b6-da83ee07228b', embedding=None, metadata={'page_label': '374', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='374 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\n15.6.2 Examination of BMI\\nBody mass index (BMI, weight divided by height2) is commonly used as an\\nobesity measure because it is well correlated with abdominal visceral fat.\\nBut it is not obvious that BMI is the correct summary of height and weight\\nfor predicting pre-clinical diabetes, and it may be the case that body size\\nmeasures other than height and weight are better predictors.\\nUse the log-log ordinal model to check the adequacy of BMI, adjusting for\\nage (without assuming linearity). This can be done by examining the ratio\\nof coeﬃcients of log height and log weight, and also by using AIC to judge\\nwhether BMI is an adequate summary of height and weight when compared\\nto nonlinear functions of the logs, and to a tensor spline interaction surface.\\nf←orm(gh ∼rcs(age,5) + log(ht) + log(wt),\\nfamily=loglog, data=w)\\nprint(f, latex=TRUE)\\n-log-log Ordinal Regression Model\\norm(formula = gh ~ rcs(age, 5) + log(ht) + log(wt), data = w,\\nfamily = loglog)\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 4629 LRχ21126.94R20.217ρ 0.486\\nUniqueY63d.f. 6 g 0.627\\nY0.55.5Pr(>χ2)<0.0001gr 1.872\\nmax|∂logL\\n∂β|Scoreχ21262.81|Pr(Y≥Y0.5)−1\\n2|0.153\\n1×10−6Pr(>χ2)<0.0001\\nCoef S.E. Wald ZPr(>|Z|)\\nage 0.0398 0.0055 7.29 <0.0001\\nage’ -0.0158 0.0275 -0.57 0.5657\\nage” -0.0072 0.0866 -0.08 0.9333\\nage”’ 0.0309 0.1135 0.27 0.7853\\nht -3.0680 0.2789 -11.00 <0.0001\\nwt 1.2748 0.0704 18.10 <0.0001\\naic←NULL\\nfor(mod in list(gh ∼rcs(age,5) + rcs(log(bmi),5),\\ngh∼rcs(age,5) + rcs(log(ht),5) + rcs(log(wt),5),\\ngh∼rcs(age,5) + rcs(log(ht),4) * rcs(log(wt),4)))\\naic←c(aic, AIC(orm(mod, family=loglog, data=w)))\\nprint(aic)\\n[1] 25910.77 25910.17 25906.03', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb6a38f4-2412-44b6-b835-5f3c97dc4ed9', embedding=None, metadata={'page_label': '375', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6 Ordinal Regression Applied to HbA 1c 375\\nThe ratio of the coeﬃcient of log height to the coeﬃcient of log weight is -\\n2.4, which is between what BMI uses and the more dimensionally reasonable\\nweight / height3. By AIC, a spline interaction surface between height and\\nweight does slightly better than BMI in predicting HbA 1c, but a nonlinear\\nfunction of BMI is barely worse. It will require other body size measures to\\ndisplace BMI as a predictor.\\nAs an aside, compare this model ﬁt to that from the Cox proportional\\nhazards model. The Cox model uses a conditioning argument to obtain\\na partial likelihood free of the intercepts α(and requires a second step to\\nestimate these log discrete hazard com ponents) whereas we are using a full\\nmarginal likelihood of the ranks of Y330.\\nprint(cph(Surv(gh) ∼rcs(age,5) + log(ht) + log(wt), data =w),\\nlatex=TRUE)\\nCox Proportional Hazards Model\\ncph(formula = Surv(gh) ~ rcs(age, 5) + log(ht)\\n+ log(wt), data = w)\\nModel Tests Discrimination\\nIndexes\\nObs 4629 LRχ21120.20R20.215\\nEvents 4629 d.f. 6 Dxy0.359\\nCenter 8.3792 Pr(>χ2) 0.0000 g 0.622\\nScoreχ21258.07gr1.863\\nPr(>χ2) 0.0000\\nCoef S.E. Wald ZPr(>|Z|)\\nage -0.0392 0.0054 -7.24 <0.0001\\nage’ 0.0148 0.0274 0.54 0.5888\\nage” 0.0093 0.0862 0.11 0.9144\\nage”’ -0.0321 0.1131 -0.28 0.7767\\nht 3.0477 0.2779 10.97 <0.0001\\nwt -1.2653 0.0701 -18.04 <0.0001\\nClose agreement of the two is seen, as expected.\\n15.6.3 Consideration of All Body Size Measurements\\nNext we examine all body size measures, and check their redundancies.\\nv←varclus( ∼wt + ht + bmi + leg + arml + armc + waist +\\ntri + sub + age + sex + re, data=w)\\nplot(v)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e607353-fd2e-4694-9a21-069889dba164', embedding=None, metadata={'page_label': '376', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"376 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\n# Omit wt so it won 't be removed before bmi\\nredun(∼ht + bmi + leg + arml + armc + waist + tri + sub,\\ndata=w, r2=.75)\\nRedundancy Analysis\\nredun(formula = ∼ht + bmi + leg + arml + armc + waist + tri +\\nsub, data = w, r2 = 0.75)\\nn: 3853 p: 8 nk: 3\\nNumber of NAs: 776\\nFrequencies of Missing Values Due to Each Variable\\nht bmi leg arml armc waist tri sub\\n0 0 155 127 130 164 334 655\\nTransformation of target variables forced to be linear\\nR2cutoff: 0.75 Type: ordinary\\nR2with which each variable can be predicted from all other variables:\\nht bmi leg arml armc waist tri sub\\n0.829 0.924 0.682 0.748 0.843 0.864 0.531 0.594\\nRendundant variables:\\nbmi ht\\nPredicted fromvariables:\\nleg arml armc waist tri sub\\nVariable Deleted R2R2after later deletions\\n1 bmi 0.924 0.909\\n2 ht 0.792\\nSix size measures adequat ely capture the entire set. Height and BMI are\\nremoved (Figure 15.6). An advantage of removing height is that it is age-\\ndependent due to vertebral compression in the elderly:\\nf←orm(ht ∼rcs(age,4)*sex, data=w) # Prop. oddsmodel\\nqu←Quantile(f); med ←function(x) qu(.5, x)\\nggplot( Predict(f, age, sex, fun=med, conf.int= FALSE),\\nylab= 'Predicted Median Height, cm ')\\nHowever,upperleglengthhasthesamedecliningtrend,implyingasurvival\\nbias or birth year eﬀect.\\nIn preparing to cr eate a multivariable model, degrees of freedom are allo-\\ncated according to the generalized Spearman ρ2(Figure15.7)j.\\ns←spearman2(gh ∼age + sex + re + wt + leg + arml + armc +\\nwaist + tri + sub, data=w, p=2)\\nplot(s)\\nParameters will be allocated in descending order of ρ2. But note that\\nsubscapular skinfold has a large number of NAs and other predictors also have\\nNAs. Suboptimal casewise deletion will be used until the ﬁnal model is ﬁtted\\n(Figure15.8).\\njCompetition between collinear size measures hurts interpretation of partial tests of\\nassociation in a saturated additive model.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f6d2fab4-d2f1-4d2d-9f45-fcf9f18c9778', embedding=None, metadata={'page_label': '377', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.6 Ordinal Regression Applied to HbA 1c 377bmi\\nwaist\\nwt\\narmc\\ntri\\nsub\\nsexfemale\\nleg\\nht\\narml\\nage\\nreOther Race Including Multi−Racial\\nreOther Hispanic\\nreNon−Hispanic White\\nreNon−Hispanic Black\\n1.00.80.60.40.20.0Spearman ρ2\\nFig. 15.6 Variable clustering for all potential predictors\\n155160165170175180\\n20 40 60 80\\nAge, yearsPredicted Median Height, cmsex\\nmale\\nfemale\\nFig. 15.7 Estimated median height as a smooth function of age, allowing age to\\ninteract with sex, from a proportional odds model\\nBecausetherearemanycompeting body measures,we usebackwardsstep-\\ndown to arrive at a set of predictors. The bootstrap will be used to penal-\\nize predictive ability for variable selection. First the full model is ﬁt using\\ncasewise deletion, then we do a composit e test to assess whether any of the\\nfrequently–missing predictors is important.\\nf←orm(gh ∼rcs(age,5) + sex + re + rcs(wt,3) + rcs(leg,3) + arml +\\nrcs(armc,3) + rcs(waist,4) + tri + rcs(sub,3),\\nfamily= 'loglog ', data=w, x= TRUE, y= TRUE)\\nprint(f, latex= TRUE,coefs=FALSE)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='97a25c40-6b8e-4c77-857b-354b5677d3b4', embedding=None, metadata={'page_label': '378', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='378 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nage\\nwaist\\nleg\\nsub\\narmc\\nwt\\nre\\ntri\\narml\\nsexN  df\\n4629 14502 24295 24629 44629 24499 23974 24474 24465 24629 2\\n0.00 0.05 0.10 0.15 0.20Spearman ρ2 Response : gh\\nAdjusted ρ2\\nFig. 15.8 Generalized squared rank correlations\\n-log-log Ordinal Regression Model\\norm(formula = gh ~ rcs(age, 5) + sex + re + rcs(wt, 3)\\n+ rcs(leg, 3) + arml + rcs(armc, 3) + rcs(waist, 4)\\n+ tri + rcs(sub, 3), data = w, x = TRUE, y = TRUE,\\nfamily = \"loglog\")\\nFrequencies of Missing Values Due to Each Variable\\n0 100 200 300 400 500 600 70065533416415513012700000N\\nsubtriwaistlegarmcarmlghagesexrewt', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='82c53328-fa94-4c01-b309-62aaa84b1d49', embedding=None, metadata={'page_label': '379', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.6 Ordinal Regression Applied to HbA 1c 379\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 3853 LRχ21180.13R20.265ρ 0.520\\nUniqueY60d.f. 22 g 0.732\\nY0.55.5Pr(>χ2)<0.0001gr 2.080\\nmax|∂logL\\n∂β|Scoreχ21298.88|Pr(Y≥Y0.5)−1\\n2|0.172\\n3×10−5Pr(>χ2)<0.0001\\n## Composite test:\\nlan←function(a) latex(a, table.env= FALSE, file= '')\\nlan(anova(f, leg, arml, armc, waist, tri, sub))\\nχ2d.f.P\\nleg 8 .30 2 0.0158\\nNonlinear 3.32 1 0.0685\\narml 0 .16 1 0.6924\\narmc 6 .66 2 0.0358\\nNonlinear 3.29 1 0.0695\\nwaist 29 .40 3<0.0001\\nNonlinear 4.29 2 0.1171\\ntri 16 .62 1<0.0001\\nsub 40 .75 2<0.0001\\nNonlinear 4.50 1 0.0340\\nTOTAL NONLINEAR 14 .95 5 0.0106\\nTOTAL 128 .29 11<0.0001\\nThe model achieves Spearman ρ=0.52, the rank correlation between\\npredicted and observed HbA 1c.\\nWe show the predicted mean and median HbA 1cas a function of age,\\nadjusting other variablesto their median ormode (Figure 15.9).Comparethe\\nestimateofthemedianand90thpercentilewiththatfromquantileregression.\\nM ←Mean(f)\\nqu ←Quantile(f)\\nmed ←function(x) qu(.5, x)\\np90 ←function(x) qu(.9, x)\\nfq ←Rq(formula(f), data=w)\\nfq90 ←Rq(formula(f), data=w, tau=.9)\\npmean ←Predict(f, age, fun=M, conf.int= FALSE)\\npmed ←Predict(f, age, fun=med, conf.int= FALSE)\\np90 ←Predict(f, age, fun=p90, conf.int= FALSE)\\npmedqr ←Predict(fq, age, conf.int= FALSE)\\np90qr ←Predict( fq90, age, conf.int= FALSE)\\nz←rbind( 'orm mean '=pmean, 'orm median '=pmed, 'orm P90 '=p90,\\n'QR median '=pmedqr, 'QR P90 '=p90qr)\\nggplot(z, groups= '.set. ',\\nadj.subtitle= FALSE, legend.label= FALSE)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1b3a7c0-3fc9-408b-a281-555259a79273', embedding=None, metadata={'page_label': '380', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"380 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nprint(fastbw(f, rule= 'p'), estimates= FALSE)\\n5.005.255.505.756.00\\n20 40 60 80\\nAge,  yearsorm mean\\norm median\\norm P90\\nQR median\\nQR P90\\nFig. 15.9 Estimated mean and 0.5 and 0.9 quantiles from the log-log ordinal model\\nusing casewise deletion, along with predictions of 0.5 and 0.9 quantiles from quantile\\nregression (QR). Age is varied and other predictors are held constant to medians/-\\nmodes.\\nDeleted Chi-Sq d.f. P Residual d.f. P AIC\\narml 0.16 1 0.6924 0.16 1 0.6924 -1.84\\nsex 0.45 1 0.5019 0.61 2 0.7381 -3.39\\nwt 5.72 2 0.0572 6.33 4 0.1759 -1.67\\narmc 3.32 2 0.1897 9.65 6 0.1400 -2.35\\nFactors in Final Model\\n[1] age re leg waist tri sub\\nset.seed (13)# so can reproduce results\\nv←validate(f, B =100, bw=TRUE, estimates= FALSE, rule= 'p')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b2c3ef3-6687-4fa1-b54f-a1e6de0f9c12', embedding=None, metadata={'page_label': '381', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.6 Ordinal Regression Applied to HbA 1c 381\\nBackwards Step-down - Original Model\\nDeleted Chi-Sq d.f. P Residual d.f. P AIC\\narml 0.16 1 0.6924 0.16 1 0.6924 -1.84\\nsex 0.45 1 0.5019 0.61 2 0.7381 -3.39\\nwt 5.72 2 0.0572 6.33 4 0.1759 -1.67\\narmc 3.32 2 0.1897 9.65 6 0.1400 -2.35\\nFactors in Final Model\\n[1] age re leg waist tri sub\\n# Show number of variables selected in first 30 boots\\nlatex(v, B=30, file= '', size= 'small ')\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nρ 0.5225 0 .5290 0.5208 0 .0083 0 .5142 100\\nR20.2712 0 .2788 0.2692 0 .0095 0 .2617 100\\nSlope 1 .0000 1 .0000 0.9761 0 .0239 0 .9761 100\\ng 1.2276 1 .2505 1.2207 0 .0298 1 .1978 100\\n|Pr(Y≥Y0.5)−1\\n2|0.2007 0 .2050 0.1987 0 .0064 0 .1943 100\\nFactors Retained in Backwards Elimination\\nFirst 30 Resamples\\nage sex re wt leg arml armc waist tri sub\\n••• • • • • •\\n•• • • • • •\\n•• • • • • •\\n••• • • • • •\\n••• • • • • •\\n•• • • • • •\\n••• • • • •\\n••• • • • •\\n•• •• • • • •\\n••• • • • • •\\n•• • • • • •\\n•• • • • • • •\\n•• •• • • • •\\n•• •• • • • •\\n•• • • • • •\\n•• •• • • • •\\n••• • • • • • •\\n•• • • • •\\n•• •• • • • •\\n•• •• • • • •\\n•• •• • • • •\\n•• • • • • •\\n•• •• • • • •\\n••• • • •\\n•• • • • •\\n•• •• • • • •\\n•• • • • • •\\n•• •• • • • •\\n•• •• • • • •\\n•• • • • •\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c2174ca9-f438-47e1-87dd-ae5c6f6a9e7c', embedding=None, metadata={'page_label': '382', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"382 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nFrequencies of Numbers of Factors Retained\\n5 678 9 1 0\\n11 92 94 64 1\\nNext we ﬁt the reduced model, using multiple imputation to impute miss-\\ning predictors (Figure 15.10).\\na←aregImpute( ∼gh + wt + ht + bmi + leg + arml + armc + waist +\\ntri + sub + age +re, data=w, n.impute=5, pr=FALSE)\\ng←fit.mult.impute(gh ∼rcs(age,5) + re + rcs(leg,3) +\\nrcs(waist,4) + tri + rcs(sub,4),\\norm, a, family=loglog, data=w, pr=FALSE)\\nprint(g, latex= TRUE,needspace= '1.5in ')\\n-log-log Ordinal Regression Model\\nfit.mult.impute(formula = gh ~ rcs(age, 5) + re + rcs(leg, 3)\\n+ rcs(waist, 4) + tri + rcs(sub, 4), fitter = orm,\\nxtrans = a, data = w, pr = FALSE, family = loglog)\\nModel Likelihood Discrimination Rank Discrim.\\nRatio Test Indexes Indexes\\nObs 4629 LRχ21448.42R20.269ρ 0.513\\nUniqueY63d.f. 17 g 0.743\\nY0.55.5Pr(>χ2)<0.0001gr 2.102\\nmax|∂logL\\n∂β|Scoreχ21569.21|Pr(Y≥Y0.5)−1\\n2|0.173\\n1×10−5Pr(>χ2)<0.0001\\nCoef S.E. Wald ZPr(>|Z|)\\nage 0.0404 0.0055 7.29 <0.0001\\nage’ -0.0228 0.0279 -0.82 0.4137\\nage” 0.0126 0.0876 0.14 0.8857\\nage”’ 0.0424 0.1148 0.37 0.7116\\nre=Other Hispanic -0.0766 0.0597 -1.28 0.1992\\nre=Non-Hispanic White -0.4121 0.0449 -9.17 <0.0001\\nre=Non-Hispanic Black 0.0645 0.0566 1.14 0.2543\\nre=Other Race Including Multi-Racial -0.0555 0.0750 -0.74 0.4593\\nleg -0.0339 0.0091 -3.73 0.0002\\nleg’ 0.0153 0.0105 1.46 0.1434\\nwaist 0.0073 0.0050 1.47 0.1428\\nwaist’ 0.0304 0.0158 1.93 0.0536\\nwaist” -0.0910 0.0508 -1.79 0.0732\\ntri -0.0163 0.0026 -6.28 <0.0001\\nsub -0.0027 0.0097 -0.28 0.7817\\nsub’ 0.0674 0.0289 2.33 0.0198\\nsub” -0.1895 0.0922 -2.06 0.0398\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b4ce7ae8-5c16-4052-b30a-dd60f9b95b1e', embedding=None, metadata={'page_label': '383', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"15.6 Ordinal Regression Applied to HbA 1c 383\\nan←anova(g)\\nlan(an)\\nχ2d.f.P\\nage 692 .50 4<0.0001\\nNonlinear 28.47 3<0.0001\\nre 168 .91 4<0.0001\\nleg 24 .37 2<0.0001\\nNonlinear 2.14 1 0.1434\\nwaist 128 .31 3<0.0001\\nNonlinear 4.05 2 0.1318\\ntri 39 .44 1<0.0001\\nsub 39 .30 3<0.0001\\nNonlinear 6.63 2 0.0363\\nTOTAL NONLINEAR 46 .80 8<0.0001\\nTOTAL 1464 .24 17<0.0001\\nb←anova(g, leg, waist, tri, sub)\\n# Add new lines to the plot with combined effect of 4 size var.\\ns←rbind(an, size=b[ 'TOTAL ',] )\\nclass(s) ←'anova.rms '\\nplot(s)\\nleg\\nsub\\ntri\\nwaist\\nre\\nsize\\nage\\n0 100 200 300 400 500 600 700\\nχ2− df\\nFig. 15.10 ANOVA for reduced model, after multiple imputation, with addition of\\na combined eﬀect for four size variables\\nggplot(Predict (g),abbrev= TRUE,ylab=NULL) # Figure 15.11\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d1b7354a-abbb-4fa1-97a6-d620a430169b', embedding=None, metadata={'page_label': '384', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"384 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nCompare the estimated age partial eﬀects and conﬁdence intervals with\\nthose from a model using casewise deletion, and with bootstrap nonparamet-\\nric conﬁdence intervals (also with casewise deletion).\\n−1.0−0.50.00.51.01.5\\n20 40 60 80\\nAge,years−1.0−0.50.00.51.01.5\\n30 35 40 45\\nUpper Leg Length ,cmlllll\\nMxcnAmOthrHsNn−HsWNn−HsBORIM−R\\n−1.0−0.50.0 0.5 1.0 1.5Race Ethnicity\\n−1.0−0.50.00.51.01.5\\n10 20 30 40\\nSubscapular Skinfold , mm−1.0−0.50.00.51.01.5\\n10 20 30 40\\nTriceps Skinfold , mm−1.0−0.50.00.51.01.5\\n80 100 120 140\\nWaist Circumference , cm\\nFig. 15.11 Partial eﬀects (log hazard or log-log cumulative probability scale) of all\\npredictors in reduced model, after multiple imputation\\ngc←orm(gh ∼rcs(age,5) + re + rcs(leg,3) +\\nrcs(waist,4) + tri + rcs(sub,4),\\nfamily=loglog, data=w, x= TRUE, y= TRUE)\\ngb←bootcov(gc, B =300)\\nbootclb ←Predict(gb, age, boot.type= 'basic ')\\nbootclp ←Predict(gb, age, boot.type= 'percentile ')\\nmultimp ←Predict(g, age)\\nplot(Predict(gc, age), addpanel=function(...) {\\nwith(bootclb, {llines(age, lower, col= 'blue ')\\nllines(age, upper, col= 'blue ')})\\nwith(bootclp, {llines(age, lower, col= 'blue ', lty=2)\\nllines(age, upper, col= 'blue ', lty=2)})\\nwith(multimp, {llines(age, lower, col= 'red')\\nllines(age, upper, col= 'red')\\nllines(age, yhat, col= 'red')} ) },\\ncol.fill= gray(.9), adj.subtitle= FALSE) # Figure 15.12\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9d57b9ca-61b7-4277-8924-06354200df53', embedding=None, metadata={'page_label': '385', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6 Ordinal Regression Applied to HbA 1c 385\\nAge, yearslog hazard\\n−0.50.00.51.0\\n20 30 40 50 60 70 80\\nFig. 15.12 Partial eﬀect for age from multiple imputation (center red line) and\\ncasewise deletion (center blue line) with symmetric Wald 0.95 conﬁdence bands using\\ncasewise deletion (gray shaded area), basic bootstrap conﬁdence bands using case wise\\ndeletion (blue lines), percentile bootstrap conﬁdence bands using casewise deletion\\n(dashed blue lines), and symmetric Wald conﬁdence bands accounting for multiple\\nimputation (red lines).\\nFigure15.13depicts the relationship between various predicted quantities,\\ndemonstrating that the ordinal model makes fewer model assumptions that\\ndictate their connections. A Gaussian or log-Gaussian model would have a\\nstraight-line relationship between the predicted mean and median.\\nM←Mean(g)\\nqu←Quantile(g)\\nmed←function(lp) qu(.5, lp)\\nq90←function(lp) qu(.9, lp)\\nlp←predict(g)\\nlpr←quantile(predict(g), c( .002,.998), na.rm=TRUE)\\nlps←seq(lpr[1], lpr[2], length=200)\\npmn←M(lps)\\npme←med(lps)\\np90←q90(lps)\\nplot(pmn, pme, # Figure 15.13\\nxlab=expression( paste( \\'Predicted Mean \\', HbA[\"1c\"])),\\nylab= \\'Median and 0.9 Quantile \\', type= \\'l\\',\\nxlim=c(4.75, 8.0), ylim=c(4.75, 8.0), bty= \\'n\\')\\nbox(col=gray(.8))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='68d4534c-db72-48d8-9eb9-bedc5b104cb9', embedding=None, metadata={'page_label': '386', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"386 15 Regression Models for Continuous Yand Case Study in Ordinal Regression\\nlines(pmn, p90, col= 'blue ')\\nabline(a=0, b=1, col=gray(.8))\\ntext(6.5, 5.5, 'Median ')\\ntext(5.5, 6.3, '0.9', col= 'blue ')\\nnint←350\\nscat1d(M(lp), nint=nint)\\nscat1d(med(lp), side=2, nint=nint)\\nscat1d(q90(lp), side=4, col= 'blue ', nint=nint)\\n5.0 5.5 6.0 6.5 7.0 7.5 8.05.05.56.06.57.07.58.0\\nPredicted Mean HbA 1cMedian and 0.9 QuantileMedian0.9\\nFig. 15.13 Predicted mean HbA 1cvs. predicted median and 0.9 quantile along with\\ntheir marginal distributions\\nFinally, let us draw a nomogram that shows the full power of ordinal\\nmodels, by predicting ﬁve quantities of interest.\\ng ←Newlevels(g, list(re= abbreviate( levels(w$re))))\\nexprob ←ExProb(g)\\nnom←\\nnomogram(g, fun= list(Mean=M,\\n'Median Glycohemoglobin '= med,\\n'0.9 Quantile ' = q90,\\n'Prob(HbA1c ≥6.5) '=\\nfunction(x) exprob(x, y=6.5),\\n'Prob(HbA1c ≥7.0) '=\\nfunction(x) exprob(x, y=7),\\n'Prob(HbA1c ≥7.5) '=\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='854a8f13-4ccd-4dbc-8753-afe1aacec419', embedding=None, metadata={'page_label': '387', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='15.6 Ordinal Regression Applied to HbA 1c 387\\nfunction(x) exprob(x, y=7.5)),\\nfun.at=list(seq(5, 8, by=.5),\\nc(5,5.25,5.5,5.75,6,6.25),\\nc(5.5,6,6.5,7,8,10,12,14),\\nc(.01,.05,.1,.2,.3,.4),\\nc(.01,.05,.1,.2,.3,.4),\\nc(.01,.05,.1,.2,.3,.4)))\\nplot(nom, lmgp=.28) # Figure 15.14\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 00\\nAge\\n20 25 30 35 40 45 50 55 60 65 70 75 80\\nRace/Ethnicity\\nN−HW ORIMOthH\\nUpper Leg Length\\n55 45 35 30 25 20\\nWaist Circumference\\n50 70 90 100 110 120 130 140 150 160 170\\nTriceps Skinfold\\n45 35 25 15 5 0\\nSubscapular Skinfold\\n1015 20 25 30 35 40 45\\nTotal Points\\n0 20 40 60 80 100 120 140 160 180 200 220 240 260 280\\nLinear Predictor\\n−1.5 −1 −0.5 0 0.5 1 1.5 2 2.5\\nMean\\n5 5.5 6 6.5 77.58\\nMedian Glycohemoglobin\\n5 5.25 5.5 5.75 6 6.25\\n0.9 Quantile\\n5.5 6 6.5 781012 14\\nProb(HbA1c >= 6.5)\\n0.01 0.05 0.1 0.2 0.3 0.4\\nProb(HbA1c >= 7.0)\\n0.01 0.05 0.1 0.2 0.3 0.4\\nProb(HbA1c >= 7.5)\\n0.01 0.05 0.1 0.2 0.3\\nFig. 15.14 Nomogram for predicting median, mean, and 0.9 quantile of glycohe-\\nmoglobin, along with the estimated probability that HbA 1c≥6.5,7, or 7.5, all from\\nthe log-log ordinal model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='365e8718-7432-4355-88ea-caa11d852ef2', embedding=None, metadata={'page_label': '389', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 16\\nTransform-Both-Sides Regression\\n16.1 Background\\nFitting multiple regressionmodelsbythe method ofleastsquaresisone ofthe\\nmost commonly used methods in statistics. There are a number of challenges\\nto the use of least squares, even when it is only used for estimation and not\\ninference, including the following.\\n1. How should continuous predictors be transformed so as to get a good ﬁt?\\n2. Is it better to transform the response variable? How does one ﬁnd a good\\ntransformation that simpliﬁes the right-hand side of the equation?\\n3. What if Yneeds to be transformed non-monotonically (e.g., |Y−100|)\\nbefore it will have any correlation with X?\\nWhen one is trying to draw an inference abou t population eﬀects using con-\\nﬁdence limits or hypothesis tests, the most common approach is to assume\\nthat the residuals have a normal distribution. This is equivalent to assuming\\nthat the conditional distribution of the response Ygiven the set of predictors\\nXis normal with mean depending on Xand variance that is (one hopes)\\na constant independent of X. The need for a distributional assumption to\\nenable us to draw inferences creates a number of other challenges such as the\\nfollowing.\\n1. If for the untransformedoriginalscaleof the response Ythe distribution of\\ntheresidualsisnotnormalwithconstantspread,ordinarymethodswillnot\\nyield correct inferences (e.g., conﬁdence intervals will not have the desired\\ncoverage probability and the intervals will need to be asymmetric).\\n2. Quite often there is a transformation of Ythat will yield well-behaving\\nresiduals. How do you ﬁnd this transformation? Can you ﬁnd a transfor-\\nmation for the Xs at the same time?\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 16389', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bcd66d5b-f8b0-438d-97f5-d686536a1635', embedding=None, metadata={'page_label': '390', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='390 16 Transform-Both-Sides Regression\\n3. All classical statistical inferential methods assume that the full model was\\npre-speciﬁed,that is,the modelwasnot modiﬁedafterexaminingthe data.\\nHow does one correct conﬁdence limits, for example, for data-based model\\nand transformation selection?\\n16.2 Generalized Additive Models\\nHastie and Tibshirani275have developed generalized additive models (GAMs)\\nfor a variety of distributions for Y. There are semiparametric GAMs, but\\nmost GAMs for continuous Yassumethatthe conditionaldistributionof Yis\\nfrom a speciﬁc distribution family. GAMs nicely estimate the transformation\\neach continuous Xrequires so as to optimize a ﬁtting criterion such as sum\\nof squared errors or log likelihood, subject to the degrees of freedom the\\nanalyst desires to spend on each predictor. However, GAMs assume that Y\\nhas already been transformed to ﬁt the speciﬁed distribution family.\\nThere is excellent software available for ﬁtting a wide variety of GAMs,\\nsuch as the Rpackages gam,mgcv,a n drobustgam .\\n16.3 Nonparametric Estimation of Y-Transformation\\nWhen the model’s left-hand side also needs transformation, either to im-\\nproveR2or to achieve constant variance of the residuals (which increases the\\nchances of satisfying a normality assumption), there are a few approaches\\navailable. One approach is Breiman and Friedman’s alternating conditional\\nexpectation (ACE) method.68ACE simultaneously transforms both Yand\\neach of the Xs so as to maximize the multiple R2between the transformed\\nYand the transformed Xs. The model is given by\\ng(Y)=f1(X1)+f2(X2)+...+fp(Xp). (16.1)\\nACE allows the analyst to impose restrictions on the transformations such\\nas monotonicity. It allows for categorical predictors, whose categories will\\nautomaticallybe givennumericscores.The transformationfor Yis allowedto\\nbe non-monotonic. One feature of ACE is its ability to estimate the maximal\\ncorrelation betweenan Xandtheresponse Y. Unlike the ordinarycorrelation\\ncoeﬃcient (which assumes linearity) or Spearman’s rank correlation (which\\nassumes monotonicity), the maximal correlation has the property that it is\\nzero if and only if XandYare statistically independent. This property holds\\nbecause ACE allows for non-monotonic transformations of all variables. The\\n“supersmoother”(seetheS supsmufunction)isthebasisforthenonparametric\\nestimation of transformations for continuous Xs.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='87363a14-7dfb-43b8-81ac-4e8795a90658', embedding=None, metadata={'page_label': '391', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.4 Obtaining Estimates on the Original Scale 391\\nTibshirani developed a diﬀerent algorithm for nonparametric additive\\nregression based on least squares, additivity and variance stabilization\\n(AVAS).607Unlike ACE, AVAS forces g(Y) to be monotonic. AVAS’s ﬁt-\\nting criterion is to maximize R2while forcing the transformation for Yto\\nresult in nearly constant variance of residuals. The model speciﬁcation is the\\nsame as for ACE (Equation 16.3).\\nACEandAVASarepowerfulﬁttingalgorithms,buttheycanresultinover-\\nﬁtting (R2can be greatly inﬂated when one ﬁts many predictors), and they\\nprovide no statistical inferential measures. As discussed earlier, the processof\\nestimating transformations (especially those for Y) can result in signiﬁcant\\nvariance under-estimation, especially for small sample sizes. The bootstrap\\ncan be used to correct the apparent R2(R2\\napp) for overﬁtting. As before,\\nit estimates the optimism (bias) in R2\\nappand subtracts this optimism from\\nR2\\nappto get a more trustworthy estimate. The bootstrap can also be used to\\ncompute conﬁdence limits for all estimated transformations, and conﬁdence\\nlimits for estimated predictor eﬀects that take fully into account the uncer-\\ntainty associated with the transformations. To do this, all steps involved in\\nﬁtting the additive models must be repeated fresh for each re-sample.\\nLimited testing has shown that the sample size needs to exceed 100 for\\nACE and AVAS to provide stable estimates. In small sample sizes the boot-\\nstrap bias-corrected estimate of R2will be zero because the sample informa-\\ntion did not support simultaneous estimation of all transformations.\\n16.4 Obtaining Estimates on the Original Scale\\nA common practice in least squares ﬁtting is to attempt to rectify lack of\\nﬁt by taking parametric transformations of Ybefore ﬁtting; the logarithm\\nis the most common transformation.aIf after transformation the model’s\\nresiduals have a population median of zero, the inverse transformation of a\\npredicted transformed value estimates the population median of YgivenX.\\nThis is because unlike means, quantiles are transformation-preserving.Many\\nanalysts make the mistake of not reporting which population parameter is\\nbeing estimated when inverse transforming Xˆβ, and sometimes they even\\nreport that the mean is being estimated.\\nHow would one go about estimating the population mean or other param-\\neter on the untransformed scale? If the residuals are assumed to be normally\\ndistributed and if log( Y) is the transformation, the mean of the log-normal\\ndistribution, a function of both the mean and the variance of the residuals,\\ncan be used to derive the desired quantity. However, if the residuals are not\\nnormally distributed, this procedure will not result in the correct estimator.\\naA disadvantage of transform-both-sides regression is this diﬃculty of interpreting\\nestimates on the original scale. Sometimes the useof a special generalized linear model\\ncan allow for a good ﬁt without transforming Y.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fcc895e0-6f36-434d-9afa-60809678b71c', embedding=None, metadata={'page_label': '392', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='392 16 Transform-Both-Sides Regression\\nDuan165developed a “smearing” estimator for more nonparametrically ob-\\ntainingestimatesofparametersontheoriginalscale.Inthesimpleone-sample\\ncase without predictors in which one has computed ˆθ=∑n\\ni=1log(Yi)/n,t h e\\nresiduals from this ﬁtted value are given by ei= log(Yi)−ˆθ. The smearing\\nestimator of the population mean is∑exp[ˆθ+ei]/n. In this simple case the\\nresult is the ordinary sample mean Y.\\nThe worth of Duan’s smearing estimator is in regression modeling. Sup-\\npose that the regression was run on g(Y) from which estimated values\\nˆg(Yi)=Xiˆβand residualson the transformedscale ei=ˆg(Yi)−Xiˆβwere ob-\\ntained. Instead of restricting ourselvesto estimating the population mean, let\\nW(y1,y2,...,y n) denote any function of a vector of untransformed response\\nvalues. To estimate the population mean in the homogeneous one-sample\\ncase,Wis the simple average of all of its arguments. To estimate the pop-\\nulation 0.25 quantile, Wis the sample 0.25 quantile of y1,...,y n. Then the\\nsmearing estimator of the populat ion parameter estimated by WgivenXis\\nW(g−1(a+e1),g−1(a+e2),...,g−1(a+en)), where g−1is the inverse of the\\ngtransformation and a=Xˆβ.\\nWhen using the AVAS algorithm, the monotonic transformation gis es-\\ntimated from the data, and the predicted value of ˆ g(Y) is given by Equa-\\ntion16.3.So we extend the smearingestimatoras W(ˆg−1(a+e1),...,ˆg−1(a+\\nen)), where ais the predicted transformed response given X.A sˆgis non-\\nparametric (i.e., a table look-up), the areg.boot function described below\\ncomputes ˆ g−1using reverse linear interpolation.\\nIf residuals from ˆ g(Y) are assumed to be symmetrically distributed, their\\npopulation median is zero and we can estimate the median on the untrans-\\nformed scale by computing ˆ g−1(Xˆβ). To be safe, areg.boot adds the median\\nresidual to Xˆβwhen estimating the population median (the median residual\\ncan be ignored by specifying statistic=’fitted’ to functions that operate on\\nobjects created by areg.boot ).\\nWhen quantiles of Yare of major interest, a more direct way to obtain\\nestimates is throughthe use of quantile regression357. An excellentcase study\\nincluding comparisons with other methods such as Cox regression can be\\nfound in Austin et al.38.\\n16.5RFunctions\\nTheRacepackpackage’s acefunction implements all the features of the ACE\\nalgorithm, and its avasfunction does likewise for AVAS. The bootstrap and\\nsmearing capabilities mentioned above are oﬀered for these estimation func-\\ntions by the areg.boot (“additive regression using the bootstrap”) function\\nin theHmiscpackage. Unlike the aceandavasfunctions, areg.boot uses the\\nRmodeling language, making it easier fo r the analyst to specify the predic-', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c27cd8c9-693a-492d-b803-8361dcd1eaf0', embedding=None, metadata={'page_label': '393', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"16.6 Case Study 393\\ntor variables and what is assumed about their relationships with the trans-\\nformedY.areg.boot also implements a parametric transform-both-sides ap-\\nproachusingrestrictedcubicsplinesandcanonicalvariates, andoﬀersvarious\\nestimation options with and without smearing. It can estimate the eﬀect of\\nchangingonepredictor,holdingothersconstant,usingtheordinarybootstrap\\nto estimate the standard deviation of diﬀerence in two possibly transformed\\nestimates (for two values of X), assuming normality of such diﬀerences. Nor-\\nmality is assumed to avoid generating a large number of bootstrap replica-\\ntions of time-consuming model ﬁts. It would not be very diﬃcult to add non-\\nparametric bootstrap conﬁdence limit capabilities to the software. areg.boot\\nre-samples every aspect of the modeling process it uses, just as Faraway186\\ndid for parametric least squares modeling.\\nareg.boot implements a variety of methods as shown in the simple exam-\\nple below. The monotone function restricts a variable’s transformation to be\\nmonotonic, while the Ifunction restricts it to be linear.\\nf←areg.boot(Y ∼monotone(age) +\\nsex + weight + I( blood.pressure))\\nplot(f) #showtransformations, CLs\\nFunction(f) #generate S functions\\n#defining transformations\\npredict(f) #get predictions, smearing estimates\\nsummary(f) #compute CLs on effects of each X\\nsmearingEst() #generalized smearing estimators\\nMean(f) #derive S function to\\n#compute smearing mean Y\\nQuantile(f) #derive function to compute smearing quantile\\nThe methods are best described in a case study.\\n16.6 Case Study\\nConsidersimulateddatawheretheconditionaldistributionof Yislog-normal\\ngivenX, but where transform-both-sides regression methods use unlogged\\nY.P r e d i c t o r X1is linearly related to log Y,X2is related by |X2−1\\n2|,a n d\\ncategorical X3has reference group aeﬀect of zero, group beﬀect of 0.3, and\\ngroupceﬀect of 0.5.\\nrequire(rms)\\nset.seed(7)\\nn←400\\nx1←runif(n)\\nx2←runif(n)\\nx3←factor(sample(c( 'a','b','c'), n, TRUE))\\ny←exp(x1 + 2*abs(x2 - .5) + .3*(x3== 'b') + .5*(x3== 'c')+\\n.5*rnorm(n))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90282640-0d0f-478a-92c9-233cf7b4c634', embedding=None, metadata={'page_label': '394', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='394 16 Transform-Both-Sides Regression\\n# For reference fit appropriate OLS model\\nprint(ols(log(y) ∼x1 + rcs(x2, 5) + x3), coefs= FALSE,\\nlatex=TRUE)\\nLinear Regression Model\\nols(formula = log(y) ~ x1 + rcs(x2, 5) + x3)\\nModel Likelihood Discrimination\\nRatio Test Indexes\\nObs 400 LRχ2236.87R20.447\\nσ0.4722d.f. 7 R2\\nadj0.437\\nd.f. 392 Pr(>χ2) 0.0000 g 0.482\\nResiduals\\nMin 1Q Median 3Q Max\\n−1.346−0.3075−0.0134 0.327 1.527\\nNow ﬁt the avasmodel. We use 300 bootstrap repetitions but only plot\\nthe ﬁrst 20 estimates to see clearly how the bootstrap re-estimates of trans-\\nformations vary. Had we wanted to restrict transformations to be linear, we\\nwould have speciﬁed the identity function, for example, I(x1).\\nf←areg.boot(y ∼x1 + x2 + x3, method= \\'avas \\', B=300)\\nf\\navas Additive Regression Model\\nareg.boot(x = y ∼x1 + x2 + x3, B = 300, method = \"avas\")\\nPredictor Types\\ntype\\nx1 s\\nx2 s\\nx3 c\\ny type: s\\nn= 400 p= 3\\nApparent R2 on transformed Y scale: 0.444\\nBootstrap validated R2 : 0.42\\nCoefficients of standardized transformations:\\nIntercept x1 x2 x3\\n-3.443111e-16 9.702960e-01 1.224320e+00 9.881150e-01\\nResiduals on transformed scale:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61d05109-5def-4163-ae95-610c00e2c774', embedding=None, metadata={'page_label': '395', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='16.6 Case Study 395\\nMin 1Q Median 3Q Max\\n-1.877152e+00 -5.252194e-01 -3.732200e-02 5.339122e-01 2.172680e+00\\nMean S.D.\\n8.673617e-19 7.420788e-01\\nNote that the coeﬃcients above do not mean very much as the scale of the\\ntransformations is arbitrary. We see that the model was very slightly overﬁt-\\nted (R2dropped from 0.44 to 0.42), and the R2are in agreement with the\\nOLS model ﬁt above.\\nNext we plot the transformations, 0.95 conﬁdence bands, and a sample of\\nthe bootstrap estimates.\\nplot(f, boot=20) # Figure 16.1\\n0 5 10 15 20−2−1012\\nyTransformed y\\n0.0 0.2 0.4 0.6 0.8 1.0−0.6−0.4−0.20.00.20.40.6\\nx1Transformed x1\\n0.0 0.2 0.4 0.6 0.8 1.0−0.4−0.20.00.20.40.6\\nx2Transformed x2\\nx3Transformed x3\\n−0.4−0.20.00.20.4\\nabc\\nFig. 16.1 avastransformations: overall estimates, pointwise 0 .95 conﬁdence bands,\\nand 20 bootstrap estimates (red lines).\\nThe plot is shown in Figure 16.1. The nonparametrically estimated transfor-\\nmationof x1isalmostlinear,andthetransformationof x2iscloseto |x2−0.5|.\\nWe know that the true transformation of yis log(y), so variance stabilization\\nand normality of residuals will be achieved if the estimated y-transformation\\nis close to log( y).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0783acbe-798d-47e3-8777-0e6a1128101f', embedding=None, metadata={'page_label': '396', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"396 16 Transform-Both-Sides Regression\\nys←seq(.8, 20, length=200)\\nytrans ←Function(f)$y # Function outputs all transforms\\nplot(log(ys), ytrans(ys), type= 'l')# Figure 16.2\\nabline(lm(ytrans(ys) ∼log(ys)), col=gray(.8))\\n0.0 0.5 1.0 1.5 2.0 2.5 3.0−2−1012\\nlog(ys)ytrans(ys)\\nFig. 16.2 Checking estimated against optimal transformation\\nApproximate linearity indicates that the estimated transformation is very\\nlog-like.b\\nNow let us obtain a pproximate tests of eﬀects of each predictor. summary\\ndoes this by setting all other predictors to reference values (e.g., medians),\\nand comparing predicted responses for a given level of the predictor Xwith\\npredictions for the lowest setting of X. The default predicted response for\\nsummaryis the median, which is used here. Therefore tests are for diﬀerences\\nin medians.\\nsummary(f, values=list(x1=c(.2, .8), x2 =c(.1, .5)))\\nsummary.areg.boot(object = f, values = list(x1 = c (0.2, 0.8),\\nx2 = c(0.1, 0.5)))\\nEstimates based on 300 resamples\\nValues to which predictors are set whenestimating\\neffects of other predictors:\\nyx 1x 2x 3\\n3.728843 0.500000 0.300000 2.000000\\nbBeware that use of a data–derived transformation in an ordinary model, as this will\\nresult in standard errors that are too small. This is because model selection is not\\ntaken into account.186\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7c524be3-a31e-4a0b-98ba-57ff6eb0aa8c', embedding=None, metadata={'page_label': '397', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"16.6 Case Study 397\\nEstimates of differences of effects on Median Y (from first X\\nvalue), and bootstrap standard errors of these differences.\\nSettings for X are shown as row headings.\\nPredictor: x1\\nx Differences S.E Lower 0.95 Upper 0.95 Z Pr(|Z|)\\n0.2 0.000000 NA NA NA NA NA\\n0.8 1.546992 0.2099959 1.135408 1.958577 7.366773 1.747491e-13\\nPredictor: x2\\nx Differences S.E Lower 0.95 Upper 0.95 Z Pr(|Z|)\\n0.1 0.000000 NA NA NA NA NA\\n0.5 -1.658961 0.3163361 -2.278968 -1.038953 -5.244298 1.568786e-07\\nPredictor: x3\\nx Differences S.E Lower 0.95 Upper 0.95 Z Pr(|Z|)\\na 0.0000000 NA NA NA NA NA\\nb 0.8447422 0.1768244 0.4981728 1.191312 4.777295 1.776692e-06\\nc 1.3526151 0.2206395 0.9201697 1.785061 6.130431 8.764127e-10\\nFor example, when x1increases from 0.2 to 0.8 we predict an increase in\\nmedian yby1.55withbootstrapstandarderror0.21,whenallotherpredictors\\nare held to constants. Setting them to other constants will yield diﬀerent\\nestimates of the x1eﬀect, as the transformation of yis nonlinear.\\nNext depict the ﬁtted model by plotting predicted values, with x2varying\\non thex-axis, and three curves corresponding to three values of x3.x1is set\\nto 0.5. Figure 16.3shows estimates of both the median and the mean y.\\nnewdat ←expand.grid(x2=seq(.05, .95, length=200),\\nx3=c( 'a','b','c'), x1=.5,\\nstatistic=c( 'median ','mean '))\\nyhat←c(predict(f, subset(newdat, statistic== 'median '),\\nstatistic= 'median '),\\npredict(f, subset(newdat, statistic== 'mean '),\\nstatistic= 'mean '))\\nnewdat ←\\nupData(newdat,\\nl p=x 1+2 * a b s ( x 2-. 5 )+. 3 * ( x 3 = = 'b')+\\n.5*(x3== 'c'),\\nytrue = ifelse( statistic== 'median ', exp(lp),\\nexp(lp + 0.5*(0.5∧2))), pr= FALSE)\\nInput object size: 45472 bytes; 4 variables\\nAdded variable lp\\nAdded variable ytrue\\nAdded variable pr\\nNew object size: 69800 bytes; 7 variables\\n# Use Hmisc function xYplot to produce Figure 16.3\\nxYplot(yhat ∼x2 | statistic, groups=x3,\\ndata=newdat, type= 'l', col=1,\\nylab=expression(hat(y)),\\npanel=function(...) {\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='78702752-e6eb-46f9-b0ca-606f1a93923e', embedding=None, metadata={'page_label': '398', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"398 16 Transform-Both-Sides Regression\\npanel.xYplot(...)\\ndat←subset(newdat,\\nstatistic==c( 'median ','mean ')[current.column ()])\\nfor(w in c( 'a','b','c'))\\nwith(subset(dat, x3==w),\\nllines(x2, ytrue, col= gray(.7), lwd=1.5))\\n}\\n)\\nx2y^\\n234567\\n0.2 0.4 0.6 0.8abcmedian0.2 0.4 0.6 0.8\\nabcmean\\nFig. 16.3 Predicted median (left panel) and mean (right panel) yas a function of\\nx2andx3. True population values are shown in gray.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d2d6d89-8a3e-43db-8363-d54197a65f28', embedding=None, metadata={'page_label': '399', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 17\\nIntroduction to Survival Analysis\\n17.1 Background\\nSuppose that one wished to study the occurrence of some event in a popu-\\nlation of subjects. If the time until the occurrence of the event were unim-\\nportant, the event could be analyzed as a binary outcome using the logistic\\nregression model. For example, in analyzing mortality associated with open\\nheart surgery, it may not matter whether a patient dies during the proce-\\ndure or he dies after being in a coma for two months. For other outcomes,\\nespecially those concerned with chronic conditions, the time until the event\\nis important. In a study of emphysema, death at eight years after onset of\\nsymptoms is diﬀerent from death at six months. An analysis that simply\\ncounted the number of deaths would be discarding valuable information and\\nsacriﬁcing statistical power.\\nSurvival analysis is used to analyze data in which the time until the event\\nis of interest. The response variable is the time until that event and is often\\ncalled a failure time, survival time ,o revent time . Examples of responses 1\\nof interest include the time until cardiovascular death, time until death or\\nmyocardial infarction, time until failure of a light bulb, time until pregnancy,\\nor time until occurrence of an ECG abnormality during exercise. Bull and\\nSpiegelhalter83have an excellent overview of survival analysis.\\nThe response, event time, is usually continuous, but survival analysis al-\\nlowsthe responsetobe incompletelydetermined forsomesubjects.Forexam-\\nple, suppose that after a ﬁve-yearfollow-upstudy ofsurvival after myocardial\\ninfarction a patient is still alive. That patient’s survival time is censored on\\nthe right at ﬁve years; that is, her survival time is known only to exceed ﬁve\\nyears.The response value to be used in the analysis is 5+. Censoring can also\\noccur when a subject is lost to follow-up. 2\\nIf no responses are censored, standard regression models for continuous\\nresponses could be used to analyze the failure times by writing the ex-\\npected failure time as a function of one or more predictors, assuming that\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 17399', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='313eff6a-e289-4c1d-8e97-824d39a79c04', embedding=None, metadata={'page_label': '400', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='400 17 Introduction to Survival Analysis\\nthe distribution of failure time is properly speciﬁed. However, there are still\\nseveral reasons for studying failure time using the specialized methods of\\nsurvival analysis.\\n1. Time to failure can have an unusual distribution. Failure time is restricted\\nto be positive so it has a skewed distribution and will never be normally\\ndistributed.\\n2. The probabilityofsurvivingpasta certaintime isoften morerelevantthan\\nthe expected survival time (and expected survival time may be diﬃcult to\\nestimate if the amount of censoring is large).\\n3. A function used in survival analysis, the hazard function, helps one to\\nunderstand the mechanism of failure.308\\nSurvival analysis is used often in industrial life-testing experiments, and\\nit is heavily used in clinical and epidemiologic follow-up studies. Examples\\ninclude a randomized trial comparing a new drug with placebo for its ability\\nto maintain remission in patients with leukemia, and an observational study\\nof prognostic factors in coronaryheart disease. In the latter example subjects\\nmay well be followed for varying lengths of time, as they may enter the study\\nover a period of many years.\\nWhen regression models are used for survival analysis, all the advantages\\nof these models can be brought to bear in analyzing failure times. Multiple,\\nindependentprognosticfactorscanbeanalyzedsimultaneouslyandtreatment\\ndiﬀerences can be assessed while adjusting for heterogeneity and imbalances\\nin baseline characteristics. Also, patterns in outcome over time can be pre-\\ndicted for individual subjects.\\nEven in a simple well-designed experiment, survival modeling can allow\\none to do the following in addition to making simple comparisons.\\n1. Test for and describe interactions with treatment. Subgroup analyses can\\neasily generate spurious results and they do not consider interacting fac-\\ntors in a dose-response manner. Once interactions are modeled, relative\\ntreatment beneﬁts can be estimated (e.g., hazard ratios), and analyses\\ncan be done to determine if some patients are too sick or too well to have\\neven a relative beneﬁt.\\n2. Understand prognostic factors (strength and shape).\\n3. Model absolute eﬀect of treatment. First, a model for the probability of\\nsurvivingpasttime tisdeveloped.Thendiﬀerencesinsurvivalprobabilities\\nfor patients on treatments A and B can be estimated. The diﬀerences will\\nbe due primarily to sickness (overall risk) of the patient and to treatment\\ninteractions.\\n4. Understand time course of treatment eﬀect. The period of maximum eﬀect\\nor period of any substantial eﬀect can be estimated from a plot of relative\\neﬀects of treatment over time.\\n5. Gain power for testing treatment eﬀects.\\n6. Adjust for imbalances in treatment allocation in non-randomized studies.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7e01b2e5-fde2-422b-88a4-bad5bbe4a3fd', embedding=None, metadata={'page_label': '401', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.2 Censoring, Delayed Entry, and Truncation 401\\n17.2 Censoring, Delayed Entry, and Truncation\\nResponses may be left–censored and interval–censored besides being right–\\ncensored. Interval–censoring ispresent,forexample,whenameasuringdevice\\nfunctions only for a certain range of the response; measurements outside that\\nrangearecensoredatanendof the scaleofthe device. Interval–censoringalso\\noccurswhenthepresenceofamedicalconditionisassessedduringperiodicex-\\nams. When the condition is present, the time until the condition developed is\\nonly known to be between the current and the previous exam. Left–censoring\\nmeans that an eventis known to have occurred before a certain time. In addi-\\ntion,left–truncation anddelayed entry are common. Nomenclature is confus- 3\\ning as manyauthorsreferto delayedentryasleft–truncation.Left–truncation\\nreally means that an unknown subset of subjects failed before a certain time\\nand the subjects didn’t get into the study. For example, one might study the\\nsurvival patterns of patients who were admitted to a tertiary care hospital.\\nPatients who didn’t survive long enough to be referred to the hospital com-\\npose the left-truncated group, and interesting questions such as the optimum\\ntiming of admission to the hospital cannot be answered from the data set.\\nDelayed entry occurs in follow-up studies when subjects are exposedto the\\nrisk of interest only after varying periods of survival. For example, in a study\\nof occupational exposure to a toxic compound, researchers may be interested\\nin comparing life length of employees with life expectancy in the general\\npopulation. A subject must live until the beginning of employment before\\nexposure is possible; that is, death cannot be observed before employment.\\nThe start of follow-up is delayed until the start of employment and it may be\\nright–censored when follow-up ends. In some studies, a researcher may want\\nto assume that for the purpose of modeling the shape of the hazard function,\\ntime zero is the day of diagnosis of disease, while patients enter the study\\nat various times since diagnosis. Delayed entry occurs for patients who don’t\\nenter the study until some time after their diagnosis. Patients who die before\\nstudy entry are left-truncated. Note that the choice of time origin is very\\nimportant.53,83,112,133\\nHearttransplantstudieshavebeen analyzedbyconsideringtime zerotobe\\nthe time of enrollment in the study. Pre-transplant survival is right–censored\\nat the time of transplant. Transplant survival experience is based on delayed\\nentry into the“risk set”to recognize that a transplant patient is not at risk\\nof dying from transplant failure until after a donor heart is found. In other\\nwords, survival experience is not credited to transplant surgery until the day\\nof transplant. Comparisons of transplant experience with medical treatment\\nsuﬀer from “waiting time bias” if transplant survival begins on the day of\\ntransplant instead of using delayed entry.209,438,570\\nThere are several planned mechanisms by which a response is right–\\ncensored. Fixed type I censoring occurs when a study is planned to end af-\\nter two years of follow-up, or when a measuring device will only measure\\nresponses up to a certain limit. There the responses are observed only if they', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2b16fa7-cbde-474c-91c1-4191db312ba9', embedding=None, metadata={'page_label': '402', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='402 17 Introduction to Survival Analysis\\nfall below a ﬁxed value C. In type II censoring , a study ends when there is\\na pre-speciﬁed number of events. If, for example, 100 mice are followed until\\n50 die, the censoring time is not known in advance.\\nWe are concerned primarily with random type I right-censoring in which\\neach subject’s event time is observed only if the event occurs before a certain\\ntime, but the censoring time can vary between subjects. Whatever the cause\\nofcensoring,weassumethatthecensoringis non-informative abouttheevent;\\nthat is, the censoring is caused by something that is independent of the im-\\npending failure. Censoring is non-informative when it is caused by planned\\ntermination of follow-up or by a subject moving out of town for reasons unre-\\nlated to the risk of the event. If subjects are removed from follow-up because\\nof a worsening condition, the informative censoring will result in biased esti-\\nmates and inaccurate statistical inference about the survival experience. For\\nexample, if a patient’s response is censored because of an adverse eﬀect of\\na drug or noncompliance to the drug, a serious bias can result if patients\\nwith adverse experiences or noncompliance are also at higher risk of suﬀering\\nthe outcome. In such studies, eﬃcacy can only be assessed fairly using the\\nintention to treat principle : all events should be attributed to the treatment\\nassigned even if the subject is later removed from that treatment. 4\\n17.3 Notation, Survival, and Hazard Functions\\nIn survival analysis we use Tto denote the response variable, as the response\\nis usually the time until an event. Instead of deﬁning the statistical model\\nfor the response Tin terms of the expected failure time, it is advantageous\\nto deﬁne it in terms of the survival function ,S(t), given by\\nS(t)=P r o b {T>t}=1−F(t), (17.1)\\nwhereF(t) is the cumulative distribution function for T. If the eventis death,\\nS(t) is the probability that death occurs after time t, that is, the probability\\nthat the subject will survive at least until time t.S(t)i sa l w a y s1a t t=0 ;\\nall subjects survive at least to time zero. The survival function must be\\nnon-increasing as tincreases. An example of a survival function is shown in\\nFigure17.1. In that example subjects are at very high risk of the event in the\\nearlyperiodsothatthe S(t)dropssharply.Theriskislowfor0 .1≤t≤0.6,so\\nS(t) is somewhatﬂat.After t=.6the riskagainincreases,so S(t)dropsmore\\nquickly. Figure 17.2depicts the cumulative hazard function corresponding\\nto the survival function in Figure 17.1. This function is denoted by Λ(t).\\nIt describes the accumulated risk up until time t, and as is shown later,\\nis the negative of the log of the survival function. Λ(t) is non-decreasing\\nastincreases; that is, the accumulated risk increases or remains the same.\\nAnother important function is the hazard function ,λ(t), also called the force', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='467ca903-6cc1-46c0-9ae0-bcf435b4a4e4', embedding=None, metadata={'page_label': '403', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.3 Notation, Survival, and Hazard Functions 403\\n0.0 0.2 0.4 0.6 0.8 1.00.20.40.60.81.0\\ntS(t)\\nFig. 17.1 Survival function\\n0.0 0.2 0.4 0.6 0.8 1.00.00.51.01.52.02.53.0\\ntΛ(t)\\nFig. 17.2 Cumulative hazard function\\nof mortality ,o rinstantaneous event (death, failure) rate . The hazard at time\\ntis related to the probability that the event will occur in a small interval\\naroundt, given that the event has not occurred before time t. By studying\\nthe eventrateat a giventime conditionalon the eventnot havingoccurredby', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='00754d61-3b7e-4fd0-96da-d5dcb76e8b26', embedding=None, metadata={'page_label': '404', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='404 17 Introduction to Survival Analysis\\n0.0 0.2 0.4 0.6 0.8 1.02468101214\\ntλ(t)\\nFig. 17.3 Hazard function\\nthat time, one can learn about the mechanisms and forces of risk over time.\\nFigure17.3depicts the hazard function corresponding to S(t) in Figure 17.1\\nand toΛ(t) in Figure 17.2. Notice that the hazard function allows one to\\nmore easily determine the phases of increased risk than looking for sudden\\ndrops in S(t)o rΛ(t).\\nThe hazard function is deﬁned formally by\\nλ(t) = lim\\nu→0Prob{t<T≤t+u|T>t}\\nu, (17.2)\\nwhich using the law of conditional probability becomes\\nλ(t) = lim\\nu→0Prob{t<T≤t+u}/Prob{T>t}\\nu\\n= lim\\nu→0[F(t+u)−F(t)]/u\\nS(t)\\n=∂F(t)/∂t\\nS(t)(17.3)\\n=f(t)\\nS(t),', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='977a5935-4392-4624-ac8c-9858c8e69975', embedding=None, metadata={'page_label': '405', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.3 Notation, Survival, and Hazard Functions 405\\nwheref(t) is the probability density function of Tevaluated at t, the deriva-\\ntive or slope of the cumulative distribution function 1 −S(t). Since\\n∂logS(t)\\n∂t=∂S(t)/∂t\\nS(t)=−f(t)\\nS(t), (17.4)\\nthe hazard function can also be expressed as\\nλ(t)=−∂logS(t)\\n∂t, (17.5)\\nthe negative of the slope of the log of the survival function. Working back-\\nwards, the integral of λ(t)i s :\\n∫t\\n0λ(v)dv=−logS(t). (17.6)\\nThe integral or area under λ(t) is deﬁned to be Λ(t), the cumulative hazard\\nfunction. Therefore\\nΛ(t)=−logS(t), (17.7)\\nor\\nS(t)=e x p [−Λ(t)]. (17.8)\\nSo knowing any one of the functions S(t),Λ(t), orλ(t) allows one to derive\\nthe other two functions. The three functions are diﬀerent ways of describing\\nthe same distribution.\\nOne property of Λ(t) is that the expected value of Λ(T) is unity, since if\\nT∼S(t), the density of Tisλ(t)S(t)a n d\\nE[Λ(T)] =∫∞\\n0Λ(t)λ(t)exp(−Λ(t))dt\\n=∫∞\\n0uexp(−u)du (17.9)\\n=1.\\nNow considerpropertiesofthe distribution of T.The population qth quan-\\ntile (100qth percentile), Tq, is the time by which a fraction qof the subjects\\nwill fail. It is the value tsuch that S(t)=1−q;t h a ti s\\nTq=S−1(1−q). (17.10)\\nThemedianlifelengthisthetimebywhichhalfthesubjectswillfail,obtained\\nby setting S(t)=0.5:\\nT0.5=S−1(0.5). (17.11)\\nTheqth quantile of Tcan also be computed by setting exp[ −Λ(t)] = 1−q,\\ngiving', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85010672-6e1c-4664-a9ce-3d66b192e98b', embedding=None, metadata={'page_label': '406', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='406 17 Introduction to Survival Analysis\\nTq=Λ−1[−log(1−q)] and as a special case ,\\nT.5=Λ−1(log2). (17.12)\\nThe meanor expected value of T(the expected failure time) is the areaunder\\nthe survival function for tranging from 0 to ∞:\\nμ=∫∞\\n0S(v)dv. (17.13)\\nIrwin has deﬁned mean restricted life (see[334,335]), which is the area under\\nS(t) up to a ﬁxed time (usually chosen to be a point at which there is still\\nadequate follow-up information).\\nThe random variable Tdenotes a random failure time from the survival\\ndistribution S(t). We need additionalnotation for the response and censoring\\ninformationfor the ithsubject. Let Tidenote the responsefor the ith subject.\\nThis response is the time until the event of interest, and it may be censored\\nif the subject is not followed long enough for the event to be observed. Let Ci\\ndenotethecensoringtimeforthe ithsubject,anddeﬁnetheeventindicatoras\\nei= 1 if the event was observed ( Ti≤Ci),\\n= 0 if the response was censored ( Ti>Ci).(17.14)\\nThe observed response is\\nYi= min(Ti,Ci), (17.15)\\nwhich is the time that occurred ﬁrst: the failure time or the censoring time.\\nThe pair of values ( Yi,ei) contains all the response information for most\\npurposes (i.e., the potential censoring time Ciis not usually of interest if the\\nevent occurred before Ci).\\nFigure17.4demonstrates this notation. The line segments start at study\\nentry (survival time t=0 ) .\\nA useful property of the cumulative hazard function can be derived as fol-\\nlows. Let zbe any cutoﬀ time and consider the expected value of Λevaluated\\nat the earlier of the cutoﬀ time or the actual failure time.\\nE[Λ(min(T,z))] =E[Λ(T)[T≤z]+Λ(z)[T>z]]\\n=E[Λ(T)[T≤z]]+Λ(z)S(z). (17.16)\\nThe ﬁrst term in the right–hand side is\\n∫∞\\n0Λ(t)[t≤z]λ(t)exp(−Λ(t))dt\\n=∫z\\n0Λ(t)λ(t)exp(−Λ(t))dt (17.17)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='203e375f-4d3b-402c-aa51-a4b98c5b3fb9', embedding=None, metadata={'page_label': '407', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.4 Homogeneous Failure Time Distributions 407\\nTiCiYiei\\n75 81\\n76\\n68+ 68 68\\n52\\n56 20 20 152 52+7 775 1\\n1\\n0\\n0\\nTermination of Study\\nFig. 17.4 Some censored data. Circles denote events.\\n=−[uexp(−u)+exp(−u)]|Λ(z)\\n0\\n=1−S(z)[Λ(z)+1].\\nAddingΛ(z)S(z)r e s u l t si n\\nE[Λ(min(T,z))] = 1−S(z)=F(z). (17.18)\\nIt follows that∑n\\ni=1Λ(min(Ti,z)) estimates the expected number of failures\\noccurring before time zamong the nsubjects. 5\\n17.4 Homogeneous Failure Time Distributions\\nIn this section we assume that each subject in the sample has the same dis-\\ntribution of the random variable Tthat represents the time until the event.\\nIn particular, there are no covariables that describe diﬀerences between sub-\\njects in the distribution of T.A sb e f o r ew eu s e S(t),λ(t), andΛ(t) to denote,\\nrespectively, the survival, hazard, and cumulative hazard functions.\\nThe form of the true population survival distribution function S(t)i sa l -\\nmost always unknown, and many distributional forms have been used for\\ndescribing failure time data. We consider ﬁrst the two most popular para-\\nmetric survival distributions: the exponential and Weibull distributions. The\\nexponential distribution is a very simple one in which the hazard function is\\nconstant; that is, λ(t)=λ. The cumulative hazard and survival functions\\nare then\\nΛ(t)=λtand\\nS(t)=e x p (−Λ(t)) = exp( −λt). (17.19)\\nThe median life length is Λ−1(log2) or', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='854b475d-0d86-49a3-a897-324081b54c21', embedding=None, metadata={'page_label': '408', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='408 17 Introduction to Survival Analysis\\nT0.5= log(2)/λ. (17.20)\\nThe time by which 1 /2 of the subjects will have failed is then proportional to\\nthe reciprocalof the constant hazard rate λ. This is true also of the expected\\nor mean life length, which is 1 /λ.\\nThe exponential distribution is one of the few distributions for which a\\nclosed-form solution exists for the estimator of its parameter when censoring\\nis present. This estimator is a function of the number of events and the total\\nperson-years of exposure. Methods based on person-years in fact implicitly\\nassumeanexponentialdistribution.Theexponentialdistributionisoftenused\\nto model events that occur“at random in time.”323It has the property that\\nthe future lifetime of a subject is the same, no matter how“old”it is, or\\nProb{T>t0+t|T>t0}=P r o b{T>t}. (17.21)\\nThis“ageless”property also makes the exponential distribution a poor choice\\nfor modeling human survival except over short time periods.\\nTheWeibulldistributionisageneralizationoftheexponentialdistribution.\\nIts hazard, cumulative hazard, and survival functions are given by\\nλ(t)=αγtγ−1\\nΛ(t)=αtγ(17.22)\\nS(t)=e x p (−αtγ).\\nThe Weibull distribution with γ= 1 is an exponential distribution (with\\nconstant hazard). When γ>1, its hazard is increasing with t,a n dw h e n\\nγ<1 its hazard is decreasing. Figure 17.5depicts some of the shapes of\\nthe hazard function that are possible. If Thas a Weibull distribution, the\\nmedian of Tis\\nT0.5= [(log2) /α]1/γ. (17.23)\\nTherearemanyothertraditionalparametricsurvivaldistributions,someof\\nwhich have hazards that are“bathtub shaped”as in Figure 17.3.243,323The\\nrestricted cubic spline function described in Section 2.4.5is an alternative\\nbasis for λ(t).286,287This function family allows for any shape of smooth\\nλ(t) since the number of knots can be increased as needed, subject to the\\nnumber of events in the sample. Nonlinear terms in the spline function can\\nbe tested to assess linearity of hazard (Rayleigh-ness) or constant hazard\\n(exponentiality). 6\\nThe restricted cubic spline hazard model with kknots is\\nλk(t)=a+bt+k−2∑\\nj=1γjwj(t), (17.24)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a7ef5ed9-c991-4147-a286-e22fc139ac2b', embedding=None, metadata={'page_label': '409', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.5 Nonparametric Estimation of SandΛ 409\\n0.0 0.2 0.4 0.6 0.8 1.0 1.201234567\\nt.5\\n1\\n2\\n4\\nFig. 17.5 Some Weibull hazard functions with α= 1 and various values of γ.\\nwhere the wj(t) are the restricted cubic spline terms of Equation 2.25.T h e r e\\nterms are cubic terms in t. A set of knots v1,...,v kis selected from the\\nquantiles of the uncensored failure times (see Section 2.4.5and[286]).\\nThe cumulative hazard function for this model is\\nΛ(t)=at+1\\n2t2+1\\n4×quartic terms in t . (17.25)\\nStandard maximum likelihood theory is used to obtain estimates of the k\\nunknown parameters to derive, for example, smooth estimates of λ(t) with\\nconﬁdence bands. The ﬂexible estimates of S(t) using this method are as\\neﬃcientasKaplan–Meierestimates,buttheyaresmoothandcanbe usedasa\\nbasisformodelingpredictorvariables.Thesplinehazardmodelisparticularly\\nuseful for ﬁtting steeply falling and gently rising hazard functions that are\\ncharacteristic of high-risk medical procedures.\\n17.5 Nonparametric Estimation of SandΛ\\n17.5.1 Kaplan–Meier Estimator\\nAs the true form of the survival distribution is seldom known, it is useful to\\nestimate the distribution without making any assumptions. For many anal-\\nyses, this may be the last step, while in others this step helps one select a\\nstatistical model for more in-depth analyses. When no event times are cen-\\nsored, a nonparametric estimator of S(t)i s1−Fn(t)w h e r eFn(t) is the usual', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9972364c-ed55-4313-9050-0ddcc38363e0', embedding=None, metadata={'page_label': '410', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='410 17 Introduction to Survival Analysis\\nTable 17.1 Kaplan-Meier computations\\nDay No. Subjects Deaths Censored Cumulative\\nAt Risk Survival\\n12 100 1 0 99 /100=.99\\n30 99 2 1 97 /99×99/100=.97\\n60 96 0 3 96 /96×.97 =.97\\n72 93 3 0 90 /93×.97 =.94\\n.. .. .\\n.. .. .\\nempiricalcumulativedistributionfunctionbasedontheobservedfailuretimes\\nT1,...,T n.L e tSn(t) denote this empirical survival function. Sn(t)i sg i v e n\\nby the fraction of observed failure times that exceed t:\\nSn(t) = [number of Ti>t]/n. (17.26)\\nWhen censoring is present, S(t) can be estimated (at least for tup until\\nthe end of follow-up) by the Kaplan–Meier333product-limit estimator. This\\nmethod is based on conditional probabilities. For example, suppose that ev-\\nery subject has been followed for 39 days or has died within 39 days so that\\nthe proportion of subjects surviving at least 39 days can be computed. After\\n39 days, some subjects may be lost to follow-up besides those removed from\\nfollow-up because of death within 39 days. The proportion of those still fol-\\nlowed 39 days who survive day 40 is computed. The probability of surviving\\n40 days from study entry equals the probability of surviving day 40 after\\nliving 39 days, multiplied by the chance of surviving 39 days.\\nThe life table in Table 17.1demonstrates the method in more detail. We\\nsuppose that 100 subjects enter the study and none die or are lost before\\nday 12.\\nTimes in a life table should be measure d as precisely as possible. If the\\nevent being analyzed is death, the failure time should usually be speciﬁed\\nto the nearest day. We assume that deaths occur on the day indicated and\\nthat being censoredon a certain day implies the subject survived through the\\nend of that day. The data used in computing Kaplan–Meier estimates consist\\nof (Yi,ei),i=1,2,...,nusing notation deﬁned previously. Primary data\\ncollected to derive ( Yi,ei) usually consist of entry date, event date (if subject\\nfailed), and censoring date (if subject did not fail). Instead, the entry date,\\ndate of event/censoring, and event/censoring indicator eimay be speciﬁed.\\nThe Kaplan–Meier estimator is called the product-limit estimator because\\nit is the limiting case of actuarial survival estimates as the time periods\\nshrink so that an entry is made for each failure time. An entry need not\\nbe in the table for censoring times (when no failures occur at that time) as\\nlong as the number of subjects censored is subtracted from the next number', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e7b5a2df-0396-44f7-ad39-0447a395fc0a', embedding=None, metadata={'page_label': '411', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.5 Nonparametric Estimation of SandΛ 411\\nTable 17.2 Summaries used in Kaplan-Meier computations\\nitinidi(ni−di)/ni\\n1 1 7 1 6/7\\n2 3 6 2 4/6\\n3 9 2 1 1/2\\nat risk. Kaplan–Meier estimates are preferred to actuarial estimates because\\nthey provide more resolution and make fewer assumptions. In constructing\\na yearly actuarial life table, for example, it is traditionally assumed that\\nsubjects censored between two years were followed 0.5 years.\\nThe product-limit estimator is a nonparametric maximum likelihood es-\\ntimator [ 331, pp. 10–13]. The formula for the Kaplan–Meier product-limit\\nestimator of S(t) is as follows. Let kdenote the number of failures in the\\nsample and let t1,t2,...,tkdenote the unique event times (ordered for ease\\nof calculation). Let didenote the number of failures at tiandnibe the num-\\nber of subjects at riskat timeti;t h a ti s , ni= number of failure/censoring\\ntimes≥ti. The estimator is then\\nSKM(t)=∏\\ni:ti≤t(1−di/ni). (17.27)\\nThe Kaplan–Meier estimator of Λ(t)i sΛKM(t)=−logSKM(t). An estimate\\nof quantile qof failure time is S−1\\nKM(1−q), if follow-up is long enough so that\\nSKM(t)d r o p sa sl o wa s1 −q. If the last subject followed failed so that SKM(t)\\ndrops to zero, the expected failure time can be estimated by computing the\\narea under the Kaplan–Meier curve.\\nTo demonstrate computation of SKM(t), imagine a sample of failure times\\ngiven by\\n1336+8+91 0+,\\nwhere + denotes a censored time. The quantities needed to compute SKMare\\nin Table 17.2.T h u s\\nSKM(t)=1,0≤t<1\\n=6/7=.85,1≤t<3\\n=( 6/7)(4/6)=.57,3≤t<9 (17.28)\\n=( 6/7)(4/6)(1/2)=.29,9≤t<10.\\nNote that the estimate of S(t) is undeﬁned for t>10 since not all subjects\\nhave failed by t= 10 but no follow-up extends beyond t= 10. A graph of the\\nKaplan–Meier estimate is found in Figure 17.6.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b9886b6-5740-448d-906d-2f7ca1417b80', embedding=None, metadata={'page_label': '412', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='412 17 Introduction to Survival Analysis\\nrequire(rms)\\ntt←c(1,3,3,6,8,9,10)\\nstat←c(1,1,1,0,0,1,0)\\nS←Surv(tt, stat)\\nsurvplot( npsurv(S ∼1), conf=\"bands\", n.risk= TRUE,\\nxlab=expression(t))\\nsurvplot( npsurv(S ∼1, type=\" fleming-harrington\",\\nconf.int= FALSE), add= TRUE, lty=3)\\ntSurvival Probability\\n0123456789 1 00.00.20.40.60.81.0\\n77 66 444 33 21\\nFig. 17.6 Kaplan–Meier product–limit estimator with 0 .95 conﬁdence bands. The\\nAltschuler–Nelson–Aalen–Fleming–Harrington estimator is depicted with the dott ed\\nlines.\\nThe variance of SKM(t) can be estimated using Greenwood’s formula [ 331,\\np. 14], and using normality of SKM(t) in large samples this variance can\\nbe used to derive a conﬁdence interval for S(t). A better method is to de-\\nrive an asymmetric conﬁdence interval for S(t) based on a symmetric in-\\nterval for log Λ(t). This latter method ensures that a conﬁdence limit does\\nnot exceed one or fall below zero, and is more accurate since log ΛKM(t)i s\\nmore normally distributed than SKM(t). Once a conﬁdence interval, say [ a,b]\\nis determined for log Λ(t), the conﬁdence interval for S(t) is computed by\\n[exp{−exp(b)},exp{−exp(a)}]. The formula for an estimate of the variance\\nof interest is [ 331, p. 15]:\\nVar{logΛKM(t)}=∑\\ni:ti≤tdi/[ni(ni−di)]\\n{∑\\ni:ti≤tlog[(ni−di)/ni]}2.(17.29)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='595fd47a-38a8-45ad-96eb-ad5bdd7f178e', embedding=None, metadata={'page_label': '413', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.6 Analysis of Multiple Endpoints 413\\nLettingsdenote the square root of this variance estimate, an approximate\\n1−αconﬁdence interval for log Λ(t) is given by log ΛKM(t)±zs,w h e r ezis\\nthe 1−α/2standardnormalcriticalvalue.Aftersimpliﬁcation,theconﬁdence\\ninterval for S(t) becomes\\nSKM(t)exp(±zs). (17.30)\\nEven though the log Λbasis for conﬁdence limits has theoretical advan-\\ntages, on the log −log scale the estimate of S(t) has the greatest instability\\nwhere much information is available: when S(t) falls just below 1.0. For that\\nreason, the recommended default conﬁdence limits are on the Λ(t) scale using\\nVar{ΛKM(t)}=∑\\ni:ti≤tdi\\n[ni(ni−di)]. (17.31)\\nLettingsdenote its square root, an approximate 1 −αconﬁdence interval for\\nS(t)i sg i v e nb y\\nexp(±zs)SKM(t), (17.32)\\ntruncated to [0 ,1]. 7\\n17.5.2 Altschuler–Nelson Estimator\\nAltschuler19,N e l s o n472,A a l e n1and Fleming and Harrington196proposed es-\\ntimators of Λ(t)o ro fS(t) based on an estimator of Λ(t):\\nˆΛ(t)=∑\\ni:ti≤tdi\\nni\\nSΛ(t)=e x p (−ˆΛ(t)). (17.33)\\nSΛ(t) has advantages over SKM(t). First,∑n\\ni=1ˆΛ(Yi)=∑n\\ni=1ei\\n[605, Appendix 3]. In other words, the estimator gives the correct expected\\nnumber of events. Second, there is a wealth of asymptotic theory based on\\nthe Altschuler–Nelson estimator.196\\nSee Figure 17.6for an example of the SΛ(t) estimator. This estimator has\\nthe same variance as SKM(t) for large enough samples. 8\\n17.6 Analysis of Multiple Endpoints\\nClinical studies frequently assess multiple endpoints. A cancer clinical trial\\nmay, for example, involve recurrence of disease and death, whereas a car-\\ndiovascular trial may involve nonfatal myocardial infarction and death. End-\\npoints may be combined, and the new event (e.g., time until infarction or', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='023b0483-23de-40b2-9afb-da35f0725655', embedding=None, metadata={'page_label': '414', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='414 17 Introduction to Survival Analysis\\ndeath) maybe analyzedwithanyofthetoolsofsurvivalanalysisbecauseonly\\nthe usual censoringmechanism is used. Sometimes the variousendpoints may\\nneed separate study, however, because they may have diﬀerent risk factors.\\nWhen the multiple endpoints represent multiple causes of a terminating\\nevent (e.g., death), Prentice et al. have developed standard methods for an-\\nalyzing cause-speciﬁc hazards513[331, pp. 163–178]. Their methods allow\\neach cause of failure to be analyzed separately, censoring on the other causes.\\nThey do not assume any mechanism for cause removal nor make any assump-\\ntions regarding the interrelation among causes of failure. However, analyses\\nof competing events using data where some causes of failure are removed in\\na diﬀerent way from the original dataset will give rise to diﬀerent inferences.\\nWhen the multiple endpoints represent a mixture of fatal and nonfatal\\noutcomes, the analysis may be more complex. The same is true when one\\nwishes to jointly study an event-time endpoint and a repeated measurement.\\n9\\n17.6.1 Competing Risks\\nWhen events are independent, each event may also be analyzed separately by\\ncensoringonallothereventsaswellascensoringonlosstofollow-up.Thiswill\\nyield an unbiased estimate of an easily interpreted cause-speciﬁc λ(t)o rS(t)\\nbecause censoring is non-informative [ 331, pp. 168–169]. One minus SKM(t)\\ncomputedinthismannerwillcorrectlyestimatetheprobabilityoffailingfrom\\nthe event in the absence of other events. Even when the competing events are\\nnot independent, the cause-speciﬁc hazard model may lead to valid results,\\nbut the resulting model does not allow one to estimate risks conditional on\\nremoval of one or more causes of the event. See Kay340for a nice example\\nof competing risks analysis when a treatment reduces the risk of death from\\none cause but increases the risk of death from another cause. 10\\nLarson and Dinse376have an interesting approach that jointly models the\\ntime until (any) failure and the failure type. For rfailure types, they use\\nanr-category polytomous logistic model to predict the probability of failing\\nfrom each cause. They assume that censoring is unrelated to cause of event.\\n17.6.2 Competing Dependent Risks\\nIn many medical and epidemiologic studies one is interested in analyzing\\nmultiple causes of death. If the goal is to estimate cause-speciﬁc failure prob-\\nabilities, treating subjects dying from extraneous causes as censored and\\nthen computing the ordinary Kaplan–Meier estimate results in biased (high)\\nsurvival estimates212,225.I fc a u s e mis of interest, the cause-speciﬁc hazard', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f997462c-7b6d-4a22-83e3-144974d4314e', embedding=None, metadata={'page_label': '415', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.6 Analysis of Multiple Endpoints 415\\nfunction is deﬁned as\\nλm(t) = lim\\nu→0Pr{fail from cause min [t,t+u)|alive att}\\nu.(17.34)\\nThecumulative incidence function or probability of failure from cause mby\\ntimetis given by\\nFm(t)=∫t\\n0λm(u)S(u)du, (17.35)\\nwhereS(u) is the probability of surviving (ignoring cause of death), which\\nequalsexp[ −∫u\\n0(∑λm(x))dx][212];[444,Chapter10]; [102,408].A sp r e v i o u s l y\\nmentioned, 1 −Fm(t)=e x p [−∫t\\n0λm(u)du] only if failures due to other causes\\nare eliminated and if the cause-speciﬁc hazard of interest remains unchanged\\nin doing so.212\\nAgain letting t1,t2,...,tkdenote the unique ordered failure times, a non-\\nparametric estimate of Fm(t)i sg i v e nb y\\nˆFm(t)=∑\\ni:ti≤tdmi\\nniSKM(ti−1), (17.36)\\nwheredmiis the number of failures of type mat timetiandniis the number\\nof subjects at risk of failure at ti.\\nPepe and others494,496,497showed how to use a combination of Kaplan–\\nMeier estimators to derive an estimator of the probability of being free of\\nevent 1 by time tgiven event 2 has not occurred by time t(see also [349]).\\nLetT1andT2denote, respectively, the times until events 1 and 2. Let S1(t)\\nandS2(t) denote, respectively, the two survival functions. Let us suppose\\nthat event 1 is not a terminating event (e.g., is not death) and that even\\nafter event 1 subjects are followed to ascertain occurrences of event 2. The\\nprobability that T1>tgivenT2>tis\\nProb{T1>t|T2>t}=Prob{T1>tandT2>t}\\nProb{T2>t}\\n=S12(t)\\nS2(t), (17.37)\\nwhereS12(t) is the survival function for min( T1,T2), the earlier of the two\\nevents. Since S12(t) does not involve any informative censoring (assuming as\\nalways that loss to follow-up is non-informative), S12may be estimated by\\nthe Kaplan–Meier estimator SKM12(or bySΛ). For the type of event 1 we\\nhave discussed above, S2can also be estimated without bias by SKM2.T h u s\\nwe estimate, for example, the probability that a subject still alive at time t\\nwill be free of myocardial infarction as of time tbySKM12/SKM2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0f5c1702-40a5-4b9b-98c2-abfd0c8d9ac7', embedding=None, metadata={'page_label': '416', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='416 17 Introduction to Survival Analysis\\nAnother quantity that can easily be computed from ordinary survival es-\\ntimates is S2(t)−S12(t)=[ 1−S12(t)]−[1−S2(t)], which is the probability\\nthat event 1 occurs by time tand that event 2 has not occurred by time t.\\nThe ratio estimate above is used to estimate the survival function for one\\nevent given that another has not occurred. Another function of interest is\\nthecrude survival function which is a marginal distribution; that is, it is the\\nprobability that T1>twhether or not event 2 occurs:362\\nSc(t)=1−F1(t)\\nF1(t)=P r o b {T1≤t}, (17.38)\\nwhereF1(t)i st h ecrude incidence function deﬁned previously. Note that the\\nT1≤timplies that the occurrence of event 1 is part of the probability being\\ncomputed. If event 2 is a terminating event so that some subjects can never\\nsuﬀer event 1, the crude survival function for T1will never drop to zero. The\\ncrude survival function can be interpreted as the survival distribution of W\\nwhereW=T1ifT1<T2andW=∞otherwise.36211\\n17.6.3 State Transitions and Multiple Types\\nof Nonfatal Events\\nIn manystudiesthereisoneﬁnal, absorbingstate(death, allcauses)and mul-\\ntiple livestates.Thelivestatesmayrepresentdiﬀerenthealthstatesorphases\\nof a disease. For example, subjects may be completely free of cancer, have an\\nisolated tumor, metastasize to a distant organ, and die. Unlike this example,\\nthe live states need not have a deﬁnite ordering. One may be interested in es-\\ntimating transition probabilities , for example, the probability πij(t1,t2)t h a t\\nan individual in state iat timet1is in state jafter an additional time t2.\\nStrauss and Shavelle596have developed an extended Kaplan–Meier estimator\\nfor this situation. Let Si\\nKM(t|t1) denote the ordinary Kaplan–Meier estimate\\nof the probability of not dying before time t(ignoring distinctions between\\nmultiple live states) for a cohort of subjects beginning follow-up at time t1\\nin statei. This is an estimate of the probability of surviving an additional t\\ntime units (in any live state) given that the subject was alive and in state i\\nat timet1. Strauss and Shavelle’s estimator is given by\\nπij(t1,t2)=nij(t1,t2)\\nni(t1,t2)Si\\nKM(t2|t1), (17.39)\\nwhereni(t1,t2) is the number of subjects in live state iat timet1who are\\nalive and uncensored t2time units later, and nij(t1,t2)i st h en u m b e ro fs u c h\\nsubjects in state jt2time units beyond t1. 12', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8e7d6a8d-014d-49ea-a441-1122b6b17c33', embedding=None, metadata={'page_label': '417', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.6 Analysis of Multiple Endpoints 417\\n17.6.4 Joint Analysis of Time and Severity\\nof an Event\\nIn some studies, an endpoint is given more weight if it occurs earlier or\\nif it is more severe clinically, or both. For example, the event of interest\\nmay be myocardial infarction, which may be of any severity from minimal\\ndamage to the left ventricle to a fatal infarction. Berridge and Whitehead52\\nhave provided a promising model for the analysis of such endpoints. Their\\nmethod assumes that the severity of endpoints which do occur is measured\\non an ordinal categorical scale and that severity is assessed at the time of\\nthe event. Berridge and Whitehead’s example was time until ﬁrst headache,\\nwith severity of headaches graded on an ordinal scale. They proposed a joint\\nhazard of an individual who responds with ordered category j:\\nλj(t)=λ(t)πj(t), (17.40)\\nwhereλ(t) is the hazard for the failure time and πj(t) is the probability of an\\nindividual having event severity jgiven she fails at time t. Note that a shift\\nin the distribution of response severity is allowed as the time until the event\\nincreases. 13\\n17.6.5 Analysis of Multiple Events\\nIt is common to choose as an endpoint in a clinical trial an event that can\\nrecur. Examples include myocardial infarction, gastric ulcer, pregnancy, and\\ninfection. Using only the time until the ﬁrst event can result in a loss of\\nstatisticalinformationandpower.aTherearespecializedmultivariatesurvival\\nmodels (whose assumptions are extremely diﬃcult to verify) for handling this\\nsetup, but in many cases a simpler approach will be eﬃcient.\\nThe simpler approach involves modeling the marginal distribution of the\\ntime until each event.407,495Here one forms one recordper subject per event,\\nand the survival time is the time to the ﬁrst event for the ﬁrst record, or is\\nthe time from the previous event to the next event for all later records. This\\napproachyields consistentestimates of distribution parametersas long as the\\nmarginaldistributionsare correctlyspeciﬁed.655One canallowthe numberof\\npreviousevents to inﬂuence the hazard function of anotherevent by modeling\\nthis count as a covariable.\\nThe multiple events within subject are not independent, so variance esti-\\nmates must be corrected for intracluster correlation. The clustered sandwich\\ncovariance matrix estimator described in Section 9.5and in[407]will provide\\naAn exception to this is the case in which once an event occurs for the ﬁrst time, that\\nevent is likely to recur multiple times for any patient. Then the latter occurrences are\\nredundant.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c4d02e1-dd6b-4aa5-8b7f-ee69f0d59239', embedding=None, metadata={'page_label': '418', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='418 17 Introduction to Survival Analysis\\nconsistent estimates of variances and covariances even if the events are de-\\npendent. Lin407also discussed how this method can easily be used to model\\nmultiple events of diﬀering types. 14\\n17.7RFunctions\\nTheevent.chart function of Lee et al.394will draw a variety of charts for dis-\\nplaying raw survival time data, for both single and multiple events per sub-\\nject. Relationships with covariables can also be displayed. The event.history\\nfunction of Dubin et al.166draws an event history graph for right-censored\\nsurvival data, including time-dependent covariate status. These function are\\nin theHmiscpackage.\\nThe analysesdescribedin this chapter canbe viewedasspecial casesofthe\\nCox proportional hazards model.132The programs for Cox model analyses\\nd e s c r i be di nS e c t i o n 20.13can be used to obtain the results described here, as\\nlong as there is at least one stratiﬁcationfactor in the model. There are,how-\\never, several Rfunctions that are pertinent to the homogeneous or stratiﬁed\\ncase. The Rfunction survfit, and its particular renditions of the print, plot,\\nlines, and points generic functions (all part of the survival package written\\nby Terry Therneau), will compute, print, and plot Kaplan–Meier and Nelson\\nsurvival estimates. Conﬁdence intervals for S(t)m a yb eb a s e do n S,Λ,o r\\nlogΛ.T h ermspackage’s front-end to the survival package’s survfitfunction\\nisnpsurvfor “nonparametric survival”. It and other functions described in\\nlater chapters use Therneau’s Survfunction to combine the response variable\\nand event indicator into a single R“survivaltime”object. In its simplestform,\\nuseSurv(y, event) ,w h e r e yis the failure/right–censoring time and eventis\\nthe event/censoring indicator, usually coded T/F,0=c e n s o r e d1=e v e n to r\\n1 = censored 2 = event. If the event status variable has other coding (e.g., 3\\nmeans death), use Surv(y, s==3) . To handle interval time-dependent covari-\\nables, or to use Andersen and Gill’s counting process formulation of the Cox\\nmodel,23use the notation Surv(tstart, tstop, status) . The counting process\\nnotation allows subjects to enter and leave risk sets at random. For each\\ntime interval for each subject, the interval is made up of tstart<t≤tstop.\\nFor time-dependent stratiﬁcation, there is an optional originargument to\\nSurvthat indicates the hazard shape time origin at the time of crossover\\nto a new stratum. A typeargument is used to handle left– and interval–\\ncensoring, especially for parametric survival models. Possible values of type\\nare\"right\",\"left\",\"interval\",\"counting\",\"interval2\",\"mstate\" .\\nTheSurvexpression will usually be used inside another function, but it is\\nﬁne to save the result of Survin another object and to use this object in the\\nparticular ﬁtting function.\\nnpsurvis invoked by the following, with default parameter settings indi-\\ncated.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4434422-13d0-40eb-af55-a4b58ccef720', embedding=None, metadata={'page_label': '419', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.7RFunctions 419\\nrequire(rms)\\nunits(y) ←\"Month\"\\n# Default is \"Day\" - used for axis labels, etc.\\nnpsurv(Surv(y, event) ∼svar1 + svar2 + ... , data,subset,\\ntype=c(\"kaplan-meier\", \"fleming-harrington\", \"fh2\"),\\nerror=c(\"greenwood\", \"tsiatis\"), se.fit= TRUE,\\nconf.int=.95,\\nconf.type =c(\"log\",\" log-log\",\" plain\",\"none\"), ...)\\nIf thereareno stratiﬁcationvariables( svar1,...),omitthem.T oprintatable\\nof estimates, use\\nf←npsurv(...)\\nprint(f) # print brief summary of f\\nsummary(f, times, censored= FALSE) # in survival\\nFor failure times stored in days, use\\nf←npsurv(Surv(futime, event) ∼sex)\\nsummary(f, seq(30, 180, by =30))\\nto print monthly estimates.\\nThere is a plotmethod To plot the object returned by survfitandnpsurv.\\nThis invokes plot.survfit .\\nObjects created by npsurvcan be passed to the more comprehensive plot-\\nting function survplot (here, actually survplot.npsurv ) for other options that\\ninclude automatic curve labeling and showing the number of subjects at risk\\nat selected times. See Figure 17.6for an example. Stratiﬁed estimates, with\\nfour treatments distinguished by line type and curve labels, could be drawn\\nby\\nunits(y) ←\"Year\"\\nf←npsurv(Surv(y, stat) ∼treatment)\\nsurvplot(f, ylab=\"Fraction Pain-Free\")\\nThegroupkminrmscomputes and optionally plots SKM(u)o rl o gΛKM(u)( i f\\nloglog=TRUE )f o rﬁ x e d uwith automatic stratiﬁcation on a continuous predic-\\ntorx.A si n cut2(Section 6.2) you can specify the number of subjects per\\ninterval (default is m=50), the number ofquantile groups( g), or the actualcut-\\npoints ( cuts).groupkmplots the survival or log–log survival estimate against\\nmeanxin each xinterval.\\nThebootkmfunction in the Hmiscpackage bootstraps Kaplan–Meier sur-\\nvival estimates or Kaplan–Meier estimates of quantiles of the survival time\\ndistribution.Itiseasytouse bootkmtocompute,forexample,anonparametric\\nconﬁdence interval for the ratio of median survival times for two groups.\\nSee the Web site for a list of functions from other users for nonparametric\\nestimationof S(t)withleft–,right–,andinterval–censoreddata.Theadaptive\\nlinear spline log-hazard ﬁtting function heft361is freely available.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ea6706e8-3ab4-4e11-bf47-d33d46cdd153', embedding=None, metadata={'page_label': '420', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='420 17 Introduction to Survival Analysis\\n17.8 Further Reading\\n1Some excellent general references for survival analysis are [ 57,83,114,133,154,\\n197,282,308,331,350,382,392,444,484,574,604]. Govindarajulu et al.229have\\na nice review of frailty models in survival analysis, for handling clustered time-\\nto-event data.\\n2See Goldman,220Bull and Spiegelhalter,83, Lee et al.394, and Dubin et al.166\\nfor ways to construct descriptive graphs depicting right–censored data.\\n3Some useful references for left–truncation are [ 83,112,244,524]. Mandel435care-\\nfully described the diﬀerence between censoring and truncation.\\n4See [384, p. 164] for some ideas for detecting informative censoring. Bilker and\\nWang54discussright–truncation and contrast it with right–censoring.\\n5Arjas29has applications based on properties of the cumulative hazard function.\\n6Kooperberg et al.361,594have an adaptive method for ﬁtting hazard functions\\nusinglinearsplinesintheloghazard. Binquetetal.56studied a related approach\\nusing quadratic splines. Mudholkar et al.466presented a generalized Weibull\\nmodel allowing for a variety of hazard shapes.\\n7Hollander et al.299provide a nonparametric simultaneous conﬁdence band for\\nS(t), surprisingly using likelihood ratio methods. Miller459showed that if the\\nparametric form of S(t)isknown tobeWeibullwith knownshapeparameter (an\\nunlikely scenario), the Kaplan–Meier estimator is very ineﬃcient (i.e., has high\\nvariance) when compared with the parametric maximum likelihood estimator.\\nSee [666] for a discussion of how the eﬃciency of Kaplan–Meier estimators can\\nbe improved by interpolation as opposed to piecewise ﬂat step functions. That\\npaper also discusses a variety of other estimators, some of which are signiﬁcantly\\nmore eﬃcient than Kaplan–Meier.\\n8See [112,244,438,570,614,619] for methods of estimating SorΛin the presence\\nof left–truncation. See Turnbull616for nonparametric estimation of S(t)w i t h\\nleft–, right–, and interval–censoring, and Kooperberg and Clarkson360for a\\nﬂexible parametric approach to modeling that allows for interval–censoring.\\nLindsey and Ryan413have a nice tutorial on the analysis of interval–censored\\ndata.\\n9Hogan and Laird297,298developed methods for dealing with mixtures of fa-\\ntal and nonfatal outcomes, including some ideas for handling outcome-related\\ndropoutsonthe repeated measurements. See alsoFinkelsteinandSchoenfeld.193\\nThe 30 April 1997 issue of Statistics in Medicine (Vol. 16) is devoted to methods\\nfor analyzing multiple endpoints as well as designing multiple endpoint stud-\\nies. The papers in that issue are invaluable, as is Therneau and Hamilton606\\nand Therneau and Grambsch.604Huang and Wang311presented a joint model\\nfor recurrent events and a terminating event, addressing such issues as the fre-\\nquency of recurrent events by the time of the terminating event.\\n10See Lunn and McNeil429and Marubini and Valsecchi [ 444, Chapter 10] for\\npractical approaches to analyzing competing risks using ordinary Cox propor-\\ntional hazards models. A nice overview of competing risks with comparisons of\\nvarious approaches is found in Tai et al.599,G e s k u s214, and Koller et al.358.\\nBryant and Dignam78developed a semiparametric procedure in which com-\\npeting risks are adjusted for nonparametrically while a parametric cumulative\\nincidence function is used for the event of interest, to gain precision. Fine and\\nGray192developed methods for analyzing competing risks by estimating sub-\\ndistribution functions. Nishikawa et al.478developed some novel approaches to\\ncompeting risk analysis involving time to adverse drug events competing with\\ntime to withdrawal from therapy. They also dealt with diﬀerent severities of\\nevents in an interesting way. Putter et al.517has a nice tutorial on competing\\nrisks, multi-state models, and associated Rsoftware. Fiocco et al.194developed', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e161f7ac-e1e8-45d3-9341-9944bfaee053', embedding=None, metadata={'page_label': '421', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='17.9 Problems 421\\nan approach to avoid the problems caused by having to estimate a large num-\\nber of regression coeﬃcients in multi-state models. Ambrogi et al.22provide\\nclinically useful estimates from competing risks analyses.\\n11Jiang, Chappell, and Fine322present methods for estimating the distribution\\nof event times of nonfatal events in the presence of terminating events such as\\ndeath.\\n12Shen and Thall568have developed a ﬂexible parametric approach to multi-state\\nsurvival analysis.\\n13Lancar et al.372developed a method for analyzing repeated events of varying\\nseverities.\\n14Lawless and Nadeau384have a very good description of models dealing with\\nrecurrent events. They use the notion of the cumulative mean function ,w h i c h\\nis the expected number of events experienced by a subject by a certain time.\\nLawless383contrasts this approach with other approaches. See Aalen et al.3\\nfor a nice example in which multivariate failure times (time to failure of ﬁll-\\nings in multiple teeth per subject) are analyzed. Francis and Fuller204devel-\\noped a graphical device for depicting complex event history data. Therneau and\\nHamilton606have very informative comparisons of various methods for model-\\ning multiple events, showing the importance of whether the analyst starts the\\nclock over after each event. Kelly and Lim343have another very useful paper\\ncomparing various methods for analyzing recurrent events. Wang and Chang650\\ndemonstrated the diﬃculty of using Kaplan–Meier estimates for recurrence time\\ndata.\\n17.9 Problems\\n1. Make a rough drawing of a hazard function from birth for a man who de-\\nvelopssigniﬁcantcoronaryarterydiseaseatage50andundergoescoronary\\nartery bypass surgery at age 55.\\n2. Deﬁne in words the relationship between the hazard function and the sur-\\nvival function.\\n3. In a study of the life expectancy of light bulbs as a function of the bulb’s\\nwattage, 100 bulbs of various wattage ratings were tested until each had\\nfailed. What is wrong with using the product-moment linear correlation\\ntest to test whether wattage is associated with life length concerning (a)\\ndistributional assumptions an d (b) other assumptions?\\n4. A placebo-controlled study is undertaken to ascertain whether a new drug\\ndecreases mortality. During the study, some subjects are withdrawn be-\\ncause of moderate to severe side eﬀects. Assessment of side eﬀects and\\nwithdrawal of patients is done on a blinded basis. What statistical tech-\\nnique can be used to obtain an unbiased treatment comparison of survival\\ntimes?Stateatleastoneeﬃcacyendpointthatcanbeanalyzedunbiasedly.\\n5. Consider long-term follow-up of patients in the supportdataset. What pro-\\nportion of the patients have censored survival times? Does this imply that\\none cannot make accurate estimates of chances of survival? Make a his-\\ntogram or empirical distribution function estimate of the censored follow-\\nup times. What is the typical follow-up duration for a patient in the study', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fab951a4-b4fd-4408-8aed-ba21df090084', embedding=None, metadata={'page_label': '422', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='422 17 Introduction to Survival Analysis\\nwho has survivedso far?What is the typicalsurvivaltime for patients who\\nhavedied? Takingcensoringintoaccount,whatisthe mediansurvivaltime\\nfrom the Kaplan–Meierestimate of the overallsurvivalfunction? Estimate\\nthe median graphically or using any other sensible method.\\n6. Plot Kaplan–Meier survival function estimates stratiﬁed by dzclass.E s t i -\\nmate the median survival time and the ﬁrst quartile of time until death\\nfor each of the four disease classes.\\n7. Repeat Problem 6except for tertiles of meanbp.\\n8. The commonly used log-rank test for comparing survival times between\\ngroups of patients is a special case of the test of association between the\\ngrouping variable and survival time in a Cox proportional hazards regres-\\nsion model. Depending on how one handles tied failure times, the log-rank\\nχ2statistic exactly equals the score χ2statistic from the Cox model, and\\nthe likelihood ratio and Wald χ2test statistics are also appropriate. To\\nobtainglobalscoreor LR χ2tests and P-valuesyoucan use a statementas\\nthe following, where cphis in the rmspackage. It is similar to the survival\\npackage’s coxphfunction.\\ncph(Survobject ∼predictor)\\nHereSurvobject is a survival time object created by the Survfunction. Ob-\\ntain the log-rank (score) χ2statistic, degrees of freedom, and P-value for\\ntesting for diﬀerences in survival time between levels of dzclass. Interpret\\nthis test, referring to the graph you produced in Problem 6if needed.\\n9. DopreliminaryanalysesofsurvivaltimeusingtheMayoClinicprimarybil-\\niary cirrhosis dataset described in Section 8.9. Make graphs of Altschuler–\\nNelson or Kaplan–Meier survival estimates stratiﬁed separately by a few\\ncategoricalpredictors and by categorizedversionsof one or two continuous\\npredictors. Estimate median failure time for the various strata. You may\\nwant to suppress conﬁdence bands when showing multiple strata on one\\ngraph. See [361]for parametric ﬁts to the survival and hazard function for\\nthis dataset.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26efce7a-d257-4676-a2d5-aac495cbf0d9', embedding=None, metadata={'page_label': '423', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 18\\nParametric Survival Models\\n18.1 Homogeneous Models (No Predictors)\\nThe nonparametric estimator of S(t) is a very good descriptive statistic for\\ndisplaying survival data. For many purposes, however,one may want to make\\nmore assumptions to allow the data to be modeled in more detail. By speci-\\nfying a functional form for S(t) and estimating any unknown parameters in\\nthis function, one can\\n1. easily compute selected quantiles of the survival distribution;\\n2. estimate (usually by extrapolation) the expected failure time;\\n3. derive a concise equation and smooth function for estimating S(t),Λ(t),\\nandλ(t); and\\n4. estimate S(t) more precisely than SKM(t)o rSΛ(t) if the parametric form\\nis correctly speciﬁed.\\n18.1.1 Speciﬁc Models\\nParametricmodelingrequireschoosingoneormoredistributions.TheWeibull\\nand exponential distributions were discussed in Chapter 18. Other commonly\\nused survival distributions are obtained by transforming Ta n du s i n gas t a n -\\ndard distribution. The log transformation is most commonly employed. The\\nlog-normal distribution speciﬁes that log( T) has a normal distribution with\\nmeanμand variance σ2. Stated another way, log( T)∼μ+σǫ,w h e r e ǫ\\nhas a standard normal distribution. Then S(t)=1−Φ((log(t)−μ)/σ),\\nwhereΦis the standard normal cumulative distribution function. The log-\\nlogisticdistribution is given by S(t) = [1 + exp( −(log(t)−μ)/σ)]−1.H e r e\\nlog(T)∼μ+σǫwhereǫfollowsalogisticdistribution[1+exp( −u)]−1.Thelog\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 18423', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2465de0-7146-4f94-a17e-039993d55b2a', embedding=None, metadata={'page_label': '424', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='424 18 Parametric Survival Models\\nextreme value distribution is given by S(t)=e x p [−exp((log( t)−μ)/σ)], and\\nlog(T)∼μ+σǫ,w h e r eǫ∼1−exp[−exp(u)].\\nThe generalized gamma and generalized Fdistributions provide a richer\\nvariety of distribution and hazard functions127,128. Spline hazard\\nmodels286,287,361are other excellent alternatives.\\n18.1.2 Estimation\\nMaximum likelihood (ML) estimation is used to estimate the unknown\\nparameters of S(t). The general method presented in Chapter 9must be\\naugmented, however, to allow for censored failure times. The basic idea is as\\nfollows. Again let Tbe a random variable representing time until the event,\\nTibe the (possibly censored) failure time for the ith observation, and Yi\\ndenote the observed failure or censoring time min( Ti,Ci), where Ciis the\\ncensoring time. If Yiis uncensored, observation icontributes a factor to the\\nlikelihood equal to the density function for Tevaluated at Yi,f(Yi). IfYi\\ninstead represents a censored time so that Ti=Y+\\ni, it is only known that\\nTiexceedsYi. The contribution to the likelihood function is the probability\\nthatTi>Ci(equal to Prob {Ti>Yi}). This probability is S(Yi). The joint\\nlikelihood over all observations i=1,2,...,nis\\nL=n∏\\ni:Yiuncensoredf(Yi)n∏\\ni:YicensoredS(Yi). (18.1)\\nThere is one more component to L: the distribution of censoring times if\\nthese are not ﬁxed in advance. Recall that we assume that censoring is non-\\ninformative, that is, it is independent of the risk of the event. This inde-\\npendence implies that the likelihood component of the censoring distribution\\nsimply multiplies Land that the censoring distribution contains little infor-\\nmationaboutthesurvivaldistribution.Inaddition,the censoringdistribution\\nmay be very diﬃcult to specify. For these reasons we can maximize Lsepa-\\nrately to estimate parameters of S(t) and ignore the censoring distribution.\\nRecalling that f(t)=λ(t)S(t)a n dΛ(t)=−logS(t), the log likelihood\\nc a nb ew r i t t e na s\\nlogL=n∑\\ni:Yiuncensoredlogλ(Yi)−n∑\\ni=1Λ(Yi). (18.2)\\nAll observations then contribute an amount to the log likelihood equal to the\\nnegative of the cumulative hazard evaluated at the failure/censoring time.\\nIn addition, uncensored observations contribute an amount equal to the log\\nof the hazard function evaluated at the time of failure. Once Lor logL\\nis speciﬁed, the general ML methods outlined earlier can be used without', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='26f1a2cd-2d21-4a85-bbbd-21ee5eb9bdce', embedding=None, metadata={'page_label': '425', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.1 Homogeneous Models (No Predictors) 425\\nchange in most situations. The principal diﬀerence is that censored observa-\\ntions contribute less information to the statistical inference than uncensored\\nobservations. For distributions such as the log-normal that are written only\\nin terms of S(t), it may be easier to write the likelihood in terms of S(t)\\nandf(t).\\nAs an example, we turn to the exponential distribution, for which log\\nLhas a simple form that can be maximized explicitly. Recall that for this\\ndistribution λ(t)=λandΛ(t)=λt. Therefore,\\nlogL=n∑\\ni:Yiuncensoredlogλ−n∑\\ni=1λYi. (18.3)\\nLettingnudenote the number of uncensored event times,\\nlogL=nulogλ−n∑\\ni=1λYi. (18.4)\\nLettingwdenote the sum of all failure/censoring times (“person years of\\nexposure”):\\nw=n∑\\ni=1Yi, (18.5)\\nthe derivatives of log Lare given by\\n∂logL\\n∂λ=nu/λ−w\\n∂2logL\\n∂λ2=−nu/λ2. (18.6)\\nEquating the derivative of log Lto zero implies that the MLE of λis\\nˆλ=nu/w (18.7)\\nor the number of failures per person-yearsof exposure. By inserting the MLE\\nofλinto the formula for the second derivative we obtain the observed esti-\\nmated information, w2/nu. The estimated variance of ˆλis thusnu/w2and\\nthe standarderroris n1/2\\nu/w. The precision of the estimate depends primarily\\nonnu.\\nRecall that the expected life length μis 1/λfor the exponential distribu-\\ntion. The MLE of μisw/nuand its estimated variance is w2/n3\\nu.T h eM L E\\nofS(t),ˆS(t), is exp( −ˆλt), and the estimated variance of log( ˆΛ(t)) is simply\\n1/nu.\\nAs an example, consider the sample listed previously,\\n1336+8+91 0+.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7ee079cc-dc1a-4471-91d4-ba27b237a708', embedding=None, metadata={'page_label': '426', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='426 18 Parametric Survival Models\\nHerenu=4a n d w= 40, so the MLE of λis 0.1 failure per person-period.\\nThe estimated standard error is 2 /4 0=0.05. Estimated expected life length\\nis 10 units with a standard error of 5 units. Estimated median failure time is\\nlog(2)/0.1=6.931. The estimated survival function is exp( −0.1t), which at\\nt=1,3,9,10 yields 0 .90,0.74,0.41, and 0 .37, which can be compared to the\\nproduct limit estimates listed earlier (0 .85,0.57,0.29,0.29).\\nNow consider the Weibull distribution. The log likelihood function is\\nlogL=n∑\\ni:Yiuncensoredlog[αγYγ−1\\ni]−n∑\\ni=1αYγ\\ni. (18.8)\\nAlthough log Lcan be simpliﬁed somewhat, it cannot be solved explicitly for\\nαandγ. An iterative method such as the Newton–Raphson method is used\\nto compute the MLEs of αandγ. Once these estimates are obtained, the\\nestimated variance–covariance matrix and other derived quantities such as\\nˆS(t) can be obtained in the usual manner.\\nFor the dataset used in the exponential ﬁt, the Weibull ﬁt follows.\\nˆα=0.0728\\nˆγ=1.164\\nˆS(t)=e x p (−0.0728t1.164) (18.9)\\nˆS−1(0.5) = [(log2) /ˆα]1/ˆγ=6.935 (estimated median) .\\nThis ﬁt is very close to the exponential ﬁt since ˆ γis near 1.0. Note that the\\ntwo medians are almost equal. The predicted survival probabilities for the\\nWeibull model for t=1,3,9,10 are, respectively, 0 .93,0.77,0.39,0.35.\\nSometimes a formal test can be made to assess the ﬁt of the proposed\\nparametric survival distribution. For the data just analyzed, a formal test of\\nexponentiality versus a Weibull alternative is obtained by testing H0:γ=1\\nin the Weibull model. A score test yielded χ2=0.14 with 1 d.f., p=0.7,\\nshowing little evidence for non-exponentiality (note that the sample size is\\ntoo small for this test to have any power).\\n18.1.3 Assessment of Model Fit\\nThe ﬁt of the hypothesized survival distribution can often be checked eas-\\nily using graphical methods. Nonparametric estimates of S(t)a n dΛ(t)\\nare primary tools for this purpose. For example, the Weibull distribution\\nS(t)=e x p (−αtγ) can be rewritten by taking logarithms twice:\\nlog[−logS(t)] = logΛ(t) = logα+γ(logt). (18.10)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1cd7dcac-80a1-4c12-adf0-10fadebe3103', embedding=None, metadata={'page_label': '427', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.2 Parametric Proportional Hazards Models 427\\nThe ﬁt of a Weibull model can be assessed by plotting log ˆΛ(t)v e r s u sl o g t\\nand checking whether the curve is approximately linear. Also, the plotted\\ncurve provides approximate estimates of α(the antilog of the intercept) and\\nγ(the slope). Since an exponential distribution is a special case of a Weibull\\ndistribution when γ= 1, exponentially distributed data will tend to have a\\ngraph that is linear with a slope of 1.\\nFor any assumed distribution S(t), a graphical assessment of goodness of\\nﬁ tc a nbem a d eb yp l o t t i n g S−1[SΛ(t)] orS−1[SKM(t)] against tand checking\\nfor linearity. For log distributions, Sspeciﬁes the distribution of log( T), so\\nwe plot against log t. For a log-normal distribution we thus plot Φ−1[SΛ(t)]\\nagainst log t,w h e r eΦ−1is the inverse of the standard normal cumulative\\ndistributionfunction.Foralog-logisticdistributionweplotlogit[ SΛ(t)] versus\\nlogt. For an extreme value distribution we use log −log plots as with the\\nWeibull distribution. Parametric model ﬁts can also be checked by plotting\\nthe ﬁtted ˆS(t)a n dSΛ(t) against ton the same graph.\\n18.2 Parametric Proportional Hazards Models\\nIn this section we present one way to generalize the survival model to a\\nsurvival regression model. In other words, we allow the sample to be hetero-\\ngeneous by adding predictor variables X={X1,X2,...,X k}.A sw i t ho t h e r\\nregressionmodels, Xcan representa mixture of binary,polytomous,continu-\\nous, spline-expanded, and even ordinal predictors(if the categoriesare scored\\nto satisfy the linearity assumption). Before discussing ways in which the re-\\ngressionpartofasurvivalmodel mightbe speciﬁed, ﬁrstrecallhowregression\\neﬀects have been modeled in other settings. In multiple linear regression, the\\nregression eﬀect Xβ=β0+β1X1+β2X2+...+βkXkcan be thought of\\nas an increment in the expected value of the response Y. In binary logistic\\nregression, Xβspeciﬁes the log odds that Y=1 ,o re x p ( Xβ) multiplies the\\nodds that Y=1 .\\n18.2.1 Model\\nThe most widely used survival regression speciﬁcation is to allow the hazard\\nfunction λ(t) to be multiplied by exp( Xβ). The survival model is thus gener-\\nalized from a hazard function λ(t) for the failure time Tto a hazard function\\nλ(t)exp(Xβ) for the failure time given the predictors X:\\nλ(t|X)=λ(t)exp(Xβ). (18.11)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4408b86c-fa99-4dbf-8dff-08283abb3dd0', embedding=None, metadata={'page_label': '428', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='428 18 Parametric Survival Models\\nThis regression formulation is called the proportional hazards (PH) model.\\nTheλ(t)p a r to f λ(t|X) is sometimes called an underlying hazard function or\\nahazard function for a standard subject , which is a subject with Xβ=0 .A n y\\nparametric hazard function can be used for λ(t), and as we show later, λ(t)\\ncan be left completely unspeciﬁed without sacriﬁcing the ability to estimate\\nβ, by the use of Cox’s semi-parametric PH model.132Depending on whether\\nthe underlying hazard function λ(t) has a constant scale parameter, Xβmay\\normaynotinclude anintercept β0. The termexp( Xβ)canbe calleda relative\\nhazard function and in many cases it is the function of primary interest as it\\ndescribes the (relative) eﬀects of the predictors.\\nThe PH model can also be written in terms of the cumulative hazard and\\nsurvival functions:\\nΛ(t|X)=Λ(t)exp(Xβ)\\nS(t|X)=e x p [−Λ(t)exp(Xβ)] = exp[ −Λ(t)]exp(Xβ).(18.12)\\nΛ(t) is an “underlying”cumulative hazard function. S(t|X), the probability\\nof surviving past time tgiven the values of the predictors X, can also be\\nwritten as\\nS(t|X)=S(t)exp(Xβ), (18.13)\\nwhereS(t) is the “underlying”survival distribution, exp( −Λ(t)). The eﬀect\\nof the predictors is to multiply the hazard and cumulative hazard functions\\nby a factor exp( Xβ), or equivalently to raise the survival function to a power\\nequal to exp( Xβ).\\n18.2.2 Model Assumptions and Interpretation\\nof Parameters\\nIn the general regression notation of Section 2.2, the log hazard or log cumu-\\nlative hazardcan be used as the property of the response Tevaluated at time\\ntthat allows distributional and regression parts to be isolated and checked.\\nThe PH model can be linearized with respect to Xβusing the following\\nidentities.\\nlogλ(t|X) = logλ(t)+Xβ\\nlogΛ(t|X) = logΛ(t)+Xβ. (18.14)\\nNo matter which of the three model statements are used, there are certain\\nassumptions in a parametricPHsurvivalmodel. These assumptionsarelisted\\nbelow.\\n1. The true form of the underlyingfunctions ( λ,Λ,an dS) shouldbe speciﬁed\\ncorrectly.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d3f3d827-284d-4410-86cc-6c78589cb54a', embedding=None, metadata={'page_label': '429', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.2 Parametric Proportional Hazards Models 429\\n2. The relationship between the predic tors and log hazard or log cumulative\\nhazard should be linear in its simplest form. In the absence of interaction\\nterms, the predictors should also operate additively.\\n3. The way in which the predictors aﬀect the distribution of the response\\nshould be by multiplying the hazard or cumulative hazard by exp( Xβ)\\nor equivalently by adding Xβto the log hazard or log cumulative hazard\\nat eacht. The eﬀect of the predictors is assumed to be the same at all\\nvalues of tsince log λ(t) can be separated from Xβ. In other words, the\\nPH assumption implies no tby predictor interaction.\\nThe regression coeﬃcient for Xj,βj, is the increase in log hazard or log\\ncumulative hazard at any ﬁxed point in time if Xjis increased by one unit\\nand all other predictors are held constant. This can be written formally as\\nβj=l o gλ(t|X1,X2,...,X j+1,Xj+1,...,X k)−logλ(t|X1,...,X j,...,X k),\\n(18.15)\\nwhich is equivalent to the log of the ratio of the hazards at time t.T h e\\nregressioncoeﬃcientcanjustaseasilybewrittenintermsofaratioofhazards\\nat timet. The ratio of hazards at Xj+dversusXj, all other factors held\\nconstant, is exp( βjd). Thus the eﬀect of increasing Xjbydis to increase the\\nhazard of the event by a factor of exp( βjd) at all points in time, assuming Xj\\nis linearly related to log λ(t). In general,the ratio of hazards for an individual\\nwith predictor variable values X∗compared to an individual with predictors\\nXis\\nX∗:Xhazard ratio = [ λ(t)exp(X∗β)]/[λ(t)exp(Xβ)]\\n=e x p (X∗β)/exp(Xβ)=e x p [ ( X∗−X)β].(18.16)\\nIf there is only one predictor X1and that predictor is binary, the PH model\\ncan be written\\nλ(t|X1=0 )=λ(t)\\nλ(t|X1=1 )=λ(t)exp(β1). (18.17)\\nHere exp( β1)i st h e X1=1:X1= 0 hazard ratio. This simple case has\\nno regression assumption but assumes PH and a form for λ(t). If the single\\npredictor X1is continuous, the model becomes\\nλ(t|X1)=λ(t)exp(β1X). (18.18)\\nWithout further modiﬁcation (such as taking a transformation of the predic-\\ntor), the model assumes a straight line in the log hazard or that for all t,a n\\nincrease in Xby one unit increases the hazard by a factor of exp( β1).\\nAs in logistic regression, much more general regression speciﬁcations can\\nbe made, including interaction eﬀects. Unlike logistic regression, however, a\\nmodel containing, say age, sex, and age ×sex interaction is not equivalent to', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4322533c-fb2f-406e-b788-44b37c0e67bb', embedding=None, metadata={'page_label': '430', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='430 18 Parametric Survival Models\\nﬁtting two separate models. This is because even though males and females\\nare allowed to have unequal age slopes, both sexes are assumed to have the\\nTable 18.1 Mortality diﬀerences and ratios when hazard ratio is 0 .5\\nSubject 5-Year Diﬀerence Mortality\\nSurvival Ratio (T/C)\\nCT\\n1 0.98 0.99 0.01 0.01/0.02 = 0.5\\n2 0.80 0.89 0.09 0.11/0.2 = 0.55\\n3 0.25 0.50 0.25 0.5/0.75 = 0.67\\nunderlying hazard function proportional to λ(t) (i.e., the PH assumption\\nholds for sex in addition to age).\\n18.2.3 Hazard Ratio, Risk Ratio, and Risk\\nDiﬀerence\\nOther ways of modeling predictors can also be speciﬁed besides a multiplica-\\ntive eﬀect on the hazard. For example, one could postulate that the eﬀect of\\na predictor is to add to the hazard of failure instead of to multiply it by a\\nfactor. The eﬀect of a predictor could also be described in terms of a mor-\\ntality ratio (relative risk), risk diﬀerence, odds ratio, or increase in expected\\nfailure time. However, just as an odds ratio is a natural way to describe an\\neﬀect on a binary response, a hazard ratio is often a natural way to describe\\nan eﬀect on survival time. One reason is that a hazard ratio canbe constant.\\nTable18.1provides treated (T) to control (C) survival (mortality) dif-\\nferences and mortality ratios for three hypothetical types of subjects. We\\nsuppose that subjects 1, 2, and 3 have increasingly worse prognostic factors.\\nFor example,the ageatbaseline ofthe subjectsmightbe 30,50,and70years,\\nrespectively. We assume that the treatment aﬀects the hazard by a constant\\nmultiple of 0.5 (i.e., PH is in eﬀect and the constant hazard ratio is 0.5). Note\\nthatST=S0.5\\nC. Notice that the mortality diﬀerence and ratio depend on the\\nsurvival of the control subject. A control subject having “good” predictor\\nvalues will leave little room for an improved prognosis from the treatment.\\nThe hazardratiois a basisfordescribingthe mechanismofan eﬀect. In the\\nabove example, it is reasonablethat the treatment aﬀect each subject by low-\\nering her hazard of death by a factor of 2, even though less sick subjects have\\na low mortality diﬀerence. Hazard ratios also lead to good statistical tests\\nfor diﬀerences in survival patterns and to predictive models. Once the model\\nis developed, however, survival diﬀerences may better capture the impact of\\na risk factor. Absolute survival diﬀe rences rather than relative diﬀerences', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3b0d5033-cbfe-40a2-b0b6-4a58a62dd9a0', embedding=None, metadata={'page_label': '431', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.2 Parametric Proportional Hazards Models 431\\n(hazard ratios) also relate more closely to statistical power. For example,\\neven if the eﬀect of a treatment is to halve the hazard rate, a population\\nwhere the control survival is 0.99 will require a much larger sample than will\\na population where the control survival is 0.3.\\nFigure18.1depicts the relationship between survival S(t)o fac o n t r o l\\nsubject at any time t, relative reduction in hazard ( h), and diﬀerence in\\nsurvivalS(t)−S(t)h. This ﬁgure demonstrates that absolute clinical beneﬁt\\n0.0 0.2 0.4 0.6 0.8 1.00.00.10.20.30.40.50.60.7\\nSurvival for Control SubjectImprovement in Survival0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\nFig. 18.1 Absolute clinical beneﬁt as a function of survival in a control subject and\\nthe relative beneﬁt (hazard ratio). The hazard ratios are given for each curve.\\nis primarily a function of the baseline risk of a subject. Clinical beneﬁt will\\nalso be a function of factors that interact with treatment, that is, factors\\nthat modify the relative beneﬁt of treatment. Once a model is developed\\nfor estimating S(t|X), this model can be used to estimate absolute beneﬁt\\nas a function of baseline risk factors as well as factors that interact with a\\ntreatment. Let X1be a binary treatment indicator and let A={X2,...,X p}\\nbe the other factors (which for convenience we assume do not interact with\\nX1). Then the estimate of S(t|X1=0,A)−S(t|X1=1,A) can be plotted\\nagainstS(t|X1= 0) or against levels of variables in Ato display absolute\\nbeneﬁt versus overall risk or speciﬁc subject characteristics. 1\\n18.2.4 Speciﬁc Models\\nLetXβdenote the linear combination of predictors excluding an intercept\\nterm. Using the PH formulation, an exponential survival regression model218\\ncan be stated as', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='731778aa-ecd5-4f9c-9529-430f7b9a7245', embedding=None, metadata={'page_label': '432', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='432 18 Parametric Survival Models\\nλ(t|X)=λexp(Xβ)\\nS(t|X)=e x p [−λtexp(Xβ)] = exp( −λt)exp(Xβ).(18.19)\\nThe parameter λcan be thought of as the antilog of an intercept term since\\nthe model could be written λ(t|X) = exp[(log λ)+Xβ]. The eﬀect of Xon\\nthe expected or median failure time is as follows.\\nE{T|X}=1/[λexp(Xβ)]\\nT0.5|X= (log2) /[λexp(Xβ)]. (18.20)\\nThe exponentialregressionmodelcanbe writtenin anotherformthatis more\\nnumerically stable by replacing the λparameter with an intercept term in\\nXβ, speciﬁcally λ=e x p (β0). After redeﬁning Xβto include β0,λcan be\\ndropped in all the above formulas.\\nThe Weibull regression model is deﬁned by one of the following functions\\n(assuming that Xβdoes not contain an intercept).\\nλ(t|X)=αγtγ−1exp(Xβ)\\nΛ(t|X)=αtγexp(Xβ)\\nS(t|X)=e x p [−αtγexp(Xβ)] (18.21)\\n=[ e x p (−αtγ)]exp(Xβ).\\nNote that the parameter αin the homogeneous Weibull model has been\\nreplaced with αexp(Xβ). The median survival time is given by\\nT0.5|X={log2/[αexp(Xβ)]}1/γ. (18.22)\\nAs with the exponential model, the parameter αcould be dropped (and\\nreplaced with exp( β0)) if an intercept β0is added to Xβ.\\nFor numerical reasons it is sometimes advantageous to write the Weibull\\nPH model as\\nS(t|X)=e x p (−Λ(t|X)), (18.23)\\nwhere\\nΛ(t|X)=e x p (γlogt+Xβ). (18.24)\\n18.2.5 Estimation\\nThe parameters in λandβare estimated by maximizing a log likelihood\\nfunction constructed in the same manner as described in Section 18.1.T h e\\nonly diﬀerence is the insertion of exp( Xiβ) in the likelihood function:', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bdfa1f20-b4ec-479f-9188-7ec8861e7e18', embedding=None, metadata={'page_label': '433', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.2 Parametric Proportional Hazards Models 433\\nlogL=n∑\\ni:Yiuncensoredlog[λ(Yi)exp(Xiβ)]−n∑\\ni=1Λ(Yi)exp(Xiβ).(18.25)\\nOnceˆβ,t h eM L Eo f β, is computed along with the large-sample standard\\nerror estimates, hazard ratio estimates and their conﬁdence intervals can\\nreadily be computed. Letting sdenote the estimated standard error of ˆβj,\\na1−αconﬁdence interval for the Xj+1:Xjhazard ratio is given by\\nexp[ˆβj±zs], where zis the 1−α/2 critical value for the standard normal\\ndistribution.\\nOnce the parameters of the underlying hazard function are estimated, the\\nMLE of λ(t),ˆλ(t), can be derived. The MLE of λ(t|X), the hazard as a\\nfunction of tandX,i sg i v e nb y\\nˆλ(t|X)=ˆλ(t)exp(Xˆβ). (18.26)\\nThe MLE of Λ(t),ˆΛ(t), can be derived from the integral of ˆλ(t) with respect\\ntot. Then the MLE of S(t|X) can be derived:\\nˆS(t|X)=e x p [−ˆΛ(t)exp(Xˆβ)]. (18.27)\\nFor the Weibull model, we denote the MLEs of the hazard parameters αand\\nγby ˆαand ˆγ.T h eM L Eo f λ(t|X),Λ(t|X), andS(t|X)f o rt h i sm o d e la r e\\nˆλ(t|X)=ˆαˆγtˆγ−1exp(Xˆβ)\\nˆΛ(t|X)=ˆαtˆγexp(Xˆβ) (18.28)\\nˆS(t|X)=e x p [−ˆΛ(t|X)].\\nConﬁdenceintervalsfor S(t|X)arebestderivedusinggeneralmatrixnotation\\ntoobtainanestimate softhestandarderroroflog[ ˆλ(t|X)]fromtheestimated\\ninformation matrix of all hazard and regression parameters. A conﬁdence\\ninterval for ˆSwill be of the form\\nˆS(t|X)exp(±zs). (18.29)\\nThe MLEs of βand of the hazard shape parameters lead directly to MLEs\\nof the expected and median life length. For the Weibull model the MLE of\\nthe median life length given Xis\\nˆT0.5|X={log2/[ˆαexp(Xˆβ)]}1/ˆγ. (18.30)\\nFor the exponential model, the MLE of the expected life length for a subject\\nhaving predictor values Xis given by\\nˆE(T|X)=[ˆλexp(Xˆβ)]−1, (18.31)\\nwhereˆλis the MLE of λ.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a1ed5a49-f4b5-49ff-8af3-ec5d9d5c3b30', embedding=None, metadata={'page_label': '434', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='434 18 Parametric Survival Models\\nβ1\\ntX1 =1\\nX1 =0\\nFig. 18.2 PH model with one binary predictor. Y-axis is log λ(t)o rl o g Λ(t). For\\nlogΛ(t), the curves must be non-decreasing. For log λ(t), they may be any shape.\\n18.2.6 Assessment of Model Fit\\nThree assumptions of the parametric PH model were listed in Section 18.2.2.\\nWe now lay out in more detail what relationships need to be satisﬁed. We\\nﬁrst assume a PH model with a single binary predictor X1. For a general\\nunderlying hazard function λ(t), all assumptions of the model are displayed\\nin Figure 18.2. In this case, the assumptions are PH and a shape for λ(t).\\nIfλ(t) is Weibull, the two curves will be linear if log tis plotted instead\\nofton thex-axis. Note also that if there is no association between Xand\\nsurvival ( β1= 0), estimates of the two curves will be close and will intertwine\\ndue to random variability. In this case, PH is not an issue.\\nIf the single predictor is continuous, the relationships in Figures 18.3\\nand18.4must hold. Here linearity is assumed (unless otherwise speciﬁed)\\nbesides PH and the form of λ(t). In Figure 18.3, the curves must be parallel\\nfor any choices of times t1andt2as well as each individual curve being lin-\\near. Also, the diﬀerence between ordinates needs to conform to the assumed\\ndistribution. This diﬀerence is log[ λ(t2)/λ(t1)] or log[ Λ(t2)/Λ(t1)].\\nFigure18.4highlights the PH assumption. The relationship between the\\ntwo curves must hold for any two values canddofX1. The shape of the\\nfunction for a given value of X1must conform to the assumed λ(t). For a\\nWeibull model, the functions should each be linear in log t.\\nWhentherearemultiplepredictors,thePHassumptioncanbedisplayedin\\na way similar to Figures 18.2and18.4but with the population additionally\\ncross-classiﬁed by levels of the other predictors besides X1. If there is one\\nbinary predictor X1and one continuous predictor X2, the relationship in', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7fcd9beb-5d42-4c05-803f-0ade93e538a2', embedding=None, metadata={'page_label': '435', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.2 Parametric Proportional Hazards Models 435\\nt=t1t=t2\\nX1\\nFig. 18.3 PH model with one continuous predictor. Y-axis is log λ(t)o rl o gΛ(t); for\\nlogΛ(t), drawn for t2>t1. The slope of each line is β1.\\n(d-c) β1\\nX1= cX1= d\\nt\\nFig. 18.4 PH model with one continuous predictor. Y-axis is log λ(t)o rl o gΛ(t). For\\nlogλ, the functions need not be monotonic.\\nFigure18.5must hold at each time tif linearity is assumed for X2and there\\nis no interaction between X1andX2. Methods for verifying the regression\\nassumptions (e.g., splines and residua ls) and the PH assumption are covered\\nin detail under the Cox PH model in Chapter 20.\\nThemethodforverifyingtheassumedshapeof S(t)inSection 18.1.3isalso\\nuseful when there are a limited number of categorical predictors. To validate\\na Weibull PH model one can stratify on Xand plot log ΛKM(t|Xstratum)\\nagainst log t. This graph simultaneously assesses PH in addition to shape\\nassumptions—all curves should be parallel as well as straight. Straight but\\nnonparallel (non-PH) curves indicate that a series of Weibull models with\\ndiﬀering γparameters will ﬁt.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='43e08ddd-b766-4a7e-98e0-9ff13fdc4d5c', embedding=None, metadata={'page_label': '436', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='436 18 Parametric Survival Models\\nslope = β2β1X1 =1\\nX2X1 =0\\nFig. 18.5 Regression assumptions, linear additive PH or AFT model with two pre-\\ndictors. For PH, Y-axis is log λ(t)o rl o gΛ(t)f o raﬁ x e d t.F o rA F T , Y-axis is log( T).\\n18.3 Accelerated Failure Time Models\\n18.3.1 Model\\nBesides modeling the eﬀect of predictors by a multiplicative eﬀect on the\\nhazard function, other regression eﬀects can be speciﬁed. The accelerated\\nfailure time (AFT) model is commonly used; it speciﬁes that the predictors\\nact multiplicatively on the failure time or additively on the log failure time.\\nThe eﬀect of a predictor is to alter the rate at which a subject proceeds along\\nthe time axis (i.e., to accelerate the time to failure [ 331, pp. 33–35]). The\\nmodel is 2\\nS(t|X)=ψ((log(t)−Xβ)/σ), (18.32)\\nwhereψisanystandardizedsurvivaldistributionfunction.Theparameter σis\\ncalledthe scale parameter .Themodelcanalsobestatedas(log( T)−Xβ)/σ∼\\nψor log(T)=Xβ+σǫ,w h e r eǫis a random variable from the distribution\\nψ. Sometimes the untransformed Tis used in place of log( T). When the log\\nform is used, the models are said to be log-normal, log-logistic, and so on.\\nThe exponential and Weibull are the only two distributions that can de-\\nscribe either a PH or an AFT model. 3\\n18.3.2 Model Assumptions and Interpretation\\nof Parameters\\nThe logλor logΛtransformation of the PH model has the following equiva-\\nlent for AFT models.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6c7bfb45-7f51-4b5a-be49-0e50382f2f34', embedding=None, metadata={'page_label': '437', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.3 Accelerated Failure Time Models 437\\nψ−1[S(t|X)] = (log( t)−Xβ)/σ. (18.33)\\nLetting as before ǫdenote a random variable from the distribution S,t h e\\nmodel is also\\nlog(T)=Xβ+σǫ. (18.34)\\nSo thepropertyoftheresponse Tofinterestforregressionmodelingislog( T).\\nIn the absence of censoring, we could check the model by plotting an X\\nagainst log Tand checking that the residuals log( T)−Xˆβare distributed as\\nψto within a scale factor.\\nThe assumptions of the AFT model are thus the following.\\n1. The true form of ψ(the distributional family) is correctly speciﬁed.\\n2. In the absence of nonlinear and interaction terms, each Xjaﬀects log( T)\\norψ−1[S(t|X)] linearly.\\n3. Implicit in these assumptions is that σis a constant independent of X.\\nA one-unit change in Xjis then most simply understood as a βjchange in\\nthe log of the failure time. The one-unit change in Xjincreases the failure\\ntime by a factor of exp( βj).\\nThe median survival time is obtained by solving ψ((log(t)−Xβ)/σ)=0.5\\ngiving\\nT0.5|X=e x p [Xβ+σψ−1(0.5)] (18.35)\\n18.3.3 Speciﬁc Models\\nCommon choices for the distribution function ψin Equation 18.32are the\\nextreme value distribution ψ(u)=e x p ( −exp(u)), the logistic distribution\\nψ(u)=[ 1+e x p ( u)]−1, and the normal distribution ψ(u)=1−Φ(u). The\\nAFT model equivalent of the Weibull model is obtained by using the extreme\\nvalue distribution, negating β, and replacing γwith 1/σin Equation 18.24:\\nS(t|X)=e x p [−exp((log( t)−Xβ)/σ)]\\nT0.5|X= [log(2)]σexp(Xβ). (18.36)\\nThe exponential model is obtained by restricting σ= 1 in the extreme value\\ndistribution.\\nThe log-normal regression model is\\nS(t|X)=1−Φ((log(t)−Xβ)/σ), (18.37)\\nand the log-logistic model is\\nS(t|X)=[1+exp((log( t)−Xβ)/σ)]−1. (18.38)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d731287b-5cd2-4b6b-8185-96ed9fd6130e', embedding=None, metadata={'page_label': '438', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='438 18 Parametric Survival Models\\nThetdistribution allows for more ﬂexibility by varying the degrees of free-\\ndom. Figure 18.6depicts possible hazard functions for the log tdistribution\\nfor varying σand degrees of freedom. However, this distribution does not\\nhave a late increasing hazard phase typical of human survival.\\nrequire(rms)\\nhaz←survreg.auxinfo$t$ hazard\\ntimes ←c(seq(0, .25, length =100), seq(.26, 2, length=150))\\nhigh←c(6, 1.5, 1.5, 1.75)\\nlow←c(0, 0, 0, .25)\\ndfs←c(1, 2, 3, 5, 7, 15, 500)\\ncols←rep(1, 7)\\nltys←1:7\\ni←0\\nfor(scale in c(.25, .6, 1, 2)) {\\ni←i+1\\nplot(0, 0, xlim=c(0,2), ylim=c(low [i],high[i]),\\nxlab=expression (t),ylab=expression(lambda(t)), type=\"n\")\\ncol←1.09\\nj←0\\nfor(df in dfs) {\\nj←j+1\\n## Divide by t to get hazard for log t distribution\\nlines(times,\\nhaz(log(times), 0, c(log(scale), df))/ times,\\ncol=cols [j], lty= ltys[j])\\nif(i==1) text(1.7, .23 + haz(log(1.7), 0,\\nc(log(scale),df))/1.7, format(df))\\n}\\ntitle(paste(\"Scale:\", format(scale)))\\n}# Figure 18.6\\nAll three of these parametric survi val models have median survival time\\nT0.5|X=e x p (Xβ).\\n18.3.4 Estimation\\nMaximum likelihood estimation is used much the same as in Section 18.2.5.\\nCare must be taken in the choice of initial values; iterative methods are\\nespeciallypronetoproblemsinchoosingtheinitial ˆ σ.Estimationworksbetter\\nifσisparameterizedasexp( δ). Onceβandσ(exp(δ)) areestimated,MLEsof\\nsecondary parameters such as survival probabilities and medians can readily\\nbe obtained:\\nˆS(t|X)=ψ((log(t)−Xˆβ)/ˆσ)\\nˆT0.5|X=e x p [Xˆβ+ˆσψ−1(0.5)]. (18.39)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ccba901f-6080-4682-8469-7dbf3e9396f2', embedding=None, metadata={'page_label': '439', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.3 Accelerated Failure Time Models 439\\n0.0 0.5 1.0 1.5 2.00123456\\nt1235715500Scale: 0.25\\n0.0 0.5 1.0 1.5 2.00.00.51.01.5\\ntScale: 0.6\\n0.0 0.5 1.0 1.5 2.00.00.51.01.5\\ntλ(t)\\nλ(t)λ (t)λ(t)\\nScale: 1\\n0.0 0.5 1.0 1.5 2.00.51.01.5\\ntScale: 2\\nFig. 18.6 log(T) distribution for σ=0.25,0.6,1,2 and for degrees of freedom\\n1,2,3,5,7,15,500 (almost log-normal). The top left plot has degrees of freedom writ-\\nten in the plot.\\nFor normal and logistic distributions, ˆT0.5|X=e x p (Xˆβ). The MLE of the\\neﬀect on log( T) of increasing Xjbydunits isˆβjdifXjis linear and additive.\\nThe delta (statistical diﬀerential) method can be used to compute an esti-\\nmate of the variance of f= [log(t)−Xˆβ]/ˆσ.L e t(ˆβ,ˆδ) denote the estimated\\nparameters, and let ˆVdenote the estimated covariance matrix for these pa-\\nrameter estimates. Let Fdenote the vector of derivatives of fwith respect to\\n(β0,β1,...,β p,δ); that is, F=[−1,−X1,−X2,...,−Xp,−(log(t)−Xˆβ)]/ˆσ.\\nThe variance of fis then approximately\\nVar(f)=FˆVF′. (18.40)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='416f6bb9-2466-4091-adf5-5e81e7931404', embedding=None, metadata={'page_label': '440', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='440 18 Parametric Survival Models\\nLettingsbe the square root of the variance estimate and z1−α/2be the\\nnormal critical value, a 1 −αconﬁdence limit for S(t|X)i s\\nψ((log(t)−Xˆβ)/ˆσ±z1−α/2×s). (18.41)\\n18.3.5 Residuals\\nFor an AFT model, standardized residuals are simply\\nr=( l o g (T)−Xˆβ)/σ. (18.42)\\nWhenTis right-censored, ris right-censored. Censoring must be taken into 4\\naccount, for example, by displaying Kaplan–Meier estimates based on groups\\nof residuals rather than showing individual residuals. The residuals can be\\nused to check for lack of ﬁt as described in the next section. Note that exam-\\nining individual uncensored residuals is not appropriate, as their distribution\\nis conditional on Ti<Ci,w h e r eCiis the censoring time.\\nCox and Snell134proposed a type of general residuals that also work for\\ncensored data. Using their method on the cumulative probabilityscale results\\nin the probability integral transformation. If the probability of failure before\\ntimetgivenXisS(t|X),F(T|X)=1−S(T|X) has a uniform [0 ,1] distri-\\nbution, where Tis a subject’s actual failure time. When Tis right-censored,\\nso is 1−S(T|X). Substituting ˆSforSresults in an approximate uniform\\n[0,1] distribution for any value of X. One minus the Kaplan–Meier estimate\\nof 1−ˆS(T|X) (using combined data for all X) is compared against a 45◦\\nline to check for goodness of ﬁt. A more stringent assessment is obtained by\\nrepeating this process while stratifying on X.\\n18.3.6 Assessment of Model Fit\\nFor a single binary predictor, all assumptions of the AFT model are depicted\\nin Figure 18.7. That ﬁgure also shows the assumptions for any two values of\\na single continuous predictor that behaves linearly. For a single continuous\\npredictor, the relationships in Figure 18.8must hold for any two follow-up\\ntimes. The regression assumptions are isolated in Figure 18.5.\\nTo verify the ﬁt of a log-logistic model with age as the only predictor, one\\ncould stratify by quartiles of ageand check for linearity and parallelismof the\\nfour logit SΛ(t)o rSKM(t) curves over increasing tas in Figure 18.7,w h i c h\\nstressesthedistributionalassumption(no TbyXinteractionandlinearityvs.\\nlog(t)). To stress the linear regression assumption while checking for absence\\nof time interactions (part of the distributional assumptions), one could make', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='25be49fd-260e-4533-bf11-8b034127599f', embedding=None, metadata={'page_label': '441', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.3 Accelerated Failure Time Models 441\\nlog t(d-c) β1 /σX1= dX1= c\\nFig. 18.7 AFT model with one predictor. Y-axis isψ−1[S(t|X)] = (log( t)−Xβ)/σ.\\nDrawn for d>c. The slope of the lines is σ−1.\\nt=t1t=t2\\nX1\\nFig. 18.8 AFT model with one continuous predictor. Y-axis is ψ−1[S(t|X)] =\\n(log(t)−Xβ)/σ.D r a w nf o r t2>t1. The slope of each line is β1/σand the diﬀerence\\nbetween the lines is log( t2/t1)/σ.\\na plot like Figure 18.8. For each decile of age, the logit transformation of the\\n1-, 3-, and 5-year survival estimates for that decile would be plotted against\\nthe mean age in the decile. This checks for linearity and constancy of the\\nage eﬀect over time. Regression splines will be a more eﬀective method for\\nchecking linearity and determining transformations. This is demonstrated in\\nChapter 20with the Cox model, but identical methods apply here.\\nAs an example, consider data from Kalbﬂeisch and Prentice [ 331, pp. 1–2],\\nwho present data from Pike508on the time from exposure to the carcinogen\\nDMBA to mortality from vaginal cancer in rats. The rats are divided into\\ntwo groups on the basis of a pre-treatment regime. Survival times in days\\n(with censored times marked+) are found in Table 18.2.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5d30067f-36c0-4b24-8ab9-134c10a867d0', embedding=None, metadata={'page_label': '442', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='442 18 Parametric Survival Models\\nTable 18.2 Rat vaginal cancer data from Pike508\\nGroup 1 143 164 188 188 190 192 206 209 213 216\\n220 227 230 234 246 265 304 216+244+\\nGroup 2 142 156 163 198 205 232 232 233 233 233\\n233 239 240 261 280 280 296 296 323 204+\\n344+\\ngetHdata(kprats)\\nkprats$group ←factor(kprats$ group, 0:1, c( \\'Group 1 \\',\\'Group 2 \\'))\\ndd←datadist(kprats); options(datadist=\"dd\")\\nS←with(kprats, Surv(t, death))\\nf←npsurv(S ∼group, type=\"fleming\", data=kprats)\\nsurvplot(f, n.risk= TRUE,conf= \\'none\\',# Figure 18.9\\nlabel.curves=list(keys= \\'lines \\'), levels.only=TRUE)\\ntitle(sub=\"Nonparametric estimates\", adj=0, cex=.7)\\n# Check fits of Weibull, log-logistic, log-normal\\nxl←c(4.8, 5.9)\\nsurvplot(f, loglog= TRUE,logt=TRUE,conf=\"none\", xlim=xl,\\nlabel.curves=list(keys= \\'lines \\'), levels.only=TRUE)\\ntitle(sub=\"Weibull (extreme value)\", adj=0, cex=.7)\\nsurvplot(f, fun=function(y)log(y/(1-y)), ylab=\"logit S(t)\",\\nlogt=TRUE, conf=\"none\", xlim=xl,\\nlabel.curves=list(keys= \\'lines \\'), levels.only=TRUE)\\ntitle(sub=\"Log-logistic\", adj=0, cex=.7)\\nsurvplot(f, fun= qnorm, ylab=\"Inverse Normal S(t)\",\\nlogt=TRUE, conf=\"none\",\\nxlim=xl,cex.label=.7,\\nlabel.curves=list(keys= \\'lines \\'), levels.only=TRUE)\\ntitle(sub=\"Log-normal\", adj=0, cex=.7)\\nThe top left plot in Figure 18.9displays nonparametric survival estimates for\\nthe two groups,with the number of rats“atrisk”ateach 30-daymarkwritten\\nabove the x-axis. The remaining three plots are for checking assumptions of\\nthree models. None of the parametric models presented will completely allow\\nfor such a long period with no deaths. Neither will any allow for the early\\ncrossingofsurvivalcurves.Log-normala ndlog-logisticmodelsyieldverysim-\\nilar results due to the similarity in shapes between Φ(z) and [1+exp( −z)]−1\\nfor non-extreme z. All three transformations show good parallelism after the\\nearly crossing. The log-logistic and log-normal transformations are slightly\\nmore linear. The ﬁtted models are:\\nfw←psm(S∼group, data=kprats, dist= \\'weibull \\')\\nfl←psm(S∼group, data=kprats, dist= \\'loglogistic \\',\\ny=TRUE)\\nfn←psm(S∼group, data=kprats, dist= \\'lognormal \\')\\nlatex(fw, fi= \\'\\')', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='be42aa85-7530-4331-b1eb-70572405ad11', embedding=None, metadata={'page_label': '443', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"18.3 Accelerated Failure Time Models 443\\nsSurvival Probability\\n0 35 105 175 245 3150.00.20.40.60.81.0\\n19 19 19 19 19 17 11 3 1 Group 1\\n21 21 21 21 21 18 15 7 6 2 Group 2Group 1\\nGroup 2\\nNonparametric estimateslog Survival Time in slog(−log Survival Probability)\\n4.8 5.0 5.2 5.4 5.6 5.8\\n−4−3−2−101\\nGroup 1\\nGroup 2\\nWeibull (extreme value)\\nlog Survival Time in slogit S(t)\\n4.8 5.0 5.2 5.4 5.6 5.8 6.0−4−3−2−101234\\nGroup 1\\nGroup 2\\nLog−logisticlog Survival Time in sInverse Normal S(t)\\n4.8 5.0 5.2 5.4 5.6 5.8−2.0−1.5−1.0−0.50.00.51.01.52.0\\nGroup 1\\nGroup 2\\nLog−normal\\nFig. 18.9 Altschuler–Nelson–Fleming–Harrington nonparametric survival estimates\\nfor rats treated with DMBA,508along with various transformations of the estimates\\nfor checking distributional assumptions of three parametric survival models.\\nProb{T≥t}=e x p [−exp(log(t)−Xβ\\n0.1832976)] where\\nXˆβ=\\n5.450859\\n+0.131983[Group 2]\\nand [c] = 1 if subject is in group c,0o t h e r w i s e .\\nlatex(fl, fi= '')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de06fe55-7f79-4a19-a36e-65e628d668b1', embedding=None, metadata={'page_label': '444', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"444 18 Parametric Survival Models\\nTable 18.3 Group eﬀects from three survival models\\nModel Group 2:1 Median Survival Time\\nFailure Time Ratio Group 1 Group 2\\nExtreme Value (Weibull) 1.14 217 248\\nLog-logistic 1.11 217 241\\nLog-normal 1.10 217 238\\nProb{T≥t}=[ 1+e x p (log(t)−Xβ\\n0.1159753)]−1where\\nXˆβ=\\n5.375675\\n+0.1051005[Group 2]\\nand [c] = 1 if subject is in group c,0o t h e r w i s e .\\nlatex(fn, fi= '')\\nProb{T≥t}=1−Φ(log(t)−Xβ\\n0.2100184)w h e r e\\nXˆβ=\\n5.375328\\n+0.0930606[Group 2]\\nand [c] = 1 if subject is in group c,0o t h e r w i s e .\\nThe estimated failure time ratios and median failure times for the two\\ngroups are given in Table 18.3. For example, the eﬀect of going from Group 1\\ntoGroup2istoincreaselogfailuretimeby0.132fortheextremevaluemodel,\\ngiving a Group 2:1 failure time ratio of exp(0 .132) = 1 .14. This ratio is also\\nthe ratio of median survival times. We choose the log-logistic model for its\\nsimpler form. The ﬁtted survival curves are plotted with the nonparametric\\nestimates in Figure 18.10. Excellent agreement is seen, except for 150 to 180\\ndays for Group 2. The standard error of the regression coeﬃcient for group\\nin the log-logistic model is 0.0636 giving a Wald χ2for group diﬀerences of\\n(.105/.0636)2=2.73,P=0.1.\\nsurvplot(f, conf.int= FALSE, # Figure 18.10\\nlevels.only= TRUE,label.curves= list(keys= 'lines '))\\nsurvplot(fl, add= TRUE,label.curves= FALSE, conf.int= FALSE)\\nThe Weibull PH form of the ﬁtted extreme value model, using Equa-\\ntion18.24,i s\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6b14dd8b-e66f-4744-b15c-8fe106f7fd94', embedding=None, metadata={'page_label': '445', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"18.3 Accelerated Failure Time Models 445\\nsSurvival Probability\\n0 35 70 140 210 280 3500.00.20.40.60.81.0\\nGroup 1\\nGroup 2\\nFig. 18.10 Agreement between ﬁtted log-logistic model and nonparametric survival\\nestimates for rat vaginal cancer data.\\nProb{T≥t}=e x p{−t5.456exp(Xˆβ)}where\\nXˆβ=\\n−29.74\\n−0.72[Group 2]\\nand [c] = 1 if subject is in group c,0o t h e r w i s e .\\nA sensitive graphical veriﬁcation of the distributional assumptions of the\\nAFT model is obtained by plotting the estimated survival distribution of\\nstandardized residuals (Equation 18.3.5), censored identically to the way T\\nis censored. This distribution is plotted along with the theoretical distri-\\nbutionψ. The assessment may be made more stringent by stratifying the\\nresiduals by important subject characteristics and plotting separate survival\\nfunction estimates; they should all have the same standardized distribution\\n(e.g., same σ).\\nr←resid(fl, 'cens ')\\nsurvplot( npsurv(r ∼group, data=kprats),\\nconf= 'none ', xlab= 'Residual ',\\nlabel.curves= list(keys= 'lines '), levels.only= TRUE)\\nsurvplot( npsurv(r ∼1), conf= 'none ', add=TRUE, col= 'red')\\nlines(r, lwd=1, col= 'blue ')# Figure 18.11\\nAs an example, Figure 18.11shows the Kaplan–Meier estimate of the dis-\\ntribution of residuals, Kaplan–Meier estimates stratiﬁed by group, and the\\nassumed log-logistic distribution. 5\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5edc5857-d757-457f-8ed6-022a0c5c0d97', embedding=None, metadata={'page_label': '446', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='446 18 Parametric Survival Models\\nResidualSurvival Probability\\n−6 −5 −4 −3 −2 −1 0 1 2 3 40.00.20.40.60.81.0\\nGroup 1\\nGroup 2\\nFig. 18.11 Kaplan–Meier estimates of distribution of standardized censored residu-\\nals from the log-logistic model, along with the assumed standard log-logistic distri -\\nbution (dashed curve). The step functions in red is the estimated distribution of all\\nresiduals, and the step functions in black are the estimated distributions of residuals\\nstratiﬁed by group, as indicated. The blue curve is the assumed log-logistic distribu-\\ntion.\\nSection19.2has a more in-depth example of this approach.\\n18.3.7 Validating the Fitted Model\\nAFT models may be validated for both calibration and discrimination accu-\\nracy using the same methods that are presented for the Cox model in Sec-\\ntion20.11.The methods discussedthere for checking calibrationare based on\\nchoosing a single follow-up time. Checking the distributional assumptions of\\nthe parametric model is also a check of calibration accuracy in a sense. An-\\nother indirectcalibrationassessmentmaybe obtainedfromasetofCox–Snell\\nresiduals (Section 18.3.5) or by using ordinary residuals as just described. A\\nhigher resolution indirect calibration assessment based on plotting individual\\nuncensored failure times is available when the theoretical censoring times for\\nthose observations are known. Let Cdenote a subject’s censoring time and F\\nthe cumulativedistributionofafailuretime T.Theexpected valueof F(T|X)\\nis 0.5 when Tis an actual failure time random variable. The expected value\\nfor an event time that is observed because it is uncensored is the expected\\nvalue of F(T|T≤C,X)=0.5F(C|X). A smooth plot (using, say, loess)o f\\nF(T|X)−0.5F(C|X) against Xˆβshould be a ﬂat line through y= 0 if the\\nmodel is well calibrated. A smooth plot of 2 F(T|X)/F(C|X) against Xˆβ(or\\nanything else) should be a ﬂat line through y= 1. This method assumes that\\nthe model is calibrated well enough that we can substitute 1 −ˆS(C|X)f o r\\nF(C|X).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4fadf63d-acbe-4547-8239-d22eef8afb27', embedding=None, metadata={'page_label': '447', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.8 Time-Dependent Covariates 447\\n18.4 Buckley–James Regression Model\\nBuckley and James81developed a method for estimating regression coeﬃ-\\ncients using least squares after imputing censored residuals. Their method\\ndoes not assume a distribution for survival time or the residuals, but is aimed\\nat estimating expected survival time or expected log survival time given pre-\\ndictor variables. This method has been generalized to allow for smooth non-\\nlinear eﬀects and interactions in the S bjfunction in the rmspackage, written\\nby Stare and Harrell585.\\n18.5 Design Formulations\\nVarious designs can be formulated with survival regression models just as\\nwith other regression models. By constructing the proper dummy variables,\\nANOVA and ANOCOVAmodelscan easilybe speciﬁed fortesting diﬀerences\\nin survival time between multiple treatments. Interactions and complex non-\\nlinear eﬀects may also be modeled.\\n18.6 Test Statistics\\nAs discussed previously, likelihood ratio, score, and Wald statistics can be\\nderived from the maximum likelihood analysis, and the choice of test statistic\\ndepends on the circumstance and on computational convenience.\\n18.7 Quantifying Predictive Ability\\nSee Section 20.10for a generalizedmeasureofconcordancebetweenpredicted\\nandobservedsurvivaltime(orprobabilityofsurvival)forright-censoreddata.\\n18.8 Time-Dependent Covariates\\nTime-dependent covariates (predictors) requires special likelihood functions\\nand add signiﬁcant complexity to analyses in exchange for greater ver-\\nsatility and enhanced predictive discrimination604. Nicolaie et al.477and\\nD’Agostino et al.145provide useful static covariate approaches to modeling\\ntime-dependent predictors using landmark analysis.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe39ca52-1190-4005-acc6-efe430ef7478', embedding=None, metadata={'page_label': '448', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='448 18 Parametric Survival Models\\n18.9RFunctions\\nTherneau’s survregfunction (part of his survival package) can ﬁt regression\\nmodels in the AFT family with left–, right–, or interval–censoring. The time\\nvariable canbe untransformedor log-transformed(the default). Distributions\\nsupported are extreme value (Weibull and exponential), normal, logistic, and\\nStudent-t. The version of survreginrmsthat ﬁts parametric survival models\\nin the same framework as lrm,ols,a n dcphis called psm.psmworks with\\nprint,coef,formula,specs,summary,anova,predict,Predict,fastbw,latex,\\nnomogram,validate,calibrate ,survest,a n dsurvplot functions for obtaining\\nandplottingpredictedsurvivalprobabilities.The distargumentto psmcanbe\\n\"exponential\" ,\"extreme\" ,\"gaussian\" ,\"logistic\" ,\"loglogistic\" ,\"lognormal\" ,\\n\"t\",o r\"weibull\" . To ﬁt a model with no covariables, use the command\\npsm(Surv(d.time, event) ∼1)\\nTo restatea Weibull orexponential model in PH form, use the pphsmfunction.\\nAn example of how many of the functions are used is found below.\\nunits(d.time) ←\"Year\"\\nf←psm(Surv(d.time,cdeath) ∼lsp(age,65)*sex)\\n# default is Weibull\\nanova(f)\\nsummary(f) # summarize effects withdelta log T\\nlatex(f) # typeset math. form of fitted model\\nsurvest(f, times=1) # 1y survival est. for all subjects\\nsurvest(f, expand.grid(sex=\" female\", age=30:80), times=1:2)\\n# 1y, 2y survival estimates vs. age, for females\\nsurvest(f, data.frame(sex=\" female\",age=50))\\n# survival curve for an individual subject\\nsurvplot(f, sex=NA, age=50, n.risk=T)\\n# survival curves for each sex, adjusting age to 50\\nf.ph←pphsm(f) # convert from AFT to PH\\nsummary( f.ph) # summarize withhazard ratios\\n# instead of changes in log(T)\\nSpecial functions work with objects created by psmto create S functions that\\ncontain the analytic form for predicted survival probabilities ( Survival), haz-\\nard functions ( Hazard), quantiles of survival time ( Quantile), and mean or\\nexpected survival time ( Mean). Once the S functions are constructed, they can\\nbe used in a variety of contexts. The survplot andsurvestfunctions have\\na special argument for psmﬁts:what. The default is what=\"survival\" to esti-\\nmate or plot survival probabilities. Specifying what=\"hazard\" will plot hazard\\nfunctions. Predictalso has a special argument for psmﬁts:time. Specifying a\\nsingle value for timeresults in survival probability for that time being plotted\\ninstead of Xˆβ. Examples of many of the functions appear below, with the\\noutput of the survplot c o m m a n ds h o w ni nF i g u r e 18.12.\\nmed ←Quantile(fl)\\nmeant ←Mean(fl)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='98accf58-8f63-4d13-b30a-7254b4cd9917', embedding=None, metadata={'page_label': '449', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.9RFunctions 449\\nhaz ←Hazard(fl)\\nsurv ←Survival(fl)\\nlatex(surv,file= \\'\\', type= \\'Sinput \\')\\nsurv←function ( times = NULL, lp = NULL,\\nparms = -2.15437773933124)\\n{\\n1/(1 + exp((logb(times) - lp)/exp(parms)))\\n}\\n# Plot estimated hazard functions and add median\\n# survival times to graph\\nsurvplot(fl, group, what=\"hazard\") # Figure 18.12\\n# Compute median survival time\\nm←med(lp=predict(fl,\\ndata.frame( group=levels(kprats$group))))\\nm\\n12\\n216.0857 240.0328\\nmed(lp=range(fl$ linear.predictors))\\n[1] 216.0857 240.0328\\nm←format(m, digits=3)\\ntext(68, .02, paste(\"Group 1 median: \", m[1], \"\\\\n\",\\n\"Group 2 median: \", m[2], sep=\"\"))\\n# Compute survival probability at 210 days\\nxbeta ←predict(fl,\\ndata.frame( group=c(\"Group 1\",\"Group 2\")))\\nsurv(210,xbeta)\\n12\\n0.5612718 0.7599776\\nThe S object called survreg.distributions in Therneau’s survival package\\nand the object survreg.auxinfo in thermspackage have detailed information\\nfor extreme-value, logistic, normal, and tdistributions. For each distribution,\\ncomponentsincludethedeviancefunction,analgorithmforobtainingstarting\\nparameter estimates, a L ATEX representation of the survival function, and S\\nfunctions deﬁning the survival, hazard, quantile functions, and basic survival\\ninverse function (which could have been used in Figure 18.9). See Figure 18.6\\nfor examples. rms’sval.surv function is useful for indirect external valida-\\ntion of parametric models using Cox–Snell residuals and other approaches of\\nSection18.3.7.T h eplotmethod for an object created by val.surv makes it\\neasy to stratify all computations by a variable of interest to more stringently\\nvalidate the ﬁt with respect to that variable.\\nrms’sbjfunction ﬁts the Buckley–James model for right-censored re-\\nsponses.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a3f0e21d-1b6c-4fc4-a5d0-d33bd67f0f83', embedding=None, metadata={'page_label': '450', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='450 18 Parametric Survival Models\\nDaysHazard Function\\n0 30 60 90 120 180 240 3000.0000.0050.0100.0150.0200.0250.030\\nGroup 1\\nGroup 2Group 1 median: 216\\nGroup 2 median: 240\\nFig. 18.12 Estimated hazard functions for log-logistic ﬁt to rat vaginal cancer data,\\nalong with median survival times.\\nKooperberg et al.’s adaptive linear spline log-hazard model360,361,594has\\nbeenimplementedintheSfunction hare. Theirproceduresearchesforsecond-\\norder interactions involving predictors (and linear splines of them) and linear\\nsplines in follow-up time (allowing for non-proportional hazards). hareis also\\nused to estimate calibration curves for parametric survival models ( rmsfunc-\\ntioncalibrate )a si ti sf o rC o xm o d e l s .\\n18.10 Further Reading\\n1Wellek657developed a test statistic for a speciﬁed maximum survival diﬀerence\\nafter relating this diﬀerence to a hazard ratio.\\n2Hougaard308compared accelerated failure time models with proportional haz-\\nard models.\\n3Gore et al.226discuss how an AFT model (the log-logistic model) gives rise to\\nvarying hazard ratios.\\n4See Hillis293for other types of residuals and plots that use them.\\n5S e eG o r ee ta l .226and Lawless382for other methods of checking assumptions for\\nAFT models. Lawless is an excellent text for in-depth discussion of parametric\\nsurvival modeling. Kwong and Hutton369present other methods of choosing\\nparametric survivalmodels, and discussthe robustnessof estimates when ﬁtting\\nan incorrectly chosen accelerated failure time model.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='da150522-c7ab-4ee9-96d7-4fa701a5db5d', embedding=None, metadata={'page_label': '451', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='18.11 Problems 451\\n18.11 Problems\\n1. For the failure times (in days)\\n133+6+7+\\ncompute MLEs of the following parameters of an exponential distribution\\nby hand: λ,μ,T0.5,a n dS(3 days). Compute 0.95 conﬁdence limits for λ\\nandS(3), basing the latter on log[ Λ(t)].\\n2. ForthesamedatainProblem1,computeMLEsofparametersofaWeibull\\ndistribution. Also compute the MLEs of S(3) andT0.5.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9ceaeed1-4137-4e91-a691-450d71818d37', embedding=None, metadata={'page_label': '453', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Chapter 19\\nCase Study in Parametric Survival\\nModeling and Model Approximation\\nConsider the random sample of 1000 patients from the SUPPORT study,352\\ndescribed in Section 3.12. In this case study we develop a parametric sur-\\nvival time model (accelerated failure time model) for time until death for the\\nacute disease subset of SUPPORT (acute respiratory failure, multiple organ\\nsystem failure, coma). We eliminate the chronic disease categories because\\nthe shapes of the survival curves are diﬀerent between acute and chronic dis-\\nease categories. To ﬁt both acute and c hronic disease classes would require a\\nlog-normal model with σparameter that is disease-speciﬁc.\\nPatients had to survive until day 3 of the study to qualify. The baseline\\nphysiologic variables were measured during day 3.\\n19.1 Descriptive Statistics\\nFirst we create a variable acuteto ﬂag the categories of interest, and print\\nunivariable descriptive statistics for the data subset.\\nrequire(rms)\\ngetHdata(support) # Get data framefrom web site\\nacute ←support$dzclass %in% c( 'ARF/MOSF ','Coma ')\\nlatex(describe(support[ acute,]), file= '')\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 19453\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ae325960-afe3-422b-8343-23eec1dec3be', embedding=None, metadata={'page_label': '454', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='454 19 Parametric Survival Modeling and Model Approximation\\nsupport[acute, ]\\n35 Variables 537 Observations\\nage : Age\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 529 1 60.7 28.49 35.22 47.93 63.67 74.49 81.54 85.56\\nlowest : 18.04 18.41 19.76 20.30 20.31\\nhighest: 91.62 91.82 91.93 92.74 95.51\\ndeath : Death at any time up to NDI date:31DEC94\\nn missing unique Info Sum Mean\\n537 0 2 0.67 356 0.6629\\nsexn missing unique\\n537 0 2\\nfemale (251, 47%), male (286, 53%)\\nhospdead : Death in Hospital\\nn missing unique Info Sum Mean\\n537 0 2 0.7 201 0.3743\\nslos : Days from Study Entry to Discharge\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 85 1 23.44 4.0 5.0 9.0 15.0 27.0 47.4 68.2\\nlowest:34567 , highest: 145 164 202 236 241\\nd.time : Days of Follow-Up\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 340 1 446.1 4 6 16 182 724 1421 1742\\nlowest : 3 4 5 6 7, highest: 1977 1979 1982 2011 2022\\ndzgroup\\nn missing unique\\n537 0 3\\nARF/MOSF w/Sepsis (391, 73%), Coma (60, 11%), MOSF w/Malig (86, 16%)\\ndzclassn missing unique\\n537 0 2\\nARF/MOSF (477, 89%), Coma (60, 11%)\\nnum.co : number of comorbidities\\nn missing unique Info Mean\\n537 0 7 0.93 1.525\\n0 1 2345 6\\nFrequency 111 196 133 51 31 10 5\\n% 2 13 62 5962 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c9d4f087-03fb-4338-aec5-a3f9876c6dfb', embedding=None, metadata={'page_label': '455', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='19.1 Descriptive Statistics 455\\nedu : Years of Education\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n411 126 22 0.96 12.03 7 8 10 12 14 16 17\\nlowest : 0 1 2 3 4, highest: 17 18 19 20 22\\nincomen missing unique\\n335 202 4\\nunder $11k (158, 47%), $11-$25k (79, 24%), $25-$50k (63, 19%)\\n>$50k (35, 10%)\\nscoma : SUPPORT Coma Score based on Glasgow D3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 11 0.82 19.24 0 0 0 0 37 55 100\\n0 9 26 37 41 44 55 61 89 94 100\\nFrequency 301 50 44 19 17 43 11 6 8 6 32\\n% 5 6984382111 6\\ncharges : Hospital Charges\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n517 20 516 1 86652 11075 15180 27389 51079 100904 205562 283411\\nlowest : 3448 4432 4574 5555 5849\\nhighest: 504660 538323 543761 706577 740010\\ntotcst : Total RCC cost\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n471 66 471 1 46360 6359 8449 15412 29308 57028 108927 141569\\nlowest : 0 2071 2522 3191 3325\\nhighest: 269057 269131 338955 357919 390460\\ntotmcst : Total micro-cost\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n331 206 328 1 39022 6131 8283 14415 26323 54102 87495 111920\\nlowest : 0 1562 2478 2626 3421\\nhighest: 144234 154709 198047 234876 271467\\navtisst : Average TISS, Days 3–25\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n536 1 205 1 29.83 12.46 14.50 19.62 28.00 39.00 47.17 50.37\\nlowest : 4.000 5.667 8.000 9.000 9.500\\nhighest: 58.500 59.000 60.000 61.000 64.000\\nracen missing unique\\n535 2 5\\nwhite black asian other hispanic\\nFrequency 417 84 4 8 22\\n% 7 8 1611 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b0e794d-a429-45f0-8e4f-fa74de979631', embedding=None, metadata={'page_label': '456', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='456 19 Parametric Survival Modeling and Model Approximation\\nmeanbp : Mean Arterial Blood Pressure Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 109 1 83.28 41.8 49.0 59.0 73.0 111.0 124.4 135.0\\nlowest : 0 20 27 30 32, highest: 155 158 161 162 180\\nwblc : White Blood Cell Count Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n532 5 241 1 14.1 0.8999 4.5000 7.9749 12.3984 18.1992 25.1891 30.1873\\nlowest : 0.05000 0.06999 0.09999 0.14999 0.19998\\nhighest: 51.39844 58.19531 61.19531 79.39062 100.00000\\nhrt : Heart Rate Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 111 1 105 51 60 75 111 126 140 155\\nlowest : 0 11 30 36 40, highest: 189 193 199 232 300\\nresp : Respiration Rate Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 45 1 23.72 8 10 12 24 32 39 40\\nlowest : 0 4 6 7 8, highest: 48 49 52 60 64\\ntemp : Temperature (celcius) Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 61 1 37.52 35.50 35.80 36.40 37.80 38.50 39.09 39.50\\nlowest : 32.50 34.00 34.09 34.90 35.00\\nhighest: 40.20 40.59 40.90 41.00 41.20\\npaﬁ : PaO2/(.01*FiO2) Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n500 37 357 1 227.2 86.99 105.08 137.88 202.56 290.00 390.49 433.31\\nlowest : 45.00 48.00 53.33 54.00 55.00\\nhighest: 574.00 595.12 640.00 680.00 869.38\\nalb : Serum Albumin Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n346 191 34 1 2.668 1.700 1.900 2.225 2.600 3.100 3.400 3.800\\nlowest : 1.100 1.200 1.300 1.400 1.500\\nhighest: 4.100 4.199 4.500 4.699 4.800\\nbili : Bilirubin Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n386 151 88 1 2.678 0.3000 0.4000 0.6000 0.8999 2.0000 6.5996 13.1743\\nlowest : 0.09999 0.19998 0.29999 0.39996 0.50000\\nhighest: 22.59766 30.00000 31.50000 35.00000 39.29688\\ncrea : Serum creatinine Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 84 1 2.232 0.6000 0.7000 0.8999 1.3999 2.5996 5.2395 7.3197\\nlowest : 0.3 0.4 0.5 0.6 0.7, highest: 10.4 10.6 11.2 11.6 11.8', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1a204dab-7029-4646-ad96-d534ebeb43ea', embedding=None, metadata={'page_label': '457', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='19.1 Descriptive Statistics 457\\nsod : Serum sodium Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 38 1 138.1 129 131 134 137 142 147 150\\nlowest : 118 120 121 126 127, highest: 156 157 158 168 175\\nph : Serum pH (arterial) Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n500 37 49 1 7.416 7.270 7.319 7.380 7.420 7.470 7.510 7.529\\nlowest : 6.960 6.989 7.069 7.119 7.130\\nhighest: 7.560 7.569 7.590 7.600 7.659\\nglucose : Glucose Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n297 240 179 1 167.7 76.0 89.0 106.0 141.0 200.0 292.4 347.2\\nlowest : 30 42 52 55 68, highest: 446 468 492 576 598\\nbun : BUN Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n304 233 100 1 38.91 8.00 11.00 16.75 30.00 56.00 79.70 100.70\\nlowest:13456 , highest: 123 124 125 128 146\\nurine : Urine Output Day 3\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n303 234 262 1 2095 20.3 364.0 1156.5 1870.0 2795.0 4008.6 4817.5\\nlowest : 0 5 8 15 20, highest: 6865 6920 7360 7560 7750\\nadlp : ADL Patient Day 3\\nn missing unique Info Mean\\n104 433 8 0.87 1.577\\n0 1234567\\nFrequency 51 19 7 6 4 7 8 2\\n% 4 91 8764782\\nadls : ADL Surrogate Day 3\\nn missing unique Info Mean\\n392 145 8 0.89 1.86\\n01234567\\nFrequency 185 68 22 18 17 20 39 23\\n% 4 7 1 76545 1 06\\nsfdm2n missing unique\\n468 69 5\\nno(M2 and SIP pres) (134, 29%), adl>=4 (>=5 if sur) (78, 17%)\\nSIP>=30 (30, 6%), Coma or Intub (5, 1%), <2 mo. follow-up (221, 47%)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eeb0ce98-9f5f-4486-9629-0b88a129f39a', embedding=None, metadata={'page_label': '458', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"458 19 Parametric Survival Modeling and Model Approximation\\nadlsc : Imputed ADL Calibrated to Surrogate\\nn missing unique Info Mean .05 .10 .25 .50 .75 .90 .95\\n537 0 144 0.96 2.119 0.000 0.000 0.000 1.839 3.375 6.000 6.000\\nlowest : 0.0000 0.4948 0.4948 1.0000 1.1667\\nhighest: 5.7832 6.0000 6.3398 6.4658 7.0000\\nNext, patterns of missing data are displayed.\\nplot(naclus( support[ acute,])) # Figure 19.1\\nTheHmisc varclus functionisusedtoquantifyanddepictassociationsbetween\\npredictors, allowing for general nonmonotonic relationships. This is done by\\nusing Hoeﬀding’s Das a similarity measure for all possible pairs of predictors\\ninstead of the default similarity, Spearman’s ρ.\\nac←support[acute,]\\nac$dzgroup ←ac$dzgroup[drop=TRUE] # Remove unused levels\\nlabel(ac$dzgroup) ←'Disease Group '\\nattach(ac)\\nvc←varclus( ∼age + sex + dzgroup + num.co + edu + income +\\nscoma + race + meanbp + wblc + hrt + resp +\\ntemp + pafi + alb + bili + crea + sod + ph +\\nglucose + bun + urine + adlsc, sim= 'hoeffding ')\\nplot(vc) # Figure 19.2\\n19.2 Checking Adequacy of Log-Normal Accelerated\\nFailure Time Model\\nLet us check whether a parametric survival time model will ﬁt the data, with\\nrespectto the keyprognosticfactors.First,Kaplan–Meierestimatesstratiﬁed\\nby disease group are computed, and plotted after inverse normal transforma-\\ntion, against log t. Parallelism and linearity indicate goodness of ﬁt to the\\nlog normal distribution for disease group. Then a more stringent assessment\\nis made by ﬁtting an initial model and computing right-censored residuals.\\nThese residuals, after dividing by ˆ σ, should all have a normal distribution\\nif the model holds. We compute Kaplan–Meier estimates of the distribution\\nof the residuals and overlay the estimated survival distribution with the the-\\noretical Gaussian one. This is done overall, and then to get more stringent\\nassessments of ﬁt, residuals are stratiﬁed by key predictors and plots are\\nproduced that contain multiple Kaplan–Meier curves along with a single the-\\noretical normal curve. All curves should hover about the normal distribution.\\nTo gauge the natural variability of stratiﬁed residual distribution estimates,\\nthe residuals are also stratiﬁed by a random number that has no bearing on\\nthe goodness of ﬁt.\\ndd←datadist(ac)\\n# describe distributions of variables to rms\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='567d7f04-a9c7-490f-95c2-78412e29bf58', embedding=None, metadata={'page_label': '459', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"19.2 Checking Adequacy of Log-Normal Model 459\\nadlsc\\nsod\\ncrea\\ntemp\\nresp\\nhrt\\nmeanbp\\nrace\\navtisst\\nwblc\\ncharges\\ntotcst\\nscoma\\npafi\\nph\\nsfdm2\\nalb\\nbili\\ntotmcst\\nadlp\\nurine\\nglucose\\nbun\\nadls\\nedu\\nincome\\nnum.co\\ndzclass\\ndzgroup\\nd.time\\nslos\\nhospdead\\nsex\\nage\\ndeath\\n0.50.40.30.20.10.0Fraction Missing\\nFig. 19.1 Cluster analysis showing which predictors tend to be missing on the same\\npatients\\nraceasian\\nraceother\\nracehispanic\\nraceblack\\nurine\\nglucose\\nbun\\nnum.co\\nadlsc\\nresp\\nhrt\\ntemp\\nmeanbp\\npafi\\nph\\nalb\\nbili\\nage\\ncrea\\nedu\\nincome$11−$25k\\nincome$25−$50k\\nincome>$50k\\nsexmale\\nsod\\ndzgroupComa\\nscoma\\ndzgroupMOSF w/Malig\\nwblc\\n 0.20 0.15 0.10 0.05 0.0030 * Hoeffding D\\nFig. 19.2 Hierarchical clustering of potential predictors using Hoeﬀding Das a\\nsimilarity measure. Categorical predictors are automatically expanded into dummy\\nvariables.\\noptions(datadist= 'dd')\\n# Generate right-censored survival timevariable\\nyears ←d.time/365.25\\nunits(years) ←'Year '\\nS←Surv(years, death)\\n# Show normal inverse Kaplan-Meier estimates\\n# stratified by dzgroup\\nsurvplot( npsurv(S ∼dzgroup), conf= 'none ',\\nfun=qnorm, logt=TRUE) # Figure 19.3\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6ddf9ffe-9bf2-45da-ba6f-5072743b8879', embedding=None, metadata={'page_label': '460', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"460 19 Parametric Survival Modeling and Model Approximation\\nf←psm(S∼dzgroup + rcs(age,5) + rcs( meanbp,5),\\ndist= 'lognormal ', y=TRUE)\\nr←resid(f)\\nsurvplot(r, dzgroup, label.curve=FALSE)\\nsurvplot(r, age, label.curve=FALSE)\\nsurvplot(r, meanbp, label.curve=FALSE)\\nrandom ←runif(length(age)); label(random) ←'Random Number '\\nsurvplot(r, random, label.curve=FALSE) # Fig. 19.4\\nNow remove from consideration predictors that are missing in more than 0.2\\nof patients. Many of these were collected only for the second half of SUP-\\nPORT. Of those variables to be included in the model, ﬁnd which ones have\\nenough potential predictive power to justify allowing for nonlinear relation-\\nships or multiple categories,which spend more d.f. For each variablecompute\\nSpearman ρ2basedonmultiple linearregressionofrank( x),rank(x)2,andthe\\nlog Survival Time in Years−3 −2 −1 0 1 2−2−1012\\ndzgroup=ARF/MOSF w/Sepsis\\ndzgroup=Comadzgroup=MOSF w/Malig\\nFig. 19.3 Φ−1(SKM(t)) stratiﬁed by dzgroup. Linearity and semi-parallelism indi-\\ncate a reasonable ﬁt to the log-normal accelerated failure time model with respect to\\none predictor.\\nsurvival time, truncating survival time at the shortest follow-up for survivors\\n(356 days; see Section 4.1).\\nshortest.follow.up ←min(d.time[death ==0],na.rm=TRUE)\\nd.timet ←pmin(d.time, shortest.follow.up)\\nw←spearman2(d.timet ∼age + num.co + scoma + meanbp +\\nhrt + resp + temp + crea + sod + adlsc +\\nwblc + pafi + ph + dzgroup + race, p=2)\\nplot(w, main= '') # Figure 19.5\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f4ed0808-6f4a-4f6e-8f49-d9c042888c62', embedding=None, metadata={'page_label': '461', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"19.2 Checking Adequacy of Log-Normal Model 461\\nAbetterapproachistousethecompleteinformationinthefailureandcensor-\\ning times by computing Somers’ Dxyrank correlation allowing for censoring.\\nw←rcorrcens(S ∼age + num.co + scoma + meanbp + hrt + resp +\\ntemp + crea + sod + adlsc + wblc + pafi + ph +\\ndzgroup + race)\\nplot(w, main= '') # Figure 19.6\\nRemaining missing values are imputed using the “most normal” values, a\\nprocedurefound toworkadequatelyforthisparticularstudy.Raceisimputed\\nusing the modal category.\\n# Compute number of missing values per variable\\nsapply(llist(age,num.co, scoma,meanbp,hrt, resp,temp,crea,sod,\\nadlsc,wblc,pafi,ph), function(x) sum( is.na(x)))\\nage num.co scoma meanbp hrt resp temp crea sod adlsc\\n0000000000\\nwblc pafi ph\\n53 73 7\\nResidualSurvival Probability\\n−3.0 −2.0 −1.0 0.0 0.5 1.0 1.5 2.00.00.20.40.60.81.0Disease Group\\nResidualSurvival Probability\\n−3.0 −2.0 −1.0 0.0 0.5 1.0 1.5 2.00.00.20.40.60.81.0Age\\nResidualSurvival Probability\\n−3.0 −2.0 −1.0 0.0 0.5 1.0 1.5 2.00.00.20.40.60.81.0Mean Arterial Blood Pressure Day 3\\nResidualSurvival Probability\\n−3.0 −2.0 −1.0 0.0 0.5 1.0 1.5 2.00.00.20.40.60.81.0Random Number\\nFig. 19.4 Kaplan-Meier estimates of distributions of normalized, right-censored\\nresiduals from the ﬁtted log-normal survival model. Residuals are stratiﬁed by im-\\nportant variables in the model (by quartiles of continuous variables), plus a r andom\\nvariabletodepictthenaturalvariability(inthelower right plot).Theoretical standard\\nGaussian distributions of residuals are shown with a thick solid line.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='13f790ab-0eda-4219-82fb-362057413dd8', embedding=None, metadata={'page_label': '462', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='462 19 Parametric Survival Modeling and Model Approximation\\nscoma\\nmeanbp\\ndzgroup\\ncrea\\npafi\\nph\\nsod\\nhrt\\nadlsc\\ntemp\\nwblc\\nnum.co\\nage\\nresp\\nraceN  df\\n535 4537 2537 2537 2532 2537 2537 2537 2537 2500 2500 2537 2537 2537 2537 2\\n0.00 0.02 0.04 0.06 0.08 0.10 0.12\\nAdjusted ρ2\\nFig. 19.5 Generalized Spearman ρ2rank correlation between predictors and trun-\\ncated survival time\\nmeanbp\\ncrea\\ndzgroup\\nscoma\\npafi\\nph\\nadlsc\\nage\\nnum.co\\nhrt\\nresp\\nrace\\nsod\\nwblc\\ntempN\\n537532537535537537537537537500500537537537537\\n0.00 0.05 0.10 0.15 0.20\\n|Dxy|\\nFig. 19.6 Somers’ Dxyrank correlation between predictors and original survival\\ntime. For dzgroup orrace, the correlation coeﬃcient is the maximum correlation from\\nusing a dummy variable to represent the most frequent or one to represent the second\\nmost frequent category.’,scap=’Somers’ Dxyrank correlation between predictors and\\noriginal survival time', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6f7ff56f-0fcb-433b-b782-39b5623bca31', embedding=None, metadata={'page_label': '463', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"19.2 Checking Adequacy of Log-Normal Model 463\\n# Can also do naplot(naclus(support[ acute,]))\\n# Can also use the Hmisc naclus and naplot functions\\n# Impute missing values withnormal or modal values\\nwblc.i ←impute( wblc, 9)\\npafi.i ←impute( pafi, 333.3)\\nph.i ←impute(ph, 7.4)\\nrace2 ←race\\nlevels(race2) ←list(white= 'white ',other=levels(race)[-1])\\nrace2[is.na(race2)] ←'white '\\ndd←datadist(dd, wblc.i, pafi.i, ph.i,race2)\\nNow that missing values have been imputed, a formal multivariable redun-\\ndancy analysis can be undertaken. The Hmiscpackage’s redunfunction goes\\nfarther than the varcluspairwise correlation approach and allows for non-\\nmonotonic transformations in predicting each predictor from all the others.\\nredun(∼crea + age + sex + dzgroup + num.co + scoma + adlsc +\\nrace2 + meanbp + hrt + resp + temp + sod + wblc.i +\\npafi.i + ph.i, nk=4)\\nRedundancy Analysis\\nredun(formula = ∼crea + age + sex + dzgroup + num.co + scoma +\\nadlsc + race2 + meanbp + hrt + resp + temp + sod + wblc.i +\\npafi.i + ph.i, nk = 4)\\nn: 537 p: 16 nk: 4\\nNumber of NAs: 0\\nTransformation of target variables forced to be linear\\nR2cutoff: 0.9 Type: ordinary\\nR2with which each variable can be predicted from all other variables:\\ncrea age sex dzgroup num.co scoma adlsc race2 meanbp\\n0.133 0.246 0.132 0.451 0.147 0.418 0.153 0.151 0.178\\nhrt resp temp sod wblc.i pafi.i ph.i\\n0.258 0.131 0.197 0.135 0.093 0.143 0.171\\nNo redundant variables\\nNow turn to a more eﬃcient approach for gauging the potential of each\\npredictor, one that makes maximal use of failure time and censored data is to\\nall continuous variables to have a maximum number of knots in a log-normal\\nsurvival model. This approach must use imputation to have an adequate\\nsamplesize.Asemi-saturatedmaineﬀectsadditivelog-normalmodelisﬁtted.\\nIt is necessary to limit restricted cubic splines to 4 knots, force scomato be\\nlinear, and to omit ph.iin order to avoid a singular covariance matrix in\\nthe ﬁt.\\nk←4\\nf←psm(S∼rcs(age,k)+sex+dzgroup+pol( num.co,2)+scoma+\\npol(adlsc ,2)+race+rcs(meanbp,k)+rcs(hrt,k)+\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='201b0656-000a-4c04-a402-8ddeb5222560', embedding=None, metadata={'page_label': '464', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='464 19 Parametric Survival Modeling and Model Approximation\\nrcs(resp,k)+rcs(temp,k)+rcs(crea ,3)+rcs(sod,k)+\\nrcs(wblc.i,k)+rcs(pafi.i,k), dist= \\'lognormal \\')\\nplot(anova(f)) # Figure 19.7\\nFigure19.7properly blinds the analyst to the form of eﬀects (tests of lin-\\nearity). Next ﬁt a log-normal survival model with number of parameters\\ncorresponding to nonlinear eﬀects determined from the partial χ2tests in\\nFigure19.7. For the most promising predictors, ﬁve knots can be allocated,\\nas there are fewer singularity problems once less promising predictors are\\nsimpliﬁed.\\nsex\\ntemp\\nrace\\nsod\\nnum.co\\nhrt\\nwblc.i\\nadlsc\\nresp\\nscoma\\npafi.i\\nage\\nmeanbp\\ncrea\\ndzgroup\\n01 0 2 0 3 0\\nχ2 − df\\nFig. 19.7 Partialχ2statistics for association of each predictor with response from\\nsaturated main eﬀects model, penalized for d.f.\\nf←psm(S∼rcs(age,5)+sex+ dzgroup+ num.co+\\nscoma+pol( adlsc,2)+race2+rcs(meanbp,5)+\\nrcs(hrt,3)+rcs( resp,3)+temp+\\nrcs(crea ,4)+sod+rcs(wblc.i,3)+rcs(pafi.i,4),\\ndist= \\'lognormal \\')\\nprint(f, latex= TRUE,coefs=FALSE)\\nParametric Survival Model: Log Normal Distribution\\npsm(formula = S ~ rcs(age, 5) + sex + dzgroup + num.co + scoma +\\npol(adlsc, 2) + race2 + rcs(meanbp, 5) + rcs(hrt, 3) + rcs(resp,\\n3) + temp + rcs(crea, 4) + sod + rcs(wblc.i, 3) + rcs(pafi.i,\\n4), dist = \"lognormal\")', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='72c1a1f6-d0de-4686-a60f-c05eea2f6000', embedding=None, metadata={'page_label': '465', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='19.2 Checking Adequacy of Log-Normal Model 465\\nModel Likelihood Discrimination\\nRatio Test Indexes\\nObs 537 LRχ2236.83R20.594\\nEvents 356 d.f. 30 Dxy0.485\\nσ2.230782 Pr(>χ2)<0.0001g 0.033\\ngr1.959\\na←anova(f)\\nTable 19.1 Wald Statistics for S\\nχ2d.f.P\\nage 15 .99 4 0.0030\\nNonlinear 0.23 3 0.9722\\nsex 0 .11 1 0.7354\\ndzgroup 45 .69 2<0.0001\\nnum.co 4 .99 1 0.0255\\nscoma 10 .58 1 0.0011\\nadlsc 8 .28 2 0.0159\\nNonlinear 3.31 1 0.0691\\nrace2 1 .26 1 0.2624\\nmeanbp 27 .62 4<0.0001\\nNonlinear 10.51 3 0.0147\\nhrt 11 .83 2 0.0027\\nNonlinear 1.04 1 0.3090\\nresp 11 .10 2 0.0039\\nNonlinear 8.56 1 0.0034\\ntemp 0 .39 1 0.5308\\ncrea 33 .63 3<0.0001\\nNonlinear 21.27 2<0.0001\\nsod 0 .08 1 0.7792\\nwblc.i 5 .47 2 0.0649\\nNonlinear 5.46 1 0.0195\\npaﬁ.i 15 .37 3 0.0015\\nNonlinear 6.97 2 0.0307\\nTOTAL NONLINEAR 60 .48 14<0.0001\\nTOTAL 261 .47 30<0.0001', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b89836bf-09d4-4ffa-980b-3b7df9cd3c03', embedding=None, metadata={'page_label': '466', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"466 19 Parametric Survival Modeling and Model Approximation\\n19.3 Summarizing the Fitted Model\\nFirst let’s plot the shape of the eﬀect of each predictor on log survival time.\\nAll eﬀects are centered so that they can be placed on a common scale. This\\nallows the relative strength of various predictors to be judged. Then Wald\\nχ2statistics, penalized for d.f., are plotted in descending order. Next, rela-\\ntive eﬀects of varying predictors over reasonable ranges (survival time ratios\\nvarying continuouspredictorsfromthe ﬁrstto the thirdquartile)arecharted.\\nggplot( Predict(f, ref.zero= TRUE), vnames= 'names ',\\nsepdiscrete= 'vertical ', anova=a) # Figure 19.8\\nlatex(a, file= '',label= 'tab:support-anovat ')# Table 19.1\\nplot(a) # Figure 19.9\\noptions( digits=3)\\nplot(summary(f), log= TRUE,main= '')# Figure 19.10\\n19.4 Internal Validation of the Fitted Model\\nUsing the Bootstrap\\nLetusdecidewhethertherewassigniﬁcantoverﬁttingduringthedevelopment\\nof this model, using the bootstrap.\\n# First add data to model fit so bootstrap can re-sample\\n# from the data\\ng←update(f, x= TRUE, y= TRUE)\\nset.seed (717)\\nlatex(validate(g, B =300), digits=2, size= 'Ssize ')\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy 0.49 0.51 0.46 0 .05 0 .43 300\\nR20.59 0.66 0.54 0 .12 0 .47 300\\nIntercept 0 .00 0.00−0.05 0 .05−0.05 300\\nSlope 1 .00 1.00 0.90 0 .10 0 .90 300\\nD 0.48 0.55 0.42 0 .13 0 .35 300\\nU 0.00 0.00−0.01 0 .01−0.01 300\\nQ 0.48 0.56 0.43 0 .12 0 .36 300\\ng 1.96 2.05 1.87 0 .19 1 .77 300\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='90f51cbd-6dea-4e87-a151-034c37b6e6d1', embedding=None, metadata={'page_label': '467', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='19.4 Internal Validation of the Fitted Model Using the Bootstrap 467\\nχ22=8.3\\nχ42=16 χ32=33.6 χ22=11.8\\nχ42=27.6 χ12=5 χ32=15.4 χ22=11.1\\nχ12=10.6 χ12=0.1 χ12=0.4 χ22=5.5adlsc age crea hrt\\nmeanbp num.co pafi.i resp\\nscoma sod temp wblc.i−3−2−1012\\n−3−2−1012\\n−3−2−10120246 20 40 60 80 2.5 5.0 7.5 50 100 150\\n30 60 90 120 150 0246 100 200 300 400 500 10 20 30 40\\n0 25 50 75 100 130 135 140 145 150155 35 36 37 38 39 40 01 0 2 0 3 0 4 0log(T)\\nlll\\nχ22= 45.7 llχ12=1.3\\nll\\nχ12=0.1\\nARF/MOSF w/SepsisComaMOSF w/Malig\\nwhiteother\\nfemalemale\\n−3 −2 −1 0 1 2 −3 −2 −1 0 1 2 −3 −2 −1 0 1 2\\nlog(T)dzgroup race2 sex\\nFig. 19.8 Eﬀect of each predictor on log survival time. Predicted values have been\\ncentered so that predictions at predictor reference values are zero. Pointwise 0.95\\nconﬁdence bands are also shown. As all y-axes have the same scale, it is easy to see\\nwhich predictors are strongest.\\nJudging from DxyandR2there is a moderate amount of overﬁtting. The\\nslope shrinkage factor (0.9) is not troublesome, however. An almost unbiased\\nestimate of future predictive discrimination on similar patients is given by\\nthe corrected Dxyof 0.43. This index equals the diﬀerence between the prob-\\nability of concordanceand the probability of discordanceof pairs of predicted\\nsurvival times and pairs of observed survival times, accounting for censoring.\\nNext, a bootstrap overﬁtting-corrected calibration curve is estimated. Pa-\\ntients are stratiﬁed by the predicted probability of surviving one year, such\\nthat there are at least 60 patients in each group.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='16ea1c32-2967-4044-a471-1e6729184b1f', embedding=None, metadata={'page_label': '468', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"468 19 Parametric Survival Modeling and Model Approximation\\nsod\\nsex\\ntemp\\nrace2\\nwblc.i\\nnum.co\\nadlsc\\nresp\\nscoma\\nhrt\\nage\\npafi.i\\nmeanbp\\ncrea\\ndzgroup\\n0 1 02 03 04 0\\nχ2 − df\\nFig. 19.9 Contribution of variables in predicting survival time in log-normal model\\n0.10 0.50 1.00 2.00 3.50\\nage − 74.5:47.9\\nnum.co − 2:1\\nscoma − 37:0\\nadlsc − 3.38:0\\nmeanbp − 111:59\\nhrt − 126:75\\nresp − 32:12\\ntemp − 38.5:36.4\\ncrea − 2.6:0.9\\nsod − 142:134\\nwblc.i − 18.2:8.1\\npafi.i − 323:142\\nsex − female:male\\ndzgroup − Coma:ARF/MOSF w/Sepsis\\ndzgroup − MOSF w/Malig:ARF/MOSF w/Sepsis\\nrace2 − other:white\\nFig. 19.10 Estimated survival time ratios for default settings of predictors. For\\nexample, when age changes from its lower quartile to the upper quartile (47.9y to\\n74.5y), median survival time decreases by more than half. Diﬀerent shaded areas of\\nbars indicate diﬀerent conﬁdence levels (.9, 0.95, 0.99).\\nset.seed (717)\\ncal←calibrate(g, u=1, B =300)\\nplot(cal, subtitles= FALSE)\\ncal←calibrate(g, cmethod= 'KM', u=1, m=60, B=120, pr= FALSE)\\nplot(cal, add=TRUE) # Figure 19.11\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='44aaba9e-fdc5-4153-83ce-79c3b963574b', embedding=None, metadata={'page_label': '469', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='19.5 Approximating the Full Model 469\\n0.0 0.2 0.4 0.6 0.80.00.20.40.60.8\\nPredicted  1 Year SurvivalFraction Surviving 1 Years\\nFig. 19.11 Bootstrap validation of calibration curve. Dots represent apparent cali-\\nbration accuracy; ×are bootstrap estimates corrected for overﬁtting, based on bin-\\nning predicted survival probabilities and computing Kaplan-Meier estimates. Black\\ncurve is the estimated observed relationship using hareand the blue curve is the\\noverﬁtting-corrected hareestimate. The gray-scale line depicts the ideal relationship.\\n19.5 Approximating the Full Model\\nThe ﬁtted log-normal model is perhaps too complex for routine use and for\\nroutine data collection. Let us develop a simpliﬁed model that can predict\\nthe predicted values of the full model with high accuracy ( R2=0.967). The\\nsimpliﬁcation is done using a fast backward step-down against the full model\\npredicted values.\\nZ←predict(f) #X*beta hat\\na←ols(Z∼rcs(age,5)+sex+ dzgroup+ num.co+\\nscoma+pol( adlsc,2)+race2+\\nrcs(meanbp,5)+rcs(hrt,3)+rcs( resp,3)+\\ntemp+rcs( crea,4)+sod+rcs(wblc.i,3)+\\nrcs(pafi.i,4), sigma=1)\\n# sigma=1 is used to prevent sigma hat frombeingzero when\\n# R2=1.0 since we start out by approximating Z with all\\n# component variables\\nfastbw(a, aics =10000) # fast backward stepdown\\nDeleted Chi-Sq d.f. P Residual d.f. P AIC R2\\nsod 0.43 1 0.512 0.43 1 0.5117 -1.571.000\\nsex 0.57 1 0.451 1.00 2 0.6073 -3.000.999\\ntemp 2.20 1 0.138 3.20 3 0.3621 -2.800.998\\nrace2 6.81 1 0.009 10.01 4 0.0402 2.01 0.994\\nwblc.i 29.52 2 0.000 39.53 6 0.0000 27.53 0.976', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a351c031-870a-4268-8b14-08d4ecc6a066', embedding=None, metadata={'page_label': '470', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"470 19 Parametric Survival Modeling and Model Approximation\\nnum.co 30.84 1 0.000 70.36 7 0.0000 56.36 0.957\\nresp 54.18 2 0.000 124.55 9 0.0000 106.55 0.924\\nadlsc 52.46 2 0.000 177.00 11 0.0000 155.00 0.892\\npafi.i 66.78 3 0.000 243.79 14 0.0000 215.79 0.851\\nscoma 78.07 1 0.000 321.86 15 0.0000 291.86 0.803\\nhrt 83.17 2 0.000 405.02 17 0.0000 371.02 0.752\\nage 68.08 4 0.000 473.10 21 0.0000 431.10 0.710\\ncrea 314.47 3 0.000 787.57 24 0.0000 739.57 0.517\\nmeanbp 403.04 4 0.000 1190.61 28 0.0000 1134.61 0.270\\ndzgroup 441.28 2 0.000 1631.89 30 0.0000 1571.89 0.000\\nApproximate Estimates after Deleting Factors\\nCoef S.E.Wald Z P\\n[1,] -0.5928 0.04315 -13.74 0\\nFactors in Final Model\\nNone\\nf.approx ←ols(Z∼dzgroup + rcs( meanbp,5) + rcs( crea,4) +\\nrcs(age,5) + rcs(hrt,3) + scoma +\\nrcs(pafi.i,4) + pol( adlsc,2)+\\nrcs(resp,3), x= TRUE)\\nf.approx$ stats\\nn Model L.R. d.f. R2 g Sigma\\n537.000 1688.225 23.000 0.957 1.915 0.370\\nWe can estimate the variance–covariance matrix of the coeﬃcients of the\\nreduced model using Equation 5.2in Section 5.5.2. The computations below\\nresult in a covariance matrix that does not include elements related to the\\nscale parameter. In the code xis the matrix Tin Section 5.5.2.\\nV←vcov(f, regcoef.only= TRUE) # var(full model)\\nX←cbind(Intercept=1, g$x) # full model design\\nx←cbind(Intercept=1, f.approx$x) # approx. model design\\nw←solve(t(x) %*% x, t(x)) %*% X # contrast matrix\\nv←w %*% V %*% t(w)\\nLet’s compare the variance estimates (diagonalsof v) with variance estimates\\nfrom a reduced model that is ﬁtted against the actual outcomes.\\nf.sub ←psm(S∼dzgroup + rcs( meanbp,5) + rcs(crea,4) +\\nrcs(age,5) + rcs(hrt,3) + scoma + rcs(pafi.i,4) +\\npol(adlsc,2)+ rcs( resp,3), dist= 'lognormal ')\\ndiag(v)/diag(vcov(f.sub, regcoef.only= TRUE))\\nIntercept dzgroup=Coma dzgroup=MOSF w/Malig\\n0.981 0.979 0.979\\nmeanbp meanbp ' meanbp ''\\n0.977 0.979 0.979\\nmeanbp ''' crea crea'\\n0.979 0.979 0.979\\ncrea'' age age '\\n0.979 0.982 0.981\\nage'' age''' hrt\\n0.981 0.980 0.978\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3e334510-a711-482a-a51e-d2e2947b17a3', embedding=None, metadata={'page_label': '471', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"19.5 Approximating the Full Model 471\\nhrt' scoma pafi.i\\n0.976 0.979 0.980\\npafi.i ' pafi.i '' adlsc\\n0.980 0.980 0.981\\nadlsc∧2 resp resp'\\n0.981 0.978 0.977\\nr←diag(v)/diag(vcov( f.sub,regcoef.only= TRUE))\\nr[c(which.min(r), which.max(r))]\\nhrt'age\\n0.976 0.982\\nThe estimated variances from the reduced model are actually slightly smaller\\nthan those that would have been obtained from stepwise variable selection\\nin this case, had variable selection used a stopping rule that resulted in the\\nsame set of variables being selected. Now let us compute Wald statistics for\\nthe reduced model.\\nf.approx$var ←v\\nlatex(anova(f.approx, test= 'Chisq ', ss=FALSE), file= '',\\nlabel= 'tab:support-anovaa ')\\nThe results are shown in Table 19.2. Note the similarity of the statistics\\nto those found in the table for the full model. This would not be the case had\\ndeleted variables been very collinear with retained variables.\\nThe equation for the simpliﬁed model follows. The model is also depicted\\ngraphically in Figure 19.12.The nomogram allows one to calculate mean and\\nmedian survival time. Survival probabilities could have easily been added as\\nadditional axes.\\n# Typeset mathematical form of approximate model\\nlatex(f.approx, file= '')\\nE(Z) =Xβ,where\\nXˆβ=\\n−2.51\\n−1.94[Coma] −1.75[MOSF w /Malig]\\n+0.068meanbp −3.08×10−5(meanbp −41.8)3\\n++7.9×10−5(meanbp −61)3\\n+\\n−4.91×10−5(meanbp −73)3\\n++2.61×10−6(meanbp −109)3\\n+−1.7×10−6(meanbp −135)3\\n+\\n−0.553crea −0.229(crea −0.6)3\\n++0.45(crea −1.1)3\\n+−0.233(crea −1.94)3\\n+\\n+0.0131(crea −7.32)3\\n+\\n−0.0165age −1.13×10−5(age−28.5)3\\n++4.05×10−5(age−49.5)3\\n+\\n−2.15×10−5(age−63.7)3\\n+−2.68×10−5(age−72.7)3\\n++1.9×10−5(age−85.6)3\\n+\\n−0.0136hrt + 6 .09×10−7(hrt−60)3\\n+−1.68×10−6(hrt−111)3\\n++1.07×10−6(hrt−140)3\\n+\\n−0.0135 scoma\\n+0.0161paﬁ .i−4.77×10−7(paﬁ.i−88)3\\n++9.11×10−7(paﬁ.i−167)3\\n+\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e71ae02-4f97-493f-af92-bd9a358e2488', embedding=None, metadata={'page_label': '472', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"472 19 Parametric Survival Modeling and Model Approximation\\nTable 19.2 Wald Statistics for Z\\nχ2d.f.P\\ndzgroup 55 .94 2<0.0001\\nmeanbp 29 .87 4<0.0001\\nNonlinear 9.84 3 0.0200\\ncrea 39 .04 3<0.0001\\nNonlinear 24.37 2<0.0001\\nage 18 .12 4 0.0012\\nNonlinear 0.34 3 0.9517\\nhrt 9 .87 2 0.0072\\nNonlinear 0.40 1 0.5289\\nscoma 9 .85 1 0.0017\\npaﬁ.i 14 .01 3 0.0029\\nNonlinear 6.66 2 0.0357\\nadlsc 9 .71 2 0.0078\\nNonlinear 2.87 1 0.0904\\nresp 9 .65 2 0.0080\\nNonlinear 7.13 1 0.0076\\nTOTAL NONLINEAR 58 .08 13<0.0001\\nTOTAL 252 .32 23<0.0001\\n−5.02×10−7(paﬁ.i−276)3\\n++6.76×10−8(paﬁ.i−426)3\\n+−0.369 adlsc + 0 .0409 adlsc2\\n+0.0394resp −9.11×10−5(resp−10)3\\n++0.000176(resp −24)3\\n+−8.5×10−5(resp−39)3\\n+\\nand [c] = 1 if subject is in group c,0o t h e r w i s e ; ( x)+=xifx>0,0\\notherwise.\\n# Derive S functions thatexpress mean and quantiles\\n# of survival time for specific linear predictors\\n# analytically\\nexpected.surv ←Mean(f)\\nquantile.surv ←Quantile(f)\\nlatex(expecte d.surv, file= '', type= 'Sinput ')\\nexpected.surv ←function (lp = NULL,\\nparms = 0 .802352037606488)\\n{\\nnames(parms) ←NULL\\nexp(lp + exp(2 * parms)/2)\\n}\\nlatex(quantil e.surv, file= '', type= 'Sinput ')\\nquantile.surv ←function (q = 0.5, lp = NULL,\\nparms = 0 .802352037606488)\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='70937e16-a1c3-4594-a862-da97115a06a9', embedding=None, metadata={'page_label': '473', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"19.6 Problems 473\\n{\\nnames(parms) ←NULL\\nf←function(lp, q, parms) lp + exp(parms) * qnorm(q)\\nnames(q) ←format(q)\\ndrop(exp(outer(lp, q, FUN = f, parms = parms)))\\n}\\nmedian.surv ←function(x) quantile.surv(lp=x)\\n# Improve variable labels for the nomogram\\nf.approx ←Newlabels(f.approx, c( 'Disease Group ',\\n'Mean Arterial BP ','Creatinine ','Age','Heart Rate ',\\n'SUPPORT Coma Score ','PaO2/(.01*FiO2) ','ADL',\\n'Resp. Rate '))\\nnom←\\nnomogram(f.approx,\\npafi.i=c(0, 50, 100, 200, 300, 500, 600, 700, 800,\\n900),\\nfun=list( 'Median Survival Time '=median.surv,\\n'Mean Survival Time '=expected.surv),\\nfun.at=c(.1,.25,.5,1,2,5,10,20,40))\\nplot(nom, cex.var=1, cex.axis=.75, lmgp=.25)\\n# Figure 19.12\\n19.6 Problems\\nAnalyze the Mayo Clinic PBC dataset.\\n1. Graphically assess whether Weibull (extreme value), exponential, log-\\nlogistic,orlog-normaldistributionswillﬁt thedata,usingafewapparently\\nimportant stratiﬁcation factors.\\n2. For the best ﬁtting parametric model from among the four examined,\\nﬁt a model containing several sensible covariables, both categorical and\\ncontinuous. Do a Wald test for whether each factor in the model has an\\nassociation with survival time, and a likelihood ratio test for the simulta-\\nneous contribution of all predictors. For classiﬁcation factors having more\\nthan two levels, be sure that the Wald test has the appropriate degrees\\nof freedom. For continuous factors, verify or relax linearity assumptions.\\nIf using a Weibull model, test whether a simpler exponential model would\\nbe appropriate. Interpret all estimated coeﬃcients in the model. Write the\\nfull survival model in mathematical form. Generate a predicted survival\\ncurve for a patient with a given set of characteristics.\\nSee[361]for an analysis of this dataset using linear splines in time and in the\\ncovariables.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='bd727f60-d624-4b0f-8a88-204ffc796c55', embedding=None, metadata={'page_label': '474', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='474 19 Parametric Survival Modeling and Model Approximation\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 00\\nDisease Group\\nComa ARF/MOSF w/SepsisMOSF w/Malig\\nMean Arterial BP\\n02 0 4 0 6 080120\\nCreatinine\\n53 2 1 067 8 91 0 1 1 1 2\\nAge\\n100 70 60 50 30 10\\nHeart Rate\\n300 200 100 50 0\\nSUPPORT Coma\\nScore100 70 50 30 10\\nPaO2/(.01*FiO2)\\n0 50 100 200300\\n500 700 900\\nADL\\n4.5 2 1 057\\nResp. Rate\\n05 1 565 60 55 50 45 40 35 30\\nTotal Points\\n0 50 100 150 200 250 300 350 400 450\\nLinear Predictor\\n−7 −5 −3 −1 1 2 3 4\\nMedian Survival Time\\n0.10.25 0.5125102040\\nMean Survival Time\\n0.10.250.5125102040\\nFig. 19.12 Nomogram for predicting median and mean survival time, based on ap-\\nproximation of full model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='610c138b-ee40-4255-bd1a-0bf1b3d83235', embedding=None, metadata={'page_label': '475', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 20\\nCox Proportional Hazards Regression\\nModel\\n20.1 Model\\n20.1.1 Preliminaries\\nThe Cox proportional hazards model132is the most popular model for the\\nanalysis of survival data. It is a semiparametric model; it makes a parametric 1\\nassumption concerning the eﬀect of the predictors on the hazard function,\\nbut makes no assumption regarding the nature of the hazard function λ(t)\\nitself. The Cox PH model assumes that predictors act multiplicatively on the\\nhazardfunctionbutdoesnotassumethatthehazardfunctionisconstant(i.e.,\\nexponential model), Weibull, or any other particular form. The regression\\nportion of the model is fully parametric; that is, the regressors are linearly\\nrelated to log hazard or log cumulative hazard. In many situations, either\\nthe form of the true hazard function is unknown or it is complex, so the\\nCox model has deﬁnite advantages. Also, one is usually more interested in\\nthe eﬀects of the predictors than in the shape of λ(t), and the Cox approach\\nallows the analyst to essentially ignore λ(t), which is often not of primary\\ninterest.\\nThe Cox PH model uses only the rankorderingofthe failure and censoring\\ntimes and thus is less aﬀected by outliers in the failure times than fully\\nparametric methods. The model contains as a special case the popular log-\\nrank test for comparing survival of two groups. For estimating and testing\\nregression coeﬃcients, the Cox model is as eﬃcient as parametric models\\n(e.g., Weibull model with PH) even when all assumptions of the parametric\\nmodel are satisﬁed.171\\nWhen aparametricmodel’sassumptionsarenottrue (e.g.,whena Weibull\\nmodel is used and the population is not from a Weibull survival distribution\\nso that the choice of model is incorrect), the Cox analysis is more eﬃcient\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 20475', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='620ad330-f768-4101-b786-8d14465f0e30', embedding=None, metadata={'page_label': '476', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='476 20 Cox Proportional Hazards Regression Model\\nthan the parametric analysis. As shown below, diagnostics for checking Cox\\nmodel assumptions are very well developed.\\n20.1.2 Model Deﬁnition\\nThe Cox PH model is most often stated in terms of the hazard function:\\nλ(t|X)=λ(t)exp(Xβ). (20.1)\\nWe do not include an intercept parameter in Xβhere. Note that this is\\nidentical to the parametric PH model stated earlier. There is an important\\ndiﬀerence, however,in that now we do not assume any speciﬁc shape for λ(t).\\nFor the moment, we are not even interested in estimating λ(t). The reason\\nfor this departure from the fully parametric approach is due to an ingenious\\nconditional argument by Cox.132Cox argued that when the PH model holds,\\ninformation about λ(t) is not very useful in estimating the parameters of\\nprimary interest, β. By special conditioning in formulating the log likelihood\\nfunction, Coxshowedhowto deriveavalidestimateof βthat doesnotrequire\\nestimation of λ(t)a sλ(t) dropped out of the new likelihood function. Cox’s\\nderivation focuses on using the information in the data that relates to the\\nrelative hazard function exp( Xβ).\\n20.1.3 Estimation of β\\nCox’s derivation of an estimator of βcan be loosely described as follows. Let\\nt1<t2<...<t krepresent the unique ordered failure times in the sample of\\nnsubjects; assume for now that there areno tied failure times (tied censoring\\ntimes are allowed) so that k=n. Consider the set of individuals at risk of\\nfailing an instant before failure time ti. This set of individuals is called the\\nrisk setat timeti, and we use Rito denote this risk set. Riis the set of\\nsubjects jsuch that the subject had not failed or been censored by time ti;\\nthat is, the risk set Riincludes subjects with failure/censoring time Yj≥ti.\\nThe conditional probability that individual iis the one that failed at ti,\\ngiven that the subjects in the set Riare at risk of failing, and given further\\nthat exactly one failure occurs at ti,i s\\nProb{subjectifails atti|Riand one failure at ti}=\\nProb{subjectifails atti|Ri}\\nProb{one failure at ti|Ri}(20.2)\\nusing the rules of conditional probability. This conditional probability equals\\nλ(ti)exp(Xiβ)∑\\nj∈Riλ(ti)exp(Xjβ)=exp(Xiβ)∑\\nj∈Riexp(Xjβ)=exp(Xiβ)∑\\nYj≥tiexp(Xjβ)(20.3)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85155d71-0d8b-4e32-884b-f085b381b7f0', embedding=None, metadata={'page_label': '477', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.1 Model 477\\nindependent of λ(t). To understand this likelihood, consider a special case\\nwhere the predictors have no eﬀect; that is, β=0[93, pp. 48–49]. Then\\nexp(Xiβ)=e x p ( Xjβ)=1a n dP r o b {subjectiis the subject that failed at\\nti|Riandonefailureoccurredat ti}is 1/niwhereniisthe numberofsubjects\\nat risk at time ti.\\nBy arguing that these conditional probabilities are themselves condition-\\nally independent across the diﬀerent failure times, a total likelihood can be\\ncomputed by multiplying these individual likelihoods over all failure times.\\nCox termed this a partial likelihood forβ:\\nL(β)=∏\\nYiuncensoredexp(Xiβ)∑\\nYj≥Yiexp(Xjβ). (20.4)\\nThe log partial likelihood is\\nlogL(β)=∑\\nYiuncensored{Xiβ−log[∑\\nYj≥Yiexp(Xjβ)]}.(20.5)\\nCox and others have shown that this partial log likelihood can be treated as\\nan ordinary log likelihood to derive valid (partial) MLEs of β. Note that this\\nlog likelihood is unaﬀected by the addition of a constant to any or all of the\\nXs. This is consistentwith the fact that an intercept term is unnecessary and\\ncannot be estimated since the Cox model is a model for the relative hazard\\nand does not directly estimate the underlying hazard λ(t).\\nWhen there are tied failure times in the sample, the true partial log likeli-\\nhood function involvespermutationssoit canbe time-consumingto compute.\\nWhen the number of ties is not large, Breslow70has derived a satisfactory\\napproximate log likelihood function. The formula given above, when applied\\nwithout modiﬁcation to samples containing ties, actually uses Breslow’s ap-\\nproximation. If there are ties so that k<nandt1,...,tkdenote the unique\\nfailure times as we originally intended, Breslow’s approximation is written as\\nlogL(β)=k∑\\ni=1{Siβ−dilog[∑\\nYj≥tiexp(Xjβ)]}, (20.6)\\nwhereSi=∑\\nj∈DiXj,Diis the set of indexes jfor subjects failing at time\\nti,a n ddiis the number of failures at ti.\\nEfron171derived another approximation to the true likelihood that is sig-\\nniﬁcantly more accurate than the Breslow approximation and often yields\\nestimates that are very close to those from the more cumbersome permuta-\\ntion likelihood:288\\nlogL(β)=k∑\\ni=1{Siβ−di∑\\nj=1log[∑\\nYj≥tiexp(Xjβ)\\n−j−1\\ndi∑\\nl∈Diexp(Xlβ)]}. (20.7)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5777ab0a-92e7-49b8-89aa-fa8a72b9d23d', embedding=None, metadata={'page_label': '478', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='478 20 Cox Proportional Hazards Regression Model\\nIn the special case when all tied failure times are from subjects with iden-\\nticalXiβ, the Efron approximation yields the exact (permutation) marginal\\nlikelihood (Therneau, personal communication, 1993).\\nKalbﬂeisch and Prentice330showed that Cox’s partial likelihood, in the\\nabsence of predictors that are functions of time, is a marginal distribution of\\ntheranksof the failure/censoring times.\\nSee Therneau and Grambsch604and Huang and Harrington310for descrip-\\ntions of penalized partial likelihood estimation methods for improving mean\\nsquared error of estimates of βin a similar fashion to what was discussed in\\nSection9.10.\\n20.1.4 Model Assumptions and Interpretation\\nof Parameters\\nThe Cox PH regression model has the same assumptions as the parametric\\nPH model except that no assumption is made regarding the shape of the\\nunderlying hazard or survival functions λ(t)a n dS(t). The Cox PH model\\nassumes, in its most basic form, linearity and additivity of the predictors\\nwith respect to log hazard or log cumula tive hazard. It also assumes the PH\\nassumption of no time by predictor interactions; that is, the predictors have\\nthe same eﬀect on the hazard function at all values of t. The relative hazard\\nfunction exp( Xβ) is constant through time and the survival functions for\\nsubjects with diﬀerent values of Xare powers of each other. If, for example,\\nthe hazard of death at time tfor treated patients is half that of control\\npatients at time t, this same hazard ratio is in eﬀect at any other time point.\\nIn other words, treated patients have a consistently better hazard of death\\nover all follow-up time.\\nThe regression parameters are interp reted the same as in the parametric\\nPH model. The only diﬀerence is the absence of hazard shape parameters\\nin the model, since the hazard shape is not estimated in the Cox partial\\nlikelihood procedure.\\n20.1.5 Example\\nConsider again the rat vaginal cancer data from Section 18.3.6. Figure 20.1\\ndisplays the nonparametric survival estimates for the two groups along with\\nestimates derived from the Cox model (by a method discussed later).\\nrequire(rms)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='83b8a6ff-2280-4c15-be87-e56cee86a670', embedding=None, metadata={'page_label': '479', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"20.1 Model 479\\ngroup ←c(rep( 'Group 1 ',19),rep( 'Group 2 ',21))\\ngroup ←factor(group)\\ndd ←datadist( group); options(datadist= 'dd')\\ndays←\\nc(143,164,188,188,190,192,206,209,213,216,220,227,230,\\n234,246,265,304,216,244,142,156,163,198,205,232,232,\\n233,233,233,233,239,240,261,280,280,296,296,323,204,344)\\ndeath ←rep(1,40)\\ndeath[c(18,19,39,40)] ←0\\nunits(days) ←'Day'\\ndf←data.frame( days, death, group)\\nS←Surv(days,death)\\nf←npsurv(S ∼group, type= 'fleming ')\\nfor(meth in c( 'exact ','breslow ','efron ')) {\\ng←cph(S∼group, method= meth,surv=TRUE, x=TRUE, y= TRUE)\\n# print(g) to see results\\n}\\nf.exp ←psm(S∼group, dist= 'exponential ')\\nfw ←psm(S∼group, dist= 'weibull ')\\nphform ←pphsm(fw)\\nco←gray(c(0, .8))\\nsurvplot(f, lty=c(1, 1), lwd=c(1, 3), col=co,\\nlabel.curves=FALSE, conf= 'none ')\\nsurvplot(g, lty=c(3, 3), lwd=c(1, 3), col=co, # Efron approx.\\nadd=TRUE, label.curves=F ALSE,conf.type= 'none ')\\nlegend(c(2, 160), c(.38, .54),\\nc('Nonparametric Estimates ','Cox-Breslow Estimates '),\\nlty=c(1, 3), cex=.8, bty= 'n')\\nlegend(c(2, 160), c(.18, .34), cex=.8,\\nc('Group 1 ','Group 2 '), lwd=c(1,3), col=co, bty= 'n')\\nThe predicted survival curvesfrom the ﬁtted Cox model are in good agree-\\nment with the nonparametric estimates, again verifying the PH assumption\\nfor these data. The estimates of the group eﬀect from a Cox model (using the\\nexact likelihood since there are ties, along with both Efron’s and Breslow’s\\napproximations) as well as from a Weibull model and an exponential model\\nare shown in Table 20.1. The exponential model, with its constant hazard,\\ncannot accommodate the long early period with no failures. The group pre-\\ndictor was coded as X1=0a n d X1= 1 for Groups 1 and 2, respectively. For\\nthis example, the Breslow likelihood approximation resulted in ˆβcloser to\\nthat from maximizing the exact likelihood. Note how the group eﬀect (47%\\nreduction in hazard of death by the exact Cox model) is underestimated by\\nthe exponential model (9% reduction in hazard). The hazard ratio from the\\nWeibull ﬁt agrees with the Cox ﬁt.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e92431d1-4c45-47bc-ad1a-8f6821311674', embedding=None, metadata={'page_label': '480', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='480 20 Cox Proportional Hazards Regression Model\\nDaysSurvival Probability\\n0 30 60 90 120 150 180 210 240 270 300 3300.00.20.40.60.81.0\\nNonparametric Estimates\\nCox−Breslow Estimates\\nGroup 1\\nGroup 2\\nFig. 20.1 Altschuler–Nelson–Fleming–Harrington nonparametric survival estimates\\nand Cox-Breslow estimates for rat data508\\nTable 20.1 Group eﬀects using three versions of the partial likelihood and three\\nparametric models\\nModel Group Regression S.E. Wald Group 2:1\\nCoeﬃcient P-Value Hazard Ratio\\nCox (Exact) −0.629 0 .361 0.08 0 .533\\nCox (Efron) −0.569 0 .347 0.10 0 .566\\nCox (Breslow) −0.596 0 .348 0.09 0 .551\\nExponential −0.093 0 .334 0.78 0 .911\\nWeibull (AFT) 0 .132 0 .061 0.03 –\\nWeibull (PH) −0.721 – – 0 .486\\n20.1.6 Design Formulations\\nDesigns are no diﬀerent for the Cox PH model than for other models except\\nfor one minor distinction. Since the Cox model does not have an intercept\\nparameter, the group omitted from Xin an ANOVA model will go into the\\nunderlying hazard function. As an example, consider a three-group model for\\ntreatments A, B, and C. We use the two dummy variables\\nX1= 1 if treatment is A ,0o t h e r w i s e ,and\\nX2= 1 if treatment is B ,0o t h e r w i s e .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9a348782-c037-4663-b85d-65cf2fbef27e', embedding=None, metadata={'page_label': '481', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.1 Model 481\\nThe parameter β1is the A : C log hazard ratio or diﬀerence in hazards at\\nany time tbetween treatment A and treatment C. β2is the B : C log hazard\\nratio (exp( β2) is the B : C hazard ratio, etc.). Since there is no intercept\\nparameter, there is no direct estimate of the hazard function for treatment\\nC or any other treatment; only relative hazards are modeled.\\nAs with all regression models, a Wald, score, or likelihood ratio test for\\ndiﬀerences between any treatments is conducted by testing H0:β1=β2=0\\nwith 2 d.f.\\n20.1.7 Extending the Model by Stratiﬁcation\\nA unique feature of the Cox PH model is its ability to adjust for factors that\\nare not modeled. Such factors usually take the form of polytomous stratiﬁ-\\ncation factors that either are too diﬃcult to model or do not satisfy the PH\\nassumption. For example, a subject’s occupation or clinical study site may\\ntake on dozens of levels and the sample size may not be large enough to\\nmodel this nominal variable with dozens of dummy variables. Also, one may\\nknow that a certain predictor (either a polytomous one or a continuous one\\nthat is grouped) may not satisfy PH and it may be too complex to model the\\nhazard ratio for that predictor as a function of time.\\nThe idea behind the stratiﬁed Cox PH model is to allow the form of the\\nunderlying hazard function to vary across levels of the stratiﬁcation factors.\\nA stratiﬁed Cox analysis ranks the failure times separately within strata.\\nSuppose that there are bstrata indexed by j=1,2,...,b.L e tCdenote the\\nstratum identiﬁcation. For example, C= 1 or 2 may stand for the female and\\nmale strata, respectively. The stratiﬁed PH model is\\nλ(t|X,C=j)=λj(t)exp(Xβ),or\\nS(t|X,C=j)=Sj(t)exp(Xβ). (20.8)\\nHereλj(t)a n dSj(t) are, respectively, the underlying hazard and survival\\nfunctions for the jth stratum. The model does not assume any connection\\nbetween the shapes of these functions for diﬀerent strata.\\nIn this stratiﬁed analysis, the data are stratiﬁed by Cb u t ,b yd e f a u l t ,a\\ncommonvectorofregressioncoeﬃcientsisﬁttedacrossstrata.Thesecommon\\nregression coeﬃcients can be thought of as“pooled”estimates. For example,\\na Cox model with age as a (modeled) predictor and sex as a stratiﬁcation\\nvariableessentiallyestimatesthe commonslope ofagebypoolinginformation\\nabout the age eﬀect over the two sexes. The eﬀect of age is adjusted by sex\\ndiﬀerences, but no assumption is made about how sex aﬀects survival. There\\nisnoPHassumptionforsex.Levelsofthestratiﬁcationfactor Ccanrepresent\\nmultiple stratiﬁcation factors that are cross-classiﬁed. Since these factors are\\nnot modeled, no assumption is made regarding interactions among them.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c60dd4bb-e0d1-4ef8-9f13-bc83f6a6c56b', embedding=None, metadata={'page_label': '482', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='482 20 Cox Proportional Hazards Regression Model\\nAt ﬁrst glance it appears that stratiﬁcation causes a loss of eﬃciency.\\nHowever, in most cases the loss is small as long as the number of strata is not\\ntoo large with regard to the total number of events. A stratum that contains\\nno events contributes no information to the analysis, so such a situation\\nshould be avoided if possible.\\nThe stratiﬁed or “pooled” Cox model is ﬁtted by formulating a separate\\nlog likelihood function for each stratum, but with each log likelihoodhaving a\\ncommon βvector.If diﬀerent strata aremade up ofindependent subjects, the\\nstrata are independent and the likelihood functions are multiplied together\\nto form a joint likelihood over strata. Log likelihood functions are thus added\\nover strata. This total log likelihood function is maximized once to derive a\\npooled or stratiﬁed estimate of βand to make an inference about β.N oi n f e r -\\nence can be made about the stratiﬁcation factors. They are merely“adjusted\\nfor.”\\nStratiﬁcation is useful for checking the PH and linearity assumptions for\\none or more predictors. Predicted Cox survival curves (Section 20.2)c a n\\nbe derived by modeling the predictors in the usual way, and then stratiﬁed\\nsurvival curves can be estimated by using those predictors as stratiﬁcation\\nfactors. Other factors for which PH is assumed can be modeled in both in-\\nstances. By comparing the modeled versus stratiﬁed survival estimates, a\\ngraphical check of the assumptions can be made. Figure 20.1demonstrates\\nthis method although there are no other factors being adjusted for and strat-\\niﬁed Cox estimates are KM estimates. The stratiﬁed survival estimates are\\nderived by stratifying the dataset to obtain a separate underlying survival\\ncurve for each stratum, while pooling information across strata to estimate\\ncoeﬃcients of factors that are modeled.\\nBesides allowing a factor to be adjusted for without modeling its eﬀect,\\na stratiﬁed Cox PH model can also allow a modeled factor to interact with\\nstrata.143,180,603For the age–sex example, consider the following model with\\nX1denoting age and C=1,2 denoting females and males, respectively.\\nλ(t|X1,C=1 )=λ1(t)exp(β1X1)\\nλ(t|X1,C=2 )=λ2(t)exp(β1X1+β2X1). (20.9)\\nThis model can be simpliﬁed to\\nλ(t|X1,C=j)=λj(t)exp(β1X1+β2X2) (20.10)\\nifX2is a product interaction term equal to 0 for females and X1for males.\\nTheβ2parameter quantiﬁes the interaction between age and sex: it is the\\ndiﬀerence in the age slope between males and females. Thus the interaction\\nbetween age and sex can be quantiﬁed and tested, even though the eﬀect of\\nsex is not modeled!\\nThe stratiﬁed Cox model is commonly used to adjust for hospital diﬀer-\\nences in a multicenter randomized trial. With this method, one can allow', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8ae8a61d-9e6d-4423-b9ff-af6558596e3d', embedding=None, metadata={'page_label': '483', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.2 Survival Probability and Secondary Parameters 483\\nfor diﬀerences in outcome between qhospitals without estimating q−1p a -\\nrameters. Treatment ×hospital interactions can be tested eﬃciently without\\ncomputational problems by estimating only the treatment main eﬀect, after\\nstratifying on hospital. The score statistic (with q−1 d.f.) for testing q−1\\ntreatment ×hospital interaction terms is then computed (“residual χ2”ina\\nstepwise procedure with treatment ×hospital terms as candidate predictors).\\nThe stratiﬁed Cox model turns out to be a generalization of the condi-\\ntional logistic model for analyzing matched set (e.g., case-control) data.71\\nEach stratum represents a set, and the number of“failures”in the set is the\\nnumber of“cases”in that set. For r: 1 matching ( rm a yv a r ya c r o s ss e t s ) ,t h e\\nBreslow70likelihood may be used to ﬁt the conditional logistic model exactly.\\nForr:mmatching, an exact Cox likelihood must be computed.\\n20.2 Estimation of Survival Probability and Secondary\\nParameters\\nAs discussed above, once a partial log likelihood function is derived, it is\\nused as if it were an ordinary log likelihood function to estimate β, estimate\\nstandarderrorsof β,obtainconﬁdencelimits,andmakestatisticaltests.Point\\nand interval estimates of hazard ratios are obtained in the same fashion as\\nwith parametric PH models discussed earlier.\\nThe Cox model and parametricsurvivalmodels diﬀer markedlyin how one\\nestimates S(t|X). Since the Cox model does not depend on a choice of the\\nunderlying survival function S(t), ﬁtting a Cox model does not result directly\\nin an estimate of S(t|X). However, several authors have derived secondary\\nestimates of S(t|X). One method is the discrete hazard model of Kalbﬂeisch\\nand Prentice [ 331, pp. 36–37, 84–87]. Their estimator has two advantages: it\\nis an extension of the Kaplan–Meier estimator and is identical to SKMif the\\nestimated value of βhappened to be zero or there are no covariables being\\nmodeled; and it is not aﬀected by the choice of what constitutes a“standard”\\nsubject having the underlying survival function S(t). In other words,it would\\nnot matter whether the standard subject is one having age equal to the mean\\nage in the sample or the median age in the sample; the estimate of S(t|X)\\nas a function of X= age would be the same (this is also true of another\\nestimator which follows).\\nLett1,t2,...,tkdenotetheuniquefailuretimesin thesample.Thediscrete\\nhazard model assumes that the probability of failure is greater than zero only\\nat observed failure times. The probability of failure at time tjgiven that the\\nsubject has not failed before that time is also the hazard of failure at time\\ntjsince the model is discrete. The hazard at tjfor the standard subject is\\nwrittenλj. Letting αj=1−λj, the underlying survival function can be\\nwritten', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cbfd6f0b-a8fe-4f23-99de-9e0068cb6de2', embedding=None, metadata={'page_label': '484', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='484 20 Cox Proportional Hazards Regression Model\\nS(ti)=i−1∏\\nj=0αj,i=1,2,...,k (α0=1 ). (20.11)\\nA separate equation can be solved using the Newton–Raphson method to\\nestimate each αj. If there is only one failure at time ti, there is a closed-form\\nsolution for the maximum likelihood estimate of αi,ai, letting jdenote the\\nsubject who failed at ti.ˆβdenotes the partial MLE of β.\\nˆαi=[ 1−exp(Xjˆβ)/∑\\nYm≥Yjexp(Xmˆβ)]exp(−Xjˆβ).(20.12)\\nIfˆβ= 0, this formula reduces to a conditional probability component of the\\nproduct-limit estimator, 1 −(1/number at risk).\\nThe estimator of the underlying survival function is\\nˆS(t)=∏\\nj:tj≤tˆαj, (20.13)\\nand the estimate of the probability of survival past time tfor a subject with\\npredictor values Xis\\nˆS(t|X)=ˆS(t)exp(Xˆβ). (20.14)\\nWhen the model is stratiﬁed, estimation ofthe αjandSis carriedout sep-\\narately within each stratum once ˆβis obtained by pooling over strata. The\\nstratiﬁed survival function estimates can be thought of as stratiﬁed Kaplan–\\nMeier estimates adjusted for X, with the adjustment made by assuming PH\\nand linearity. As mentioned previously, these stratiﬁed adjusted survival es-\\ntimates are useful for checking model assumptions and for providing a simple\\nway to incorporate factors that violate PH.\\nThe stratiﬁed estimates are also useful in themselves as descriptive statis-\\ntics without making assumptions about a major factor. For example, in a\\nstudy from Caliﬀ et al.88to compare medical therapy with coronary artery\\nbypass grafting (CABG), the model was stratiﬁed by treatment but adjusted\\nfor a variety of baseline characteristics by modeling. These adjusted survival\\nestimates do not assume a form for the eﬀect of surgery. Figure 20.2displays\\nunadjusted (Kaplan–Meier) and adjusted survival curves, with baseline pre-\\ndictors adjusted to their mean levels in the combined sample. Notice that\\nvalid adjusted survival estimates are obtained even though the curves cross\\n(i.e., PH is violated for the treatment variable). These curves are essentially\\nproduct limit estimateswith respecttotreatment andCoxPHestimateswith\\nrespect to the baseline descriptor variables.\\nThe Kalbﬂeisch–Prentice discrete underlying hazard model estimates of\\ntheαjare one minus estimates of the hazard function at the discrete failure\\ntimes. However, these estimated hazard functions are usually too“noisy”to\\nbe useful unless the sample size is very large or the failure times have been\\ngrouped (say by rounding).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e8ef7e5c-cddb-4123-81da-345969cc034b', embedding=None, metadata={'page_label': '485', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.2 Survival Probability and Secondary Parameters 485\\nUnadjusted Adjusted\\n0.000.250.500.751.00\\n05 1 0 05 1 0\\nYears of FollowupSurvival ProbabilityTreatment\\nSurgical\\nMedical\\nFig. 20.2 Unadjusted (Kaplan–Meier) and adjusted (Cox–Kalbﬂeisch–Prentice) es-\\ntimates of survival. Left, Kaplan–Meier estimates for patients treated medica lly and\\nsurgically at Duke University Medical Center from November 1969 through December\\n1984. These survival curves are not adjusted for baseline prognostic factors. Right ,\\nsurvival curves for patients treated medically or surgically after adjusting for all\\nknown important baseline prognostic characteristics.88\\nJust as Kalbﬂeisch and Prentice have generalized the Kaplan–Meier es-\\ntimator to allow for covariables, Breslow70has generalized the Altschuler–\\nNelson–Aalen–Fleming–Harrington estimator to allow for covariables. Using\\nthe notation in Section 20.1.3, Breslow’s estimate is derived through an esti-\\nmate of the cumulative hazard function:\\nˆΛ(t)=∑\\ni:ti<tdi∑\\nYi≥tiexp(Xiˆβ). (20.15)\\nFor anyX, the estimates of ΛandSare\\nˆΛ(t|X)=ˆΛ(t)exp(Xˆβ)\\nˆS(t|X)=e x p [−ˆΛ(t)exp(Xˆβ)]. (20.16)\\nMore asymptotic theory has been derived from the Breslow estimator than\\nfor the Kalbﬂeisch–Prentice estimator. Another advantage of the Breslow\\nestimator is that it does not require iterative computations for di>1. Law-\\nless [382, p. 362] states that the two survival function estimators diﬀer little\\nexcept in the right-hand tail when all dis are unity. Like the Kalbﬂeisch–\\nPrentice estimator, the Breslow estimator is invariant under diﬀerent choices\\nof“standard subjects”for the underlying survival S(t). 2\\nSomewhat complex formulas are available for computing conﬁdence limits\\nofˆS(t|X).6153', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='cc826e98-f592-4bb0-8af8-3ff1187ed0ce', embedding=None, metadata={'page_label': '486', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='486 20 Cox Proportional Hazards Regression Model\\n20.3 Sample Size Considerations\\nOne way of estimating the minimum sample size for a Cox model analy-\\nsis aimed at estimating survival probabilities is to consider the simplest case\\nwhere there areno covariates.Thus the problem reducesto using the Kaplan-\\nMeier estimate to estimate S(t). Let’s further simplify things to assume there\\nis no censoring. Then the Kaplan-Meier estimate is just one minus the em-\\npirical cumulative distribution function. By the Dvoretzky-Kiefer-Wolfowitz\\ninequality, the maximum absolute error in an empirical distribution function\\nestimate of the true continuous distribution function is less than or equal to\\nǫwith probability of at least 1 −2e−2nǫ2. For the probability to be at least\\n0.95,n= 184. Thus in the case of no censoring, one needs 184 subjects to\\nestimate the survival curve to within a margin of error of 0.1 everywhere.\\nTo estimate the subject-speciﬁc survival curves ( S(t|X)) will require greater\\nsample sizes, as will having censored data. It is a fair approximation to think\\nof 184 as the needed number of subjects suﬀering the event or being censored\\n“late.”\\nTurning to estimation of a hazard ratio for a single binary predictor X\\nthat has equal numbers of X=0a n d X= 1, if the total sample size is n\\nand the number of events in the two categories are respectively e0ande1,\\nthe variance of the log hazard ratio is approximately v=1\\ne0+1\\ne1. Letting z\\ndenote the 1 −α/2 standard normal critical value, the multiplicative margin\\nof error (MMOE) with conﬁdence 1 −αis given by exp( z√v). To achieve\\na MMOE of 1.2 in estimating eˆβwith equal numbers of events in the two\\ngroups and α=0.05 requires a total of 462 events.\\n20.4 Test Statistics\\nWald, score, and likelihood ratio statistics are useful and valid for drawing\\ninferences about βin the Cox model. The score test deserves special mention\\nhere. If there is a single binary predictor in the model that describes two\\ngroups, the score test for assessing the importance of the binary predictor\\nis virtually identical to the Mantel–Haenszel log-rank test for comparing the\\ntwo groups. If the analysis is stratiﬁed for other (nonmodeled) factors, the\\nscore test from a stratiﬁed Cox model is equivalent to the corresponding\\nstratiﬁed log-rank test. Of course, the likelihood ratio or Wald tests could\\nalso be used in this situation, and in fact the likelihood ratio test may be\\nbetter than the score test (i.e., type I errors by treating the likelihood ratio\\ntest statistic as having a χ2distribution may be more accurate than using\\nthe log-rank statistic).\\nThe Cox model can be thought of as a generalization of the log-rank pro-\\ncedure since it allowsone to test continuous predictors,perform simultaneous', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='95a8d9c9-f7d6-4567-b604-ec14738ba545', embedding=None, metadata={'page_label': '487', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.6 Assessment of Model Fit 487\\ntests of various predictors, and adjust for other continuous factors without\\ngrouping them. Although a stratiﬁed log-rank test does not make assump-\\ntions regarding the eﬀect of the adjustment (stratifying) factors,it makes the\\nsame assumption (i.e., PH) as the Cox model regarding the treatment eﬀect\\nfor the statistical test of no diﬀerence in survival between groups.\\n20.5 Residuals\\nTherneau et al.605discussed four types of residuals from the Cox model:\\nmartingale, score, Schoenfeld, and deviance. The ﬁrst three have been proven\\nto be very useful, as indicated in Table 20.2. 4\\nTable 20.2 Types of residuals for the Cox model\\nResidual Purposes\\nMartingale Assessing adequacy of a hypothesized predictor\\ntransformation. Graphing an estimate of a\\npredictor transformation (Section 20.6.1).\\nScore Detecting overly inﬂuential observations\\n(Section 20.9). Robust estimate of\\ncovariance matrix of ˆβ(Section 9.5).410\\nSchoenfeld Testing PH assumption (Section 20.6.2).\\nGraphing estimate of hazard ratio function\\n(Section 20.6.2).\\n20.6 Assessment of Model Fit\\nAs stated before, the Cox model makes the same assumptions as the para-\\nmetric PH model except that it does not assume a given shape for λ(t)o r\\nS(t). Becausethe Cox PHmodel is sowidely used, methods ofassessingits ﬁt\\nare dealt with in more detail than was done with the parametric PH models.\\n20.6.1 Regression Assumptions\\nRegressionassumptions (linearity,additivity) for the PH model are displayed\\ninFigures 18.3and18.5.Asmentionedearlier,theregressionassumptionscan\\nbe veriﬁed by stratifying by Xand examining log ˆΛ(t|X) or log[ΛKM(t|X)]\\nestimates as a function of Xat ﬁxed time t. However, as was pointed out', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='63896eee-39ba-47c0-9c49-dfc8c030a978', embedding=None, metadata={'page_label': '488', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"488 20 Cox Proportional Hazards Regression Model\\nin logistic regression, the stratiﬁcation method is prone to problems of high\\nvariability of estimates. The sample size must be moderately large before\\nestimates are precise enough to observe trends through the “noise.” If one\\nwished to divide the sample by quintiles of age and 15 events were thought to\\nbe needed in each stratum to derive a reliable estimate of log[ ΛKM(2 years)],\\nthere would need to be 75 events in the entire sample. If the Kaplan–Meier\\nestimateswereneededtobeadjustedforanoth erfactorthatwasbinary,twice\\nas many events would be needed to allow the sample to be stratiﬁed by that\\nfactor.\\nFigure20.3displays Kaplan–Meier three-year log cumulative hazard esti-\\nmates stratiﬁed by sex and decile of age. The simulated sample consists of\\n2000 hypothetical subjects (389 of whom had events), with 1174 males (146\\ndeaths) and 826 females (243 deaths). The sample was drawn from a pop-\\nulation with a known survival distribution that is exponential with hazard\\nfunction\\nλ(t|X1,X2)=.02exp[.8X1+.04(X2−50)], (20.17)\\nwhereX1represents the sex group (0 = male, 1 = female) and X2age in\\nyears, and censoring is uniform. Thus for this population PH, linearity, and\\nadditivity hold. Notice the amount of variability and wide conﬁdence limits\\nin the stratiﬁed nonparametric survival estimates.\\nn←2000\\nset.seed(3)\\nage←50 + 12 * rnorm(n)\\nlabel(age) ←'Age'\\nsex←factor(1 + (runif(n) ≤.4), 1:2, c( 'Male ','Female '))\\ncens←15 * runif(n)\\nh←.02 * exp(.04 * (age - 50) + .8 * (sex == 'Female '))\\nft←-log(runif(n)) / h\\ne←ifelse(ft ≤cens, 1, 0)\\nprint(table(e))\\ne\\n01\\n1611 389\\nft←pmin(ft, cens)\\nunits(ft) ←'Year '\\nSrv←Surv(ft, e)\\nage.dec ←cut2(age, g=10, levels.mean=TRUE)\\nlabel(age.dec) ←'Age'\\ndd←datadist(age, sex, age.dec); options(datadist= 'dd')\\nf.np←cph(Srv ∼strat(age.dec) + strat(sex), surv=TRUE)\\n# surv=TRUE speeds up computations, and confidence limits when\\n# there are no covariables are stillaccurate.\\np←Predict( f.np, age.dec, sex, time=3, loglog=TRUE)\\n# Treat age.dec as a numeric variable ( meanswithin deciles)\\np$age.dec ←as.numeric( as.character(p$ age.dec))\\nggplot(p, ylim=c(-5, -.5))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c1f4eef0-5ccf-4f10-ae91-fa9cbf7d9fc9', embedding=None, metadata={'page_label': '489', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.6 Assessment of Model Fit 489\\n−5−4−3−2−1\\n30 40 50 60 70\\nAgelog[−log S(3)]sex\\nMale\\nFemale\\nFig. 20.3 Kaplan–Meier log Λestimates by sex and deciles of age, with 0 .95 conﬁ-\\ndence limits. Solid line is for males, dashed line for females.\\nAswiththe logisticmodelandotherregressionmodels,the restrictedcubic\\nspline function is an excellent tool for modeling the regression relationship\\nwith veryfew assumptions.A four-knotsplineCoxPHmodel intwo variables\\n(X1,X2) that assumes linearity in X1and noX1×X2interaction is given by\\nλ(t|X)=λ(t)exp(β1X1+β2X2+β3X′\\n2+β4X′′\\n2),\\n=λ(t)exp(β1X1+f(X2)), (20.18)\\nwhereX′\\n2andX′′\\n2are spline component variables as described earlier and\\nf(X2) is the spline function or spline transformation of X2given by\\nf(X2)=β2X2+β3X′\\n2+β4X′′\\n2. (20.19)\\nIn linear form the Cox model without assuming linearity in X2is\\nlogλ(t|X) = logλ(t)+β1X1+f(X2). (20.20)\\nBy computing partial MLEs of β2,β3,a n dβ4, one obtains the estimated\\ntransformation of X2that yields linearity in log hazard or log cumulative\\nhazard.\\nA similarmodel that does not assumePHin X1is the Coxmodel stratiﬁed\\nonX1. Letting the stratiﬁcation factor be C=X1,t h i sm o d e li s', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='43aecbce-0bed-4f89-ae4b-99be5f78c254', embedding=None, metadata={'page_label': '490', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"490 20 Cox Proportional Hazards Regression Model\\nlogλ(t|X2,C=j) = logλj(t)+β1X2+β2X′\\n2+β3X′′\\n2\\n=l o gλj(t)+f(X2). (20.21)\\nThis model does assume no X1×X2interaction.\\nFigure20.4displays the estimated spline function relating age and sex to\\nlog[Λ(3)] in the simulated dataset, using the additive model stratiﬁed on sex.\\nf.noia ←cph(Srv ∼rcs(age,4) + strat(sex), x= TRUE, y= TRUE)\\n# Get accurate C.L. for any age by specifying x= TRUE y=TRUE\\n# Note: for evaluating shape of regression, we would not\\n# ordinarily bother to get 3-year survival probabilities -\\n# would just use X *beta\\n# We do so here to use same scale as nonparametric estimates\\nw←latex(f.noia, inline= TRUE,digits=3)\\nlatex(anova(f.noia), table.env= FALSE, file= '')\\nχ2d.f.P\\nage 72 .33 3<0.0001\\nNonlinear 0.69 2 0.7067\\nTOTAL 72 .33 3<0.0001\\np←Predict( f.noia, age, sex, time=3, loglog=TRUE)\\nggplot(p, ylim =c(-5, -.5))\\n−5−4−3−2−1\\n20 40 60 80\\nAgelog[−log S(3)]sex\\nMale\\nFemale\\nFig. 20.4 Cox PH model stratiﬁed on sex, using spline function for age, no inter-\\naction. 0 .95 conﬁdence limits also shown. Solid line is for males, dashed line is for\\nfemales.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='de665477-d1f2-4496-a659-6b753f7f452f', embedding=None, metadata={'page_label': '491', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"20.6 Assessment of Model Fit 491\\nA formal test of the linearity assumption of the Cox PH model in the\\nabove example is obtained by testing H0:β2=β3=0 .T h e χ2statistic with\\n2 d.f. is 0.69, P=0.7. The ﬁtted equation, after simplifying the restricted\\ncubic spline to simpler (unrestricted) form, is Xˆβ=−1.46 + 0.0255age+\\n2.59×10−5(age−30.3)3−0.000101(age −45.1)3\\n++9.73×10−5(age−54.6)3\\n+−\\n2.22×10−5(age−69.6)3\\n+. Notice that the spline estimates are closer to the\\ntrue linear relationships than were the Kaplan–Meier estimates, and the con-\\nﬁdence limits are much tighter. The spline estimates impose a smoothness\\non the relationship and also use more information from the data by treating\\nageas a continuousorderedvariable.Also,unlikethestratiﬁedKaplan–Meier\\nestimates, the modeled estimates can make the assumption of no age ×sex\\ninteraction. When this assumption is true, modeling eﬀectively boosts the\\nsample size in estimating a common function for age across both sex groups.\\nOf course, this assumption can be tested and interactions can be modeled if\\nnecessary.\\nA Cox model that still does not assume PH for X1=Cbut which allows\\nfor anX1×X2interaction is\\nlogλ(t|X2,C=j) = logλj(t)+β1X2+β2X′\\n2+β3X′′\\n2\\n+β4X1X2+β5X1X′\\n2(20.22)\\n+β6X1X′′\\n2.\\nThis model allowsthe relationshipbetween X2andloghazardto bea smooth\\nnonlinear function and the shape of the X2eﬀect to be completely diﬀerent\\nfor each level of X1ifX1is dichotomous. Figure 20.5displays a ﬁt of this\\nmodel at t= 3 years for the simulated dataset.\\nf.ia←cph(Srv ∼rcs(age,4) * strat(sex), x= TRUE, y=TRUE,\\nsurv=TRUE)\\nw←latex(f.ia,inline= TRUE,digits=3)\\nlatex(anova(f.ia), table.env= FALSE, file= '')\\nχ2d.f.P\\nage (Factor+Higher Order Factors) 72 .82 6<0.0001\\nAll Interactions 1.05 3 0.7886\\nNonlinear (Factor+Higher Order Factors) 1.80 4 0.7728\\nage×sex (Factor+Higher Order Factors) 1 .05 3 0.7886\\nNonlinear 1.05 2 0.5911\\nNonlinear Interaction : f(A,B) vs. AB 1.05 2 0.5911\\nTOTAL NONLINEAR 1 .80 4 0.7728\\nTOTAL NONLINEAR + INTERACTION 1 .80 5 0.8763\\nTOTAL 72 .82 6<0.0001\\np←Predict( f.ia, age, sex, time=3, loglog=TRUE)\\nggplot(p, ylim =c(-5, -.5))\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4e26a258-d1e6-4201-ad72-a2771680eccc', embedding=None, metadata={'page_label': '492', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='492 20 Cox Proportional Hazards Regression Model\\n−5−4−3−2−1\\n20 40 60 80\\nAgelog[−log S(3)]sex\\nMale\\nFemale\\nFig. 20.5 Cox PH model stratiﬁed on sex, with interaction between age spline and\\nsex. 0.95 conﬁdence limits are also shown. Solid line is for males, dashed line for\\nfemales.\\nThe ﬁtted equation is Xˆβ=−1.8+0.0493age−2.15×10−6(age−30.3)3\\n+−\\n2.82×10−5(age−45.1)3\\n++5.18×10−5(age−54.6)3\\n+−2.15×10−5(age−69.6)3\\n++\\n[Female][ −0.0366age+ 4 .29×10−5(age−30.3)3\\n+−0.00011(age −45.1)3\\n++\\n6.74×10−5(age−54.6)3\\n+−2.32×10−7(age−69.6)3\\n+]. The test for interaction\\nyieldedχ2=1.05 with 3 d.f., P=0.8. The simultaneous test for linearity\\nand additivity yielded χ2=1.8 with 5 d.f., P=0.9. Note that allowing the\\nmodel to be very ﬂexible (not assuming linearity in age, additivity between\\nage and sex, and PH for sex) still resulted in estimated regression functions\\nthat are very close to the true functions. However, conﬁdence limits in this\\nunrestricted model are much wider.\\nFigure20.6displays the estimated relationship between left ventricular\\nejection fraction (LVEF) and log hazard ratio for cardiovascular death in a\\nsample of patients with signiﬁcant coronary artery disease. The relationship\\nis estimatedusingthree knotsplacedatquantiles0.05,0.5,and0.95ofLVEF.\\nHere there is signiﬁcant nonlinearity (Wald χ2=9.6 with 1 d.f.). The graphs\\nleads to a transformation of LVEF that better satisﬁes the linearity assump-\\ntion: min(LVEF, 0.5).This transformationhas the best log likelihood“fort he\\nmoney” as judged by the Akaike information criterion (AIC = −2l o gL . R .\\n−2×no. parameters= 127).The AICs for 3, 4, 5, and 6-knot spline ﬁts were,\\nrespectively, 126, 124, 122, and 120.\\nHad the suggested transformation been more complicated than a trunca-\\ntion, a tentative transformation could have been checked for adequacy by\\nexpanding the new transformed variable into a new spline function and test-\\ning it for linearity.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e8cf8eb-8a0d-4dc1-a435-efae22a50843', embedding=None, metadata={'page_label': '493', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.6 Assessment of Model Fit 493\\nLVEF0.2-4.0-3.5-3.0 -2.5 -2.0-1.5-1.0\\n0.4 0.6 0.8log Relative HazardCox Regression Model, n=979 events=198\\n    Statistic        X2  df\\nModel        L.R.  129.92 2  AIC= 125.92\\nAssociation  Wald  157.45 2  p=  0.000\\nLinearity    Wald    9.59 1  p=  0.002\\nFig. 20.6 Restricted cubic spline estimate of relationship between LVEF and relative\\nlog hazard from a sample of 979 patients and 198 cardiovascular deaths. Data from\\nthe Duke Cardiovascular Disease Databank.\\nOther methods based on smoothed residual plots are also valuable tools\\nfor selecting predictor transformations. Therneau et al.605describe residuals\\nbased on martingale theory that can estimate transformations of any number\\nof predictors omitted from a Cox model ﬁt, after adjusting for other vari-\\nables included in the ﬁt. Figure 20.7used various smoothing methods on the\\npoints (LVEF, residual). First, the Rloessfunction96w a su s e dt oo b t a i na\\nsmoothed scatterplot ﬁt and approximate 0.95 conﬁdence bars. Second, an 5\\nordinary least squares model, representing LVEF as a restricted cubic spline\\nwith ﬁvedefaultknots,wasﬁtted.Ideally,bothﬁtsshouldhaveusedweighted\\nregression as the residuals do not have equal variance. Predicted values from\\nthis ﬁt along with 0.95 conﬁdence limits are shown. The loessand spline-\\nlinear regression agree extremely well. Third, Cleveland’s lowessscatterplot\\nsmoother111was used on the martingale residuals against LVEF. The sug-\\ngested transformationfrom allthree is verysimilar to that ofFigure 20.6.F or\\nsmaller sample sizes, the raw residuals should also be displayed. There is one\\nvector of martingale residuals that is plotted against all of the predictors.\\nWhen correlations among predictors are mild, plots of estimated predictor\\ntransformationswithoutadjustment forotherpredictors(i.e., marginaltrans-\\nformations) may be useful. Martingale residuals may be obtained quickly by\\nﬁxingˆβ= 0 for all predictors. Then smoothed plots of predictor against\\nresidual may be made for all predictors. Table 20.3summarizes some of the', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7dd0c82f-8e06-4d31-b4df-5a949d8ca3c9', embedding=None, metadata={'page_label': '494', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='494 20 Cox Proportional Hazards Regression Model\\nLVEF0.20.0 0.5 1.0\\n0.4 0.6 0.8Martingale Residualloess Fit and 0.95 Confidence Bars\\nols Spline Fit and 0.95 Confidence Limits\\nlowess Smoother\\nFig. 20.7 Three smoothed estimates relating martingale residuals605to LVEF.\\nTable 20.3 Uses of martingale residuals for estimating predictor transformations\\nPurpose Method\\nEstimate transformation for Force ˆβ1= 0 and compute\\na single variable residuals from the null regression\\nCheck linearity assumption for Compute ˆβ1and compute\\na single variable residuals from the linear regression\\nEstimate marginal Force ˆβ1,...,ˆβp= 0 and compute\\ntransformations for pvariables residuals from the global null model\\nEstimate transformation for Estimate p−1βs, forcing ˆβi=0\\nvariableiadjusted for other Compute residuals from mixed\\np−1 variables global/null model\\nways martingaleresiduals may be used. See section 10.5for moreinformation 6\\non checking the regression assumptions. The methods for examining interac-\\ntion surfaces described there apply without modiﬁcation to the Cox model\\n(except that the nonparametric regression surface does not apply because of\\ncensoring).\\n20.6.2 Proportional Hazards Assumption\\nEven though assessment of ﬁt of the regression part of the Cox PH model\\ncorresponds with other regression models such as the logistic model, the Cox\\nmodel has its own distributional assumption in need of validation. Here, of\\ncourse,the distributionalassumptionisnotasstringentaswithothersurvival', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='df4d48b4-5d67-4362-83cc-a1f5eb78f499', embedding=None, metadata={'page_label': '495', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.6 Assessment of Model Fit 495\\nmodels, but we do need to validate how the survival or hazard functions\\nfor various subjects are connected. There are many graphical and analyti-\\ncal methods of verifying the PH assumption. Two of the methods have al-\\nready been discussed: a graphical examination of parallelism of log Λplots,\\nand a comparison of stratiﬁed with unstratiﬁed models (as in Figure 20.1).\\nMuenz467suggested a simple modiﬁcation that will make nonproportional\\nhazards more apparent: plot ΛKM1(t)/ΛKM2(t) against tand check for ﬂat-\\nness.Thepointsonthiscurvecanbepassedthroughasmoother.Onecanalso\\nplot diﬀerences in log( −logS(t)) against t.143Arjas29developed a graphical\\nmethod based on plotting the estimated cumulative hazard versus the cumu-\\nlative number of events in a stratum as tprogresses.\\nThere areother methods for assessingwhether PH holds that may be more\\ndirect. Gore et al.,226Harrell and Lee,266and Kay340(see also Anderson and\\nSenthilselvan27) describe a method for allowing the log hazard ratio (Cox\\nregression coeﬃcient) for a predictor to be a function of time by ﬁtting spe-\\ncially stratiﬁed Cox models. Their method assumes that the predictor being\\nexamined for PH already satisﬁes the linear regression assumption. Follow-\\nup time is stratiﬁed into intervals and a separate model is ﬁtted to compute\\nthe regression coeﬃcient within each interval, assuming that the eﬀect of the\\npredictor is constant only within that small interval. It is recommended that\\nintervals be constructed so that there is roughly an equal number of events\\nin each. The number of intervals should allow at least 10 or 20 events per\\ninterval.\\nThe interval-speciﬁc log hazard ratiois estimated by excluding all subjects\\nwith event/censoring time before the start of the interval and censoring all\\nevents that occur after the end of the interval. This process is repeated for\\nall desired time intervals. By plotting the log hazard ratio and its conﬁdence\\nlimits versus the interval, one can assess the importance of a predictor as\\na function of follow-up time and learn how to model non-PH using more\\ncomplicated models containing predictor by time interactions. If the hazard\\nratio is approximately constant within broad time intervals, the time strat-\\niﬁcation method can be used for ﬁtting and testing the predictor ×time\\ninteraction [ 266, p. 827]; [98].\\nConsider as an example the rat vaginal cancer data used in Figures 18.9,\\n18.10,a n d20.1. Recall that the PH assumption appeared to be satisﬁed for\\nthe two groups although Figure 18.9demonstrated some non-Weibullness.\\nFigure20.8contains a Λratio plot.467\\nf←cph(S∼strat(group), surv=TRUE)\\n# For both strata, eval. S(t) at combined set of death times\\ntimes ←sort(unique(days[death == 1]))\\nest ←survest(f, data.frame( group=levels(group)),\\ntimes=times, conf.type=\" none\")$surv\\ncumhaz ←- log(est)\\nplot(times, cumhaz[2,] / cumhaz[1,], xlab=\"Days\",\\nylab=\"Cumulative Hazard Ratio\", type =\"s\")\\nabline(h=1, col=gray(.80))', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b79bac78-5f30-4ca4-b11b-82bfb440d2b8', embedding=None, metadata={'page_label': '496', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='496 20 Cox Proportional Hazards Regression Model\\n150 200 250 3000.51.01.52.02.5\\nDaysCumulative Hazard Ratio\\nFig. 20.8 Estimate of Λ2/Λ1based on −log of Altschuler–Nelson–Fleming–\\nHarrington nonparametric survival estimates.\\nTable 20.4 Interval-speciﬁc group eﬀects from rat data by artiﬁcial censoring\\nTime Observations Deaths Log Hazard Standard\\nInterval Ratio Error\\n[0,209) 40 12 −0.47 0 .59\\n[209,234) 27 12 −0.72 0 .58\\n234+ 14 12 −0.50 0 .64\\nhazard.ratio.plot(g$x, g$y, e=12, pr= TRUE)\\nThe number of observations is declining over time because computations in\\neach interval were based on animals followed at least to the start of that\\ninterval. The overall Cox regression coeﬃcient was −0.57 with a standard\\nerror of 0 .35. There does not appear to be any trend in the hazard ratio over\\ntime, indicating a constanthazardratio or proportionalhazards(Table 20.4).\\nNow consider the Veterans Administration Lung Cancer dataset [ 331, pp.\\n60, 223–4]. Log Λplots indicated that the four cell types did not satisfy\\nPH. To simplify the problem, omit patients with “large” cell type and let\\nthe binary predictor be 1 if the cell type is“squamous”and 0 if it is“small”\\nor “adeno.” We are assessing whether survival patterns for the two groups\\n“squamous”versus“small”or“adeno”have PH. Interval-speciﬁc estimates of\\nthe squamous : small,adeno log hazard ratios (using Efron’s likelihood) are\\nfound in Table 20.5.T i m e sa r ei nd a y s .', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c4eee3e1-7921-4616-a887-dde104e1497e', embedding=None, metadata={'page_label': '497', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"20.6 Assessment of Model Fit 497\\nTable 20.5 Interval-speciﬁc eﬀects of squamous cell cancer in VA lung cancer data\\nTime Observations Deaths Log Hazard Standard\\nInterval Ratio Error\\n[0,21) 110 26 −0.46 0 .47\\n[21,52) 84 26 −0.90 0 .50\\n[52,118) 59 26 −1.35 0 .50\\n118+ 28 26 −1.04 0 .45\\nTable 20.6 Interval-speciﬁc eﬀects of performance status in VA lung cancer data\\nTime Observations Deaths Log Hazard Standard\\nInterval Ratio Error\\n[0,19] 137 27 −0.053 0 .010\\n[19,49) 112 26 −0.047 0 .009\\n[49,99) 85 27 −0.036 0 .012\\n99+ 28 26 −0.012 0 .014\\ngetHdata( valung)\\nwith(valung, {\\nhazard.ratio.plot(1 * ( cell == 'Squamous '), Surv(t, dead),\\ne=25, subset=cell != 'Large ',\\npr=TRUE, pl= FALSE)\\nhazard.ratio.plot(1 * kps, Surv(t, dead), e=25,\\npr=TRUE, pl= FALSE) })\\nThere is evidence of a trend of a decreasing hazard ratio over time which\\nis consistent with the observation that squamous cell patients had equal or\\nworse survival in the early period but decidedly better survival in the late\\nphase.\\nFrom the same dataset now examine the PH assumption for Karnofsky\\nperformance status using data from all subjects, if the linearity assumption is\\nsatisﬁed. Interval-speciﬁc regressioncoeﬃcients for this predictor are given in\\nTable20.6. Thereis goodevidence thatthe importance ofperformancestatus\\nis decreasing over time and that it is not a prognostic factor after roughly\\n99 days. In other words, once a patient survives 99 days, the performance\\nstatus doesnot containmuchinformationconcerningwhether the patientwill\\nsurvive120days.Thisnon-PHwouldbemorediﬃculttodetectfromKaplan–\\nMeier plots stratiﬁed on performance status unless performance status was\\nstratiﬁed carefully. 7\\nFigure20.9displays a log hazard ratio plot for a larger dataset in which\\nmoretimestratacanbeformed.In3299patientswithcoronaryarterydisease,\\n827 suﬀered cardiovascular death or nonfatal myocardial infarction. Time\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0109b449-fa37-4343-9e13-33224f73c15a', embedding=None, metadata={'page_label': '498', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='498 20 Cox Proportional Hazards Regression Model\\nt02468 1 0-0.1 0.0 0.1 0.2Log Hazard Ratio\\nPredictor:Pain/Ischemia Index\\nEvent:cdeathmiSubset Estimate\\n0.95 C.L.\\nSmoothed\\nFig. 20.9 Stratiﬁed hazard ratios for pain/ischemia index over time. Data from the\\nDuke Cardiovascular Disease Databank.\\nwas stratiﬁed into intervals containing approximately 30 events, and within\\neach interval the Cox regression coeﬃcient for an index of anginal pain and\\nischemia was estimated. The pain/ischemia index, one component of which is\\nunstableangina,isseentohaveastrongeﬀectforonlysixmonths.Afterthat,\\nsurvivors have stabilized and knowledge of the angina status in the previous\\nsix months is not informative.\\nAnother method for graphically assessing the log hazard ratio over time is\\nbased onSchoenfeld’s partial residuals503,557with respectto eachpredictorin\\nthe ﬁtted model. The residual is the contribution of the ﬁrst derivative of the\\nlog likelihood function with respect to the predictor’s regression coeﬃcient,\\ncomputed separately at each risk set or unique failure time. In Figure 20.10\\nthe“loess-smoothed”96(with approximate 0.95 conﬁdence bars) and“super-\\nsmoothed”207relationship between the residual and unique failure time is\\nshown for the same data as Figure 20.9. For smaller n, the raw residuals\\nshould also be displayed to convey the proper sense of variability. The agree-\\nment with the pattern in Figure 20.9is evident.\\nPettitt and Bin Daud503suggest scaling the partial residuals by the infor-\\nmation matrix components. They also propose a score test for PH based on\\nthe Schoenfeld residuals. Grambsch and Therneau233found that the Pettitt–\\nBin Daud standardization is sometimes misleading in that non-PH in one\\nvariable may cause the residual plot for another variable to display non-\\nPH. The Grambsch–Therneau weighted residual solves this problem and also\\nyields a residual that is on the same scale as the log relative hazard ratio.\\nTheir residual is\\nˆβ+dRˆV, (20.23)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e6c0c8f7-3a44-402a-9866-1fb5a8991d0d', embedding=None, metadata={'page_label': '499', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.6 Assessment of Model Fit 499\\ntScaled Schoenfeld Residualloess Smoother, span=0.5, 0.95 C.L.\\nSuper Smoother\\n02468 1 0 12-0.1 0.0 0.1 0.2\\nFig. 20.10 Smoothed weighted233Schoenfeld557residuals for the same data in Fig-\\nure20.9. Test for PH based on the correlation ( ρ) between the individual weighted\\nSchoenfeld residuals and the rank of failure time yielded ρ=−0.23,z=−6.73,P=\\n2×10−11.\\nwheredis the total number of events, Ris then×pmatrix of Schoenfeld\\nresiduals, and ˆVis the estimated covariance matrix for ˆβ. This new residual\\ncan also be the basis for tests for PH, by correlating a user-speciﬁed function\\nof unique failure times with the weighted residuals. 8\\nTheresidualplotiscomputationallyveryattractivesincethescoreresidual\\ncomponents are byproducts of Cox maximum likelihood estimation. Another\\nattractive feature is the lack of need to categorize the time axis. Unless ap-\\nproximate conﬁdence intervals are derived from smoothing techniques, a lack\\nof conﬁdence intervals from most software is one disadvantageof the method.\\n9\\nFormal tests for PH can be based on time-stratiﬁed Cox regression esti-\\nmates.27,266Alternatively,morecomplex(andprobablymoreeﬃcient)formal\\ntests for PH can be derived by specifying a form for the time by predictor in-\\nteraction (using what is calleda time-dependent covariablein the Coxmodel)\\nand testing coeﬃcients of such interactions for signiﬁcance. The obsolete Ver-\\nsion 5 SAS PHGLMprocedure used a computationally fast procedure based on\\nan approximate score statistic that tests for linear correlation between the\\nrank order of the failure times in the sample and Schoenfeld’s partial resid-\\nuals.258,266This test is available in R(for both weighted and unweighted 10\\nresiduals) using Therneau’s cox.zphfunction in the survival package. For the\\nresults in Figure 20.10, the test for PH is highly signiﬁcant (correlation coef-\\nﬁcient = −0.23,normal deviate z=−6.73).Since there is only one regression\\nparameter, the weighted residuals are a constant multiple of the unweighted\\nones, and have the same correlation coeﬃcient. 11', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4c6eb23d-a0ca-4dd6-9c71-2be37ad7c7bb', embedding=None, metadata={'page_label': '500', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='500 20 Cox Proportional Hazards Regression Model\\nTable 20.7 Time-speciﬁc hazard ratio estimates of squamous cell cancer eﬀect in VA\\nlung cancer data, by ﬁtting two Weibull distributions with unequal shape parameters\\ntlog Hazard\\nRatio\\n10−0.36\\n36−0.64\\n83.5−0.83\\n200−1.02\\nAnother method for checking the PH assumption which is especially ap-\\nplicable to a polytomous predictor involves taking ratios of parametrically\\nestimated hazard functions estimated sep arately for each level of the predic-\\ntor. For example, suppose that a risk factor Xis either present ( X=1 )o r\\nabsent (X= 0), and suppose that separate Weibull distributions adequately\\nﬁt the survival pattern of each group. If there are no other predictors to ad-\\njust for, deﬁne the hazard function for X=0a sαγtγ−1and the hazard for\\nX=1a sδθtθ−1.T h eX=1:X= 0 hazard ratio is\\nαγtγ−1\\nδθtθ−1=αγ\\nδθtγ−θ. (20.24)\\nThe hazard ratio is constant if the two Weibull shape parameters ( γandθ)\\nare equal. These Weibull parameters can be estimated separately and a Wald\\ntest statistic of H0:γ=θcan be computed by dividing the square of their\\ndiﬀerence by the sum of the squares of their estimated standard errors, or\\nbetter by a likelihood ratio test. A plot of the estimate of the hazard ratio 12\\nabove as a function of tmay also be informative.\\nIn the VA lung cancer data, the MLEs of the Weibull shape parameters\\nfor squamous cell cancer is 0.77 and for the combined small + adeno is 0.99.\\nEstimates of the reciprocals of these parameters, provided by some software\\npackages, are 1.293 and 1.012 with respective standard errors of 0.183 and\\n0.0912. A Wald test for diﬀerences in these reciprocals provides a rough test\\nfor a diﬀerence in the shape estimates. The Wald χ2is 1.89 with 1 d.f. indi-\\ncating slight evidence for non-PH.\\nThe ﬁtted Weibull hazard function for squamous cell cancer is .0167t0.23\\nand for adeno + small is 0 .0144t−0.01. The estimated hazard ratio is then\\n1.16t−0.22and the log hazard ratio is 0 .148−0.22logt. By evaluating this\\nWeibull log hazard ratio at interval midpoints (arbitrarily using t= 200\\nfor the last (open) interval) we obtain log hazard ratios that are in good\\nagreementwiththoseobtainedbytime-stratifyingtheCoxmodel(Table 20.5)\\nas shown in Table 20.7.\\nThere are many methods of assessing PH using time-dependent covari-\\nables in the Cox model.226,583Gray237,238mentions a ﬂexible and eﬃcient\\nmethod of estimating the hazard ratio function using time-dependent covari-\\nablesthatare X×splineterminteractions.Gray’smethodusesB-splinesand', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7d5b2535-2d43-476d-9cdc-e8479ffaaf3a', embedding=None, metadata={'page_label': '501', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.7 What to Do When PH Fails 501\\nrequires one to maximize a penalized log-likelihood function. Verweij and van\\nHouwelingen641developed a more nonparametric version of this approach.\\nHess289uses simple restricted cubic splines to model the time-dependent co-\\nvariable eﬀects (see also [4,287,398,498]). Suppose that k=4k n o t sa r eu s e d\\nand that a covariable Xis already transformed correctly. The model is\\nlogλ(t|X) = logλ(t)+β1X+β2Xt+β3Xt′+β4Xt′′,(20.25)\\nwheret′,t′′are constructed spline variables (Equation 2.25). TheX+1:X\\nlog hazard ratio function is estimated by\\nˆβ1+ˆβ2t+ˆβ3t′+ˆβ4t′′. (20.26)\\nThis method can be generalized to allow for simultaneous estimation of the\\nshape of the Xeﬀect and X×tinteraction using spline surfaces in ( X,t)\\ninstead of ( X1,X2) (Section 2.7.2). 13\\nTable20.8summarizes many facets of verifying assumptions for PH mod-\\nels. The trade-oﬀs of the various methods for assessing proportional hazards\\nare given in Table 20.9. 14\\n2 0 . 7W h a tt oD oW h e nP HF a i l s\\nWhen a factor violates the PH assumption and a test of association is not\\nneeded, the factor can be adjusted for through stratiﬁcation as mentioned\\nearlier. This is especially attractive if the factor is categorical.For continuous\\npredictors, one may want to stratify into quantile groups. The continuous\\nversion of the predictor can still be adjusted for as a covariable to account\\nfor any residual linearity within strata.\\nWhen a test of signiﬁcance is needed and the P-value is impressive, the\\n“principle of conservatism” could be invoked, as the P-value would likely\\nhave been more impressive had the factor been modeled correctly. Predicted\\nsurvival probabilities using this approach will be erroneous in certain time\\nintervals.\\nAn eﬃcient test of association can be done using time-dependent covari-\\nables [444, pp. 208–217]. For example, in the model\\nλ(t|X)=λ0(t)exp(β1X+β2X×log(t+1)) (20.27)\\none tests H0:β1=β2= 0 with 2 d.f. This is similar to the approach used\\nby[72]. Stratiﬁcation on time intervals can also be used:27,226,266\\nλ(t|X)=λ0(t)exp(β1X+β2X×[t>c]). (20.28)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b00c3775-65c6-4503-9887-469e7a00405f', embedding=None, metadata={'page_label': '502', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='502 20 Cox Proportional Hazards Regression Model\\nTable 20.8 Assumptions of the Proportional Hazards Model\\nVariables Assumptions Veriﬁcation\\nResponse Variable T\\nTime Until EventShape of λ(t|X)f o rﬁ x e d X\\nast↑\\nCox: none\\nWeibull: tθShape of SKM(t)\\nInteraction Between X\\nandTProportional hazards—eﬀect\\nofXdoes not depend on T\\n(e.g., treatment eﬀect is con-\\nstant over time)•Categorical X:\\ncheck parallelism of strati-\\nﬁed log[ −logS(t)] plots as\\nt↑\\n•Muenz467cum. hazard ra-\\ntio plots\\n•Arjas29cum. hazard plots\\n•Check agreement of strati-\\nﬁed and modeled estimates\\n•Hazard ratio plots\\n•Smoothed Schoenfeld resid-\\nual plots and correlation\\ntest (time vs. residual)\\n•Test time-dependent co-\\nvariable such as X×log(t+\\n1)\\n•Ratio of parametrically es-\\ntimated λ(t)\\nIndividual Predictors XShape of λ(t|X)f o rﬁ x e d tas\\nX↑\\nLinear:\\nlogλ(t|X)=l o gλ(t)+βX\\nNonlinear: log λ(t|X)=\\nlogλ(t)+f(X)•k-level ordinal X: linear\\nterm + k−2 dummy vari-\\nables\\n•Continuous X: polynom-\\nials, spline functions,\\nsmoothed martingale\\nresidual plots\\nInteraction Between X1\\nandX2Additive eﬀects: eﬀect of X1\\non logλis independent of X2\\nand vice versaTest nonadditive terms (e.g.,\\nproducts)\\nIf this step-function model holds, and if a suﬃcient number of subjects have\\nlate follow-up, you can also ﬁt a model for early outcomes and a separate\\none for late outcomes using interval-speciﬁc censoring as discussed in Section\\n20.6.2. The dual model approach provides easy to interpret models, assuming\\nthat proportional hazards is satisﬁed within each interval.\\nKronborgandAaby367andDabrowskaetal.143providetestsfordiﬀerences\\ninΛ(t)a ts p e c i ﬁ c tbased on stratiﬁed PH models. These can also be used\\nto test for treatment eﬀects when PH is violated for treatment but not for', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d766cf0d-71b7-4ae9-9161-3c336044c185', embedding=None, metadata={'page_label': '503', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.8 Collinearity 503\\nadjustment variables. Diﬀerences in mean restricted life length (diﬀerences in\\nareas under survival curves up to a ﬁxed ﬁnite time) can also be useful for\\ncomparing therapies when PH fails.335\\nTable 20.9 Comparison of methods for checking the proportional hazards assump-\\ntion and for allowing for non-proportional hazards\\nMethod Requires Requires Computa- Yields Yields Requires Must Choose\\nGrouping Grouping tional Formal Estimate of Fitting 2 Smoothing\\nX tEﬃciency Test λ2(t)/λ1(t) Models Parameter\\nlog[−log],\\nMuenz,xx x\\nArjas plots\\nDabrowska\\nlogˆΛxx xx\\ndiﬀerence\\nplots\\nStratiﬁed vs. xx x\\nModeled\\nEstimates\\nHazard ratio\\nplotx? x x ?\\nSchoenfeld\\nresidualxx x\\nplot\\nSchoenfeld\\nresidualxx\\ncorrelation\\ntest\\nFit time-\\ndependentxx\\ncovariables\\nRatio of\\nparametricxx x x x\\nestimates\\nofλ(t)\\nParametric models that assume an eﬀect other than PH, for example, the\\nlog-logistic model,226can be used to allow a predictor to have a constantly\\nincreasing or decreasing eﬀect over time. If one predictor satisﬁes PH but\\nanother does not, this approach will not work. 15\\n20.8 Collinearity\\nSee Section 4.6for the general approach using variance inﬂation factors.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5f893258-1a40-4e7b-b042-eec6e056ad6c', embedding=None, metadata={'page_label': '504', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='504 20 Cox Proportional Hazards Regression Model\\n20.9 Overly Inﬂuential Observations\\nTherneau et al.605describe the use of score residuals for assessinginﬂuence in\\nCoxand relatedregressionmodels.Theyshowthat the inﬁnitesimal jackknife\\nestimate of the inﬂuence of observation ionβequalsVs′,w h e r eVis the\\nestimated variance–covariancematrix of the pregression estimates bands=\\n(si1,si2,...,s ip)isthevectorofscoreresidualsforthe pregressioncoeﬃcients\\nfor theith observation. Let Sn×pdenote the matrix of score residuals over\\nall observations. Then an approximation to the unstandardized change in b\\n(DFBETA) is SV. Standardizing by the standard errors of bfound from the\\ndiagonals of V,e=(V11,V22,...,V pp)1/2, yields\\nDFBETAS = SVDiag(e)−1, (20.29)\\nwhere Diag( e) is a diagonal matrix containing the estimated standard errors.\\nAs discussed in Section 20.13, identiﬁcation of overly inﬂuential observa-\\ntions is facilitated by printing, for each predictor, the list of observations\\ncontaining DFBETAS >ufor any parameter associated with that predictor.\\nThe choice of cutoﬀ udepends on the sample size among other things. A\\ntypical choice might be u=0.2 indicating a change in a regression coeﬃcient\\nof 0.2 standard errors.\\n20.10 Quantifying Predictive Ability\\nTo obtain a unitless measure of predictive ability for a Cox PH model we\\ncan use the Rindex described in Section 9.8.3, which is the square root of\\nthe fraction of log likelihood explained by the model of the log likelihood\\nthat could be explained by a perfect model, penalized for the complexity of\\nthe model. The lowest (best) possible −2 log likelihood for the Cox model is\\nzero, which occurs when the predictors can perfectly rank order the survival\\ntimes. Therefore, as was the case with the logistic model, the quantity L∗\\nfrom Section 9.8.3is zero and an Rindex that is penalized for the number of\\nparameters in the model is given by\\nR2=( L R−2p)/L0, (20.30)\\nwherepisthe numberofparametersestimatedand L0isthe−2loglikelihood\\nwhenβis restricted to be zero (i.e., there are no predictors in the model). R\\nwill be near one for a perfectly predictive model and near zero for a model\\nthat does not discriminate between short and long survival times. The R\\nindex does not take into account any stratiﬁcation factors. If stratiﬁcation\\nfactorsarepresent, Rwillbenearoneifsurvivaltimescanbeperfectlyranked\\nwithin strata even though there is overlap between strata.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f0d6c6c5-d77f-4e90-8f2b-e15704bf0dc5', embedding=None, metadata={'page_label': '505', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.10 Quantifying Predictive Ability 505\\nSchemper546and Korn and Simon365have reported that R2is too sen-\\nsitive to the distribution of censoring times and have suggested alterna-\\ntives based on the distance between estimated Cox survival probabilities\\n(using predictors) and Kaplan–Meier e stimates (ignoring predictors). Kent\\nand O’Quigley345also report problems with R2and suggest a more complex\\nmeasure. Schemper548investigated the Maddala–Magee431,432indexR2\\nLRde-\\nscribed in Section 9.8.3, applied to Cox regression:\\nR2\\nLR=1−exp(−LR/n)\\n=1−ω2/n, (20.31)\\nwhereωis the null model likelihood divided by the ﬁtted model likelihood.\\nFor many situations, R2\\nLRperformed as well as Schemper’s more complex\\nmeasure546,549and hence it is preferred because of its ease of calculation\\n(assuming that PH holds). Ironically, Schemper548demonstrated that the n\\nin the formula for this index is the total number of observations, not the\\nnumber of events (but see O’Quigley, Xu, and Stare481). To make the R2\\nindex have a maximum value of 1.0, we use the Nagelkerke471R2\\nNdiscussed\\nin Section 9.8.3. 16\\nAn easily interpretable index of discrimination for survival models is de-\\nrived from Kendall’s τand Somers’ Dxyrank correlation,579the Gehan–\\nWilcoxon statistic for comparing two samples for survival diﬀerences, and\\nthe Brown–Hollander–Korwarnonparametric test of association for censored\\ndata.76,170,262,268Thisindex, c, isageneralizationofthe areaunderthe ROC\\ncurve discussed under the logistic model, in that it applies to a continuous\\nresponse variable that can be censored. The cindex is the proportion of all\\npairs of subjects whose survival time can be ordered such that the subject\\nwith the higher predicted survival is the one who survived longer. Two sub-\\njects’ survival times cannot be ordered if both subjects are censored or if one\\nhas failed and the follow-up time of the other is less than the failure time\\nof the ﬁrst. The cindex is a probability of concordance between predicted\\nand observed survival, with c=0.5 for random predictions and c=1f o ra\\nperfectly discriminating model. The cindex is mildly aﬀected by the amount\\nof censoring. Dxyis obtained from 2( c−0.5). While c(andDxy) is a good\\nmeasure of pure discrimination ability of a single model, it is not sensitive\\nenough to allow multiple models to be compared447. 17\\nSince high hazard means short survival time, when the linear predictor\\nXˆβfrom a Cox model is compared with observed survival time, Dxywill be\\nnegative. Some analysts may want to negate reported values of Dxy.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='334298b3-a688-4678-a32d-c119cdc4a7fe', embedding=None, metadata={'page_label': '506', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='506 20 Cox Proportional Hazards Regression Model\\n20.11 Validating the Fitted Model\\nSeparate bootstrap or cross-validation assessments can be made for calibra-\\ntion and discrimination of Cox model survival and log relative hazard esti-\\nmates. 18\\n20.11.1 Validation of Model Calibration\\nOne approach to validation of the calibration of predictions is to obtain un-\\nbiased estimates of the diﬀerence between Cox predicted and Kaplan–Meier\\nsurvival estimates at a ﬁxed time u. Here is one sequence of steps.\\n1. Obtain cutpoints (e.g., deciles) of predicted survival at time uso as to\\nhave a given number of subjects (e.g., 50) in each interval of predicted\\nsurvival. These cutpoints are based on the distribution of ˆS(u|X)i nt h e\\nwholesampleforthe“ﬁnal”model(fordata-splitting,insteadusethemodel\\ndeveloped in the training sample). Let kdenote the number of intervals\\nused.\\n2. Compute the average ˆS(u|X)i ne a c hi n t e r v a l .\\n3. Compare this with the Kaplan–Meier survival estimates at time u,s t r a t -\\niﬁed by intervals of ˆS(u|X). Let the diﬀerences be denoted by d=\\n(d1,...,d k).\\n4. Use bootstrapping or cross-validation to estimate the overoptimism in d\\nand then to correct dto get a more fair assessment of these diﬀerences.\\nFor each repetition, repeat any stepwise variable selection or stagewise\\nsigniﬁcance testing using the same stopping rules as were used to derive\\nthe“ﬁnal”model. No more than B= 200 replications are needed to obtain\\naccurate estimates.\\n5. If desired, the bias-corrected dcan be added to the original stratiﬁed\\nKaplan–Meier estimates to obtain a bias-corrected calibration curve.\\nHowever , anystatisticalmethodthat usesbinningofcontinuousvariables\\n(here, the predicted risk), is arbitrary and has lower precision than smooth\\nestimates that allow for interpolation. A far better approach to estimating\\ncalibration curves for survival models is to use the ﬂexible adaptive hazard\\nregression approach of Kooperberg et al.361as discussed on P. 450.T h e i r\\nmethod does not assume linearity or proportional hazards. Hazard regres-\\nsion can be used to estimate the relationship between (suitably transformed)\\npredicted survival probabilities and observed outcomes, i.e., to derive a cali-\\nbration curve. The bootstrap is used to de-bias the estimates to correct for\\noverﬁtting, allowing estimation of the likely future calibration performance\\nof the ﬁtted model.\\nAs an example, consider a dataset of 20 random uniformly distributed\\npredictors for a sample of size 200. Let the failure time be another random', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='845ad055-8fd4-4a6a-8d1c-83794f0b5207', embedding=None, metadata={'page_label': '507', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.11 Validating the Fitted Model 507\\nuniform variable that is independent of allthe predictors, and censor half of\\nthe failure times at random. Due to ﬁtting 20 predictors to 100 events, there\\nwill apparently be fair agreement between predicted and observed survival\\nover all strata (smooth black curve from hazard regression in Figure 20.11).\\nHowever, the bias-corrected calibration ( blue curve from hazard regression)\\ngives a more truthful answer: examining the Xs across levels of predicted\\nsurvivaldemonstratethat predicted and observedsurvivalareweeklyrelated,\\nin more agreement with how the data were generated. For the more arbitrary\\nKaplan-Meierapproach,we divide the observationsinto quintiles ofpredicted\\n0.5-year survival, so that there are 40 observations per stratum.\\nn←200\\np←20\\nset.seed(6)\\nxx←matrix(rnorm(n * p), nrow=n, ncol=p)\\ny←runif(n)\\nunits(y) ←\"Year\"\\ne←c(rep(0, n / 2), rep(1, n / 2))\\nf←cph(Surv(y, e) ∼xx, x=TRUE, y=TRUE,\\ntime.inc=.5, surv=TRUE)\\ncal←calibrate(f, u=.5, B =200)\\nUsing Cox survival estimates at 0.5 Years\\nplot(cal, ylim =c(.4, 1), subtitles= FALSE)\\ncalkm ←calibrate(f, u=.5, m=40, cmethod= \\'KM\\', B=200)\\nUsing Cox survival estimates at 0.5 Years\\nplot(calkm, add= TRUE) # Figure 20.11\\n20.11.2 Validation of Discrimination and Other\\nStatistical Indexes\\nHere bootstrapping and cross-validation are used as for logistic models (Sec-\\ntion10.9). We can obtain bootstrap bias-corrected estimates of cor equiv-\\nalentlyDxy. To instead obtain a measure of relative calibration or slope\\nshrinkage, we can bootstrap the apparent estimate of γ=1i nt h em od e l\\nλ(t|X)=λ(t)exp(γXb). (20.32)\\nBesides being a measure of calibration in itself, the bootstrap estimate of\\nγalso leads to an unreliability index Uwhich measures how far the model\\nmaximum log likelihood (which allows for an overall slope correction) is from\\ntheloglikelihoodevaluatedat“frozen”regressioncoeﬃcients( γ=1 )( s e e [267]\\nand Section 10.9).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fdf43e8d-6800-404e-b054-2c82067669b6', embedding=None, metadata={'page_label': '508', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='508 20 Cox Proportional Hazards Regression Model\\n0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.850.40.50.60.70.80.91.0\\nPredicted  0.5 Year SurvivalFraction Surviving 0.5 Year\\nFig. 20.11 Calibration of random predictions using Efron’s bootstrap with B= 200\\nresamples. Dataset has n= 200, 100 uncensored observations, 20 random predic-\\ntors, model χ2\\n20= 19. The smooth black line is the apparent calibration estimated\\nby adaptive linear spline hazard regression361, and the blue line is the bootstrap\\nbias– (overﬁtting–) corrected calibration curve estimated also by hazard regression.\\nThe gray scale line is the line of identity representing perfect calibration. Black dot s\\nrepresent apparent calibration accuracy obtained by stratifying into intervals of pre-\\ndicted 0.5y survival containing 40 events per interval and plotting the mean predicted\\nvalue within the interval against the stratum’s Kaplan-Meier estimate. The bl ue×\\nrepresent bootstrap bias-corrected Kaplan-Meier estimates.\\nU=LR(ˆγXb)−LR(Xb)\\nL0, (20.33)\\nwhereL0is the−2 log likelihood for the null model (Section 9.8.3). Similarly,\\na discrimination index D267can be derived from the −2 log likelihood at the\\nshrunken linear predictor, penalized for estimating one parameter ( γ)( s e e\\nalso [633, p. 1318] and [123]):\\nD=LR(ˆγXb)−1\\nL0. (20.34)\\nDi st h es a m ea s R2discussed above when p= 1 (indicating only one reesti-\\nmated parameter, γ), the penalized proportion of explainable log likelihood\\nthat was explained by the model. Because of the remark of Schemper,546all\\nof these indexes may unfortunately be functions of the censoring pattern.\\nAn index ofoverallquality that penalizes discrimination for unreliability is\\nQ=D−U=LR(Xb)−1\\nL0. (20.35)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fe64946d-9e78-4603-9aef-1664649fc3b0', embedding=None, metadata={'page_label': '509', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"20.12 Describing the Fitted Model 509\\nQis a normalized and penalized −2 log likelihood that is evaluated at the\\nuncorrected linear predictor.\\nFor the random predictions used in Figure 20.11, the bootstrap estimates\\nwithB= 200 resamples are found in Table 20.10.\\nlatex(validate(f, B =200), digits=3, file= '', caption= '',\\ntable.env= TRUE,label= 'tab:cox-val-random ')\\nTable 20.10 Bootstrap validation of a Cox model with random predictors\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy0.213 0.335 0.147 0 .188 0 .025 200\\nR20.092 0.191 0.042 0 .150−0.058 200\\nSlope 1 .000 1.000 0.389 0 .611 0 .389 200\\nD 0.021 0.048 0.009 0 .039−0.019 200\\nU−0.002−0.002 0.028−0.031 0 .028 200\\nQ 0.023 0.050−0.020 0 .070−0.047 200\\ng 0.516 0.878 0.339 0 .539−0.023 200\\nIt can be seen that the apparent correlation ( Dxy=−0.21) does not hold\\nup after correcting for overﬁtting ( Dxy=−0.02). Also, the slope shrinkage\\n(0.39) indicates extreme overﬁtting.\\nSee [633, Section 6] and [640]and Section 18.3.7for still more useful meth-\\nods for validating the Cox model.\\n20.12 Describing the Fitted Model\\nAs with logistic modeling, once a Cox PH model has been ﬁtted and all\\nits assumptions veriﬁed, the ﬁnal model needs to be presented and inter-\\npreted. The fastest way to describe the model is to interpret each eﬀect in\\nit. For each predictor the change in log hazard per desired units of change\\nin the predictor value may be computed, or the antilog of this quantity,\\nexp(βj×change in Xj), may be used to estimate the hazard ratio holding\\nallotherfactorsconstant.When Xjisanonlinearfactor,changesinpredicted\\nXβfor sensible values of Xjsuch as quartiles can be used as described in\\nSection10.10. Of course for nonmodeled stratiﬁcation factors, this method is\\nof no help. Figure 20.12depicts a way to display estimated surgical : medical\\nhazard ratios in the presence of a signiﬁcant treatment by disease severity\\ninteraction and a secular trend in the beneﬁt of surgical therapy (treatment\\nby year of diagnosis interaction).\\nOften, the use of predicted survival probabilities may make the model\\nmore interpretable. If the eﬀect of only one factor is being displayed and\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='dc4fc209-9ec2-465a-9016-931df06755cb', embedding=None, metadata={'page_label': '510', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='510 20 Cox Proportional Hazards Regression Model\\n95% Left Main75% Left Main3−Vessel Disease2−Vessel Disease1−Vessel Disease\\n95% Left Main75% Left Main3−Vessel Disease2−Vessel Disease1−Vessel Disease\\n95% Left Main75% Left Main3−Vessel Disease2−Vessel Disease1−Vessel Disease1970 1977 1984\\n0.125 0.25 0.5 1.0 1.5 2.0 2.5\\nHazard Ratio\\nFig. 20.12 A display of an interaction between treatment and extent of disease, and\\nbetween treatment and calendar yearofstartof treatment. Comparison ofmedical and\\nsurgical average hazard ratios for patients treated in 1970, 1977, and 1984 according\\nto coronary disease severity. Circles represent point estimates; bars represent 0 .95\\nconﬁdence limits of hazard ratios. Ratios less than 1.0 indicate that coronar y bypass\\nsurgery is more eﬀective.88\\nthat factor is polytomous or predictions are made for speciﬁc levels, survival\\ncurves(with orwithoutadjustmentforotherfactorsnotshown)canbedrawn\\nfor each level of the predictor of interest, with follow-up time on the x-axis.\\nFigure20.2demonstrated this for a factor which was a stratiﬁcation factor.\\nFigure20.13extends this by displaying survival estimates stratiﬁed by treat-\\nment but adjusted to variouslevels of two modeled factors,one of which, year\\nof diagnosis, interacted with treatment.\\nWhen a continuous predictor is of interest, it is usually more informative\\nto display that factor on the x-axis with estimated survival at one or more\\ntime points on the y-axis. When the model contains only one predictor, even\\nif that predictor is represented by multiple terms such as a spline expansion,\\none may simply plot that factor against the predicted survival. Figure 20.14\\ndepicts the relationship between treadmill exercise score, which is a weighted\\nlinear combination of several predictors in a Cox model, and the probability\\nof surviving ﬁve years.\\nWhen displaying the eﬀect of a single factor after adjusting for multiple\\npredictors which are not displayed, care only need be taken for the values\\nto which the predictors are adjusted (e.g., grand means). When instead the\\ndesire is to display the eﬀect of multiple predictors simultaneously, an im-\\nportant continuous predictor can be displayed on the x-axis while separate', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5d93b32c-cc17-4a68-9be7-bcf5737a8282', embedding=None, metadata={'page_label': '511', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.12 Describing the Fitted Model 511\\ncurves or graphs are made for levels of other factors. Figure 20.15,w h i c h\\ncorresponds to the log Λplots in Figure 20.5, displays the joint eﬀects of age\\nand sex on the three-year survival probability. Age is modeled with a cubic\\nspline function, and the model includes terms for an age ×sex interaction.\\np←Predict( f.ia, age, sex, time=3)\\nggplot(p)\\n1970 1977 1984\\n0.000.250.500.751.00\\n0.000.250.500.751.00LVEF=0.4 LVEF=0.6\\n012345 012345 012345\\nYears of FollowupSurvival ProbabilityTreatment\\nSurgical\\nMedical\\nFig. 20.13 Cox–Kalbﬂeisch–Prentice survivalestimates stratifyingon treatment and\\nadjusting for several predictors, showing a secular trend in the eﬃcacy of coronary\\nartery bypass surgery. Estimates are for patients with left main disease and normal\\n(LVEF=0.6) or impaired (LVEF=0.4) ventricular function.516\\nBesides making graphs of survival probabilities estimated for given levels\\nof the predictors, nomograms have some utility in specifying a ﬁtted Cox\\nmodel.Anomogramcanbeusedtocompute Xˆβ,theestimatedloghazardfor\\na subject with a set of predictor values Xrelative to the“standard”subject.\\nThe centrallineinthenomogramwill beonthislinearscaleunlikethelogistic\\nmodel nomograms given in Section 10.10which further transformed Xˆβinto\\n[1 +exp( −Xˆβ)]−1. Alternatively, the central line could be on the nonlinear\\nexp(Xˆβ) hazard ratio scale or survival at ﬁxed t. 19\\nA graph of the estimated underlying survival function ˆS(t) as a function\\noftcan be coupled with the nomogram used to compute Xˆβ.T h es u r v i v a l\\nfor a speciﬁc subject, ˆS(t|X) is obtained from ˆS(t)exp(Xˆβ). Alternatively, one\\ncould graph ˆS(t)exp(Xˆβ)for various values of Xˆβ(e.g.,Xˆβ=−2,−1,0,1,2)', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d4b8ab97-a3c6-4ac1-9cb8-42d1a07bca88', embedding=None, metadata={'page_label': '512', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='512 20 Cox Proportional Hazards Regression Model\\n0.50.60.70.80.91.0\\nTreadmill Score−20−15−10−50 5 10 155−Year Survival Probability\\nFig. 20.14 Cox model predictions with respect to a continuous variable. X-axis\\nshows the range of the treadmill score seen in clinical practice and Y-axis shows the\\ncorresponding ﬁve-year survival probability predicted by the Cox regression model\\nfor the 2842 study patients.440\\n0.50.60.70.80.91.0\\n20 40 60 80\\nAge3 Year Survival Probabilitysex\\nMale\\nFemale\\nFig. 20.15 Survival estimates for model stratiﬁed on sex, with interaction.\\nsothatthedesiredsurvivalcurvecouldbereaddirectly,atleasttothe nearest\\ntabulated Xˆβ. For estimating survivalata ﬁxedtime, say two years,one only\\nneed to provide the constant ˆS(t). The nomogram could even be adapted to\\ninclude a nonlinear scale ˆS(2)exp(Xˆβ)to allow direct computation of two-year\\nsurvival.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4baa7f58-f727-46f8-bf82-68b9b720ff23', embedding=None, metadata={'page_label': '513', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.13RFunctions 513\\n20.13RFunctions\\nHarrell’s cpower,spower,a n dciapower (in theHmiscpackage) perform power\\ncalculations for Cox tests in follow-up studies. cpowercomputes power for\\na two-sample Cox (log-rank) test with random patient entry over a ﬁxed\\nduration and a given length of minimum follow-up. The expected number of\\nevents in each group is estimated by assuming exponential survival. cpower\\nuses a slight modiﬁcation of the method of Schoenfeld558(see[501]). Separate\\nspeciﬁcationofnoncomplianceintheactivetreatmentarmand“drop-in”from\\nthe control arm into the active arm are allowed, using the method of Lachin\\nand Foulkes.370Theciapower function computes power of the Cox interaction\\ntest in a 2 ×2 setup using the method of Peterson and George.501It does\\nnot take noncompliance into account. The spowerfunction simulates power\\nfor two-sample tests (the log-rank test by default) allowing for very complex\\nconditions such as continuously varying treatment eﬀect and noncompliance\\nprobabilities.\\nThermspackage cphfunction is a slight modiﬁcation of the coxphfunc-\\ntion written by Terry Therneau (in his survival package to work in the rms\\nframework. cphcomputes MLEs of Cox and stratiﬁed Cox PH models, overall\\nscoreandlikelihoodratio χ2statisticsforthemodel, martingaleresiduals,the\\nlinear predictor ( Xˆβcentered to have mean 0), and collinearity diagnostics.\\nEfron, Breslow, and exact partial likelihoods are supported (although the\\nexact likelihood is very computationally intensive if ties are frequent). The\\nfunction also ﬁts the Andersen–Gill23generalization of the Cox PH model.\\nThis model allows for predictor values to change over time in the form of step\\nfunctions aswell asallowingtime-dependent stratiﬁcation(subjectscan jump\\nto diﬀerent hazard function shapes). The Andersen–Gill formulation allows\\nmultiple eventsper subject and permits subjects to move in and out of risk at\\nany desired time points. The latter feature allows time zero to have a more\\ngeneral deﬁnition. (See Section 9.5for methods of adjusting the variance–\\ncovariance matrix of ˆβfor dependence in the events per subject.) The print-\\ning function corresponding to cphprints the Nagelkerke index R2\\nNdescribed\\nin Section 20.10, and has a latexoption for better output. cphworks in con-\\njunction with the generic functions such as specs, predict, summary, anova,\\nfastbw, which.influence, latex, residuals, coef, nomogram ,a n dPredictde-\\ns c r i b e di nS e c t i o n 20.13,the same as the logistic regressionfunction lrmdoes.\\nFor the purpose of plotting predicted survival at a single time, Predicthas an\\nadditional argument timefor plotting cphﬁts. It also has an argument loglog\\nwhich if TRUEcauses instead log-log survival to be plotted on the y-axis.cph\\nhas all the arguments described in Section 20.13and some that are speciﬁc\\nto it.\\nSimilartofunctionsfor psm,t h e reare Survival,Quantile,a n dMeanfunctions\\nwhich create other Rfunctions to evaluate survival probabilities and perform\\nother calculations, based on a cphﬁt with surv=TRUE . These functions, un-\\nlike all the others, allow polygon (linear interpolation) estimation of survival', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67abbed2-10a5-4a84-9407-c242dff37950', embedding=None, metadata={'page_label': '514', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"514 20 Cox Proportional Hazards Regression Model\\nprobabilities, quantiles, and mean survival time as an option. Quantile is the\\nonly automatic way for obtaining survival quantiles with cph. Quantile esti-\\nmates will be missing when the survival curve does not extend long enough.\\nLikewise, survival estimates will be missing for t>maximum follow-up time,\\nwhen the last event time is censored. Meancomputes the mean survival time\\nif the last failure time in each stratum is uncensored. Otherwise, Meanmay\\nbe used to compute restricted mean lifetime using a user-speciﬁed trunca-\\ntion point.334Quantile andMeanare especially useful with plotandnomogram.\\nSurvival is useful with nomogram.\\nTheRprogrambelow demonstrateshowseveral cph-related functions work\\nwell with the nomogram function. Here predicted three-year survival probabil-\\nities and median survival time (when deﬁned) are displayed against age and\\nsex from the previously simulated dataset. The fact that a nonlinear eﬀect\\ninteracts with a stratiﬁed factor is taken into account.\\nsurv ←Survival( f.ia)\\nsurv.f ←function(lp) surv(3, lp, stratum= 'sex=Female ')\\nsurv.m ←function(lp) surv(3, lp, stratum= 'sex=Male ')\\nquant ←Quantile( f.ia)\\nmed.f ←function(lp) quant(.5, lp, stratum= 'sex=Female ')\\nmed.m ←function(lp) quant(.5, lp, stratum= 'sex=Male ')\\nat.surv ←c(.01, .05, seq(.1,.9,by=.1), .95, .98, .99, .999)\\nat.med ←c(0, .5, 1, 1.5, seq(2, 14, by=2))\\nn←nomogram( f.ia, fun= list(surv.m, surv.f, med.m,med.f),\\nfunlabel=c( 'S(3 | Male) ','S(3 | Female) ',\\n'Median (Male) ','Median (Female) '),\\nfun.at=list(c(.8,.9,.95,.98,.99),\\nc(.1,.3,.5,.7,.8,.9,.95,.98),\\nc(8,10,12),c(1,2,4,8 ,12)))\\nplot(n, col.grid= FALSE, lmgp=.2)\\nlatex(f.ia,file= '', digits=3)\\nProb{T≥t|sex =i}=Si(t)eXβ,where\\nXˆβ=\\n−1.8\\n+0.0493age−2.15×10−6(age−30.3)3\\n+−2.82×10−5(age−45.1)3\\n+\\n+5.18×10−5(age−54.6)3\\n+−2.15×10−5(age−69.6)3\\n+\\n+[Female][ −0.0366age+4 .29×10−5(age−30.3)3\\n+−0.00011(age −45.1)3\\n+\\n+6.74×10−5(age−54.6)3\\n+−2.32×10−7(age−69.6)3\\n+]\\nand [c] = 1 if subject is in group c,0o t h e r w i s e ; ( x)+=xifx>0,0\\notherwise.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='353f34d6-bf68-4ece-991c-b9b168f2404c', embedding=None, metadata={'page_label': '515', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.13RFunctions 515\\ntSMale(t)SFemale(t)\\n01.000 1 .000\\n10.993 0 .902\\n20.984 0 .825\\n30.975 0 .725\\n40.967 0 .648\\n50.956 0 .576\\n60.947 0 .520\\n70.938 0 .481\\n80.928 0 .432\\n90.920 0 .395\\n10 0.909 0 .358\\n11 0.904 0 .314\\n12 0.892 0 .268\\n13 0.886 0 .223\\n14 0.877 0 .203\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 0 0\\nage (sex=Male)\\n10 20 30 40 50 60 70 90\\nage (sex=Female)\\n10 30 50 60 70 80 90 100\\nTotal Points\\n0 1 02 03 04 05 06 07 08 09 0 1 0 0\\nLinear Predictor\\n−2 −1.5 −1 −0.5 0 0.5 1 1.5 2\\nS(3 | Male)\\n0.9 0.95 0.98 0.99\\nS(3 | Female)\\n0.1 0.3 0.5 0.7 0.8 0.9 0.95\\nMedian (Male)\\n1012\\nMedian (Female)\\n1 2 4 8 12\\nFig. 20.16 Nomogram from a ﬁtted stratiﬁed Cox model that allowed for interaction\\nbetween age and sex, and nonlinearity in age. The axis for median survival tim e is\\ntruncated on the left where the median is beyond the last follow-up time.\\nrcspline.plot( lvef,d.time, event=cdeath, nk=3)\\nThecorrespondingsmoothedmartingaleresidualplotforLVEFinFigure 20.7\\nwas created with', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='135d8975-5510-45c7-82a1-67906754fde2', embedding=None, metadata={'page_label': '516', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='516 20 Cox Proportional Hazards Regression Model\\ncox←cph(Surv(d.time, cdeath) ∼lvef,iter.max=0)\\nres←resid(cox)\\ng∼loess(res ∼lvef)\\nplot(g, coverage=0.95, confidence=7, xlab=\"LVEF\",\\nylab=\"Martingale Residual\")\\ng←ols(res ∼rcs(lvef ,5))\\nplot(g, lvef=NA, add=T, lty=2)\\nlines(lowess(lvef, res, iter=0), lty=3)\\nlegend(.3, 1.15, c(\"loess Fit and 0.95 Confidence Bars\",\\n\"ols Spline Fit and 0.95 Confidence Limits\",\\n\"lowess Smoother\"), lty =1:3, bty=\"n\")\\nBecause we desired residuals with respect to the omitted predictor LVEF,\\nthe parameter iter.max=0 had to be given to make cphstop the estimation\\nprocessatthestartingparameterestimates(defaultofzero).Theeﬀectofthis\\nis to ignore the predictors when computing the residuals; that is, to compute\\nresiduals from a ﬂat line rather than the usual residuals from a ﬁtted straight\\nline.\\nTheresiduals function is a slight modiﬁcation of Therneau’s residuals.-\\ncoxphfunction to obtain martingale, Schoenfeld, score, deviance residuals, or\\napproximate DFBETA or DFBETAS. Since martingale residuals are always\\nstored by cph(assuming there are covariables present), residuals merely has\\nto pick them oﬀ the ﬁt object and reinsert rows that were deleted due to\\nmissing values. For other residuals, you must have stored the design matrix\\nandSurvobject with the ﬁt by using ..., x=TRUE, y=TRUE . Storing the design\\nmatrix with x=TRUEensures that the same transformation parameters (e.g.,\\nk n o t s )a r eu s e di ne v a l u a t i n gt h em o d e la sw e r eu s e di nﬁ t t i n gi t .T ou s e\\nresiduals you can use the abbreviation resid. See the help ﬁle for residuals\\nfor an example of how martingale residuals may be used to quickly plot\\nunivariable (unadjusted) relationships for several predictors.\\nFigure20.10, which used smoothed scaled Schoenfeld partial residuals557\\nto estimate the form of a predictor’s log hazard ratio over time, was made\\nwith\\nSrv←Surv(dm.time,cdeathmi)\\ncox←cph(Srv ∼pi, x=T, y=T)\\ncox.zph(cox, \"rank\") # Test for PH for each column of X\\nres←resid(cox, \"scaledsch\")\\ntime←as.numeric(names(res))\\n# Use dimnames(res)[[1]] if more than one predictor\\nf←loess(res ∼time, span=0.50)\\nplot(f, coverage=0.95, confidence=7, xlab=\"t\",\\nylab=\"Scaled Schoenfeld Residual\", ylim=c(-.1,.25))\\nlines(supsmu(time, res),lty=2)\\nlegend(1.1,.21,c(\"loess Smoother with span=0.50 and 0.95 C.L.\",\\n\"Super Smoother\"), lty =1:2, bty=\"n\")\\nThe computation and plotting of scaled Schoenfeld residuals could have been\\ndone automatically in this case by using the single command plot(cox.zph\\n(cox)), although cox.zphdefaults to plotting againstthe Kaplan–Meiertrans-\\nformation of follow-up time.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fc357964-cfbf-42e9-aa16-b9bd65e45dcf', embedding=None, metadata={'page_label': '517', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.14 Further Reading 517\\nThehazard.ratio.plot function in rmsrepeatedly estimates Cox regression\\ncoeﬃcients and conﬁdence limits within time intervals. The log hazard ra-\\ntios are plotted against the mean failure/censoring time within the interval.\\nFigure20.9was created with\\nhazard.ratio.plot(pi, S) # S was Surv(dm.time, ...)\\nIf you have multiple degree of freedom factors, you may want to score them\\ninto linear predictors before using hazard.ratio.plot .T h epredictfunction\\nwith argument type=\"terms\" will producea matrixwith one column perfactor\\nto do this (Section 20.13).\\nTherneau’s cox.zphfunction implements Harrell’s Schoenfeld residual cor-\\nrelationtestforPH. Thisfunctionalsostoresresultsthatcaneasilybe passed\\nto a plotting method for cox.zphto automatically plot smoothed residuals\\nthat estimate the eﬀect of each predictor over time.\\nTherneau has also written an Rfunction survdiff that compares two or\\nmore survival curves using the G−ρfamily of rank tests (Harrington and\\nFleming273).\\nThercorr.cens function in the Hmisclibrary computes the cindex and the\\ncorresponding generalization of Somers’ Dxyrank correlation for a censored\\nresponse variable. rcorr.cens also works for uncensored and binary responses\\n(see ROC areain Section 10.8),althoughits useof all possiblepairings makes\\nit slow for this purpose. The survival package’s survConcordance has an ex- 20\\ntremely fast algorithm for the cindex and a fairly accurate estimator of its\\nstandard error.\\nThecalibrate function for cphconstructs a bootstrap or cross-validation\\noptimism-corrected calibration curve for a single time point by resampling\\nthe diﬀerences between averageCox predicted survival and Kaplan–Meier es-\\ntimates (see Section 20.11.1). But more precise is calibrate ’s default method\\nbased on adaptive semiparametric regression discussed in the same section.\\nFigure20.11is an example.\\nThevalidate function for cphﬁts validatesseveralstatisticsdescribingCox\\nmodel ﬁts—slope shrinkage, R2\\nN,D,U,Q,a n dDxy.T h eval.surv function\\ncan also be of use in externally validating a Cox model using the methods\\npresented in Section 18.3.7.\\n20.14 Further Reading\\n1Good general texts fortheCox PHmodel includeCox andOakes,133Kalbﬂeisch\\nand Prentice,331Lawless,382Collett,114Marubini and Valsecchi,444and Klein\\nand Moeschberger.350Therneau and Grambsch604describe the many ways the\\nstandard Cox model may be extended.\\n2Cupples et al.141and Marubini and Valsecchi [ 444, pp. 201–206] present good\\ndescription of various methods of computing“adjusted survival curves.”\\n3See Altman and Andersen15for simpler approximate formulas. Cheng et al.103\\nderived methods for obtaining pointwise and simultaneous conﬁdence bands for', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='85d374d4-bf88-44c0-961f-d0540c1137f7', embedding=None, metadata={'page_label': '518', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='518 20 Cox Proportional Hazards Regression Model\\nS(t) for future subjects, and Henderson282has a comprehensive discussion of\\nthe use of Cox models to estimate survival time for individual subjects.\\n4Aalen2and Valsecchiet al.625discussotherresidualsusefulingraphicallycheck-\\ning survival model assumptions. Le´ on and Tsai400derived residuals for estimat-\\ning covariate transformations that are diﬀerent from martingale residuals.\\n5[411] has other methods for generating conﬁdence intervals for martingale resid-\\nual plots.\\n6Lin et al.411describe other methods of checking transformations using cumu-\\nlativemartingale residuals.\\n7A parametric analysis of the VA dataset using linear splines and incorporating\\nX×tinteractions is found in [ 361].\\n8Winnett and Sasieni671show how to use scaled Schoenfeld residuals in an iter-\\native fashion to actually model eﬀects that are not in proportional hazards.\\n9See [233,503] for some methods for obtaining conﬁdence bands for Schoen-\\nfeld residual plots. Winnett and Sasieni670discuss conditions in which the\\nGrambsch–Therneau scaling of the Schoenfeld residuals does not perform ade-\\nquately for estimating β(t).\\n10[475,519] compared the power of the test for PH based on the correlation be-\\ntween failure time and Schoenfeld residuals with the power of several other\\ntests.\\n11See Lin et al.411for another approach to deriving a formal test of PH using\\nresiduals. Other graphical methods for examining the PH assumption are due\\nto Gray,236who used hazard smoothing to estimate hazard ratios as a function\\nof time, and Thaler,602who developed a nonparametric estimator of the hazard\\nratio over time for time-dependent covariables. See Valsecchi et al.625for other\\nuseful graphical assessments of PH.\\n12A related test of constancy of hazard ratios may be found in [ 519]. Also, see\\nSchemper547for related methods.\\n13See [547] for a variation of the standard Cox likelihood to allow for non-PH.\\n14An excellent review of graphical methods for assessing PH may be found in\\nHess.290. Sahoo and Sengupta537provide some new graphical methods for as-\\nsessing PH irrespective of satisfaction of the other model assumptions.\\n15Schemper547provides a way to determine the eﬀect of falsely assuming PH by\\ncomparing the Cox regression coeﬃcient with a well-described average log haz-\\nard ratio. Zucker691showshowdependentaweighted log-rank testison thetrue\\nhazard ratio function, when the weights are derived from a hypothesiz ed hazard\\nratio function. Valsecchi et al.625proposed a method that is robust to non-PH\\nthat occurs in the late follow-up period. Their method uses down-weighting of\\ncertain types of “outliers.” See Herndon and Harrell287for a ﬂexible paramet-\\nric PH model with time-dependent covariables, which uses the restricted cubic\\nspline function to specify λ(t). Putter et al.518and Muggeo and Tagliavia468\\nhave nice approaches that use time-dependent covariates to model time inter -\\nactions to allow non-proportional hazards. Perperoglou et al.498,499developed\\na systematic approach that allows one to continuously vary the amount of non\\nPH allowed, through the use of a structure matrix that connects predictors\\nwith functions of time. Schuabel et al.543have a good exposition of internal\\ntime-dependent covariates.\\n16See van Houwelingen and le Cessie [ 633, Eq.61] and Verweij and van Houwelin-\\ngen640for an interesting index of cross-validated predictive accuracy. Schemper\\nand Henderson552relate explained variation to predictive accuracy in Cox mod-\\nels. Hielscher et al.291compares and illustrates several measures of explained\\nvariation as does Choodari-Oskooei et al.106. Choodari-Oskooei et al.105stud-\\nied explained randomness and predictive accuracy measures.\\n17See similar indexes in Schemper544and a related idea in [ 633, Eq.63]. Man-\\ndel, Galai, and Simchen436presented a time-varying cindex. See Korn and', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f2f7d428-a73c-4761-bb64-5bcb5ea832a9', embedding=None, metadata={'page_label': '519', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='20.14 Further Reading 519\\nSimon,365Schemper and Stare,554and Henderson282for nice comparisons of\\nvarious measures. Pencina and D’Agostino489provide more details about the c\\nindex and derived new interval estimates. They also discussed the relationship\\nbetween cand a version of Kendall’s τ. Pencina et al.491found advantages of c.\\nUno et al.618described exactly how cdepends on the amount of censoring and\\nproposed a new index, requiring one to choose a time cutoﬀ, that is invariant t o\\nthe amount of censoring. Henderson et al.283discussed the beneﬁts of using the\\nprobability of a serious prognostication error (e.g., being oﬀ by a facto r of 2.0\\nor worse on the time scale) as an accuracy measure. Schemper550shows that\\nmodels with very important predictors can have very low absolute predicti on\\nability, and he discusses measures of predictive accuracy from a general stand-\\npoint. Lawless and Yuan386present prediction error estimators and conﬁdence\\nlimits, focusing on such measures as error in predicted median or mean survival\\ntime. Schmid and Potapov555studied the bias of several variations on the cin-\\ndex under non-proportional hazards and/or nonrandom censoring. G ¨onen and\\nHeller223developed a c-index that is censoring-independent.\\n18Altman and Royston18havea good discussionof validation of prognostic models\\nand present several examples of validation using a simple discrimination index.\\nThomas Gerds has an Rpackage pecthat provides many validation methods\\nand accuracy indexes.\\n19Kattan et al.338describe how to make nomograms for deriving predicted sur-\\nvival probabilities when there are competing risks.\\n20Hielscher et al.291provides an overview of software for computing accuracy\\nindexes with censored data.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='c3969dcc-7b73-44ed-99db-4e81692d73df', embedding=None, metadata={'page_label': '521', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chapter 21\\nCase Study in Cox Regression\\n21.1 Choosing the Number of Parameters and Fitting\\nthe Model\\nConsider the randomized tr ial of estrogen for treat ment of prostate cancer87\\ndescribed in Chapter 8. Let us now develop a model for time until death\\n(of any cause). There are 354 deaths among the 502 patients. To be able\\nto eﬃciently estimate treatment beneﬁt, to test for diﬀerential treatment\\neﬀect, or to estimate prognosis or absolute treatment beneﬁt for individual\\npatients, we need a multivariable survival model. In this case study we do not\\nmake use of data reductions obtained in Chapter 8but show simpler (partial)\\napproaches to data reduction. We do use the transcan results for imputation.\\nFirst let’s assess the wisdom of ﬁtting a full additive model that does not\\nassume linearity of eﬀect for any predictor. Categorical predictors are ex-\\npanded using dummy variables. For pfwe could lump the last two categories\\nas before since the last category has only two patients. Likewise, we could\\ncombine the last two levels of ekg. Continuous predictors are expanded by\\nﬁtting four-knot restricted cubic spline functions, which contain two nonlin-\\near terms and thus have a total of three d.f. Table 21.1deﬁnes the candidate\\npredictors and lists their d.f. The variable stageis not listed as it can be\\npredicted with high accuracy from sz,sg,ap,bm (stagecould have been used\\nas a predictor for imputing missing values on sz, sg). There are a total of 36\\ncandidate d.f. that should not be artiﬁcially reduced by“univariable screen-\\ning” or graphical assessments of association with death. This is about 1 /10\\nas many predictor d.f. as there are deaths, so there is some hope that a ﬁtted\\nmodel may validate. Let us also examine this issue by estimating the amount\\nof shrinkage using Equation 4.3.W eﬁ r s tu s e transcan impute missing data.\\nrequire(rms)\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7 21521', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9c52acef-8bae-4a62-a0b4-b51f8e97d3d6', embedding=None, metadata={'page_label': '522', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"522 21 Case Study in Cox Regression\\nTable 21.1 Initial allocation of degrees of freedom\\nPredictor Name d.f. Original Levels\\nDose of estrogen rx3 placebo, 0.2, 1.0, 5.0 mg\\nestrogen\\nAge in years age3\\nWeight index: wt(kg) −ht(cm)+200 wt3\\nPerformance rating pf2 normal, in bed <50% of\\ntime, in bed >50%, in\\nbed always\\nHistory of cardiovascular disease hx1 present/absent\\nSystolic blood pressure/10 sbp3\\nDiastolic blood pressure/10 dbp3\\nElectrocardiogram code ekg5 normal, benign, rhythm\\ndisturb., block, strain,\\nold myocardial infarction,\\nnew MI\\nSerum hemoglobin (g/100ml) hg3\\nTumor size (cm2) sz3\\nStage/histologic grade combination sg3\\nSerum prostatic acid phosphatase ap3\\nBone metastasis bm1 present/absent\\ngetHdata(prostate)\\nlevels( prostate$ekg)[ levels( prostate$ekg) %in%\\nc('old MI ','recent MI ')]←'MI'\\n# combines last 2 levels and uses a new name, MI\\nprostate$pf.coded ←as.integer(prostate$pf)\\n# save original pf, re-code to 1-4\\nlevels( prostate$pf) ←c(levels( prostate$pf )[1:3],\\nlevels( prostate$pf )[3])\\n# combine last 2 levels\\nw←transcan( ∼sz + sg + ap + sbp + dbp + age +\\nwt + hg + ekg + pf + bm + hx, imputed= TRUE,\\ndata=prostate, pl= FALSE, pr= FALSE)\\nattach( prostate)\\nsz←impute(w, sz, data= prostate)\\nsg←impute(w, sg, data= prostate)\\nage←impute(w, age,data= prostate)\\nwt←impute(w, wt, data= prostate)\\nekg←impute(w, ekg,data= prostate)\\ndd←datadist(prostate); options(datadist= 'dd')\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0e1a0474-e44f-45b3-bdc3-d86909fe08fb', embedding=None, metadata={'page_label': '523', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"21.1 Choosing Number of Parameters/Fitting the Model 523\\nunits(dtime) ←'Month '\\nS←Surv(dtime, status != 'alive ')\\nf←cph(S∼rx + rcs(age,4) + rcs(wt,4) + pf + hx +\\nrcs(sbp,4) + rcs(dbp,4) + ekg + rcs(hg,4) +\\nrcs(sg,4) + rcs(sz,4) + rcs(log(ap),4) + bm)\\nprint(f, latex= TRUE,coefs=FALSE)\\nCox Proportional Hazards Model\\ncph(formula =S~r x+ rcs(age, 4) + rcs(wt, 4) + pf + hx\\n+ rcs(sbp, 4) + rcs(dbp, 4) + ekg + rcs(hg, 4)\\n+ rcs(sg, 4) + rcs(sz, 4) + rcs(log(ap), 4) + bm)\\nModel Tests Discrimination\\nIndexes\\nObs 502 LRχ2136.22R20.238\\nEvents 354 d.f. 36 Dxy0.333\\nCenter -2.9933 Pr(>χ2) 0.0000 g 0.787\\nScoreχ2143.62gr2.196\\nPr(>χ2) 0.0000\\nThe likelihood ratio χ2statistic is 136.2 with 36 d.f. This test is highly\\nsigniﬁcant so some modeling is warranted. The AIC value (on the χ2scale) is\\n136.2−2×36= 64.2.Theroughshrinkageestimateis0.74(100.2/136.2)sowe\\nestimate that 0.26 of the model ﬁtting will be noise, especially with regard to\\ncalibration accuracy. The approachof Spiegelhalter582is to ﬁt this full model\\nand to shrink predicted values. We instead try to do data reduction (blinded\\nto individual χ2statistics from the above model ﬁt) to see if a reliable model\\ncan be obtained without shrinkage. A good approach at this point might\\nbe to do a variable clustering analysis followed by single degree of freedom\\nscoring for individual predictors or for clusters of predictors. Instead we do\\nan informal data reduction. The strategy is described in Table 21.2.F o rap,\\nmore explorationis desired to be able to model the shape of eﬀect with such a\\nhighly skewed distribution. Since we expect the tumor variables to be strong\\nprognostic factors we retain them as separate variables. No assumption is\\nmade for the dose-response shape for estrogen, as there is reason to expect a\\nnon-monotonic eﬀect due to competing risks for cardiovascular death.\\nheart ←hx + ekg %nin% c( 'normal ','benign ')\\nlabel(heart) ←'HeartDisease Code '\\nmap ←(2*dbp + sbp)/3\\nlabel(map) ←'MeanArterial Pressure/10 '\\ndd←datadist(dd, heart, map)\\nf←cph(S∼rx + rcs(age,4) + rcs(wt,3) + pf.coded +\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='59cdf957-501c-4035-a1a9-16316bc1be1e', embedding=None, metadata={'page_label': '524', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='524 21 Case Study in Cox Regression\\nTable 21.2 Final allocation of degrees of freedom\\nVariables Reductions d.f. Saved\\nwt Assume variable not important enough 1\\nfor 4 knots; use 3 knots\\npf Assume linearity 1\\nhx,ekg Make new 0,1,2 variable and assume 5\\nlinearity: 2 = hxandekgnot normal\\nor benign, 1 = either, 0 = none\\nsbp,dbp Combine into mean arterial bp and 4\\nuse 3 knots: map = (2 dbp+sbp)/3\\nsg Use 3 knots 1\\nsz Use 3 knots 1\\nap Look at shape of eﬀect of apin detail, −1\\nand take log before expanding as spline\\nto achieve numerical stability: add 1 knots\\nheart + rcs(map,3) + rcs(hg,4) +\\nrcs(sg,3) + rcs(sz,3) + rcs(log(ap),5) + bm,\\nx=TRUE, y=TRUE, surv=TRUE,time.inc=5*12)\\nprint(f, latex= TRUE,coefs=3)\\nCox Proportional Hazards Model\\ncph(formula =S~r x+ rcs(age, 4) + rcs(wt, 3) + pf.coded +\\nheart + rcs(map, 3) + rcs(hg, 4) + rcs(sg, 3) +\\nrcs(sz, 3) + rcs(log(ap), 5) + bm, x = TRUE, y = TRUE,\\nsurv = TRUE, time.inc =5*1 2 )\\nModel Tests Discrimination\\nIndexes\\nObs 502 LRχ2118.37R20.210\\nEvents 354 d.f. 24 Dxy0.321\\nCenter -2.4307 Pr(>χ2) 0.0000 g 0.717\\nScoreχ2125.58gr2.049\\nPr(>χ2) 0.0000\\nCoef S.E. Wald ZPr(>|Z|)\\nrx=0.2 mg estrogen -0.0002 0.1493 0.00 0.9987\\nrx=1.0 mg estrogen -0.4160 0.1657 -2.51 0.0121\\nrx=5.0 mg estrogen -0.1107 0.1571 -0.70 0.4812\\n...', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='7a94579e-b408-49d9-b30d-21ce28cf9846', embedding=None, metadata={'page_label': '525', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"21.2 Checking Proportional Hazards 525\\nTable 21.3 Wald Statistics for S\\nχ2d.f.P\\nrx 8 .01 3 0.0459\\nage 13 .84 3 0.0031\\nNonlinear 9.06 2 0.0108\\nwt 8 .21 2 0.0165\\nNonlinear 2.54 1 0.1110\\npf.coded 3 .79 1 0.0517\\nheart 23 .51 1<0.0001\\nmap 0 .04 2 0.9779\\nNonlinear 0.04 1 0.8345\\nhg 12 .52 3 0.0058\\nNonlinear 8.25 2 0.0162\\nsg 1 .64 2 0.4406\\nNonlinear 0.05 1 0.8304\\nsz 12 .73 2 0.0017\\nNonlinear 0.06 1 0.7990\\nap 6 .51 4 0.1639\\nNonlinear 6.22 3 0.1012\\nbm 0 .03 1 0.8670\\nTOTAL NONLINEAR 23 .81 11 0.0136\\nTOTAL 119 .09 24<0.0001\\n# x, y for predict, validate, calibrate;\\n# surv, time.inc for calibrate\\nlatex(anova (f),file= '',label= 'tab:coxcase-anova1 ')# Table 21.3\\nThe total savings is thus 12 d.f. The likelihood ratio χ2is 118 with 24 d.f.,\\nwith a slightly improved AIC of 70. The rough shrinkage estimate is slightly\\nbetter at 0 .80, but still worrisome. A further data reduction could be done,\\nsuch as using the transcan transformations det ermined from self-consistency\\nof predictors, but we stop here and use this model.\\nFrom Table 21.3there are 11 parameters associated with nonlinear eﬀects,\\nand the overall test of linearity indicates the strong presence of nonlinearity\\nfor at least one of the variables age,wt,map,hg,sz,sg,ap . There is no strong\\nevidence for a diﬀerence in survival time between doses of estrogen.\\n21.2 Checking Proportional Hazards\\nNowthatwehaveatentativemodel,letusexaminethe model’sdistributional\\nassumptions using smoothed scaled Schoenfeld residuals. A messy detail is\\nhow to handle multiple regression coeﬃcients per predictor. Here we do an\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4171068-b068-4c78-b0fe-ef2e9c72b366', embedding=None, metadata={'page_label': '526', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"526 21 Case Study in Cox Regression\\napproximate analysis in which each predictor is scored by adding up all that\\npredictor’stermsinthe model,totransformthat predictortooptimallyrelate\\nto the log hazard (at least if the shapeof the eﬀect does not change with\\ntime). In doing this we are temporarily ignoring the fact that the individual\\nregression coeﬃcients were estimated from the data. For dose of estrogen,\\nfor example, we code the eﬀect as 0 (placebo), −0.00025 (0 .2m g ) ,−0.416\\n(1.0m g ) ,a n d −0.111 (5.0m g ) ,a n d ageis transformed using its ﬁtted spline\\nfunction. In the rmspackage the predictfunction easily summarizes multiple\\nterms and produces a matrix (here, z) containing the total eﬀects for each\\npredictor. Matrix factors can easily be included in model formulas.\\nz←predict(f, type= 'terms ')\\n# required x=T above to store design matrix\\nf.short ←cph(S∼z, x=TRUE, y= TRUE)\\n# store raw x, y so can get residuals\\nThe ﬁt f.shortbased on the matrix of single d.f. predictors zhas the\\nsame LR χ2of 118 as the ﬁt f, but with a falsely low 11 d.f. All regression\\ncoeﬃcients are unity.\\nNow we compute scaled Schoenfeld residuals separately for each predictor\\nand test the PH assumption using the“correlation with time”test. Also plot\\nsmoothed trends in the residuals. The plotmethod for cox.zphobjects uses\\ncubic splines to smooth the relationship.\\nphtest ←cox.zph( f.short, transform= 'identity ')\\nphtest\\nrho chisq p\\nrx 0.10232 4.00823 0.0453\\nage -0.05483 1.05850 0.3036\\nwt 0.01838 0.11632 0.7331\\npf.coded -0.03429 0.41884 0.5175\\nheart 0.02650 0.30052 0.5836\\nmap 0.02055 0.14135 0.7069\\nhg -0.00362 0.00511 0.9430\\nsg -0.05137 0.94589 0.3308\\nsz -0.01554 0.08330 0.7729\\nap 0.01720 0.11858 0.7306\\nbm 0.04957 0.95354 0.3288\\nGLOBAL NA 7.18985 0.7835\\nplot(phtest, var= 'rx')# Figure 21.1\\nPerhaps only the drug eﬀect signiﬁcantly changes over time ( P=0.05 for\\ntesting the correlation rhobetween the scaled Schoenfeld residual and time),\\nbut when a global test of PH is done penalizing for 11 d.f., the Pvalue is\\n0.78. A graphical examination of the trends doesn’t ﬁnd anything interesting\\nfor the last 10 variables. A residual plot is drawn for rxalone and is shown in\\nFigure21.1. We ignore the possible increase in eﬀect of estrogen over time. If\\nthis non-PH is real, a more accurate model might be obtained by stratifying\\nonrxor by using a time ×rxinteraction as a time-dependent covariable.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='62885408-215e-4d53-b204-d8adca12001d', embedding=None, metadata={'page_label': '527', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='21.4 Describing Predictor Eﬀects 527\\n0 2 04 06 0−15−10−50510\\nTimeBeta(t) for rx\\nll\\nll\\nll\\nll\\nlll\\nll\\nll\\nll\\nll\\nlll\\nll\\nl\\nll\\nlllll\\nlll\\nll\\nlllll\\nll\\nllll\\nll\\nl\\nlll\\nlll\\nlll\\nl\\nll\\nl\\nll\\nl\\nll\\nlll\\nllll\\nl\\nlll\\nl\\nlll\\nlll\\nl\\nll\\nlll\\nl\\nllll\\nlll\\nl\\nll\\nll\\nlllll\\nll\\nlll\\nll\\nlll\\nl\\nllllll\\nll\\nll\\nll\\nlll\\nl\\nllll\\nl\\nl\\nlll\\nl\\nl\\nlll\\nl\\nll\\nlll\\nlll\\nl\\nll\\nll\\nll\\nll\\nlll\\nl\\nlll\\nll\\nll\\nl\\nllll\\nl\\nll\\nll\\nll\\nll\\nllll\\nll\\nll\\nll\\nlll\\nll\\nll\\nllll\\nl\\nlll\\nll\\nl\\nlll\\nll\\nll\\nlll\\nll\\nll\\nl\\nll\\nllll\\nlll\\nll\\nlll\\nlll\\nl\\nll\\nl\\nlllll\\nllll\\nll\\nl\\nll\\nllll\\nl\\nlll\\nl\\nlll\\nll\\nllll\\nlll\\nll\\nlll\\nll\\nl\\nlll\\nlll\\nlll\\nlll\\nlll\\nll\\nl\\nll\\nlll\\nll\\nll\\nllll\\nlll\\nllll\\nl\\nll\\nlll\\nlll\\nlll\\nll\\nFig. 21.1 Raw and spline-smoothed scaled Schoenfeld residuals for dose of estrogen,\\nnonlinearly coded from the Cox model ﬁt, with ±2 standard errors.\\n21.3 Testing Interactions\\nNote that the model has several insigniﬁcant predictors. These are not\\ndeleted, as that would not improve predictive accuracy and it would make\\naccurate conﬁdence intervals hard to obtain. At this point it would be rea-\\nsonable to test prespeciﬁed interactions. Here we test all interactions with\\ndose. Since the multiple terms for many of the predictors (and for rx)m a k e\\nfor a great number of d.f. for testing interaction (and a loss of power), we do\\napproximate tests on the data-driven coding of predictors. P-values for these\\ntests are likely to be somewhat anti-conservative.\\nz.dose ←z[,\"rx\"] # same as saying z[,1] - get first column\\nz.other ←z[,-1] # all but the first column of z\\nf.ia←cph(S∼z.dose * z.other) # Figure 21.4:\\nlatex(anova(f.ia), file= \\'\\', label= \\'tab:coxcase-anova2 \\')\\nThe global test of additivity in Table 21.4hasP=0.27, so we ignore the\\ninteractions (and also forget to penalize for having looked for them below!).\\n21.4 Describing Predictor Eﬀects\\nLet us plot how each predictor is related to the log hazard of death, including\\n0.95 conﬁdence bands. Note in Figure 21.2that due to a peculiarity of the\\nCox model the standard error of the predicted Xˆβis zero at the reference\\nvalues (medians here, for continuous predictors).', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0b2fd34b-9880-46c6-ad5c-86b3b48ff3a2', embedding=None, metadata={'page_label': '528', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='528 21 Case Study in Cox Regression\\nTable 21.4 Wald Statistics for S\\nχ2d.f.P\\nz.dose (Factor+Higher Order Factors) 18 .74 11 0.0660\\nAll Interactions 12.17 10 0.2738\\nz.other (Factor+Higher Order Factors) 125 .89 20<0.0001\\nAll Interactions 12.17 10 0.2738\\nz.dose×z.other (Factor+Higher Order Factors) 12 .17 10 0.2738\\nTOTAL 129.10 21<0.0001\\nage ap hg\\nmap sg sz\\nwt−1.5−1.0−0.50.00.51.0\\n−1.5−1.0−0.50.00.51.0\\n−1.5−1.0−0.50.00.51.060 70 80 05 0 1 00 9 1 11 31 51 7\\n81 0 1 2 7.5 10.0 12.5 15.0 0 1 02 03 04 05 0\\n80 90 100 110 120 130log Relative Hazard\\nll\\nlll\\nllll\\nllllbm heart\\npf.coded rx01\\n012\\n1234\\nplacebo0.2 mg estrogen1.0 mg estrogen5.0 mg estrogen\\n−1.5 −1.0 −0.5 0.0 0.5 1.0 −1.5 −1.0 −0.5 0.0 0.5 1.0\\nlog Relative Hazard\\nFig. 21.2 Shape of each predictor on log hazard of death. Y-axis shows Xˆβ, but\\nthe predictors not plotted are set to reference values. Note the highly non-mo notonic\\nrelationship with ap, and the increased slope after age 70 which occurs in outcome\\nmodels for various diseases.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='2f5a803c-6a20-46f9-860b-d4a35f506327', embedding=None, metadata={'page_label': '529', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"21.5 Validating the Model 529\\nggplot( Predict(f), sepdiscrete= 'vertical ', nlevels=4,\\nvnames= 'names ')# Figure 21.2\\n21.5 Validating the Model\\nWe ﬁrst validate this model for Somers’ Dxyrank correlation between pre-\\ndicted log hazard and observed survival time, and for slope shrinkage. The\\nbootstrap is used (with 300 resamples) to penalize for possible overﬁtting, as\\ndiscussed in Section 5.3.\\nset.seed(1) # so can reproduce results\\nv←validate(f, B =300)\\nDivergence or singularity in 83 samples\\nlatex(v, file= '')\\nIndex Original Training Test Optimism Corrected n\\nSample Sample Sample Index\\nDxy0.3208 0.3454 0.2954 0 .0500 0 .2708 217\\nR20.2101 0.2439 0.1754 0 .0685 0 .1417 217\\nSlope 1 .0000 1.0000 0.7941 0 .2059 0 .7941 217\\nD0.0292 0.0348 0.0238 0 .0110 0 .0182 217\\nU−0.0005−0.0005 0.0023−0.0028 0 .0023 217\\nQ0.0297 0.0353 0.0216 0 .0138 0 .0159 217\\ng0.7174 0.7918 0.6273 0 .1645 0 .5529 217\\nHere“training”refers to accuracywhen evaluated on the bootstrapsample\\nused to ﬁt the model, and “test” refers to the accuracy when this model is\\napplied without modiﬁcation to the original sample. The apparent Dxyis\\n0.32, but a better estimate of how well the model will discriminate prognoses\\nin the future is Dxy=0.27.The bootstrapestimateof slope shrinkageis 0.79,\\nclose to the simple heuristic estimate. The shrinkage coeﬃcient could easily\\nbe used to shrink predictions to yield better calibration.\\nFinally, we validate the model (without using the shrinkage coeﬃcient) for\\ncalibration accuracy in predicting the probability of surviving ﬁve years. The\\nbootstrap is used to estimate the opt imism in how well predicted ﬁve-year\\nsurvival from the ﬁnal Cox model tracks ﬂexible smooth estimates, with-\\nout any binning of predicted survival probabilities or assuming proportional\\nhazards.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3d1c3ca5-fbf3-439e-8db9-b53bb938292c', embedding=None, metadata={'page_label': '530', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"530 21 Case Study in Cox Regression\\ncal←calibrate(f, B =300, u=5*12, maxdim=4)\\nUsing Cox survival estimates at 60 Months\\nplot(cal, subtitles= FALSE) # Figure 21.3\\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.70.00.10.20.30.40.50.60.7\\nPredicted  60 Month SurvivalFraction Surviving 60 Month\\nFig. 21.3 Bootstrap estimate of calibration accuracy for 5-year estimates from the\\nﬁnal Cox model, usingadaptive linear splinehazard regression361. The line nearer the\\nideal line corresponds to apparent predictive accuracy. The blue curve corresp onds to\\nbootstrap-corrected estimates.\\nThe estimated calibrationcurves are shown in Figure 21.3, similar to what\\nwas done in Figure 19.11. Bootstrap calibration demonstrates some overﬁt-\\nting, consistent with regressionto the mean. The absoluteerroris appreciable\\nfor 5-year survival predicted to be very low or high.\\n21.6 Presenting the Model\\nTo present point and interval estimates of predictor eﬀects we draw a hazard\\nratio chart (Figure 21.4), and to make a ﬁnal presentation of the model\\nwe draw a nomogram having multiple “predicted value” axes. Since the ap\\nrelationship is so non-monotonic, use a 20 : 1 hazard ratio for this variable.\\nplot(summary(f, ap=c(1,20)), log= TRUE,main= '')# Figure 21.4\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='0ad919e8-ac8b-4abe-91ac-8a751299745c', embedding=None, metadata={'page_label': '531', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"21.7 Problems 531\\n0.50 1.00 2.00 3.50 5.50\\nage − 76:70\\nwt − 107:90\\npf.coded − 4:1\\nheart − 2:0\\nmap − 11:9.333333\\nhg − 14.69922:12.29883\\nsg − 11:9\\nsz − 21:5\\nap − 20:1\\nbm − 1:0\\nrx − 0.2 mg estrogen:placebo\\nrx − 1.0 mg estrogen:placebo\\nrx − 5.0 mg estrogen:placebo\\nFig. 21.4 Hazard ratios and multi-level conﬁdence bars for eﬀects of predictors in\\nmodel, using default ranges except for ap\\nThe ultimate graphical display for this model will be a nomogram relating\\nthe predictors to Xˆβ, estimated three– and ﬁve-year survival probabilities\\nand median survival time. It is easy to add as many“output”axes as desired\\nto a nomogram.\\nsurv ←Survival(f)\\nsurv3 ←function(x) surv(3*12,lp=x)\\nsurv5 ←function(x) surv(5*12,lp=x)\\nquan ←Quantile(f)\\nmed ←function(x) quan(lp=x)/12\\nss ←c(.05,.1,.2,.3,.4,.5,.6,.7,.8,.9,.95)\\nnom←nomogram(f, ap=c(.1,.5,1,2,3,4,5,10,20,30 ,40),\\nfun=list(surv3, surv5, med),\\nfunlabel=c( '3-year Survival ','5-year Survival ',\\n'Median Survival Time (years) '),\\nfun.at=list(ss, ss, c(.5,1:6)))\\nplot(nom, xfrac=.65, lmgp=.35) # Figure 21.5\\n21.7 Problems\\nPerform Cox regression analyses of survival time using the Mayo Clinic PBC\\ndataset described in Section 8.9. Provide model descriptions, parameter esti-\\nmates, and conclusions.\\n1. Assess the nature of the association of several predictors of your choice.\\nForpolytomouspredictors,performalog-rank-typescoretest(or k-sample\\nANOVA extension if there are more than two levels). For continuous pre-\\ndictors, plot a smooth curve that estimates the relationship between the\\npredictor and the log hazard or log–log survival. Use both parametric\\nand nonparametric (using martingale residuals) approaches. Make a test\\nofH0: predictor is not associated with outcome versus Ha:p r e d i c t o r\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca8b29b7-d460-494e-8801-3232f708e4c3', embedding=None, metadata={'page_label': '532', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='532 21 Case Study in Cox Regression\\nPoints0 1 02 03 04 05 06 07 08 09 0 1 0 0\\nrx\\n1.0 mg estrogen 0.2 mg estrogen5.0 mg estrogen\\nAge in Years\\n70 5075 80 85 90\\nWeight Index = wt(kg)−ht(cm)+200\\n110 90 80 70 60120\\npf.coded\\n1324\\nHeart Disease Code\\n021\\nMean Arterial Pressure/10\\n4822 12\\nSerum Hemoglobin (g/100ml)\\n14 12 10 8 6 416 18 20 22\\nCombined Index of Stage and Hist.\\nGrade5678 1 01 2 1 5\\nSize of Primary Tumor (cm^2)\\n0 5 15 25 30 35 40 45 50 55 60 65 70\\nSerum Prostatic Acid Phosphatase\\n0.5 0.112 3 5\\n40\\nBone Metastases\\n01\\nTotal Points\\n0 20 40 60 80 100 140 180 220 260\\nLinear Predictor\\n−2 −1.5 −1 −0.5 0 0.5 1 1.5 2\\n3−year Survival\\n0.050.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\\n5−year Survival\\n0.050.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\\nMedian Survival Time (years)\\n0.5 1 2 3 456\\nFig. 21.5 Nomogram for predicting death in prostate cancer trial\\nis associated (by a smooth function). The test should have more than 1\\nd.f. If there is no evidence that the predictor is associated with outcome.\\nMake a formal test of linearity of each remaining continuous predictor.\\nUse restricted cubic spline functions with four knots. If you feel that you\\ncan’t narrow down the number of candidate predictors without examining\\nthe outcomes, and the number is too great to be able to derive a reliable\\nmodel, use a data reduction technique and combine many of the variables\\ninto a summary index.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='88ae4cba-4b38-44c1-9bc7-c467f1423c03', embedding=None, metadata={'page_label': '533', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='21.7 Problems 533\\n2. Forfactorsthatremain,assessthe PHassumptionusingatleasttwometh-\\nods, after ensuring that continuous predictors are transformed to be as\\nlinear as possible. In addition, for polytomous predictors, derive log cu-\\nmulative hazard estimates adjusted for continuous predictors that do not\\nassume anything about the relationship between the polytomous factor\\nand survival.\\n3. Derive a ﬁnal Cox PH model. Stratify on polytomous factors that do not\\nsatisfy the PH assumption. Decide whether to categorize and stratify on\\ncontinuous factors that may strongly violate PH. Remember that in this\\ncase you can still model the continuous factor to account for any residual\\nregression after adjusting for strata intervals. Include an interaction be-\\ntweentwopredictorsofyourchoosing.Interpretthe parametersin the ﬁnal\\nmodel. Also interpret the ﬁnal model by providing some predicted survival\\ncurves in which an important continuous predictor is on the x-axis, pre-\\ndicted survival is on the y-axis, separate curves are drawn for levels of\\nanother factor, and any other factors in the model are adjusted to speci-\\nﬁed constants or to the grand mean. The estimated survival probabilities\\nshould be computed at t= 730 days.\\n4. Verify, in an unbiased fashion, your“ﬁnal”model, for either calibration or\\ndiscrimination. Validate intermediate steps, not just the ﬁnal parameter\\nestimates.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b1b5acbd-1a6c-4703-bcd2-e6087916699f', embedding=None, metadata={'page_label': '535', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Appendix A\\nDatasets, RPackages, and Internet\\nResources\\nCentral Web Site and Datasets\\nThewebsiteforinformationrelatedtothisbookis biostat.mc.vanderbilt.\\nedu/rms,andarelatedwebsiteforafull-semestercoursebasedonthe bookis\\nhttp://biostat.mc.vanderbilt.edu/CourseBios330 . The main site con-\\ntains links to several other web sites and a link to the dataset repository that\\nholds most of the datasets mentioned in the text for downloading. These\\ndatasets are in fully annotated Rsave(.savsuﬃxes) ﬁlesa;s o m eo ft h e s e\\nare also available in other formats. The datasets were selected because of\\nthe variety of types of response and predictor variables, sample size, and\\nnumbers of missing values. In Rthey may be read using the loadfunction,\\nload(url()) to read directly from the Web, or by using the Hmiscpackage’s\\ngetHdata function to do the same (as is done in code in the case studies).\\nFrom the web site there are links to other useful dataset sources. Links to\\npresentations and technical reports related to the text are also found on this\\nsite, as is information for instructors for obtaining quizzes and answer sheets,\\nextra problems, and solutions to these and to many of the problems in the\\ntext. Details about short courses based on the text are also found there. The\\nmain site also has Chapter 7 from the ﬁrst edition, which is a case study in\\nordinary least squares modeling.\\nRPackages\\nThermspackage written by the author maintains detailed information about\\na model’s design matrix so that many analyses using the model ﬁt are au-\\ntomated. rmsis a large package of Rfunctions. Most of the functions in rms\\nanalyze model ﬁts, validate them, or make presentation graphics from them,\\naBy convention these should have had .rdasuﬃxes.\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7535', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fbc9a27c-9fe4-4f4b-ad48-01c858545d97', embedding=None, metadata={'page_label': '536', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='536 A Datasets, RPackages, and Internet Resources\\nbut the packages also contain special model–ﬁtting functions for binary and\\nordinal logistic regression (optionally using penalized maximum likelihood),\\nunpenalized ordinal regression with a variety of link functions, penalized and\\nunpenalized least squares, and parametri c and semiparametric survival mod-\\nels. In addition, rmshandles quantile regression and longitudinal analysis\\nusing generalized least squares. The rmspackage pays special attention to\\ncomputing predicted values in that design matrix attributes (e.g., knots for\\nsplines, categories for categorical predictors) are“remembered”so that pre-\\ndictors are properly transformed while predictions are being generated. The\\nfunctions makes extensive use of a wealth of survival analysis software writ-\\nten by Terry Therneau of the Mayo Foundation. This survival package is a\\nstandard part of R.\\nThe author’s Hmiscpackage contains other miscellaneous functions used\\nin the text. These are functions that do not operate on model ﬁts that used\\nthe enhanced design attributes stored by the rmspackage. Functions in Hmisc\\ninclude facilities for data reduction, imputation, power and sample size calcu-\\nlation, advanced table making, recoding variables, translating SAS datasets\\nintoRdata frames while preserving all data attributes (including variable\\nand value labels and special missing values), drawing and annotating plots,\\nand converting certain Robjects to L ATEX371typeset form. The latter capa-\\nbility, provided by a family of latexfunctions, completes the conversion to\\nLATEX of many of the objects created by rms. The packages contain several\\nLATEX methods that create L ATEX code for typesetting model ﬁts in algebraic\\nnotation, for printing ANOVA and regression eﬀect (e.g., odds ratio) tables,\\nand other applications. The L ATEX methods were used extensively in the text,\\nespecially for writing restricted cubic spline function ﬁts in simplest notation.\\nThe latest version of the rmspackage is available from CRAN(see below).\\nIt is necessary to install the Hmiscpackage in order to use rmspackage. The\\nWeb site also contains more in-depth overviewsof the packages,which run on\\nUNIX, Linux, Mac, and Microsoft Windows systems. The packages may be\\nautomatically downloaded and installed using R’sinstall.packages function\\nor using menus under Rgraphical user interfaces.\\nR-help,CRAN, and Discussion Boards\\nTosubscribetothehighlyinformativeandhelpful R-helpe-mailgroup,seethe\\nWeb site. R-helpis appropriatefor asking generalquestions about Rincluding\\nthose about ﬁnding or writing functions to do speciﬁc analyses (for questions\\nspeciﬁc to a package, contact the author of that package). Another resource\\nis theCRANrepository at www.r-project.org . Another excellent resource\\nfor askings questions about Risstackoverflow.com/questions/tagged/r .\\nThere is a Google group regmoddevoted to the book and courses.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fd875544-7720-4d27-bff6-bc6ffe2afec7', embedding=None, metadata={'page_label': '537', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='A Datasets, RPackages, and Internet Resources 537\\nMultiple Imputation\\nTheImputeE-maillistmaintainedbyJunedSiddiqueofNorthwesternUniver-\\nsity is an invaluable source of information regarding missing data problems.\\nTo subscribe to this list, see the Web site. Other excellent sources of on-\\nline information are Joseph Schafer’s“Multiple Imputation Frequently Asked\\nQuestions” site and Stef van Buuren and Karin Oudshoorn’s “Multiple Im-\\nputation Online”site, for which links exist on the main Web site.\\nBibliography\\nAn extensive annotated bibliography containing all the references in this text\\nas well as other references concerning predictive methods, survival analysis,\\nlogistic regression, prognosis, diagnosis, modeling strategies, model valida-\\ntion, practical Bayesian methods, clinical trials, graphical methods, papers\\nfor teaching statistical methods, the bootstrap, and many other areas may\\nbe found at http://www.citeulike.org/user/harrelfe .\\nSAS\\nSAS macros for ﬁtting restricted cubic splines and for other basic operations\\nare freely available from the main Web site. The Web site also has notes on\\nSAS usage for some of the methods presented in the text.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ee7575f0-4a8f-4c6c-84f8-0dc24fa0d9c8', embedding=None, metadata={'page_label': '539', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References\\nNumbers following ⋄are page numbers of citations.\\n1. O. O. Aalen. Nonparametric inference in connection with multiple decrement\\nmodels. Scan J Stat , 3:15–27, 1976. ⋄413\\n2. O. O. Aalen. Further results on the non-parametric linear regression model in\\nsurvival analysis. Stat Med , 12:1569–1588, 1993. ⋄518\\n3. O. O. Aalen, E. Bjertness, and T. Sønju. Analysis of dependent survival data\\napplied to lifetimes of amalgam ﬁllings. Stat Med , 14:1819–1829, 1995. ⋄421\\n4. M. Abrahamowicz, T. MacKenzie, and J. M. Esdaile. Time-dependent haz-\\nard ratio: Modeling and hypothesis testing with applications in lupus nephritis.\\nJAMA, 91:1432–1439, 1996. ⋄501\\n5. A. Agresti. A survey of models for repeated ordered categorical response data.\\nStat Med , 8:1209–1224, 1989. ⋄324\\n6. A. Agresti. Categorical data analysis . Wiley, Hoboken, NJ, second edition, 2002.\\n⋄271\\n7. H. Ahn and W. Loh. Tree-structured proportional hazards regression modeling.\\nBiometrics , 50:471–485, 1994. ⋄41, 178\\n8. J. Aitchison and S. D. Silvey. The generalization of probit analysis to the case\\nof multiple responses. Biometrika , 44:131–140, 1957. ⋄324\\n9. K. Akazawa, T. Nakamura, and Y. Palesch. Power of logrank test and Cox\\nregression model in clinical trials with heterogeneous samples. Stat Med , 16:583–\\n597, 1997. ⋄4\\n10. O. O. Al-Radi, F. E. Harrell, C. A. Caldarone, B. W. McCrindle, J. P. Jacobs,\\nM. G. Williams, G. S. Van Arsdell, and W. G. Williams. Case complexity\\nscores in congenital heart surgery: A comparative study of the Aristotal Basic\\nComplexity score and the Risk Adjustment in Congenital Heart Surg (RACHS-\\n1) system. J Thorac Cardiovasc Surg , 133:865–874, 2007. ⋄215\\n11. J. M. Alho. On the computation of likelihood ratio and score test based con-\\nﬁdence intervals in generalized linear models. Stat Med , 11:923–930, 1992. ⋄\\n214\\n12. P. D. Allison. Missing Data . Sage University Papers Series on Quantitative\\nApplications in the Social Sciences, 07-136. Sage, Thousand Oaks CA, 2001. ⋄\\n49, 58\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7539', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='8fa37129-85c8-445f-b186-cd3a62958c6f', embedding=None, metadata={'page_label': '540', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='540 References\\n13. D. G. Altman. Categorising continuous covariates (letter to the editor). Brit J\\nCancer, 64:975, 1991. ⋄11, 19\\n14. D. G. Altman. Suboptimal analysis using ‘optimal’ cutpoints. Brit J Cancer ,\\n78:556–557, 1998. ⋄19\\n15. D. G. Altman and P. K. Andersen. A note on the uncertainty of a survival\\nprobability estimated from Cox’s regression model. Biometrika , 73:722–724,\\n1986.⋄11, 517\\n16. D. G. Altman and P. K. Andersen. Bootstrap investigation of the stability of a\\nCox regression model. Stat Med , 8:771–783, 1989. ⋄68, 70, 341\\n17. D. G. Altman, B. Lausen, W. Sauerbrei, and M. Schumacher. Dangers of u sing\\n‘optimal’ cutpoints in the evaluation of prognostic factors. JN a tC a n c e rI n s t ,\\n86:829–835, 1994. ⋄11, 19, 20\\n18. D. G. Altman and P. Royston. What do we mean by validating a prognostic\\nmodel? Stat Med , 19:453–473, 2000. ⋄6, 122, 519\\n19. B. Altschuler. Theory for the measurement of competing risks in animal exper-\\niments. Math Biosci , 6:1–11, 1970. ⋄413\\n20. C. F. Alzola and F. E. Harrell. An Introduction to S and the Hmisc and Design\\nLibraries, 2006. Electronic book, 310 pages. ⋄129\\n21. G. Ambler, A. R. Brady, and P. Royston. Simplifying a prognostic model: a\\nsimulation study based on clinical data. Stat Med , 21(24):3803–3822, Dec. 2002.\\n⋄121\\n22. F. Ambrogi, E. Biganzoli, and P. Boracchi. Estimates of clinically useful mea-\\nsures in competing risks survival analysis. Stat Med , 27:6407–6425, 2008. ⋄\\n421\\n23. P. K. Andersen and R. D. Gill. Cox’s regression model for counting processes:\\nA large sample study. Ann Stat , 10:1100–1120, 1982. ⋄418, 513\\n24. G. L. Anderson and T. R. Fleming. Model misspeciﬁcation in proportional\\nhazards regression. Biometrika , 82:527–541, 1995. ⋄4\\n25. J. A. Anderson. Regression and ordered categorical variables. J Roy Stat Soc\\nB, 46:1–30, 1984. ⋄324\\n26. J. A. Anderson and P. R. Philips. Regression, discrimination and measurement\\nmodels for ordered categorical variables. Appl Stat , 30:22–31, 1981. ⋄324\\n27. J. A. Anderson and A. Senthilselvan. A two-step regression model for hazard\\nfunctions. Appl Stat , 31:44–51, 1982. ⋄495, 499, 501\\n28. D. F. Andrews and A. M. Herzberg. Data. Springer-Verlag, New York, 1985. ⋄\\n161\\n29. E. Arjas. A graphical method for assessing goodness of ﬁt in Cox’s proportional\\nhazards model. J Am Stat Assoc , 83:204–212, 1988. ⋄420, 495, 502\\n30. H. R. Arkes, N. V. Dawson, T. Speroﬀ, F. E. Harrell, C. Alzola, R. Phillips,\\nN. Desbiens, R. K. Oye, W. Knaus, A. F. Connors, and T. Investigators. The\\ncovariance decomposition of the probability score and its use in evaluating prog -\\nnostic estimates. Med Decis Mak , 15:120–131, 1995. ⋄257\\n31. B. G. Armstrong and M. Sloan. Ordinal regression models for epidemiologic\\ndata.Am J Epi , 129:191–204, 1989. See letter to editor by Peterson. ⋄319, 320,\\n321, 324\\n32. D. Ashby, C. R. West, and D. Ames. The ordered logistic regression model\\nin psychiatry: Rising prevalence of dementia in old people’s homes. Stat Med ,\\n8:1317–1326, 1989. ⋄324\\n33. A. C. Atkinson. A note on the generalized information criterion for choice of a\\nmodel.Biometrika , 67:413–418, 1980. ⋄69, 204\\n34. P. C. Austin. A comparison of regression trees, logistic regression, generalized\\nadditive models, and multivariate adaptive regression splinesfor predicting AMI\\nmortality. Stat Med , 26:2937–2957, 2007. ⋄41', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='74d6e2e6-97f0-44f5-8e9e-147373705e3a', embedding=None, metadata={'page_label': '541', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 541\\n35. P. C. Austin. Bootstrap model selection had similar performance for select-\\ning authentic and noise variables compared to backward variable elimination: a\\nsimulation study. JC l i nE p i , 61:1009–1017, 2008. ⋄70\\n36. P. C. Austin and E. W. Steyerberg. Events per variable (EPV) and the relative\\nperformance of diﬀerent strategies for estimating the out-of-sample validity of\\nlogistic regression models. Statistical methods in medical research , Nov. 2014. ⋄\\n112\\n37. P. C. Austin and E. W. Steyerberg. Graphical assessment of internal and exter-\\nnal calibration of logistic regression models by using loess smoothers. Stat Med ,\\n33(3):517–535, Feb. 2014. ⋄105\\n38. P. C. Austin, J. V. Tu, P. A. Daly, and D. A. Alter. Tutorial in Biostatistics:The\\nuse of quantile regression in health care research: a case study examining gender\\ndiﬀerences in the timeliness of thrombolytic therapy. Stat Med , 24:791–816,\\n2005.⋄392\\n39. D. Bamber. The area above the ordinal dominance graph and the area below\\nthe receiver operating characteristic graph. J Mathe Psych , 12:387–415, 1975.\\n⋄257\\n40. J. Banks. Nomograms. In S. Kotz and N. L. Johnson, editors, Encyclopedia of\\nStat Scis , volume 6. Wiley, New York, 1985. ⋄104, 267\\n41. J. Barnard and D. B. Rubin. Small-sample degrees of freedom with multiple\\nimputation. Biometrika , 86:948–955, 1999. ⋄58\\n42. S.A. Barnes,S.R.Lindborg,and J.W. Seaman. Multipleimputationtechniques\\nin small sample clinical trials. Stat Med , 25:233–245, 2006. ⋄47, 58\\n43. F. Barzi and M. Woodward. Imputations of missing values in practice: Results\\nfrom imputationsof serumcholesterol in28 cohort studies. Am J Epi , 160:34–45,\\n2004.⋄50, 58\\n4 4 .R .A .B e c k e r ,J .M .C h a m b e r s ,a n dA .R .W i l k s . The New S Language .\\nWadsworth and Brooks/Cole, Paciﬁc Grove, CA, 1988. ⋄127\\n45. H. Belcher. The concept of residual confounding in regression models and som e\\napplications. Stat Med , 11:1747–1758, 1992. ⋄11, 19\\n46. D. A. Belsley. Conditioning Diagnostics: Collinearity and Weak Data in Re-\\ngression . Wiley, New York, 1991. ⋄101\\n47. D. A. Belsley, E. Kuh, and R. E. Welsch. Regression Diagnostics: Identifying\\nInﬂuential Data and Sources of Collinearity . Wiley, New York, 1980. ⋄91\\n48. R. Bender and A. Benner. Calculating ordinal regression models in SAS and\\nS-Plus.Biometrical J , 42:677–699, 2000. ⋄324\\n49. J. K. Benedetti, P. Liu, H. N. Sather, J. Seinfeld, and M. A. Epton. Eﬀective\\nsample size for tests of censored survival data. Biometrika , 69:343–349, 1982. ⋄\\n73\\n50. K. Berhane, M. Hauptmann, and B. Langholz. Using tensor product splines\\nin modeling exposure–time–response relationships: Application to the Colorado\\nPlateau Uranium Miners cohort. Stat Med , 27:5484–5496, 2008. ⋄37\\n51. K. N. Berk and D. E. Booth. Seeing a curve in multiple regression. Technomet-\\nrics, 37:385–398, 1995. ⋄272\\n52. D. M. Berridge and J. Whitehead. Analysis of failure time data with ordinal\\ncategories of response. Stat Med , 10:1703–1710, 1991. ⋄319, 320, 324, 417\\n53. C. Berzuini and D. Clayton. Bayesian analysis of survival on multiple time\\nscales.Stat Med , 13:823–838, 1994. ⋄401\\n54. W. B. Bilker and M. Wang. A semiparametric extension of the Mann-Whitney\\ntest for randomly truncated data. Biometrics , 52:10–20, 1996. ⋄420\\n55. D. A. Binder. Fitting Cox’s proportional hazards models from survey data.\\nBiometrika , 79:139–147, 1992. ⋄213, 215\\n56. C. Binquet, M. Abrahamowicz, A. Mahboubi, V. Jooste, J. Faivre, C. Bonitho n-\\nKopp, and C. Quantin. Empirical study of the dependence of the results of\\nmultivariable ﬂexible survival analyses on model selection strategy. Stat Med ,\\n27:6470–6488, 2008. ⋄420', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='183a7f57-074c-48ad-a41a-b283528ac881', embedding=None, metadata={'page_label': '542', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='542 References\\n57. E. H. Blackstone. Analysis of death (survival analysis) and other time-related\\nevents. In F. J. Macartney, editor, Current Status of Clinical Cardiology , pages\\n55–101. MTP Press Limited, Lancaster, UK, 1986. ⋄420\\n58. S. E. Bleeker, H. A. Moll, E. W. Steyerberg, A. R. T. Donders, G. Derkson-\\nLubsen, D. E. Grobbee, and K. G. M. Moons. External validation is necessary\\nin prediction research: A clinical example. JC l i nE p i , 56:826–832, 2003. ⋄122\\n59. M. Blettner and W. Sauerbrei. Inﬂuence of model-building strategies on th e\\nresults of a case-control study. Stat Med , 12:1325–1338, 1993. ⋄123\\n60. D. D. Boos. On generalized score tests. Ann Math Stat , 46:327–333, 1992. ⋄213\\n61. J. G. Booth and S. Sarkar. Monte Carlo approximation of bootstrap variances.\\nAm Statistician , 52:354–357, 1998. ⋄122\\n62. R. Bordley. Statistical decisionmaking without math. Chance, 20(3):39–44,\\n2007.⋄5\\n63. R. Brant. Assessing proportionality in the proportional odds model for ordinal\\nlogistic regression. Biometrics , 46:1171–1178, 1990. ⋄324\\n64. S. R. Brazer, F. S. Pancotto, T. T. Long III, F. E. Harrell, K. L. Lee, M. P. Tyor,\\nand D. B. Pryor. Using ordinal logistic regression to estimate the likelihood of\\ncolorectal neoplasia. JC l i nE p i , 44:1263–1270, 1991. ⋄324\\n65. A. R. Brazzale and A. C. Davison. Accurate parametric inference for small\\nsamples. Statistical Sci , 23(4):465–484, 2008. ⋄214\\n66. L. Breiman. The little bootstrap and other methods for dimensionality selection\\nin regression: X-ﬁxed prediction error. J Am Stat Assoc , 87:738–754, 1992. ⋄\\n69, 100, 112, 114, 123, 204\\n67. L. Breiman. Statistical modeling: The two cultures (with discussion). Statistical\\nSci, 16:199–231, 2001. ⋄11\\n68. L.Breiman and J.H.Friedman. Estimating optimal transformations for multiple\\nregression and correlation (with discussion). J Am Stat Assoc , 80:580–619, 1985.\\n⋄82, 176, 390\\n69. L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. Classiﬁcation and\\nRegression Trees . Wadsworth and Brooks/Cole, Paciﬁc Grove, CA, 1984. ⋄30,\\n41, 142\\n70. N. E. Breslow. Covariance analysis of censored survival data. Biometrics , 30:89–\\n99, 1974. ⋄477, 483, 485\\n71. N. E. Breslow, N. E. Day, K. T. Halvorsen, R. L. Prentice, and C. Sabai. Esti-\\nmation of multiple relative risk functions in matched case-control studies. Am\\nJE p i, 108:299–307, 1978. ⋄483\\n72. N. E. Breslow, L. Edler, and J. Berger. A two-sample censored-data rank test\\nfor acceleration. Biometrics , 40:1049–1062, 1984. ⋄501\\n73. G. W. Brier. Veriﬁcation of forecasts expressed in terms of probability. Monthly\\nWeather Rev , 78:1–3, 1950. ⋄257\\n74. W. M. Briggs and R. Zaretzki. The skill plot: A graphical technique for evaluat-\\ning continuous diagnostic tests (with discussion). Biometrics , 64:250–261, 2008.\\n⋄5, 11\\n75. G. Bron. The loss of the“Titanic”. The Sphere , 49:103, May 1912. The results\\nanalysed and shown in a special“Sphere”diagram drawn from the oﬃcial ﬁgures\\ngiven in the House of Commons. ⋄291\\n76. B. W. Brown, M. Hollander, and R. M. Korwar. Nonparametric tests of inde-\\npendence for censored data, with applications to heart transplant studies. In\\nF. Proschan and R. J. Serﬂing, editors, Reliability and Biometry , pages 327–354.\\nSIAM, Philadelphia, 1974. ⋄505\\n77. D. Brownstone. Regression strategies. In Proceedings of the 20th Symposium\\non the Interface between Computer Science and Statistics , pages 74–79, Wash-\\nington, DC, 1988. American Statistical Association. ⋄116\\n78. J. Bryant and J. J. Dignam. Semiparametric models for cumulative incidence\\nfunctions. Biometrics , 69:182–190, 2004. ⋄420', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5579ef6f-93ad-4429-8ca7-6196af26e155', embedding=None, metadata={'page_label': '543', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 543\\n79. S. F. Buck. A method of estimation of missing values in multivariate data\\nsuitable for use with an electronic computer. J Roy Stat Soc B , 22:302–307,\\n1960.⋄52\\n80. S. T. Buckland, K. P. Burnham, and N. H. Augustin. Model selection: An\\nintegral part of inference. Biometrics , 53:603–618, 1997. ⋄10, 11, 214\\n81. J. Buckley and I. James. Linear regression with censored data. Biometrika ,\\n66:429–36, 1979. ⋄447\\n82. P. Buettner, C. Garbe, and I. Guggenmoos-Holzmann. Problems in deﬁning\\ncutoﬀ points of continuous prognostic factors: Example of tumor thickness in\\nprimary cutaneous melanoma. JC l i nE p i , 50:1201–1210, 1997. ⋄11, 19\\n83. K. Bull and D. Spiegelhalter. Survival analysis in observational studies. Stat\\nMed, 16:1041–1074, 1997. ⋄399, 401, 420\\n84. K. P. Burnham and D. R. Anderson. Model Selection and Multimodel Inference:\\nA Practical Information-Theoretic Approach . Springer, 2nd edition, Dec. 2003.\\n⋄69\\n85. S. Buuren. Flexible imputation of missing data . Chapman & Hall/CRC, Boca\\nRaton, FL, 2012. ⋄54, 55, 58, 304\\n86. M. Buyse. R2: A useful measure of model performance when predicting a di-\\nchotomous outcome. Stat Med , 19:271–274, 2000. Letter to the Editor regarding\\nStat Med 18:375–384; 1999. ⋄272\\n87. D. P. Byar and S. B. Green. The choice of treatment for cancer patients based on\\ncovariate information: Application to prostate cancer. Bulletin Cancer, Paris ,\\n67:477–488, 1980. ⋄161, 275, 521\\n88. R. M. Caliﬀ, F. E. Harrell, K. L. Lee, J. S. Rankin, and Others. The evolution of\\nmedical and surgical therapy for coronary artery disease. JAMA, 261:2077–2086,\\n1989.⋄484, 485, 510\\n89. R. M. Caliﬀ, H. R. Phillips, and Others. Prognostic value of a coronary artery\\njeopardy score. JA mC o l l e g eC a r d i o l , 5:1055–1063, 1985. ⋄207\\n90. R. M. Caliﬀ, L. H. Woodlief, F. E. Harrell, K. L. Lee, H. D. White, A. Guerci,\\nG. I. Barbash, R. Simes, W. Weaver, M. L. Simoons, E. J. Topol, and T. Inves-\\ntigators. Selection of thrombolytic therapy for individual patients: Developmen t\\nof a clinical model. Am Heart J , 133:630–639, 1997. ⋄4\\n91. A.J. Canty,A.C.Davison,D.V.Hinkley,and V.Venture. Bootstrap diagnostics\\nand remedies. Can J Stat , 34:5–27, 2006. ⋄122\\n92. J. Carpenter and J. Bithell. Bootstrap conﬁdence intervals: when, w hich, what?\\nA practical guide for medical statisticians. Stat Med , 19:1141–1164, 2000. ⋄122,\\n214\\n93. W. H. Carter, G. L. Wampler, and D. M. Stablein. Regression Analysis of\\nSurvival Data in Cancer Chemotherapy . Marcel Dekker, New York, 1983. ⋄477\\n94. Centers for Disease Control and Prevention CDC. National Center for Health\\nStatistics NCHS. National Health and Nutrition Examination Survey, 2010. ⋄\\n365\\n95. M. S. Cepeda, R. Boston, J. T. Farrar, and B. L. Strom. Comparison of logistic\\nregression versus propensity score when the number of events is low and there\\nare multiple confounders. Am J Epi , 158:280–287, 2003. ⋄272\\n96. J. M. Chambers and T. J. Hastie, editors. Statistical Models in S . Wadsworth\\nand Brooks/Cole, Paciﬁc Grove, CA, 1992. ⋄x, 29, 41, 128, 142, 245, 269, 493,\\n498\\n97. L. E. Chambless and K. E. Boyle. Maximum likelihood methods for com-\\nplex sample data: Logistic regression and discrete proportional hazards models.\\nComm Stat A , 14:1377–1392, 1985. ⋄215\\n98. R. Chappell. A note on linear rank tests and Gill and Schumacher’s tests of\\nproportionality. Biometrika , 79:199–201, 1992. ⋄495\\n99. C. Chatﬁeld. Avoiding statistical pitfalls (with discussion). Statistical Sci ,\\n6:240–268, 1991. ⋄91', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e9303d86-d4b8-460c-8ea1-e62f455ef769', embedding=None, metadata={'page_label': '544', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='544 References\\n100. C. Chatﬁeld. Model uncertainty, data mining and statistical inference (with\\ndiscussion). J Roy Stat Soc A , 158:419–466, 1995. ⋄vii, 9, 10, 11, 68, 100, 123,\\n204\\n101. S. Chatterjee and A. S. Hadi. Regression Analysis by Example . Wiley, New\\nYork, ﬁfth edition, 2012. ⋄78, 101\\n102. S. C. Cheng, J. P. Fine, and L. J. Wei. Prediction of cumulative incidence\\nfunction under the proportional hazards model. Biometrics , 54:219–228, 1998.\\n⋄415\\n103. S. C. Cheng, L. J. Wei, and Z. Ying. Predicting Survival Probabilities with\\nSemiparametric Transformation Models. JASA, 92(437):227–235, Mar. 1997. ⋄\\n517\\n104. F. Chiaromonte, R. D. Cook, and B. Li. Suﬃcient dimension reduction in\\nregressions with categorical predictors. Appl Stat , 30:475–497, 2002. ⋄101\\n105. B. Choodari-Oskooei, P. Royston, and M. K. B. Parmar. A simulation study\\nof predictive ability measures in a survival model II: explained randomness and\\npredictive accuracy. Stat Med , 31(23):2644–2659, 2012. ⋄518\\n106. B. Choodari-Oskooei, P. Royston, and M. K. B. Parmar. A simulation study of\\npredictive ability measures in a survival model I: Explained variation measures.\\nStat Med , 31(23):2627–2643, 2012. ⋄518\\n107. A. Ciampi, A. Negassa, and Z. Lou. Tree-structured prediction for censored\\nsurvival data and the Cox model. JC l i nE p i , 48:675–689, 1995. ⋄41\\n108. A.Ciampi,J.Thiﬀault,J.P.Nakache,and B.Asselain. Stratiﬁcation bystepwise\\nregression, correspondence analysis and recursive partition. Comp Stat Data\\nAnalysis , 1986:185–204, 1986. ⋄41, 81\\n109. L. A. Clark and D. Pregibon. Tree-Based Models. In J. M. Chambers and T. J.\\nHastie, editors, Statistical Models in S , chapter 9, pages 377–419. Wadsworth\\nand Brooks/Cole, Paciﬁc Grove, CA, 1992. ⋄41\\n110. T. G. Clark and D. G. Altman. Developing a prognostic model in the presence\\nof missing data: an ovarian cancer case study. JC l i nE p i , 56:28–37, 2003. ⋄57\\n111. W. S. Cleveland. Robust locally weighted regression and smoothing scatterplots.\\nJ Am Stat Assoc , 74:829–836, 1979. ⋄29, 141, 238, 315, 356, 493\\n112. A. Cnaan and L. Ryan. Survival analysis in natural history studies of disease.\\nStat Med , 8:1255–1268, 1989. ⋄401, 420\\n113. T. J. Cole, C. J. Morley, A. J. Thornton, M. A. Fowler, and P. H. Hewson. A\\nscoring system to quantify illness in babies under 6 months of age. J Roy Stat\\nSoc A, 154:287–304, 1991. ⋄324\\n114. D. Collett. Modelling Survival Data in Medical Research . Chapman and Hall,\\nLondon, 1994. ⋄420, 517\\n115. D. Collett. Modelling Binary Data . Chapman and Hall, London, second edition,\\n2002.⋄213, 272, 315\\n116. A. F. Connors, T. Speroﬀ, N. V. Dawson, C. Thomas, F. E. Harrell, D. Wagner,\\nN. Desbiens, L. Goldman, A. W. Wu, R. M. Caliﬀ, W. J. Fulkerson, H. Vidaillet,\\nS. Broste, P. Bellamy, J. Lynn, W. A. Knaus, and T. S. Investigators. The eﬀec-\\ntiveness of right heart catheterization in the initial care of critically ill patients.\\nJAMA, 276:889–897, 1996. ⋄3\\n117. E. F. Cook and L. Goldman. Asymmetric stratiﬁcation: An outline for an eﬃ-\\ncient method for controlling confounding in cohort studies. Am J Epi , 127:626–\\n639, 1988. ⋄31, 231\\n118. N. R. Cook. Use and misues of the receiver operating characteristic curve in\\nrisk prediction. Circulation , 115:928–935, 2007. ⋄93, 101, 273\\n119. R. D. Cook. Fisher Lecture:Dimension reduction in regression. Statistical Sci ,\\n22:1–26, 2007. ⋄101\\n120. R. D. Cook and L. Forzani. Principal ﬁtted components for dimension reduction\\nin regression. Statistical Sci , 23(4):485–501, 2008. ⋄101', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02698b8b-7886-4fcc-8d39-ae513e25fb61', embedding=None, metadata={'page_label': '545', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 545\\n121. J. Copas. The eﬀectiveness of risk scores: The logit rank plot. Appl Stat , 48:165–\\n183, 1999. ⋄273\\n122. J. B. Copas. Regression, prediction and shrinkage (with discussion). J Roy Stat\\nSoc B, 45:311–354, 1983. ⋄100, 101\\n123. J. B. Copas. Cross-validation shrinkage of regression predictors. J Roy Stat Soc\\nB, 49:175–183, 1987. ⋄115, 123, 273, 508\\n124. J. B. Copas. Unweighted sum of squares tests for proportions. Appl Stat , 38:71–\\n80, 1989. ⋄236\\n125. J. B. Copas and T. Long. Estimating the residual variance in orthogonal regres-\\nsion with variable selection. The Statistician , 40:51–59, 1991. ⋄68\\n126. C. Cox. Location-scale cumulative odds models for ordinal data: A generalized\\nnon-linear model approach. Stat Med , 14:1191–1203, 1995. ⋄324\\n127. C. Cox. The generalized fdistribution: An umbrella for parametric survival\\nanalysis. Stat Med , 27:4301–4313, 2008. ⋄424\\n128. C. Cox, H. Chu, M. F. Schneider, and A. Mu˜ noz. Parametric survival analysis\\nand taxonomy of hazard functions for the generalized gamma distribution. Stat\\nMed, 26:4352–4374, 2007. ⋄424\\n129. D. R. Cox. The regression analysis of binary sequences (with discussion). JR o y\\nStat Soc B , 20:215–242, 1958. ⋄14, 220\\n130. D. R. Cox. Two further applications of a model for binary regression.\\nBiometrika , 45(3/4):562–565, 1958. ⋄259\\n131. D. R. Cox. Further results on tests of separate families of hypotheses. JR o y\\nStat Soc B , 24:406–424, 1962. ⋄205\\n132. D. R. Cox. Regression models and life-tables (with discussion). J Roy Stat Soc\\nB, 34:187–220, 1972. ⋄39, 41, 172, 207, 213, 314, 418, 428, 475, 476\\n133. D. R. Cox and D.Oakes. Analysis of Survival Data . Chapman andHall, London,\\n1984.⋄401, 420, 517\\n134. D. R. Cox and E. J. Snell. A general deﬁnition of residuals (with discussion). J\\nRoy Stat Soc B , 30:248–275, 1968. ⋄440\\n135. D. R. Cox and E. J. Snell. The Analysis of Binary Data . Chapman and Hall,\\nLondon, second edition, 1989. ⋄206\\n136. D. R. Cox and N. Wermuth. A comment on the coeﬃcient of determination for\\nbinary responses. Am Statistician , 46:1–4, 1992. ⋄206, 256\\n137. J. G. Cragg and R. Uhler. The demand for automobiles. Canadian Journal of\\nEconomics , 3:386–406, 1970. ⋄206, 256\\n138. S. L. Crawford, S. L. Tennstedt, and J. B. McKinlay. A comparison of analytic\\nmethods for non-random missingness of outcome data. JC l i nE p i , 48:209–219,\\n1995.⋄58\\n139. N. J. Crichton and J. P. Hinde. Correspondence analysis as a screening method\\nfor indicants for clinical diagnosis. Stat Med , 8:1351–1362, 1989. ⋄81\\n140. N. J. Crichton, J. P. Hinde, and J. Marchini. Models for diagnosing chest pain :\\nIs CART useful? Stat Med , 16:717–727, 1997. ⋄41\\n141. L. A. C upples, D. R. Gagnon, R. Ramaswamy, and R. B. D’Agostino. Age-\\nadjusted survival curves with application in the Framingham Study. Stat Med ,\\n14:1731–1744, 1995. ⋄517\\n142. E. E. Cureton and R. B. D’Agostino. Factor Analysis, An Applied Approach .\\nErlbaum, Hillsdale, NJ, 1983. ⋄81, 87, 101\\n143. D. M. Dabrowska, K. A. Doksum, N. J. Feduska, R. Husing, and P. Neville.\\nMethods for comparing cumulative hazard functions in a semi-proportional haz-\\nard model. Stat Med , 11:1465–1476, 1992. ⋄482, 495, 502\\n144. R. B. D’Agostino, A. J. Belanger, E. W. Markson, M. Kelly-Hayes, and P. A.\\nWolf. Development of health risk appraisal functions in the presence of multiple\\nindicators: The Framingham Study nursing home institutionalization model.\\nStat Med , 14:1757–1770, 1995. ⋄81, 101', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='67e9e8e5-d704-48d6-8281-cdd46e177c32', embedding=None, metadata={'page_label': '546', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='546 References\\n145. R. B. D’Agostino, M. L. Lee, A. J. Belanger, and L. A . Cupples. Relation\\nof pooled logistic regression to time dependent Cox regression analysis: The\\nFramingham Heart Study. Stat Med , 9:1501–1515, 1990. ⋄447\\n146. D’Agostino, Jr and D. B. Rubin. Estimating and using propensity scores with\\npartially missing data. J Am Stat Assoc , 95:749–759, 2000. ⋄58\\n147. C. E.Davis,J. E.Hyde,S.I.Bangdiwala, and J. J.Nelson. An example of depen-\\ndencies among variables in a conditional logistic regression. In S. H. Moolgavkar\\nand R. L. Prentice, editors, Modern Statistical Methods in Chronic Disease Epi ,\\npages 140–147. Wiley, New York, 1986. ⋄79, 138, 255\\n148. C. S. Davis. Statistical Methods for the Analysis of Repeated Measurements .\\nSpringer, New York, 2002. ⋄143, 149\\n149. R. B. Davis and J. R. Anderson. Exponential survival trees. Stat Med , 8:947–\\n961, 1989. ⋄41\\n150. A. C. Davison and D. V. Hinkley. Bootstrap Methods and Their Application .\\nCambridge University Press, Cambridge, 1997. ⋄70, 106, 109, 122\\n151. R. J. M. Dawson. The ‘Unusual Episode’ data revisited. JS t a tE d u , 3(3),\\n1995. Online journal at www.amstat.org/publications/jse/v3n3/datasets.-\\ndawson.html .⋄291\\n152. C. de Boor. A Practical Guide to Splines . Springer-Verlag, New York, revised\\nedition, 2001. ⋄23, 40\\n153. J. de Leeuw and P. Mair. Giﬁ methods for optimal scaling in r: The package\\nhomals.J Stat Software , 31(4):1–21, Aug. 2009. ⋄101\\n154. E. R. DeLong, C. L. Nelson, J. B. Wong, D. B. Pryor, E. D. Peterson, K. L.\\nLee, D. B. Mark, R. M. Caliﬀ, and S. G. Pauker. Using observational data to\\nestimate prognosis: an example using a coronary artery disease registry. Stat\\nMed, 20:2505–2532, 2001. ⋄420\\n155. S.Derksen and H. J. Keselman. Backward, forward and stepwiseautomated sub-\\nset selection algorithms: Frequency of obtaining authentic and noise variables.\\nBritish J Math Stat Psych , 45:265–282, 1992. ⋄68\\n156. T.F.Devlin andB.J.Weeks. Splinefunctionsfor logisticregression modeling. I n\\nProceedings of the El eventh Annual SAS Users Group International Conference ,\\npages 646–651, Cary, NC, 1986. SAS Institute, Inc. ⋄21, 24\\n157. T. DiCiccio and B. Efron . More accurate conﬁdence intervals in exponential\\nfamilies. Biometrika , 79:231–245, 1992. ⋄214\\n158. E. R.Dickson,P. M.Grambsch,T. R.Fleming, L. D.Fisher,and A.Langw orthy.\\nPrognosis in primary biliary cirrhosis: Model for decision making. Hepatology ,\\n10:1–7, 1989. ⋄178\\n159. P. J. Diggle, P. Heagerty, K.-Y. Liang, and S. L. Zeger. Analysis of Longitudinal\\nData. Oxford University Press, Oxford UK, second edition, 2002. ⋄143, 147\\n160. N. Doganaksoy and J. Schmee. Comparisons of approximate co nﬁdence intervals\\nfor distributions used in life-data analysis. Technometrics , 35:175–184, 1993. ⋄\\n198, 214\\n161. Donders, G. J. M. G. van der Heijden, T. Stijnen, and K. G. M. Moons. Review:\\nA gentle introduction to imputation of missingvalues. JC l i nE p i , 59:1087–1091,\\n2006.⋄49, 58\\n162. A. Donner. The relative eﬀectiveness of procedures commonly used in multiple\\nregression analysis for dealing with missing values. Am Statistician , 36:378–381,\\n1982.⋄48, 52\\n163. D. Draper. Assessment and propagation of model uncertainty (with discussion).\\nJ Roy Stat Soc B , 57:45–97, 1995. ⋄10, 11\\n164. M. Drum and P. McCullagh. Comment on regression models for discrete lon-\\ngitudinal responses by G. M. Fitzmaurice, N. M. Laird, and A. G. Rotnitzky.\\nStat Sci, 8:300–301, 1993. ⋄197\\n165. N. Duan. Smearing estimate: A nonparametric retransformation method. JA m\\nStat Assoc , 78:605–610, 1983. ⋄392', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='577a8c46-cb8f-4530-abff-9677814e6ea6', embedding=None, metadata={'page_label': '547', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 547\\n166. J. A. Dubin, H. M ¨uller, and J. Wang. Event history graphs for censored data.\\nStat Med , 20:2951–2964, 2001. ⋄418, 420\\n167. R. Dudley, F. E. Harrell, L. Smith, D. B. Mark, R. M. Caliﬀ, D. B. Pryor,\\nD. Glower, J. Lipscomb, and M. Hlatky. Comparison of analytic models for\\nestimating the eﬀect of clinical factors on the cost of coronary artery bypass\\ngraft surgery. JC l i nE p i , 46:261–271, 1993. ⋄x\\n168. S. Durrleman and R. Simon. Flexible regression models with cubic splines. Stat\\nMed, 8:551–561, 1989. ⋄40\\n169. J. P. Eaton and C. A. Haas. Titanic: Triumph and Tragedy .W .W .N o r t o n ,\\nNew York, second edition, 1995. ⋄291\\n170. B. Efron. The two sample problem with censored data. In Proceedings of the\\nFifth Berkeley Symposium on Mathematical Statistics and Probability ,v o lume4 ,\\npages 831–853. 1967. ⋄505\\n171. B. Efron. The eﬃciency of Cox’s likelihood function for censored data. JA m\\nStat Assoc , 72:557–565, 1977. ⋄475, 477\\n172. B. Efron. Estimating the error rate of a prediction rule: Improvement on cross-\\nvalidation. J Am Stat Assoc , 78:316–331, 1983. ⋄70, 113, 114, 115, 116, 123,\\n259\\n173. B. Efron. How biased is the apparent error rate of a prediction rule? JA mS t a t\\nAssoc, 81:461–470, 1986. ⋄101, 114\\n174. B. Efron. Missing data, imputation, and the bootstrap (with discussion). JA m\\nStat Assoc , 89:463–479, 1994. ⋄52, 54\\n175. B. Efron and G. Gong. A leisurely look at the bootstrap, the jackknife, and\\ncross-validation. Am Statistician , 37:36–48, 1983. ⋄114\\n176. B. Efron and C. Morris. Stein’s paradox in statistics. Sci Am, 236(5):119–127,\\n1977.⋄77\\n177. B. Efron and R. Tibshirani. Boots trap methods for standard errors, co nﬁdence\\nintervals, and other measures of statistical accuracy. Statistical Sci , 1:54–77,\\n1986.⋄70, 106, 114, 197\\n178. B. Efron and R. Tibshirani. An Introduction to the Bootstrap . Chapman and\\nHall, New York, 1993. ⋄70, 106, 114, 115, 122, 197, 199\\n179. B. Efron and R. Tibshirani. Improvements on cross-validation: The .632+ boot-\\nstrap method. J Am Stat Assoc , 92:548–560, 1997. ⋄123, 124\\n180. G. E. Eide, E. Omenaas, and A. Gulsvik. The semi-proportional hazards model\\nrevisited: Practical reparameterizations. Stat Med , 15:1771–1777, 1996. ⋄482\\n181. C. Faes, G. Molenberghs, M. Aerts, G. Verbeke, and M. G. Kenward. The\\neﬀectivesamplesizeand an alternativesmall-sample degrees-of-freedom method.\\nAm Statistician , 63(4):389–399, 2009. ⋄148\\n182. M. W. Fagerland and D. W. Hosmer. A goodness-of-ﬁt test for the proportional\\nodds regression model. Stat Med , 32(13):2235–2249, 2013. ⋄317\\n183. J. Fan and R. A. Levine. To amnio or not to amnio: That is the decision for\\nBayes.Chance, 20(3):26–32, 2007. ⋄5\\n184. D. Faraggi, M. LeBlanc, and J. Crowley. Understanding neural networks using\\nregression trees: an application to multiple myeloma survival data. Stat Med ,\\n20:2965–2976, 2001. ⋄120\\n185. D. Faraggi and R. Simon. A simulation study of cross-validation for selecting an\\noptimal cutpoint in univariate survival analysis. Stat Med , 15:2203–2213, 1996.\\n⋄11, 19\\n186. J. J. Faraway. The cost of data analysis. J Comp Graph Stat , 1:213–229, 1992.\\n⋄10, 11, 97, 100, 115, 116, 322, 393, 396\\n187. V. Fedorov, F. Mannino, and R. Zhang. Consequences of dichotomization.\\nPharm Stat , 8:50–61, 2009. ⋄5, 19\\n188. Z. Feng, D. McLerran, and J. Grizzle. A comparison of statistical methods for\\nclustered data analysis with Gaussian error. Stat Med , 15:1793–1806, 1996. ⋄\\n197, 213', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5d56b03e-44cd-4f8f-ba17-d34116dc9ae5', embedding=None, metadata={'page_label': '548', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='548 References\\n189. L. Ferr´ e. Determining the dimension in sliced inverse regression and related\\nmethods. J Am Stat Assoc , 93:132–149, 1998. ⋄101\\n190. S. E. Fienberg. The Analysis of Cross-Classiﬁed Categorical Data . Springer,\\nNew York, second edition, 2007. ⋄311, 319\\n191. P. Filzmoser, H. Fritz, and K. Kalcher. pcaPP: Robust PCA by Projection Pur-\\nsuit, 2012. R package version 1.9–48. ⋄175\\n192. J. P. Fine and R. J. Gray. A proportional hazards model for the subdistribution\\nof a competing risk. J Am Stat Assoc , 94:496–509, 1999. ⋄420\\n193. D. M. Finkelstein and D. A. Schoenfeld. Combining mortality and longitud inal\\nmeasures in clinical trials. Stat Med , 18:1341–1354, 1999. ⋄420\\n194. M. Fiocco, H. Putter, and H. C. van Houwelingen. Reduced-rank proportional\\nhazards regression and simulation-based predictino for multi-state models. Stat\\nMed, 27:4340–4358, 2008. ⋄420\\n195. G. M. Fitzmaurice. A caveat concerning independence estimating equations\\nwith multivariate binary data. Biometrics , 51:309–317, 1995. ⋄214\\n196. T. R. Fleming and D. P. Harrington. Nonparametric estimation of the survival\\ndistribution in censored data. Comm Stat Th Meth , 13(20):2469–2486, 1984. ⋄\\n413\\n197. T. R. Fleming and D. P. Harrington. Counting Processes & Survival Analysis .\\nWiley, New York, 1991. ⋄178, 420\\n198. I. Ford, J. Norrie, and S. Ahmadi. Model inconsistency, illustrated by the Cox\\nproportional hazards model. Stat Med , 14:735–746, 1995. ⋄4\\n199. E. B. Fowlkes. Some diagnostics for binary logistic regression via smoothing.\\nBiometrika , 74:503–515, 1987. ⋄272\\n200. J. Fox. Applied Regression Analysis, Linear Models, and Related Methods .\\nSAGE Publications, Thousand Oaks, CA, 1997. ⋄viii\\n201. J. Fox. An R and S-PLUS Companion to Applied Regression . SAGE Publica-\\ntions, Thousand Oaks, CA, 2002. ⋄viii\\n202. J. Fox. Applied Regression Analysis and Generalized Linear Models .S A G E\\nPublications, Thousand Oaks, CA, second edition, 2008. ⋄121\\n203. Fox, John. Bootstrapping Regression Models: An Appendix to An R and S-\\nPLUS Companion to Applied Regression, 2002. ⋄202\\n204. B. Francis and M. Fuller. Visualization of event histories. J Roy Stat Soc A ,\\n159:301–308, 1996. ⋄421\\n205. D. Freedman, W. Navidi, and S. Peters. O nt h eI m p a c to fV a r i a b l eS e l e c t i o n\\nin Fitting Regression Equations , pages 1–16. Lecture Notes in Economics and\\nMathematical Systems. Springer-Verlag, New York, 1988. ⋄115\\n206. D. A. Freedman. On the so-called “Huber sandwich estimator” and “robust\\nstandard errors”. Am Statistician , 60:299–302, 2006. ⋄213\\n207. J. H. Friedman. A variable span smoother. Technical Report 5, Laboratory for\\nComputational Statistics, Department of Statistics, Stanford University, 1984.\\n⋄29, 82, 141, 210, 273, 498\\n208. L. Friedman and M. Wall. Graphical views of suppression and multicollinearity\\nin multiple linear regression. Am Statistician , 59:127–136, 2005. ⋄101\\n209. M. H. Gail. Does cardiac transplantation prolong life? A reassessment. Ann Int\\nMed, 76:815–817, 1972. ⋄401\\n210. M. H. Gail and R. M. Pfeiﬀer. On criteria for evaluating models of absolute\\nrisk.Biostatistics , 6(2):227–239, 2005. ⋄5\\n211. J. C. Gardiner, Z. Luo, and L. A. Roman. Fixed eﬀects, random eﬀects and\\nGEE: What are the diﬀerences? Stat Med , 28:221–239, 2009. ⋄160\\n212. J. J. Gaynor, E. J. Feuer, C. C. Tan, D. H. Wu, C. R. Little, D. J. Straus,\\nD. D. Clarkson, and M. F. Brennan. On the use of cause-speciﬁc failure and\\nconditional failure probabilities: Examples from clinical oncology data. JA m\\nStat Assoc , 88:400–409, 1993. ⋄414, 415', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a4f49dde-40ab-4133-9cb6-589b24615038', embedding=None, metadata={'page_label': '549', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 549\\n213. A. Gelman. Scaling regression i nputs by dividing by two standard deviations.\\nStat Med , 27:2865–2873, 2008. ⋄121\\n214. R. B. Geskus. Cause-speciﬁc cumulative incidence estimation and the Fine\\nand Gray model under both left truncation and right censoring. Biometrics ,\\n67(1):39–49, 2011. ⋄420\\n215. A. Giannoni, R. Baruah, T. Leong, M. B. Rehman, L. E. Pastormerlo, F. E.\\nHarrell, A. J. Coats, and D. P. Francis. Do optimal prognostic thresholds in\\ncontinuous physiological variables really exist? Analysis of origin of apparent\\nthresholds, with systematic review for peak oxygen consumption, ejection frac-\\ntion and BNP. PLoS ONE , 9(1), 2014. ⋄19, 20\\n216. J. H. Giudice, J. R. Fieberg, and M. S. Lenarz. Spending degrees of freedom\\nin a poor economy: A case study of building a sightability model for moose in\\nnortheastern minnesota. J Wildlife Manage , 2011.⋄100\\n217. S. A. Glantz and B. K. Slinker. Primer of Applied Regression and Analysis of\\nVariance . McGraw-Hill, New York, 1990. ⋄78\\n218. M. Glasser. Exponential survival with covariance. J Am Stat Assoc , 62:561–568,\\n1967.⋄431\\n219. T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and\\nestimation. J Am Stat Assoc , 102:359–378, 2007. ⋄4, 5, 273\\n220. A. I. Goldman. EVENTCHARTS: Visualizing survival and other timed-events\\ndata.Am Statistician , 46:13–18, 1992. ⋄420\\n221. H. Goldstein. Restricted unbiased iterative generalized least-squares estimation.\\nBiometrika , 76(3):622–623, 1989. ⋄146, 147\\n222. R. Goldstein. The comparison of models in discrimination cases. Jurimetrics J ,\\n34:215–234, 1994. ⋄215\\n223. M. G ¨onen and G. Heller. Concordance probability and discriminatory power in\\nproportional hazards regression. Biometrika , 92(4):965–970, Dec. 2005. ⋄122,\\n519\\n224. G. Gong. Cross-validation, the jackknife, and the bootstrap: Excess error es-\\ntimation in forward logistic regression. J Am Stat Assoc , 81:108–113, 1986. ⋄\\n114\\n225. T. A. Gooley, W. Leisenring, J. Crowley, and B. E. Storer. Estimation of fail-\\nure probabilities in the presence of competing risks: New representations of old\\nestimators. Stat Med , 18:695–706, 1999. ⋄414\\n226. S. M. Gore, S. J. Pocock, and G. R. Kerr. Regression models and non-\\nproportional hazards in the analysis of breast cancer survival. Appl Stat , 33:176–\\n195, 1984. ⋄450, 495, 500, 501, 503\\n227. H. H. H. G ¨oring, J. D. Terwilliger, and J. Blangero. Large upward bias in\\nestimation of locus-speciﬁc eﬀects from genomewide scans. Am J Hum Gen ,\\n69:1357–1369, 2001. ⋄100\\n228. W. Gould. Co nﬁdence intervals in logit and probit models. S t a t aT e c hB u l l ,\\nSTB-14:26–28, July 1993. http://www.stata.com/products/stb/journals/\\nstb14.pdf .⋄186\\n229. U. S. Govindarajulu, H. Lin, K. L. Lunetta, and R. B. D’Agostino. Frailty\\nmodels: Applications to biomedical and genetic studies. Stat Med , 30(22):2754–\\n2764, 2011. ⋄420\\n230. U. S. Govindarajulu, D. Spiegelman, S. W. Thurston, B. Ganguli, and E. A.\\nEisen. Comparing smoothing techniques in Cox models for exposure-response\\nrelationships. Stat Med , 26:3735–3752, 2007. ⋄40\\n231. I. M. Graham and E. Clavel. Communicating risk — coronary risk scores. J\\nRoy Stat Soc A , 166:217–223, 2003. ⋄122\\n232. J. W. Graham, A. E. Olchowski, and T. D. Gilreath. How many imputations\\nare really needed? Some practical clariﬁcations of multiple imputation theory.\\nPrev Sci , 8:206–213, 2007. ⋄54', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='74d19cd6-5d02-4441-9e56-c9c2003c5f1b', embedding=None, metadata={'page_label': '550', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='550 References\\n233. P. Grambsch and T. Therneau. Proportional hazards tests and diagnostics\\nbased on weighted residuals. Biometrika , 81:515–526, 1994. Amendment and\\ncorrections in 82: 668 (1995). ⋄314, 498, 499, 518\\n234. P. M. Grambsch and P. C. O’Brien. The eﬀects of transformations and prelim-\\ninary tests for non-linearity in regression. Stat Med , 10:697–709, 1991. ⋄32, 36,\\n68\\n235. B. I. Graubard and E. L. Korn. Regression analysis with clustered data. Stat\\nMed, 13:509–522, 1994. ⋄214\\n236. R. J. Gray. Some diagnostic methods for Cox regression models through hazard\\nsmoothing. Biometrics , 46:93–102, 1990. ⋄518\\n237. R. J. Gray. Flexible methods for analyzing survival data using splines , with\\napplications to breast cancer prognosis. J Am Stat Assoc , 87:942–951, 1992. ⋄\\n30, 41, 77, 209, 210, 211, 345, 346, 500\\n238. R. J. Gray. Spline-based tests in survivalanalysis. Biometrics , 50:640–652, 1994.\\n⋄30, 41, 500\\n239. M. J. Greenacre. Correspondence analysis of multivariate categorical data by\\nweighted least-squares. Biometrika , 75:457–467, 1988. ⋄81\\n240. S. Greenland. Alternative models for ordinal logistic regression. Stat Med ,\\n13:1665–1677, 1994. ⋄324\\n241. S. Greenland. When should epidemiologic regressions use random coeﬃcients?\\nBiometrics , 56:915–921, 2000. ⋄68, 100, 215\\n242. S. Greenland and W. D. Finkle. A critical look at methods for handling missin g\\ncovariates in epidemiologic regression analyses. Am J Epi , 142:1255–1264, 1995.\\n⋄46, 59\\n243. A. J. Gross and V. A. Clark. Survival Distributions: Reliability Applications in\\nthe Biomedical Sciences . Wiley, New York, 1975. ⋄408\\n244. S. T. Gross and T. L. Lai. Nonparametric estimation and regression analysis\\nwith left-truncated and right-censored data. J Am Stat Assoc , 91:1166–1180,\\n1996.⋄420\\n245. A. Guisan and F. E. Harrell. Ordinal response regression models in ecology. J\\nVeg Sci, 11:617–626, 2000. ⋄324\\n246. J. Guo, G. James, E. Levina, G. Michailidis, and J. Zhu. Principal component\\nanalysis with sparse fused loadings. J Comp Graph Stat , 19(4):930–946, 2011.\\n⋄101\\n247. M. J. Gurka, L. J. Edwards, and K. E. Muller. Avoiding bias in mixed model\\ninference for ﬁxed eﬀects. Stat Med , 30(22):2696–2707, 2011. ⋄160\\n248. P.Gustafson. Bayesianregression modeling withinteractions andsmooth eﬀects.\\nJ Am Stat Assoc , 95:795–806, 2000. ⋄41\\n249. P. Hall and H. Miller. Using generalized correlation to eﬀect variable selection\\nin very high dimensional problems. J Comp Graph Stat , 18(3):533–550, 2009. ⋄\\n100\\n250. P. Hall and H. Miller. Using the bootstrap to quantify the authority of an\\nempirical ranking. Ann Stat , 37(6B):3929–3959, 2009. ⋄117\\n251. M. Halperin, W. C. Blackwelder, and J. I. Verter. Estimation of the multivariate\\nlogistic risk function: A comparison of the discriminant function and maximum\\nlikelihood approaches. J Chron Dis , 24:125–158, 1971. ⋄272\\n252. D. Hand and M. Crowder. Practical Longitudinal Data Analysis . Chapman &\\nHall, London, 1996. ⋄143\\n253. D. J. Hand. Construction and Assessment of Classiﬁcation Rules . Wiley, Chich-\\nester, 1997. ⋄273\\n254. T. L. Hankins. Blood, dirt, and nomograms. Chance, 13(1):26–37, 2000. ⋄104,\\n122, 267\\n255. J. A. Hanley and B. J. McNeil. The m eaning and use of the area under a receiver\\noperating characteristic (ROC) curve. Radiology , 143:29–36, 1982. ⋄257', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='1b9e0c98-a575-4ae9-bd4b-a642d2a9f54f', embedding=None, metadata={'page_label': '551', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 551\\n256. O. Harel and X. Zhou. Multiple imputation: Review of theory, implementation\\nand software. Stat Med , 26:3057–3077, 2007. ⋄46, 50, 58\\n257. F. E. Harrell. The LOGIST Procedure. In SUGI Supplemental Library Users\\nGuide, pages 269–293. SAS Institute, Inc., Cary, NC, Version 5 edition, 1986. ⋄\\n69\\n258. F. E. Harrell. The PHGLM Procedure. In SUGI Supplemental Library Users\\nGuide, pages 437–466. SAS Institute, Inc., Cary, NC, Version 5 edition, 1986. ⋄\\n499\\n259. F. E. Harrell. Comparison of strategies for validating binary logistic regression\\nmodels. Unpublished manuscript, 1991.⋄115, 259\\n260. F. E. Harrell. Semiparametric modeling of health care cost and resource uti-\\nlization. Available from hesweb1.med.virginia.edu/biostat/presentations ,\\n1999.⋄x\\n261. F. E. Harrell. rms: R functions for biostatistical/epidemiologic modeling, testing,\\nestimation, validation, graphics, prediction, and typesettingby storingenha nced\\nmodel design attributes in the ﬁt, 2013. Implements methods in Regression\\nModeling Strategies , New York:Springer, 2001. ⋄127\\n262. F. E. Harrell, R. M. Caliﬀ, D. B. Pryor, K. L. Lee, and R. A. Rosati. Evaluating\\nthe yield of medical tests. JAMA, 247:2543–2546, 1982. ⋄505\\n263. F. E. Harrell and R. Goldstein. A survey of microcomputer survival analysis\\nsoftware: The need for an integrated framework. Am Statistician , 51:360–373,\\n1997.⋄142\\n264. F. E. Harrell and K. L. Lee. A comparison of the discrimination of discriminant\\nanalysis and logistic regression under multivariate normality. In P. K. Sen,\\neditor,Biostatistics: Statistics in Biomedical, Public Health, and Environmental\\nSciences. The Bernard G. Greenberg Volume , pages 333–343. North-Holland,\\nAmsterdam, 1985. ⋄205, 207, 258, 272\\n265. F. E. Harrell and K. L. Lee. The practical value of logistic regression. In\\nProceedings of the Tenth Annual SAS Users Group International Co nference ,\\npages 1031–1036, 1985. ⋄237\\n266. F. E. Harrell and K. L. Lee. Verifying assumptions of the Cox proportional\\nhazards model. In Proceedings of the El eventh Annual SAS Users Group Inter-\\nnational Conference , pages 823–828, Cary, NC, 1986. SAS Institute, Inc. ⋄495,\\n499, 501\\n267. F. E. Harrell and K. L. Lee. Using logistic model calibration to assess the quality\\nof probability predictions. Unpublished manuscript, 1987.⋄259, 269, 507, 508\\n268. F. E. Harrell, K. L. Lee, R. M. Caliﬀ, D. B. Pryor, and R. A. Rosati. Regression\\nmodeling strategies for improved prognostic prediction. Stat Med , 3:143–152,\\n1984.⋄72, 101, 332, 505\\n269. F. E. Harrell, K. L. Lee, and D. B. Mark. Multivariableprognostic models: Issues\\nin developing models, evaluating assumptions and adequacy, and measuring and\\nreducing errors. Stat Med , 15:361–387, 1996. ⋄xi, 100\\n270. F. E. Harrell, K. L. Lee, D. B. Matchar, and T. A. Reichert. Regression models\\nfor prognostic prediction: Advantages, problems, and suggested solutions. Ca\\nTrt Rep, 69:1071–1077, 1985. ⋄41, 72\\n271. F. E. Harrell, K. L. Lee, and B. G. Pollock. Regression models in clinical studies:\\nDetermining relationships between predictors and response. JN a tC a n c e rI n s t ,\\n80:1198–1202, 1988. ⋄30, 40\\n272. F. E. Harrell, P. A. Margolis, S. Gove, K. E. Mason, E. K. Mulholland,\\nD. Lehmann, L. Muhe, S. Gatchalian, and H. F. Eichenwald. Development of a\\nclinical prediction model for an ordinal outcome: The World Health Organiza-\\ntion ARI Multicentre Study of clinical signs and etiologic agents of pneumonia,\\nsepsis, and meningitis in young infants. Stat Med , 17:909–944, 1998. ⋄xi, 77, 96,\\n327', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='3a6c8295-aff2-4e22-ab93-a0c76e1c582a', embedding=None, metadata={'page_label': '552', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='552 References\\n273. D. P. Harrington and T. R. Fleming. A class of rank test procedures for censored\\nsurvival data. Biometrika , 69:553–566, 1982. ⋄517\\n274. T. Hastie. Discussionof“The use of polynomial splines and their tensor products\\nin multivariate function estimation”by C. J. Stone. Appl Stat , 22:177–179, 1994.\\n⋄37\\n275. T. Hastie and R. Tibshirani. Generalized Additive Models . Chapman and Hall,\\nLondon, 1990. ⋄29, 41, 142, 390\\n276. T. J. Hastie, J. L. Botha, and C. M. Schnitzler. Regression with an ordered\\ncategorical response. Stat Med , 8:785–794, 1989. ⋄324\\n277. T. J. Hastie and R. J. Tibshirani. Generalized Additive Models . Chapman &\\nHall/CRC, Boca Raton, FL, 1990. ISBN 9780412343902. ⋄90, 359\\n278. W. W. Hauck and A. Donner. Wald’s test as applied to hypotheses in logit\\nanalysis. J Am Stat Assoc , 72:851–863, 1977. ⋄193, 234\\n279. X. He and L. Shen. Linear regression after spline transformation. Biometrika ,\\n84:474–481, 1997. ⋄82\\n280. Y. He and A. M. Zaslavsky. Diagnosing imputation models by applying target\\nanalyses to posterior replicates of completed data. Stat Med , 31(1):1–18, 2012.\\n⋄59\\n281. G. Heinze and M. Schemper. A solution to the problem of separation in logistic\\nregression. Stat Med , 21(16):2409–2419, 2002. ⋄203\\n282. R. Henderson. Problems and prediction in survival-data analysis. Stat Med ,\\n14:161–184, 1995. ⋄420, 518, 519\\n283. R. Henderson, M. Jones, and J. Stare. Accuracy of point predictions in survival\\nanalysis. Stat Med , 20:3083–3096, 2001. ⋄519\\n284. A. V. Hern´ andez, M. J. Eijkemans, and E. W. Steyerberg. Randomized con-\\ntrolled trials with time-to-event outcomes: how much does prespeciﬁed covariate\\nadjustment increase power? Annals of epidemiology , 16(1):41–48, Jan. 2006. ⋄\\n231\\n285. A. V. Hern´ andez, E. W. Steyerberg, and J. D. F. Habbema. Covariate ad-\\njustment in randomized controlled trials with dichotomous outcomes increase s\\nstatistical power and reduces sample size requirements. JC l i nE p i , 57:454–460,\\n2004.⋄231\\n286. J. E. Herndon and F. E. Harrell. The restricted cubic spline hazard model.\\nComm Stat Th Meth , 19:639–663, 1990. ⋄408, 409, 424\\n287. J. E. Herndon and F. E. Harrell. The restricted cubic splineas baseline hazard in\\nthe proportional hazards model with step function time-dependent covar iables.\\nStat Med , 14:2119–2129, 1995. ⋄408, 424, 501, 518\\n288. I. Hertz-Picciotto and B. Rockhill. Validity and eﬃciency of approximation\\nmethods for tied survival times in Cox regression. Biometrics , 53:1151–1156,\\n1997.⋄477\\n289. K. R. Hess. Assessing time-by-covariate interactions in proportional hazards\\nregression models using cubic spline functions. Stat Med , 13:1045–1062, 1994. ⋄\\n501\\n290. K. R. Hess. Graphical methods for assessing violations of the proportional\\nhazards assumption in Cox regression. Stat Med , 14:1707–1723, 1995. ⋄518\\n291. T. Hielscher, M. Zucknick, W. Werft, and A. Benner. On the prognostic value\\nof survival models with application to gene expression signatures. Stat Med ,\\n29:818–829, 2010. ⋄518, 519\\n292. J. Hilden and T. A. Gerds. A note on the evaluation of novel biomarkers: do not\\nrely on integrated discrimination improvement and net reclassiﬁcation index.\\nStatist. Med. , 33(19):3405–3414, Aug. 2014. ⋄101\\n293. S. L. Hillis. Residual plots for the censored data linear regression model. Stat\\nMed, 14:2023–2036, 1995. ⋄450\\n294. S. G. Hilsenbeck and G. M. Clark. Practical p-value adjustment for optimally\\nselected cutpoints. Stat Med , 15:103–112, 1996. ⋄11, 19', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='37e5d307-b13e-4902-85f0-6a5f4cdd9795', embedding=None, metadata={'page_label': '553', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 553\\n295. W. Hoeﬀding. A non-parametric test of independence. Ann Math Stat , 19:546–\\n557, 1948. ⋄81, 166\\n296. H. Hofmann. Simpson on board the Titanic? Interactive methods for dealing\\nwith multivariate categorical data. Stat Comp Graphics News ASA , 9(2):16–19,\\n1999.http://stat-computing.org/newsletter/issues/scgn-09-2.pdf .⋄291\\n297. J. W. Hogan and N. M. Laird. Mixture models for the joint distribution of\\nrepeated measures and event times. Stat Med , 16:239–257, 1997. ⋄420\\n298. J. W. Hogan and N. M. Laird. Model-based approaches to analysing incomplete\\nlongitudinal and failure time data. Stat Med , 16:259–272, 1997. ⋄420\\n299. M. Hollander, I. W. McKeague, and J. Yang. Likelihood ratio-based co nﬁdence\\nbands for survival functions. J Am Stat Assoc , 92:215–226, 1997. ⋄420\\n300. N. Holl ¨ander, W. Sauerbrei, and M. Schumacher. Conﬁdence intervals for the\\neﬀect of a prognostic factor after selection of an ‘optimal’ cutpoint. Stat Med ,\\n23:1701–1713, 2004. ⋄19, 20\\n301. N. J. Horton and K. P. Kleinman. Much ado about nothing: A comparison of\\nmissing data methods and software to ﬁt incomplete data regression models.\\nAm Statistician , 61(1):79–90, 2007. ⋄59\\n302. N. J. Horton and S. R. Lipsitz. Multiple imputation in practice: Comparison of\\nsoftware packages for regression models with missing variables. Am Statistician ,\\n55:244–254, 2001. ⋄54\\n303. D. W. Hosmer, T. Hosmer, S. le Cessie, and S. Lemeshow. A comparison of\\ngoodness-of-ﬁt tests for the logistic regression model. Stat Med , 16:965–980,\\n1997.⋄236\\n304. D. W. Hosmer and S. Lemeshow. Goodness-of-ﬁt tests for the multiple logisti c\\nregression model. Comm Stat Th Meth , 9:1043–1069, 1980. ⋄236\\n305. D. W. Hosmer and S. Lemeshow. Applied Logistic Regression . Wiley, New York,\\n1989.⋄255, 272\\n306. D. W. Hosmer and S. Lemes how. Conﬁdence interval estimates of an index of\\nquality performance based on logistic regression models. Stat Med , 14:2161–\\n2172, 1995. See letter to editor 16:1301-3,1997. ⋄272\\n307. T. Hothorn, F. Bretz, and P. Westfall. Simultaneous inference in general para-\\nmetric models. Biometrical J , 50(3):346–363, 2008. ⋄xii, 199, 202\\n308. P. Hougaard. F undamentals of survival data. Biometrics , 55:13–22, 1999. ⋄400,\\n420, 450\\n309. B. Hu, M. Palta, and J. Shao. Properties of R2statistics for logistic regression.\\nStat Med , 25:1383–1395, 2006. ⋄272\\n310. J. Huang and D. Harrington. Penalized partial likelihood regression for right-\\ncensored data with bootstrap selection of the penalty parameter. Biometrics ,\\n58:781–791, 2002. ⋄215, 478\\n311. Y. Huangand M.Wang. Frequency ofrecurrent eventsat failure times:Modeling\\nand inference. J Am Stat Assoc , 98:663–670, 2003. ⋄420\\n312. P. J. Huber. The behavior of maximum likelihood estimates under nonstandard\\nconditions. In Proceedings of the Fifth Berkeley Symposium on Mathematical\\nStatistics and Probability , volume 1: Statistics, pages 221–233. University of\\nCalifornia Press, Berkeley, CA, 1967. ⋄196\\n313. S. Hunsberger, D. Murray, C. Davis, and R. R. Fabsitz. Imputation strategies\\nfor missing data in a school-based multi-center study: the Pathways study. Stat\\nMed, 20:305–316, 2001. ⋄59\\n314. C. M. Hurvich and C. Tsai. Regression and time series model selection in small\\nsamples. Biometrika , 76:297–307, 1989. ⋄214, 215\\n315. C. M. Hurvich and C. Tsai. Model selection for extended quasi-likelihoodmod els\\nin small samples. Biometrics , 51:1077–1084, 1995. ⋄214\\n316. C. M. Hurvich and C. L. Tsai. The impact of model selection on inference in\\nlinear regression. Am Statistician , 44:214–217, 1990. ⋄100', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='a7bfd92a-2a54-42cc-a669-93864aec15f6', embedding=None, metadata={'page_label': '554', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='554 References\\n317. L. I. Iezzoni. Dimensions of Risk. In L. I. Iezzoni, editor, Risk Adjustment\\nfor Measuring Health Outcomes , chapter 2, pages 29–118. Fo undation of the\\nAmerican College of Healthcare Executives, Ann Arbor, MI, 1994. ⋄7\\n318. R. Ihaka and R. Gentleman. R: A language for data analysis and graphics. J\\nComp Graph Stat , 5:299–314, 1996. ⋄127\\n319. K. Imai, G. King, and O. Lau. Towards a common framework for statistical\\nanalysis and development. J Comp Graph Stat , 17(4):892–913, 2008. ⋄142\\n320. J. E. Jackson. A User’s Guide to Principal Components . Wiley, New York,\\n1991.⋄101\\n321. K. J. Janssen, A. R. Donders, F. E. Harrell, Y. Vergouwe, Q. Chen, D. E.\\nGrobbee, and K. G. Moons. Missing covariate data in medical research: To\\nimpute is better than to ignore. JC l i nE p i , 63:721–727, 2010. ⋄54\\n322. H. Jiang, R. Chapell, and J. P. Fine. Estimating the distribution of nonterminal\\nevent time in the presence of mortality or informative dropout. Controlled Clin\\nTrials, 24:135–146, 2003. ⋄421\\n323. N.L.Johnson,S.Kotz, andN.Balakrishnan. Distributions in Statistics: Contin-\\nuous Univariate Distributions , volume 1. Wiley-Interscience, New York, second\\nedition, 1994. ⋄408\\n324. I. T. Jolliﬀe. Discarding variables in a principal component analysis. I. Artiﬁcial\\ndata.Appl Stat , 21:160–173, 1972. ⋄101\\n325. I. T. Jolliﬀe. Principal Component Analysis . Springer-Verlag, New York, second\\nedition, 2010. ⋄101, 172\\n326. M. P. Jones. Indicator and stratiﬁcation methods for missing explanatory vari-\\nables in multiple linear regression. J Am Stat Assoc , 91:222–230, 1996. ⋄49,\\n58\\n327. L. Joseph, P. Belisle, H. Tamim, and J. S. Sampalis. Selection bias found in\\ninterpreting analyses with missing data for the prehospital index for trauma. J\\nClin Epi , 57:147–153, 2004. ⋄58\\n328. M. Julien and J. A. Hanley. Proﬁle-speciﬁc survival estimates: Making reports\\nof clinical trials more patient-relevant. CT, 5:107–115, 2008. ⋄122\\n329. A. C. Justice, K. E. Covinsky, and J. A. Berlin. Assessing the generalizability\\nof prognostic information. Ann Int Med , 130:515–524, 1999. ⋄122\\n330. J. D. Kalbﬂeisch and R. L. Prentice. Marginal likelihood based on Cox’s regres-\\nsion and life model. Biometrika , 60:267–278, 1973. ⋄375, 478\\n331. J. D. Kalbﬂeisch and R. L. Prentice. The Statistical Analysis of Failure Time\\nData. Wiley, New York, 1980. ⋄411, 412, 414, 420, 436, 441, 483, 496, 517\\n332. G. Kalton and D. Kasprzyk. The treatment of missing survey data. Surv Meth ,\\n12:1–16, 1986. ⋄58\\n333. E. L. Kaplan and P. Meier. Nonparametric estimation from incomplete obser-\\nvations. J Am Stat Assoc , 53:457–481, 1958. ⋄410\\n334. T. Karrison. Restricted mean life with adjustment for covariates. JA mS t a t\\nAssoc, 82:1169–1176, 1987. ⋄406, 514\\n335. T. G. Karrison. Use of Irwin’s restricted mean as an index for comparing sur-\\nvival in diﬀerent treatment groups—Interpretation and power considerations.\\nControlled Clin Trials , 18:151–167, 1997. ⋄406, 503\\n336. J. Karvanen and F. E. Harrell. Visualizing covariates in proportional hazards\\nmodel.Stat Med , 28:1957–1966, 2009. ⋄104\\n337. R. E. Kass and A. E. Raftery. Bayes factors. J Am Stat Assoc , 90:773–795,\\n1995.⋄71, 214\\n338. M. W. Kattan, G. Heller, and M. F. Brennan. A competing-risks nomogram\\nfor sarcoma-speciﬁc death following local recurrence. Stat Med , 22:3515–3525,\\n2003.⋄519\\n339. M. W. Kattan and J. Marasco. What is a real nomogram? Sem Onc , 37(1):\\n23–26, Feb. 2010. ⋄104, 122', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5842919b-7438-4661-9c5d-6bfe204ccf5c', embedding=None, metadata={'page_label': '555', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 555\\n340. R. Kay. Treatment eﬀects in competing-risks analysis of prostate cancer data.\\nBiometrics , 42:203–211, 1986. ⋄276, 414, 495\\n341. R. Kay and S. Little. Assessing the ﬁt of the logistic model: A case study of\\nchildren with the haemolytic uraemic syndrome. Appl Stat , 35:16–30, 1986. ⋄\\n272\\n342. S. Kele¸ s and M. R. Segal. Residual-based tree-structured survival analysis. Stat\\nMed, 21:313–326, 2002. ⋄41\\n343. P. J. Kelly and L. Lim. Survivalanalysis for recurrent event data: An application\\nto childhood infectious diseases. Stat Med , 19:13–33, 2000. ⋄421\\n344. D. M. Kent and R. Hayward. Limitations of applying summary results of clinical\\ntrials to individual patients. JAMA, 298:1209–1212, 2007. ⋄4\\n345. J. T. Kent and J. O’Quigley. Measures of dependence for censored survival data.\\nBiometrika , 75:525–534, 1988. ⋄505\\n346. M. G. Kenward, I. R. White, and J. R. Carpener. Should baseline be a covariate\\nor dependentvariable inanalyses ofchange from baselinein clinicaltrials ?(letter\\nto the editor). Stat Med , 29:1455–1456, 2010. ⋄160\\n347. H. J. Keselman, J. Algina, R. K. Kowalchuk, and R. D. Wolﬁnger. A comparison\\nof two approaches for selecting covariance structures in the analysis of repeated\\nmeasurements. Comm Stat - Sim Comp , 27:591–604, 1998. ⋄69, 160\\n348. V. Kipnis. Relevancy criterion for discriminating among alternative model spec-\\niﬁcations. In K. Berk and L. Malone, editors, Proceedings of the 21st Sympo-\\nsium on the Interface between Computer Science and Statistics , pages 376–381,\\nAlexandria, VA, 1989. American Statistical Association. ⋄123\\n349. J. P. Klein, N. Keiding, and E. A. Copelan. Plotting summary predictions in\\nmultistate survival models: Probabilities of relapse and death in remission for\\nbone marrow transplantation patients. Stat Med , 12:2314–2332, 1993. ⋄415\\n350. J. P. Klein and M. L. Moeschberger. Survival Analysis: Techniques for Censored\\nand Truncated Data . Springer, New York, 1997. ⋄420, 517\\n351. W. A. Knaus, F. E. Harrell, C. J. Fisher, D. P. Wagner, S. M. Opan, J. C.\\nSadoﬀ, E. A. Draper, C. A. Walawander, K. Conboy, and T. H. Grasela. The\\nclinical evaluation of new drugs for sepsis: A prospective study design based on\\nsurvival analysis. JAMA, 270:1233–1241, 1993. ⋄4\\n352. W. A. Knaus, F. E. Harrell, J. Lynn, L. Goldman, R. S. Phillips, A. F. Connors,\\nN. V. Dawson, W. J. Fulkerson, R. M. Caliﬀ, N. Desbiens, P. Layde, R. K. Oye,\\nP. E. Bellamy, R. B. Hakim, and D. P. Wagner. The SUPPORT prognostic\\nmodel: Objective estimates of survival for seriously ill hospitalized adults. Ann\\nInt Med, 122:191–203, 1995. ⋄59, 84, 86, 453\\n353. M. J. Knol, K. J. M. Janssen, R. T. Donders, A. C. G. Egberts, E. R. Heerding,\\nD. E. Grobbee, K. G. M. Moons, and M. I. Geerlings. Unpredictable bias\\nwhen using the missing indicator method or complete case analysis for missing\\nconfounder values: an empirical example. JC l i nE p i , 63:728–736, 2010. ⋄47, 49\\n354. G. G. Koch, I. A. Amara, and J. M. Singer. A two-stage procedure for the\\nanalysis of ordinal categorical data. In P. K. Sen, editor, BIOSTATISTICS:\\nStatistics in Biomedical, Public Health and Environmental Sciences . Elsevier\\nScience Publishers B. V. (North-Holland), Amsterdam, 1985. ⋄324\\n355. R. Koenker. Quantile Regression . Cambridge University Press, New York, 2005.\\nISBN-10: 0-521-60827-9; ISBN-13: 978-0-521-60827-5. ⋄360\\n356. R. Koenker. quantreg : Quantile Regression , 2009. R package version 4.38. ⋄\\n131, 360\\n357. R. Koenker and G. Bassett. Regression quantiles. Econometrica , 46:33–50, 1978.\\n⋄131, 360, 392\\n358. M. T. Koller, H. Raatz, E. W. Steyerberg, and M. Wolbers. Competing risks\\nand the clinical community: irrelevance or ignorance? Stat Med ,31(11–12):1089–\\n1097, 2012. ⋄420', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='b59fa03a-4702-4f17-a795-0a720f2767cc', embedding=None, metadata={'page_label': '556', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='556 References\\n359. S. Konishi and G. Kitagawa. Information Criteria and Statistical Modeling .\\nSpringer, New York, 2008. ISBN 978-0-387-71886-6. ⋄204\\n360. C. Kooperberg and D. B. Clarkson. Hazard regression with interval-censored\\ndata.Biometrics , 53:1485–1494, 1997. ⋄420, 450\\n361. C. Kooperberg, C. J. Stone, and Y. K. Truong. Hazard regression. JA mS t a t\\nAssoc, 90:78–94, 1995. ⋄178, 419, 420, 422, 424, 450, 473, 506, 508, 518, 530\\n362. E. L. Korn and F. J. Dorey. Applications of crude incidence curves. Stat Med ,\\n11:813–829, 1992. ⋄416\\n363. E. L. Korn and B. I. Graubard. Analysis of large health surveys: Accounting\\nfor the sampling design. J Roy Stat Soc A , 158:263–295, 1995. ⋄208\\n364. E. L. Korn and B. I. Graubard. Examples of diﬀering weighted and unweighted\\nestimates from a sample survey. Am Statistician , 49:291–295, 1995. ⋄208\\n365. E. L. Korn and R. Simon. Measures of explained variation for survival data.\\nStat Med , 9:487–503, 1990. ⋄206, 215, 505, 519\\n366. E. L. Korn and R. Simon. Explained residual variation, explained risk, and\\ngoodness of ﬁt. Am Statistician , 45:201–206, 1991. ⋄206, 215, 273\\n367. D. Kronborg and P. Aaby. Piecewise comparison of survival functions in strati-\\nﬁed proportional hazards models. Biometrics , 46:375–380, 1990. ⋄502\\n368. W. F. Kuhfeld. The PRINQUAL procedure. In SAS/STAT 9.2 User’s Guide .\\nSAS Publishing, Cary, NC, second edition, 2009. ⋄82, 167\\n369. G. P. S. Kwong and J. L. Hutton. Choice of parametric models in survival\\nanalysis: applications to monotherapy for epilepsy and cerebral palsy. Appl\\nStat, 52:153–168, 2003. ⋄450\\n370. J.M.Lachin andM.A.Foulkes. Evaluationof samplesizeand powerfor analy ses\\nof survival with allowance for nonuniform patient entry, losses to follow-up,\\nnoncompliance, and stratiﬁcation. Biometrics , 42:507–519, 1986. ⋄513\\n371. L.Lamport. LATEX: A Document Preparation System . Addison-Wesley,Reading,\\nMA, second edition, 1994. ⋄536\\n372. R. Lancar, A. Kramar, and C. Haie-Meder. Non-parametric methods for\\nanalysing recurrent complications of varying severity. Stat Med , 14:2701–2712,\\n1995.⋄421\\n373. J. M. Landwehr, D. Pregibon, and A. C. Shoemaker. Graphical methods for\\nassessing logistic regression models (with discussion). J Am Stat Assoc , 79:61–\\n83, 1984. ⋄272, 315\\n374. T. P. Lane and W. H. DuMouchel. Simultaneous conﬁdence intervals in multiple\\nregression. Am Statistician , 48:315–321, 1994. ⋄199\\n375. K. Larsen and J. Merlo. Appropriate assessment of neighborhood eﬀects on\\nindividual health: integrating random and ﬁxed eﬀects in multilevel logistic re-\\ngression. American journal of epidemiology , 161(1):81–88, Jan. 2005. ⋄122\\n376. M. G. Larson and G. E. Dinse. A mixture model for the regression analysis of\\ncompeting risks data. Appl Stat , 34:201–211, 1985. ⋄276, 414\\n377. P. W. Laud and J. G. Ibrahim. Predictive model selection. J Roy Stat Soc B ,\\n57:247–262, 1995. ⋄214\\n378. A. Laupacis, N. Sekar, and I. G. Stiell. Clinical prediction rules: A review\\nand suggested modiﬁcations of methodological standards. JAMA, 277:488–494,\\n1997.⋄x, 6\\n379. B. Lausen and M. Schumacher. Evaluating the eﬀect of optimized cutoﬀ values\\nin the assessment of prognostic factors. Comp Stat Data Analysis , 21(3):307–\\n326, 1996. ⋄11, 19\\n380. P. W. Lavori, R. Dawson, and T. B. Mueller. Causal estimation of time-varying\\ntreatment eﬀects in observational studies: Application to depressive disorder.\\nStat Med , 13:1089–1100, 1994. ⋄231\\n381. P. W. Lavori, R. Dawson, and D. Shera. A multiple imputation strategy for\\nclinical trials with truncation of patient data. Stat Med , 14:1913–1925, 1995. ⋄\\n47', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5b056f46-c678-4b79-a32b-eb30f039a4c7', embedding=None, metadata={'page_label': '557', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 557\\n382. J. F. Lawless. Statistical Models and Methods for Lifetime Data . Wiley, New\\nYork, 1982. ⋄420, 450, 485, 517\\n383. J. F. Lawless. The analysis of recurrent events for multiple subjects. Appl Stat ,\\n44:487–498, 1995. ⋄421\\n384. J. F. Lawless and C. Nadeau. Some simple robust methods for the analysis of\\nrecurrent events. Technometrics , 37:158–168, 1995. ⋄420, 421\\n385. J. F. Lawless and K. Singhal. Eﬃcient screening of nonnormal regression models.\\nBiometrics , 34:318–327, 1978. ⋄70, 137\\n386. J. F. Lawless and Y. Yuan. Estimation of prediction error for survival models.\\nStat Med , 29:262–274, 2010. ⋄519\\n387. S. le Cessie and J. C. van Houwelingen. A goodness-of-ﬁt test for binary regres-\\nsion models, based on smoothing methods. Biometrics , 47:1267–1282, 1991. ⋄\\n236\\n388. S. le Cessie and J. C. van Houwelingen. Ridge estimators in logistic regression.\\nAppl Stat , 41:191–201, 1992. ⋄77, 209\\n389. M. LeBlanc and J. Crowley. Survival trees by goodness of ﬁt. J Am Stat Assoc ,\\n88:457–467, 1993. ⋄41\\n390. M. LeBlanc and R. Tibshirani. Adaptive principal surfaces. J Am Stat Assoc ,\\n89:53–64, 1994. ⋄101\\n391. A. Leclerc, D. Luce, F. Lert, J. F. Chastang, and P. Logeay. Correspondence\\nanalysis and logistic modelling: Complementary use in the analysis of a health\\nsurvey among nurses. Stat Med , 7:983–995, 1988. ⋄81\\n392. E. T. Lee. Statistical Methods for Survival Data Analysis . Lifetime Learning\\nPublications, Belmont, CA, second edition, 1980. ⋄420\\n393. E. W. Lee, L. J. Wei, and D. A. Amato. Cox-type regression analysis for large\\nnumbers of small groups of correlated failure time observations. In J. P. Klei n\\nand P. K. Goel, editors, Survival Analysis: State of the Art ,N A T OA S I ,p a g e s\\n237–247. Kluwer Academic, Boston, 1992. ⋄197\\n394. J. J. Lee, K. R. Hess, and J. A. Dubin. Extensions and applications of event\\ncharts.Am Statistician , 54:63–70, 2000. ⋄418, 420\\n395. K. L. Lee, D. B. Pryor, F. E. Harrell, R. M. Caliﬀ, V. S. Behar, W. L. Floyd, J. J.\\nMorris, R. A. Waugh, R. E. Whalen, and R. A. Rosati. Predicting outcome in\\ncoronary disease: Statistical models versus expert clinicians. Am J Med , 80:553–\\n560, 1986. ⋄205\\n396. S. Lee, J. Z. Huang, and J. Hu. Sparse logistic principal components analysis\\nfor binary data. Ann Appl Stat , 4(3):1579–1601, 2010. ⋄101\\n397. E. L. Lehmann. Model speciﬁcation: The views of Fisher and Neyman and later\\ndevelopments. Statistical Sci , 5:160–168, 1990. ⋄8, 10\\n398. S. Lehr and M. Schemper. Parsimonious analysis of time-dependent eﬀects in\\nthe Cox model. Stat Med , 26:2686–2698, 2007. ⋄501\\n399. F. Leisch. Sweave: Dynamic Generation of Statistical Reports Using Literate\\nData Analysis. In W. H ¨ardle and B. R ¨onz, editors, Compstat 2002 — Proceed-\\nings in Computational Statistics , pages 575–580. Physica Verlag, Heidelberg,\\n2002. ISBN 3-7908-1517-9. ⋄138\\n400. L. F. Le´ on and C. Tsai. Functional form diagnostics for Cox’s proportional\\nhazards model. Biometrics , 60:75–84, 2004. ⋄518\\n401. M. A. H. Levine, A. I. El-Nahas, and B. Asa. Relative risk and odds ratio data\\nare still portrayed with inappropriate scales in the medical literature. JC l i n\\nEpi, 63:1045–1047, 2010. ⋄122\\n402. C. Li and B. E. Shepherd. A new residual for ordinal outcomes. Biometrika ,\\n99(2):473–480, 2012. ⋄315\\n403. K. Li, J. Wang, and C. Chen. Dimension reduction for censored regression data.\\nAnn Stat , 27:1–23, 1999. ⋄101\\n404. K. C. Li. Sliced inverse regression for dimension reduction. J Am Stat Assoc ,\\n86:316–327, 1991. ⋄101', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f947636c-9be9-4915-9000-e2b77ad4e120', embedding=None, metadata={'page_label': '558', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='558 References\\n405. K.-Y. Liang and S. L. Zeger. Longitudinal data analysis of continuous and\\ndiscrete responses for pre-post designs. Sankhy¯a, 62:134–148, 2000. ⋄160\\n406. J. G. Liao and D. McGee. Adjusted coeﬃcients of determination for logistic\\nregression. Am Statistician , 57:161–165, 2003. ⋄273\\n407. D.Y.Lin. Cox regression analysisofmultivariate failuretime data:Themarginal\\napproach. Stat Med , 13:2233–2247, 1994. ⋄197, 213, 417, 418\\n408. D. Y. Lin. Non-parametric inference for cumulative incidence functions in com-\\npeting risks studies. Stat Med , 16:901–910, 1997. ⋄415\\n409. D. Y. Lin. On ﬁtting Cox’s proportional hazards models to survey data.\\nBiometrika , 87:37–47, 2000. ⋄215\\n410. D. Y. Lin and L. J. Wei. The robust inference for the Cox proportional hazards\\nmodel.J Am Stat Assoc , 84:1074–1078, 1989. ⋄197, 213, 487\\n411. D. Y. Lin, L. J. Wei, and Z. Ying. Checking the Cox model with cumulative\\nsums of martingale-based residuals. Biometrika , 80:557–572, 1993. ⋄518\\n412. D. Y. Lin and Z. Ying. Semiparametric regression analysis of longitudinal data\\nwith informative drop-outs. Biostatistics , 4:385–398, 2003. ⋄47\\n413. J. C. Lindsey and L. M. Ryan. Tutorial in biostatistics: Methods for interval-\\ncensored data. Stat Med , 17:219–238, 1998. ⋄420\\n414. J. K. Lindsey. Models for Repeated Measurements . Clarendon Press, 1997. ⋄143\\n415. J. K. Lindsey and B. Jones. Choosing among generalized linear models applied\\nto medical data. Stat Med , 17:59–68, 1998. ⋄11\\n416. K. Linnet. Assessing diagnostic tests by a strictly proper scoring rule. Stat Med ,\\n8:609–618, 1989. ⋄114, 123, 257, 258\\n417. S. R. Lipsitz, L. P. Zhao, and G. Molenberghs. A semiparametric method of\\nmultiple imputation. J Roy Stat Soc B , 60:127–144, 1998. ⋄54\\n418. R. Little and H. An. Robust likelihood-based analysis of multivariate data with\\nmissing values. Statistica Sinica , 14:949–968, 2004. ⋄57, 59\\n419. R. J. Little. Missing Data. In Ency of Biostatistics , pages 2622–2635. Wiley,\\nNew York, 1998. ⋄59\\n420. R. J. A. Little. Missing-data adjustments in large surveys. J Bus Econ Stat ,\\n6:287–296, 1988. ⋄51\\n421. R. J. A. Little. Regression with missing X’s: A review. J Am Stat Assoc ,\\n87:1227–1237, 1992. ⋄50, 51, 54\\n422. R. J. A. Little and D. B. Rubin. Statistical Analysis with Missing Data . Wiley,\\nNew York, second edition, 2002. ⋄48, 52, 54, 59\\n423. G. F. Liu, K. Lu, R. Mogg, M. Mallick, and D. V. Mehrotra. Should baseline be\\na covariate or dependent variable in analyses of change from baseline in clini cal\\ntrials?Stat Med , 28:2509–2530, 2009. ⋄160\\n424. K. Liu and A. R. Dyer. A rank statistic for assessing the amount of variation\\nexplained by risk factors in epidemiologic studies. Am J Epi , 109:597–606, 1979.\\n⋄206, 256\\n425. R. Lockhart, J. Taylor, R. J. Tibshirani, and R. Tibshirani. A signiﬁcance test\\nfor the lasso. Technical report, arXiv, 2013. ⋄68\\n426. J. S. Long and L. H. Ervin. Using heteroscedasticity consistent standard errors\\nin the linear regression model. Am Statistician , 54:217–224, 2000. ⋄213\\n427. J. Lubsen, J. Pool, and E. van der Does. A practical device for the application\\nof a diagnostic or prognostic function. Meth Info Med , 17:127–129, 1978. ⋄104\\n428. D. J. Lunn, J. Wakeﬁeld, and A. Racine-Poon. Cumulative logit models for\\nordinal data: a case study involving allergic rhinitis severity scores. Stat Med ,\\n20:2261–2285, 2001. ⋄324\\n429. M. Lunn and D. McNeil. Applying Cox regression to competing risks. Biomet-\\nrics, 51:524–532, 1995. ⋄420\\n430. X. Luo, L. A. Stfanski, and D. D. Boos. Tuning variable selection procedures\\nby adding noise. Technometrics , 48:165–175, 2006. ⋄11, 100', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='58cf4b95-3116-4b9d-b533-acef119da0f8', embedding=None, metadata={'page_label': '559', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 559\\n431. G. S. Maddala. Limited-Dependent and Qualitative Variables in Econometrics .\\nCambridge University Press, Cambridge, UK, 1983. ⋄206, 256, 505\\n432. L. Magee. R2measures based on Wald and likelihood ratio joint signiﬁcance\\ntests.Am Statistician , 44:250–253, 1990. ⋄206, 256, 505\\n433. L. Magee. Nonlocal behavior in polynomial regressions. Am Statistician , 52:20–\\n22, 1998. ⋄21\\n434. C. Mallows. The zeroth problem. Am Statistician , 52:1–9, 1998. ⋄11\\n435. M. Mandel. Censoring and truncation—Highlighting the diﬀerences. Am Statis-\\ntician, 61(4):321–324, 2007. ⋄420\\n436. M. Mandel, N. Galae, and E. Simchen. Evaluating survival model performance:\\na graphical approach. Stat Med , 24:1933–1945, 2005. ⋄518\\n437. N. Mantel. Why stepdown procedures in variable selection. Technometrics ,\\n12:621–625, 1970. ⋄70\\n438. N. Mantel and D. P. Byar. Evaluation of response-time data involving transient\\nstates: An illustration using heart-transplant data. J Am Stat Assoc , 69:81–86,\\n1974.⋄401, 420\\n439. P. Margolis, E. K. Mulholland, F. E. Harrell, S. Gove, and the WHO Young\\nInfants Study Group. Clinical prediction of serious bacterial infections in young\\ninfants in developing countries. Pediatr Infect Dis J , 18S:S23–S31, 1999. ⋄327\\n440. D. B. Mark, M. A. Hlatky,F. E. Harrell, K. L. Lee, R. M. Caliﬀ, and D. B. Pryor.\\nExercise treadmill score for predicting prognosis in coronary artery disease. Ann\\nInt Med, 106:793–800, 1987. ⋄512\\n441. G. Marshall, F. L. Grover, W. G. Henderson, and K. E. Hammermeister. As-\\nsessment of predictive models for binary outcomes: An empirical approach using\\noperative death from cardiac surgery. Stat Med , 13:1501–1511, 1994. ⋄101\\n442. G. Marshall, B. Warner, S. MaWhinney, and K. Hammermeister. Prospective\\nprediction in the presence of missing data. Stat Med , 21:561–570, 2002. ⋄57\\n443. R. J. Marshall. The use of classiﬁcation and regression trees in clinical epidemi-\\nology.JC l i nE p i , 54:603–609, 2001. ⋄41\\n444. E. Marubini and M. G. Valsecchi. Analyzing Survival Data from Clinical Trials\\nand Observational Studies . Wiley, Chichester, 1995. ⋄213, 214, 415, 420, 501,\\n517\\n445. J. M. Massaro. Battery Reduction. 2005. ⋄87\\n446. S.E.Maxwell and H.D. Delaney. Bivariate median splitsand spuriousstatisti cal\\nsigniﬁcance. Psych Bull , 113:181–190, 1993. ⋄19\\n447. M. May, P. Royston, M. Egger, A. C. Justice, and J. A. C. Sterne. Develop-\\nment and validation of a prognostic model for survival time data: application\\nto prognosis of HIV positive patients treated with antiretroviral therapy. Stat\\nMed, 23:2375–2398, 2004. ⋄505\\n448. G. P. McCabe. Principal variables. Technometrics , 26:137–144, 1984. ⋄101\\n449. P. McCullagh. Regression models for ordinal data. J Roy Stat Soc B , 42:109–\\n142, 1980. ⋄313, 324\\n450. P. McCullagh and J. A. Nelder. Generalized Linear Models . Chapman and\\nHall/CRC, second edition, Aug. 1989. ⋄viii\\n451. D. R. McNeil, J. Trussell, and J. C. Turner. Spline interpolation of demographic\\ndata.Demography , 14:245–252, 1977. ⋄40\\n452. W. Q. Meeker and L. A. Escobar. Teaching about approximate co nﬁdence\\nregions based on maximum likelihood estimation. Am Statistician , 49:48–53,\\n1995.⋄214\\n453. N. Meinshausen. Hierarchical testing of variable importance. Biometrika ,\\n95(2):265–278, 2008. ⋄101\\n454. S. Menard. Coeﬃcients of determination for multiple logistic regression analysis.\\nAm Statistician , 54:17–24, 2000. ⋄215, 272\\n455. X. Meng. Multiple-imputation inferen ces with uncongenial sour ces of input.\\nStat Sci, 9:538–558, 1994. ⋄58', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ff9f1bb5-c6ea-4a96-ad73-40bca9b32127', embedding=None, metadata={'page_label': '560', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='560 References\\n456. G. Michailidis and J. de Leeuw. The Giﬁ system of descriptive multivariate\\nanalysis. Statistical Sci , 13:307–336, 1998. ⋄81\\n457. M. E. Miller, S. L. Hui, and W. M. Tierney. Validation techniques for logistic\\nregression models. Stat Med , 10:1213–1226, 1991. ⋄259\\n458. M. E. Miller, T. M. Morgan, M. A. Espeland, and S. S. Emerson. Group com-\\nparisons involving missing data in clinical trials: a comparison of estimates and\\npower (size) for some simple approaches. Stat Med , 20:2383–2397, 2001. ⋄58\\n459. R. G. Miller. What price Kaplan–Meier? Biometrics , 39:1077–1081, 1983. ⋄420\\n460. S. Minkin. Proﬁle-likelihood-based co nﬁdence intervals. Appl Stat , 39:125–126,\\n1990.⋄214\\n461. M. Mittlb ¨ock and M. Schemper. Explained variation for logistic regression. Stat\\nMed, 15:1987–1997, 1996. ⋄215, 273\\n462. K. G. M. Moons, Donders, E. W. Steyerberg, and F. E. Harrell. Penalized max-\\nimum likelihood estimation to directly adjust diagnostic and prognostic predic-\\ntion models for overoptimism: a clinical example. JC l i nE p i , 57:1262–1270,\\n2004.⋄215, 273, 356\\n463. K. G. M. Moons, R. A. R. T. Donders, T. Stijnen, and F. E. Harrell. Using the\\noutcome for imputation of missing predictor values was preferred. JC l i nE p i ,\\n59:1092–1101, 2006. ⋄54, 55, 59\\n464. B. J. T. Morgan, K. J. Palmer, and M. S. Ridout. Negative score test statistic\\n(with discussion). Am Statistician , 61(4):285–295, 2007. ⋄213\\n465. B. K. Moser and L. P. Coombs. Odds ratios for a continuous outcome variable\\nwithout dichotomizing. Stat Med , 23:1843–1860, 2004. ⋄19\\n466. G. S. M udholkar, D. K. Srivastava, and G. D. Kollia. A generalization of the\\nWeibull distribution with application to the analysis of survival data. JA mS t a t\\nAssoc, 91:1575–1583, 1996. ⋄420\\n467. L. R. Muenz. Comparing survival distributions: A review for nonstatisticians.\\nII.Ca Invest , 1:537–545, 1983. ⋄495, 502\\n468. V. M. R. Muggeo and M. Tagliavia. A ﬂexible approach to the crossing hazards\\nproblem. Stat Med , 29:1947–1957, 2010. ⋄518\\n469. H. Murad, A. Fleischman, S. Sadetzki, O. Geyer, and L. S. Freedman. Small\\nsamples and ordered logistic regression: Does it help to collapse categories of\\noutcome? Am Statistician , 57:155–160, 2003. ⋄324\\n470. R. H. Myers. Classical and Modern Regression with Applications .P W S - K e n t ,\\nBoston, 1990. ⋄78\\n471. N. J. D. Nagelkerke. A note on a general deﬁnition of the coeﬃcient of deter-\\nmination. Biometrika , 78:691–692, 1991. ⋄206, 256, 505\\n472. W. B. Nelson. Theory and applications of hazard plotting for censored failure\\ndata.Technometrics , 14:945–965, 1972. ⋄413\\n473. R. Newson. Parameters behind “nonparametric” statistics: Kendall’s tau,\\nSomers’ D and median diﬀerences. Stata Journal , 2(1), 2002. http://www.\\nstata-journal.com/article.html?article=st0007 .⋄273\\n474. R. Newson. Co nﬁdence intervals for rank statistics: Somers’ D and extensions.\\nStata J, 6(3):309–334, 2006. ⋄273\\n475. N. H. Ng’a ndu. An empirical comparison of statistical tests for assessing the\\nproportional hazards assumption of Cox’s model. Stat Med , 16:611–626, 1997.\\n⋄518\\n476. T. G. Nick and J. M. Hardin. Regression modeling strategies: An illustrative\\ncase study from medical rehabilitation outcomes research. Am J Occ Ther ,\\n53:459–470, 1999. ⋄viii, 100\\n477. M. A. Nicolaie, H. C. van Houwelingen, T. M. de Witte, and H. Putter. Dynamic\\nprediction by landmarking in competing risks. Stat Med , 32(12):2031–2047,\\n2013.⋄447\\n478. M. Nishikawa, T. Tango, and M. Ogawa. Non-parametric inference of adverse\\nevents under informative censoring. Stat Med , 25:3981–4003, 2006. ⋄420', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='fa17984d-6ade-4347-ac8f-ae1ff2c03b38', embedding=None, metadata={'page_label': '561', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 561\\n479. P. C. O’Brien. Comparing two samples: Extensions of the t, rank-sum, and\\nlog-rank test. J Am Stat Assoc , 83:52–61, 1988. ⋄231\\n480. P. C. O’Brien, D. Zhang, and K. R. Bailey. Semi-parametric and non-parametric\\nmethods for clinical trials with incomplete data. Stat Med , 24:341–358, 2005. ⋄\\n47\\n481. J.O’Quigley,R.Xu,andJ.Stare. Explainedrandomness inproportional hazards\\nmodels. Stat Med , 24(3):479–489, 2005. ⋄505\\n482. W. Original. survival : Survival analysis, including penalised likelihood , 2009.\\nR package version 2.37-7. ⋄131\\n483. M. Y. Park and T. Hastie. Penalized logistic regression for detecting gene in-\\nteractions. Biostat, 9(1):30–50, 2008. ⋄215\\n484. M. K. B. Parmar and D. Machin. Survival Analysis: A Practical Approach .\\nWiley, Chichester, 1995. ⋄420\\n485. D. Paul, E. Bair, T. Hastie, and R. Tibshirani. “Preconditioning” for feature\\nselection and regression in high-dimensional problems. Ann Stat , 36(4):1595–\\n1619, 2008. ⋄121\\n486. P. Peduzzi, J. Concato, A. R. Feinstein, and T. R. Holford. Importance of\\nevents per independent variable in proportional hazards regression analysis. II.\\nAccuracy and precision of regression estimates. JC l i nE p i , 48:1503–1510, 1995.\\n⋄100\\n487. P. Peduzzi, J. Concato, E. Kemper, T. R. Holford, and A. R. Feinstein. A simu-\\nlation study of the number of events per variable in logistic regression analysi s.\\nJC l i nE p i , 49:1373–1379, 1996. ⋄73, 100\\n488. N. Peek, D. G. T. Arts, R. J. Bosman, P. H. J. van der Voort, and N. F.\\nde Keizer. External validation of prognostic models for critically ill patients\\nrequired substantial sample sizes. JC l i nE p i , 60:491–501, 2007. ⋄93\\n489. M. J. Pencina and R. B. D’Agostino. Overall Cas a measure of discrimination\\nin survival analysis: model speciﬁc population value and conﬁdence interval\\nestimation. Stat Med , 23:2109–2123, 2004. ⋄519\\n490. M. J. Pencina, R. B. D’Agostino, and O. V. Demler. Novel metrics for eval-\\nuating improvement in discrimination: net reclassiﬁcation and integrated dis-\\ncrimination improvement for normal variables and nested models. Stat Med ,\\n31(2):101–113, 2012. ⋄101, 142, 273\\n491. M. J. Pencina, R. B. D’Agostino, and L. Song. Quantifying discrimination\\nof Framingham risk functions with diﬀerent survival C statistics. Stat Med ,\\n31(15):1543–1553, 2012. ⋄519\\n492. M. J. Pencina, R. B. D’Agostino, and E. W. Steyerberg. Extensions of net re-\\nclassiﬁcation improvement calculations to measure usefulnessof new biomarkers.\\nStat Med , 30:11–21, 2011. ⋄101, 142\\n493. M. J. Pencina, R. B. D’Agostino Sr, R. B. D’Agostino Jr, and R. S. Vasan.\\nEvaluating the added predictive ability of a new marker: From area under the\\nROC curve to reclassiﬁcation and beyond. Stat Med , 27:157–172, 2008. ⋄93,\\n101, 142, 273\\n494. M. S. Pepe. Inference for events with dependent risks in multiple end point\\nstudies. J Am Stat Assoc , 86:770–778, 1991. ⋄415\\n495. M. S. Pepe and J. Cai. Some graphical displays and marginal regression analyses\\nfor recurrent failure times and time dependent covariates. J Am Stat Assoc ,\\n88:811–820, 1993. ⋄417\\n496. M. S. Pepe, G. Longton, and M. Thornquist. A qualiﬁer Q for the survival\\nfunction to describe the prevalence of a transient condition. Stat Med , 10:\\n413–421, 1991. ⋄415\\n497. M. S. Pepe and M. Mori. Kaplan–Meier, marginal or conditional probabil-\\nity curves in summarizing competing risks failure time data? Stat Med , 12:\\n737–751, 1993. ⋄415', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d494ed1c-1f6a-4431-8697-f35e65ce3cf3', embedding=None, metadata={'page_label': '562', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='562 References\\n498. A. Perperoglou, A. Keramopoullos, and H. C. van Houwelingen. Approaches\\nin modelling long-term survival: An application to breast cancer. Stat Med ,\\n26:2666–2685, 2007. ⋄501, 518\\n499. A. Perperoglou, S. le Cessie, and H. C. van Houwelingen. Reduced-rank hazard\\nregression for modelling non-proportional hazards. Stat Med , 25:2831–2845,\\n2006.⋄518\\n500. S. A. Peters, M. L. Bots, H. M. den Ruijter, M. K. Palmer, D. E. Grobbee, J. R.\\nC r o u s e ,D .H .O ’ L e a r y ,G .W .E v a n s ,J .S .R a i c h l e n ,K .G .M o o n s ,H .K o ﬃ j be r g ,\\nand METEOR study group. Multiple imputation of missing repeated outcome\\nmeasurements did not add to linear mixed-eﬀects models. JC l i nE p i , 65(6):686–\\n695, 2012. ⋄160\\n501. B. Peterson and S. L. George. Sample size requirements and length of study for\\ntesting interaction in a 1 ×kfactorial design when time-to-failure is the outcome.\\nControlled Clin Trials , 14:511–522, 1993. ⋄513\\n502. B. Peterson and F. E. Harrell. Partial proportional odds models for ordinal\\nresponse variables. Appl Stat , 39:205–217, 1990. ⋄315, 321, 324\\n503. A. N. Pettitt and I. Bin Daud. Investigating time dependence in Cox’s propor-\\ntional hazards model. Appl Stat , 39:313–329, 1990. ⋄498, 518\\n504. A. N. Phillips,S.G. Thompson,and S.J. Pocock. Prognostic scores for detecting\\na high risk group: Estimating the sensitivity when applied to new data. Stat\\nMed, 9:1189–1198, 1990. ⋄100, 101\\n505. R. R. Picard and K. N. Berk. Data splitting. Am Statistician , 44:140–147, 1990.\\n⋄122\\n506. R. R. Picard and R. D. Cook. Cross-validation of regression models. JA mS t a t\\nAssoc, 79:575–583, 1984. ⋄123\\n507. L. W. Pickle. Maximum likelihood estimation in the new computing environ-\\nment.Stat Comp Graphics News ASA , 2(2):6–15, Nov. 1991. ⋄213\\n508. M. C. Pike. A method of analysis of certain class of experiments in carcinogen-\\nesis.Biometrics , 22:142–161, 1966. ⋄441, 442, 443, 480\\n509. J. C. Pinheiro and D. M. Bates. Mixed-Eﬀects Models in S and S-PLUS .\\nSpringer, New York, 2000. ⋄131, 143, 146, 147, 148\\n510. R. F. Potthoﬀ and S. N. Roy. A generalized multivariate analysis of variance\\nmodel useful especially for growth curve problems. Biometrika , 51:313–326,\\n1964.⋄146\\n511. D. Pregibon. Logistic regression diagnostics. Ann Stat , 9:705–724, 1981. ⋄255\\n512. D. Pregibon. Resistant ﬁts for some commonly used logistic models with medical\\napplications. Biometrics , 38:485–498, 1982. ⋄272\\n513. R. L. Prentice, J. D. Kalbﬂeisch, A. V. Peterson, N. Flournoy, V. T. Farewell,\\nand N. E. Breslow. The analysis of failure times in the presence of competing\\nrisks.Biometrics , 34:541–554, 1978. ⋄414\\n514. S. J. Press and S. Wilson. Choosing between logistic regression and discriminant\\nanalysis. J Am Stat Assoc , 73:699–705, 1978. ⋄272\\n515. D. B. Pryor, F. E. Harrell, K. L. Lee, R. M. Caliﬀ, and R. A. Rosati. Estimating\\nthe likelihood of signiﬁcant coronary artery disease. Am J Med , 75:771–780,\\n1983.⋄273\\n516. D. B. Pryor, F. E. Harrell, J. S. Rankin, K. L. Lee, L. H. Muhlbaier, H. N. Old-\\nham, M. A. Hlatky, D. B. Mark, J. G. Reves, and R. M. Caliﬀ. The changing\\nsurvival beneﬁts of coronary revascularization over time. Circulation (Supple-\\nment V) , 76:13–21, 1987. ⋄511\\n517. H. Putter, M. Fiocco, and R. B. Geskus. Tutorial in biostatistics: Competing\\nrisks and multi-state models. Stat Med , 26:2389–2430, 2007. ⋄420\\n518. H. Putter, M. Sasako, H. H. Hartgrink, C. J. H. van de Velde, and J. C. van\\nHouwelingen. Long-term survival with non-proportional hazards: results from\\nt h eD u t c hG a s t r i cC a n c e rT r i a l . Stat Med , 24:2807–2821, 2005. ⋄518', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='afd582d2-8875-43ef-850e-c7bd276d4e18', embedding=None, metadata={'page_label': '563', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 563\\n519. C. Quantin, T. Moreau, B. Asselain, J. Maccaria, and J. Lellouch. A regression\\nsurvival model for testing the proportional hazards assumption. Biometrics ,\\n52:874–885, 1996. ⋄518\\n520. R Development Core Team. R: A Language and Environment for Statistical\\nComputing . R Foundation for Statistical Computing, Vienna, Austria, 2013.⋄\\n127\\n521. D. R. Ragland. Dichotomizing continuous outcome variables: Dependence of the\\nmagnitude of association and statistical power on the cutpoint. Epi, 3:434–440,\\n1992. See letters to editor May 1993 P. 274-, Vol 4 No. 3. ⋄11, 19\\n522. B. M. Reilly and A. T. Evans. Translating clinical research into clinical practice:\\nImpact of using prediction rules to make decisions. Ann Int Med , 144:201–209,\\n2006.⋄6\\n523. M. Reilly and M. Pepe. The relationship between hot-deck multiple imputation\\nand weighted likelihood. Stat Med , 16:5–19, 1997. ⋄59\\n524. B. D. Ripley and P. J. Solomon. Statistical models for prevalent cohort data.\\nBiometrics , 51:373–374, 1995. ⋄420\\n525. J. S. Roberts and G. M. Capalbo. A SAS macro for estimating missing values\\nin multivariate data. In Proceedings of the Twelfth Annual SAS Users Group\\nInternational Conference , pages 939–941, Cary, NC, 1987. SAS Institute, Inc. ⋄\\n52\\n526. J. M. Robins, S. D. Mark, and W. K. Newey. Estimating exposure eﬀects by\\nmodeling the expectation of exposure conditional on confounders. Biometrics ,\\n48:479–495, 1992. ⋄231\\n527. L. D. Robinson and N. P. Jewell. Some surprising results about covariate ad-\\njustment in logistic regression models. Int Stat Rev , 59:227–240, 1991. ⋄231\\n528. E. B. Roecker. Prediction error and its estimation for subset-selected models.\\nTechnometrics , 33:459–468, 1991. ⋄100, 112\\n529. W. H. Rogers. Regression standard errors in clustered samples. S t a t aT e c hB u l l ,\\nSTB-13:19–23, May 1993. http://www.stata.com/products/stb/journals/\\nstb13.pdf .⋄197\\n530. P. R. Rosenbaum and D. Rubin. The central role of the propensity score in\\nobservational studies for causal eﬀects. Biometrika , 70:41–55, 1983. ⋄3, 231\\n531. P. R. Rosenbaum and D. B. Rubin. Assessing sensitivity to an unobserved\\nbinary covariate in an observational study with binary outcome. J Roy Stat Soc\\nB, 45:212–218, 1983. ⋄231\\n532. P. Royston and D. G. Altman. Regression using fractional polynomials of con-\\ntinuous covariates: Parsimonious parametric modelling. ApplStat , 43:429–453,\\n1994. Discussion pp. 453–467. ⋄40\\n533. P. Royston, D. G. Altman, and W. Sauerbrei. Dichotomizing continuous pre-\\ndictors in multiple regression: a bad idea. Stat Med , 25:127–141, 2006. ⋄19\\n534. P. Royston and S. G. Thompson. Comparing non-nested regression models.\\nBiometrics , 51:114–127, 1995. ⋄215\\n535. D. Rubin and N. Schenker. Multiple imputation in health-care data bases: An\\noverview and some applications. Stat Med , 10:585–598, 1991. ⋄46, 50, 59\\n536. D. B. Rubin. Multiple Imputation for Nonresponse in Surveys . Wiley, New\\nYork, 1987. ⋄54, 59\\n537. S. Sahoo and D. Sengupta. Some diagnostic plots and corrective adjustments for\\nthe proportional hazards regression model. J Comp Graph Stat , 20(2):375–394,\\n2011.⋄518\\n538. S. Sardy. On the practice of rescaling covariates. I n tS t a tR e v , 76:285–297, 2008.\\n⋄215\\n539. W. Sarle. The VARCLUS procedure. In SAS/STAT User’s Guide ,v o l u m e2 ,\\nchapter 43, pages 1641–1659. SAS Institute,Inc., Cary, NC, fourth edition, 1990.\\n⋄79, 81, 101', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9bdb3d72-f0bb-43ed-b9fc-e0993025e0d5', embedding=None, metadata={'page_label': '564', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='564 References\\n540. SAS Institute, Inc. SAS/STAT User’s Guide , volume 2. SAS Institute, Inc.,\\nCary NC, fourth edition, 1990. ⋄315\\n541. W. Sauerbrei and M. Schumacher. A bootstrap resampling procedure for model\\nbuilding: Application to the Cox regression model. Stat Med , 11:2093–2109,\\n1992.⋄70, 113, 177\\n542. J. L. Schafer and J. W. Graham. Missing data: Our view of the state of the art.\\nPsych Meth , 7:147–177, 2002. ⋄58\\n543. D. E. Schaubel, R. A. Wolfe, and R. M. Merion. Estimating the eﬀect of a\\ntime-dependent treatment by levels of an internal time-dependent covariate:\\nApplication to the contrast between liver wait-list and posttransplant mortality.\\nJ Am Stat Assoc , 104(485):49–59, 2009. ⋄518\\n544. M. Schemper. Analyses of associations with censored data by generalized Mantel\\nand Breslow tests and generalized Kendall correlation. Biometrical J , 26:309–\\n318, 1984. ⋄518\\n545. M. Schemper. Non-parametric analysis of treatment-covariate interaction in the\\npresence of censoring. Stat Med , 7:1257–1266, 1988. ⋄41\\n546. M. Schemper. The explained variation in proportional hazards regression\\n(correction in 81:631, 1994). Biometrika , 77:216–218, 1990. ⋄505, 508\\n547. M. Schemper. Cox analysis of survival data with non-proportional hazard func-\\ntions.The Statistician , 41:445–455, 1992. ⋄518\\n548. M. Schemper. Further results on the explained variation in proportional hazards\\nregression. Biometrika , 79:202–204, 1992. ⋄505\\n549. M. Schemper. The relative importance of prognostic factors in studies of sur-\\nvival.Stat Med , 12:2377–2382, 1993. ⋄215, 505\\n550. M. Schemper. Predictive accuracy and explained variation. Stat Med , 22:2299–\\n2308, 2003. ⋄519\\n551. M. Schemper and G. Heinze. Probability imputation revisited for prognostic\\nfactor studies. Stat Med , 16:73–80, 1997. ⋄52, 177\\n552. M. Schemper and R. Henderson. Predictive accuracy and explained variation in\\nCox regression. Biometrics , 56:249–255, 2000. ⋄518\\n553. M. Schemper and T. L. Smith. Eﬃcient evaluation of treatment eﬀects in the\\npresence of missing covariate values. Stat Med , 9:777–784, 1990. ⋄52\\n554. M. Schemper and J. Stare. Explained variation in survival analysis. Stat Med ,\\n15:1999–2012, 1996. ⋄215, 519\\n555. M. Schmid and S. Potapov. A comparison of estimators to evaluate the dis-\\ncriminatory power of time-to-event models. Stat Med , 31(23):2588–2609, 2012.\\n⋄519\\n556. C. Schmoor, K. Ulm, and M. Schumacher. Comparison of the Cox model and\\nthe regression tree procedure in analysing a randomized clinical trial. Stat Med ,\\n12:2351–2366, 1993. ⋄41\\n557. D. Schoenfeld. Partial residuals for the proportional hazards regression model.\\nBiometrika , 69:239–241, 1982. ⋄314, 498, 499, 516\\n558. D. A. Schoenfeld. Sample size formulae for the proportional hazards regression\\nmodel.Biometrics , 39:499–503, 1983. ⋄513\\n559. G. Schulgen, B. Lausen, J. Olsen, and M. Schumacher. Outcome-oriented cut-\\npoints in quantitative exposure. Am J Epi , 120:172–184, 1994. ⋄19, 20\\n560. G. Schwarz. Estimating the dimension of a model. Ann Stat , 6:461–464, 1978.\\n⋄214\\n561. S. C. Scott, M. S. Goldberg, and N. E. Mayo. Statistical assessment of ordinal\\noutcomes in comparative studies. JC l i nE p i , 50:45–55, 1997. ⋄324\\n562. M. R. Segal. Regression trees for censored data. Biometrics , 44:35–47, 1988. ⋄\\n41\\n563. S. Senn. Change from baseline and analysis of covariance revisited. Stat Med ,\\n25:4334–4344, 2006. ⋄159, 160', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='e4679287-2943-44f6-bbca-4efd21e319cf', embedding=None, metadata={'page_label': '565', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 565\\n564. S. Senn and S. Julious. Measurement in clinical trials: A neglected issue for\\nstatisticians? (with discussion). Stat Med , 28:3189–3225, 2009. ⋄313\\n565. J. Shao. Linear model selection by cross-validation. J Am Stat Assoc , 88:486–\\n494, 1993. ⋄100, 113, 122\\n566. J. Shao and R. R. Sitter. Bootstrap for imputed survey data. J Am Stat Assoc ,\\n91:1278–1288, 1996. ⋄54\\n567. X. Shen, H. Huang, and J. Ye. Inference after model selection. J Am Stat Assoc ,\\n99:751–762, 2004. ⋄102\\n568. Y. Shen and P.F. Thall. Parametric likelihoods for multiplenon-fatal competing\\nrisks and death. Stat Med , 17:999–1015, 1998. ⋄421\\n569. J. Siddique. Multipleimputation usingan iterative hot-deck with distance-ba sed\\ndonor selection. Stat Med , 27:83–102, 2008. ⋄58\\n570. R. Simon and R. W. Makuch. A non-parametric graphical representation of\\nthe relationship between survival and the occurrence of an event: Application\\nto responder versus non-responder bias. Stat Med , 3:35–44, 1984. ⋄401, 420\\n571. J. S. Simonoﬀ. The “Unusual Episode” and a second statistics course. JS t a t\\nEdu, 5(1), 1997. Online journal at www.amstat.org/publications/jse/v5n1/-\\nsimonoff.html .⋄291\\n572. S. L. Simpson, L. J. Edwards, K. E. Muller, P. K. Sen, and M. A. Styner. A\\nlinear exponent AR(1) family of correlation structures. Stat Med , 29:1825–1838,\\n2010.⋄148\\n573. J. C. Sinclair and M. B. Bracken. Clinically useful measures of eﬀect in binary\\nanalyses of randomized trials. JC l i nE p i , 47:881–889, 1994. ⋄272\\n574. J. D. Singer and J. B. Willett. Modeling the days of our lives: Using survival\\nanalysis when designing and analyzing longitudinal studies of duration and the\\ntiming of events. Psych Bull , 110:268–290, 1991. ⋄420\\n575. L. A. Sleeper and D. P. Harrington. Regression splines in the Cox model with\\napplication to covariate eﬀects in liver disease. J Am Stat Assoc , 85:941–949,\\n1990.⋄23, 40\\n576. A. F. M. Smith and D. J. Spiegelhalter. Bayes factors and choice criteria for\\nlinear models. J Roy Stat Soc B , 42:213–220, 1980. ⋄214\\n577. L. R. Smith, F. E. Harrell, and L. H. Muhlbaier. Problems and potentials\\nin modeling survival. In M. L. Grady and H. A. Schwartz, editors, Medical\\nEﬀectiveness Research Data Methods (Summary Report), AHCPR Pub. No.\\n92-0056, pages 151–159. US Dept. of Health and Human Services, Agency for\\nHealth Care Policy and Research, Rockville, MD, 1992. ⋄72\\n578. P. L. Smith. Splines as a useful and convenient statistical tool. Am Statistician ,\\n33:57–62, 1979. ⋄40\\n579. R. H. Somers. A new asymmetric measure of association for ordinal variables.\\nAm Soc Rev , 27:799–811, 1962. ⋄257, 505\\n580. A. Spanos, F. E. Harrell, and D. T. Durack. Diﬀerential diagnosis of acute\\nmeningitis: An analysis of the predictive value of initial observations. JAMA,\\n262:2700–2707, 1989. ⋄266, 267, 268\\n581. I. Spence and R. F. Garrison. A remarkable scatterplot. Am Statistician , 47:12–\\n19, 1993. ⋄91\\n582. D. J. Spiegelhalter. Probabilistic prediction in patient management and clinical\\ntrials.Stat Med , 5:421–433, 1986. ⋄97, 101, 115, 116, 523\\n583. D. M. Stablein, W. H. Carter, and J. W. Novak. Analysis of survival data with\\nnonproportional hazard functions. Controlled Clin Trials , 2:149–159, 1981. ⋄\\n500\\n584. N. Stallard. Simple tests for the external validation of mortality prediction\\nscores.Stat Med , 28:377–388, 2009. ⋄237\\n585. J. Stare, F. E. Harrell, and H. Heinzl. BJ: An S-Plus program to ﬁt linear\\nregression models to censored data using the Buckley and James method. Comp\\nMeth Prog Biomed , 64:45–52, 2001. ⋄447', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='910cbda5-8ab0-4272-a815-b3d1f1257d9d', embedding=None, metadata={'page_label': '566', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='566 References\\n586. E. W. Steyerberg. Clinical Prediction Models . Springer, New York, 2009. ⋄viii\\n587. E.W. Steyerberg, S.E. Bleeker,H. A.Moll,D.E. Grobbee,and K. G.M.Moons.\\nInternal and external validation of predictive models: A simulation study of bias\\nand precision in small samples. Journal of Clinical Epi , 56(5):441–447, May\\n2003.⋄123\\n588. E. W. Steyerberg, P. M. M. Bossuyt, and K. L. Lee. Clinical trials in acute\\nmyocardial infarction: Should we adjust for baseline characteristics? Am Heart\\nJ, 139:745–751, 2000. Editorial, pp. 761–763. ⋄4, 231\\n589. E. W. Steyerberg, M. J. C. Eijkemans, F. E. Harrell, and J. D. F. Habbema.\\nPrognostic modelling with logistic regression analysis: A comparison of selection\\nand estimation methods in small data sets. Stat Med , 19:1059–1079, 2000. ⋄69,\\n100, 286\\n590. E. W. Steyerberg, M. J. C. Eijkemans, F. E. Harrell, and J. D. F. Habbema.\\nPrognostic modeling with logistic regression analysis: In search of a sensible\\nstrategy in small data sets. Med Decis Mak , 21:45–56, 2001. ⋄100, 271\\n591. E. W. Steyerberg, F. E. Harrell, G. J. J. M. Borsboom, M. J. C. Eijkemans,\\nY. Vergouwe, and J. D. F. Habbema. Internal validation of predictive models:\\nEﬃciency of some procedures for logistic regression analysis. JC l i nE p i , 54:774–\\n781, 2001. ⋄115\\n592. E. W. Steyerberg, A. J. Vickers, N. R. Cook, T. Gerds, M. Gonen, N. Obu-\\nchowski, M. J. Pencina, and M. W. Kattan. Assessing the performance of pre-\\ndiction models:aframework fortraditional andnovelmeasures. Epi (Cambridge,\\nMass.), 21(1):128–138, Jan. 2010. ⋄101\\n593. C. J. Stone. Comment: Generalized additive models. Statistical Sci , 1:312–314,\\n1986.⋄26, 28\\n594. C. J. Stone, M. H. Hansen, C. Kooperberg, and Y. K. Truong. Polynomial\\nsplines and their tensor products in extended linear modeling (with discussion).\\nAnn Stat , 25:1371–1470, 1997. ⋄420, 450\\n595. C. J. Stone and C. Y. Koo. Additive splines in statistics. In Proceedings of the\\nStatistical Computing Section ASA , pages 45–48, Washington, DC, 1985. ⋄24,\\n28, 41\\n596. D. Strauss and R. Shavelle. An extended Kaplan–Meier estimator and its ap -\\nplications. Stat Med , 17:971–982, 1998. ⋄416\\n597. S. Suissa and L. Blais. Binary regression with continuous outcomes. Stat Med ,\\n14:247–255, 1995. ⋄11, 19\\n598. G. Sun, T. L. Shook, and G. L. Kay. Inappropriate use of bivariable analysis\\nto screen risk factors for use in multivariable analysis. JC l i nE p i , 49:907–916,\\n1996.⋄72\\n599. B. Tai, D. Machin, I. White, and V. Gebski. Competing risks analysis of patients\\nwith osteosarcoma: a comparison of four diﬀerent approaches. Stat Med , 20:661–\\n684, 2001. ⋄420\\n600. J. M. G. Taylor, A. L. Siqueira, and R. E. Weiss. The cost of adding parameters\\nto a model. J Roy Stat Soc B , 58:593–607, 1996. ⋄101\\n601. R. D. C. Team. R: A language and environment for statistical computing .R\\nFoundation for Statistical Computing, Vienna, Austria, 2015. ISBN 3-900051-\\n07-0.⋄127\\n602. H. T. Thaler. Nonparametric estimation of the hazard ratio. J Am Stat Assoc ,\\n79:290–293, 1984. ⋄518\\n603. P. F. Thall and J. M. Lachin. Assessment of stratum-covariate interactions in\\nCox’s proportional hazards regression model. Stat Med , 5:73–83, 1986. ⋄482\\n604. T. Therneau and P. Grambsch. Modeling Survival Data: Extending the Cox\\nModel. Springer-Verlag, New York, 2000. ⋄420, 447, 478, 517\\n605. T. M. Therneau, P. M. Grambsch, and T. R. Fleming. Martingale-based residu-\\nals for survival models. Biometrika , 77:216–218, 1990. ⋄197, 413, 487, 493, 494,\\n504', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='02b89ef3-9c18-4d81-a522-980db7bbfa66', embedding=None, metadata={'page_label': '567', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 567\\n606. T. M. Therneau and S. A. Hamilton. rhDNase as an example of recurrent event\\nanalysis. Stat Med , 16:2029–2047, 1997. ⋄420, 421\\n607. R. Tibshirani. Estimating transformations for regression via additivity and\\nvariance stabilization. J Am Stat Assoc , 83:394–405, 1988. ⋄391\\n608. R. Tibshirani. Regression shrinkage and selection via the lasso. J Roy Stat Soc\\nB, 58:267–288, 1996. ⋄71, 215, 356\\n609. R. Tibshirani. The lasso method for variable selection in the Cox model. Stat\\nMed, 16:385–395, 1997. ⋄71, 356\\n610. R. Tibshirani and K. Knight. Model search and inference by bootstrap“bump-\\ning”. Technical report, Department of Statistics, University of Toronto, 1997.\\nhttp://www-stat.stanford.edu/tibs . Presented at the Joint Statistical Meet-\\nings, Chicago, August 1996. ⋄xii, 214\\n611. R. Tibshirani and K. Knight. The covariance inﬂation criterion for adaptive\\nmodel selection. J Roy Stat Soc B , 61:529–546, 1999. ⋄11, 123\\n612. N. H. Timm. The estimation of variance-covariance and correlation matrices\\nfrom incomplete data. Psychometrika , 35:417–437, 1970. ⋄52\\n613. T. Tjur. Coeﬃcients of determination in logistic regression models—A new pro-\\nposal: The coeﬃcient of discrimination. Am Statistician , 63(4):366–372, 2009.\\n⋄257, 272\\n614. W. Y. Tsai, N. P. Jewell, and M.C. Wang. A note on the product limit estimator\\nunder right censoring and left truncation. Biometrika , 74:883–886, 1987. ⋄420\\n615. A. A. Tsiatis. A large sample study of Cox’s regression model. Ann Stat ,\\n9:93–108, 1981. ⋄485\\n616. B. W. Turnbull. Nonparametric estimation of a survivorship function with dou-\\nbly censored data. J Am Stat Assoc , 69:169–173, 1974. ⋄420\\n617. J. Twisk, M. de Boer, W. de Vente, and M. Heymans. Multiple imputation of\\nmissing values was not necessary before pe rforming a longitudinal mixed-model\\nanalysis. JC l i nE p i , 66(9):1022–1028, 2013. ⋄58\\n618. H. Uno, T. Cai, M. J. Pencina, R. B. D’Agostino, and L. J. Wei. On the\\nC-statistics for evaluating overall adequacy of risk prediction procedures with\\ncensored survival data. Stat Med , 30:1105–1117, 2011. ⋄519\\n619.¨U. Uzuno=gullari and J.-L. Wang. A comparison of hazard rate estimators for\\nleft truncated and right censored data. Biometrika , 79:297–310, 1992. ⋄420\\n620. W. Vach. Logistic Regression with Missing Values in the Covariates ,v o l u m e8 6\\nofLecture Notes in Statistics . Springer-Verlag, New York, 1994. ⋄59\\n621. W. Vach. Some issues in estimating the eﬀect of prognostic factors from incom-\\nplete covariate data. Stat Med , 16:57–72, 1997. ⋄52, 59\\n622. W. Vach and M. Blettner. Logistic regression with incompletely observed cate-\\ngorical covariates—Investigating the sensitivity against violation of the missing\\nat random assumption. Stat Med , 14:1315–1329, 1995. ⋄59\\n623. W. Vach and M. Blettner. Missing Data in Epidemiologic Studies. In Ency of\\nBiostatistics , pages 2641–2654. Wiley, New York, 1998. ⋄52, 58, 59\\n624. W. Vach and M. Schumacher. Logistic regression with incompletely observed\\ncategorical covariates: A comparison of three approaches. Biometrika , 80:353–\\n362, 1993. ⋄59\\n625. M. G. Valsecchi, D. Silvestri, and P. Sasieni. Evaluation of long-term survi val:\\nUse of diagnostics and robust estimators with Cox’s proportional hazards model.\\nStat Med , 15:2763–2780, 1996. ⋄518\\n626. S. van Buuren, H. C. Boshuizen, and D. L. Knook. Multiple imputation of\\nmissing blood pressure covariates in survival analysis. Stat Med , 18:681–694,\\n1999.⋄58\\n627. S. van Buuren, J. P. L. Brand, C. G. M. Groothuis-Oudshoorn, and D. B. Rubi n.\\nFully conditional speciﬁcation in multivariate imputation. J Stat Computation\\nSim, 76(12):1049–1064, 2006. ⋄55', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='4d92ddd7-5945-407e-a772-df91ff76933e', embedding=None, metadata={'page_label': '568', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='568 References\\n628. G. J. M. G. van der Heijden, Donders, T. Stijnen, and K. G. M. Moons. Impu-\\ntation of missing values is superior to complete case analysis and the missing-\\nindicator method in multivariable diagnostic research: A clinical example. J\\nClin Epi , 59:1102–1109, 2006. ⋄48, 49\\n629. T. van der Ploeg, P. C. Austin, and E. W. Steyerberg. Modern modelling\\ntechniques are data hungry: a simulation study for predicting dichotomous end-\\npoints.BMC Medical Research Methodology , 14(1):137+, Dec. 2014. ⋄41, 100\\n630. M. J. van Gorp, E. W. Steyerberg, M. Kallewaard, and Y. var der Graaf. Clin-\\nical prediction rule for 30-day mortality in Bj ¨ork-Shiley convexo-concave valve\\nreplacement. JC l i nE p i , 56:1006–1012, 2003. ⋄122\\n631. H. C. van Houwelingen and J. Thorogood. Construction, validation and updat-\\ning of a prognostic model for kidney graft survival. Stat Med , 14:1999–2008,\\n1995.⋄100, 101, 123, 215\\n632. J. C. van Houwelingen and S. le Cessie. Logistic regression, a review. Statistica\\nNeerlandica , 42:215–232, 1988. ⋄271\\n633. J. C. van Houwelingen and S. le Cessie. Predictive value of statistical models .\\nStat Med , 9:1303–1325, 1990. ⋄77, 101, 113, 115, 123, 204, 214, 215, 258, 259,\\n273, 508, 509, 518\\n634. W. N. Venables and B. D. Ripley. Modern Applied Statistics with S-Plus .\\nSpringer-Verlag, New York, third edition, 1999. ⋄101\\n635. W. N. Venables and B. D. Ripley. Modern Applied Statistics with S . Springer-\\nVerlag, New York, fourth edition, 2003. ⋄xi, 127, 129, 143, 359\\n636. D. J. Venzon and S. H. Moolgavkar. A method for computing proﬁle-likelihood-\\nbased conﬁdence intervals. Appl Stat , 37:87–94, 1988. ⋄214\\n637. G. Verbeke and G. Molenberghs. Linear Mixed Models for Longitudinal Data .\\nSpringer, New York, 2000. ⋄143\\n638. Y. Vergouwe, E. W. Steyerberg, M. J. C. Eijkemans, and J. D. F. Habbema.\\nSubstantial eﬀective sample sizes were required for external validation studies\\nof predictive logistic regression models. JC l i nE p i , 58:475–483, 2005. ⋄122\\n639. P. Verweij and H. C. van Houwelingen. Penalized likelihood in Cox regression.\\nStat Med , 13:2427–2436, 1994. ⋄77, 209, 210, 211, 215\\n640. P. J. M. Verweij and H. C. van Houwelingen. Cross-validation in survival anal-\\nysis.Stat Med , 12:2305–2314, 1993. ⋄100, 123, 207, 215, 509, 518\\n641. P. J. M. Verweij and H. C. van Houwelingen. Time-dependent eﬀects of ﬁxed\\ncovariates in Cox regression. Biometrics , 51:1550–1556, 1995. ⋄209, 211, 501\\n642. A. J. Vickers. Decision analysis for the evaluation of diagnostic tests, prediction\\nmodels, and molecular markers. Am Statistician , 62(4):314–320, 2008. ⋄5\\n643. S. K. Vines. Simple principal components. Appl Stat , 49:441–451, 2000. ⋄101\\n644. E. Vittinghoﬀ and C. E. McCulloch. Relaxing the rule of ten events per variable\\nin logistic and Cox regression. Am J Epi , 165:710–718, 2006. ⋄100\\n645. P.T.von Hippel. Regression with missingys:Animproved strategy foranalyzing\\nmultiple imputed data. Soc Meth , 37(1):83–117, 2007. ⋄47\\n646. H. Wainer. Finding what is not there through the unfortunate binningof results:\\nThe Mendel eﬀect. Chance, 19(1):49–56, 2006. ⋄19, 20\\n647. S. H. Walker and D. B. Duncan. Estimation of the probability of an event as a\\nfunction of several independent variables. Biometrika , 54:167–178, 1967. ⋄14,\\n220, 311, 313\\n648. A. R. Walter, A. R. Feinstein, and C. K. Wells. Coding ordinal independent\\nvariables in multiple regression analyses. Am J Epi , 125:319–323, 1987. ⋄39\\n649. A. Wang and E. A. Gehan. Gene selection for microarray data analysis using\\nprincipal component analysis. Stat Med , 24:2069–2087, 2005. ⋄101\\n650. M. Wang and S. Chang. Nonparametric estimation of a recurrent survival func-\\ntion.J Am Stat Assoc , 94:146–153, 1999. ⋄421\\n651. R. Wang, J. Sedransk, and J. H. Jinn. Secondary data analysis when there are\\nmissing observations. J Am Stat Assoc , 87:952–961, 1992. ⋄53', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='61589cd0-b8d8-4133-bb21-4dfcffdceacc', embedding=None, metadata={'page_label': '569', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='References 569\\n652. Y. Wang and J. M. G. Taylor. Inference for smooth curves in longitudinal data\\nwith application to an AIDS clinical trial. Stat Med , 14:1205–1218, 1995. ⋄215\\n653. Y. Wang, G. Wahba, C. Gu, R. Klein, and B. Klein. Using smoothing splin e\\nANOVA to examine the relation of risk factors to the incidence and progression\\nof diabetic retinopathy. Stat Med , 16:1357–1376, 1997. ⋄41\\n654. Y. Wax. Collinearity diagnosis for a relative risk regression analysis: An appli-\\ncation to assessment of diet-cancer relationship in epidemiological studies. Stat\\nMed, 11:1273–1287, 1992. ⋄79, 138, 255\\n655. L. J. Wei, D. Y. Lin, and L. Weissfeld. Regression analysis of multivariate\\nincomplete failure time data by modeling marginal distributions. JA mS t a t\\nAssoc, 84:1065–1073, 1989. ⋄417\\n656. R. E. Weiss. Th e inﬂuence of variable selection: A Bayesian diagnostic perspec-\\ntive.J Am Stat Assoc , 90:619–625, 1995. ⋄100\\n657. S. Wellek. A log-rank test for equivalence of two survivor functions. Biometrics ,\\n49:877–881, 1993. ⋄450\\n658. T. L. Wenger, F. E. Harrell, K. K. Brown, S. Lederman, and H. C. Strauss.\\nVentricularﬁbrillation followingcaninecoronary reperfusion:Diﬀerentoutcomes\\nwith pentobarbital and α-chloralose. Can J Phys Pharm , 62:224–228, 1984. ⋄\\n266\\n659. H. White. A heteroskedasticity-consistent covariance matrix estimator and a\\ndirect test for heteroskedasticity. Econometrica , 48:817–838, 1980. ⋄196\\n660. I. R. White and J. B. Carlin. Bias and eﬃciency of multiple imputation\\ncompared with complete-case analysis for missing covariate values. Stat Med ,\\n29:2920–2931, 2010. ⋄59\\n661. I. R. White and P. Royston. Imputing missing covariate values for the Cox\\nmodel.Stat Med , 28:1982–1998, 2009. ⋄54\\n662. I. R. White, P. Royston, and A. M. Wood. Multiple imputation using chained\\nequations: Issues and guidance for practice. Stat Med , 30(4):377–399, 2011. ⋄\\n53, 54, 58\\n663. A. Whitehead, R. Z. Omar, J. P. T. Higgins, E. Savaluny, R. M. Turner, and\\nS. G. Thompson. Meta-analysis of ordinal outcomes using individual patient\\ndata.Stat Med , 20:2243–2260, 2001. ⋄324\\n664. J. Whitehead. Sample size calculations for ordered categorical data. Stat Med ,\\n12:2257–2271, 1993. See letter to editor SM 15:1065-6 for binary case;see errata\\nin SM 13:871 1994;see kol95com, jul96sam. ⋄2, 73, 313, 324\\n665. J. Whittaker. Model interpretation from the additive elements of the likelihood\\nfunction. Appl Stat , 33:52–64, 1984. ⋄205, 207\\n666. A.S.Whittemore and J.B.Keller. Survivalestimation usingsplines. Biometrics ,\\n42:495–506, 1986. ⋄420\\n667. H. Wickham. ggplot2: elegant graphics for data analysis . Springer, New York,\\n2009.⋄xi\\n668. R. E. Wiegand. Performance of using multiple stepwise algorithms for variable\\nselection. Stat Med , 29:1647–1659, 2010. ⋄100\\n669. A. R. Willan, W. Ross, and T. A. MacKenzie. Comparing in-patient classiﬁca-\\ntion systems: A problem of non-nested regression models. Stat Med , 11:1321–\\n1331, 1992. ⋄205, 215\\n670. A. Winnett and P. Sasieni. A note on scaled Schoenfeld residuals for the p ro-\\nportional hazards model. Biometrika , 88:565–571, 2001. ⋄518\\n671. A. Winnett and P. Sasieni. Iterated residuals and time-varying covariate eﬀects\\nin Cox regression. J Roy Stat Soc B , 65:473–488, 2003. ⋄518\\n672. D. M. Witten and R. Tibshirani. Testing signiﬁcance of features by lassoed\\nprincipal components. Ann Appl Stat , 2(3):986–1012, 2008. ⋄175\\n673. A. M. Wood, I. R. White, and S. G. Thompson. Are missing outcome data\\nadequately handled?Areviewof publishedrandomized controlled trialsin major\\nmedical journals. Clin Trials , 1:368–376, 2004. ⋄58', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='96b8c77d-3636-4f67-87e3-501c42195c23', embedding=None, metadata={'page_label': '570', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='570 References\\n674. S. N. Wood. Generalized Additive Models: An Introduction with R . Chapman\\n& Hall/CRC, Boca Raton, FL, 2006. ISBN 9781584884743. ⋄90\\n675. C. F. J. Wu. Jackknife, bootstrap and other resampling methods in regression\\nanalysis. Ann Stat , 14(4):1261–1350, 1986. ⋄113\\n676. Y. Xiao and M. Abrahamowicz. Bootstrap-based methods for estimating stan-\\ndard errors in Cox’s regression analyses of clustered event times. Stat Med ,\\n29:915–923, 2010. ⋄213\\n677. Y. Xie. knitr: A general-purpose package for dynamic report generation in R ,\\n2013. R package version 1.5. ⋄xi, 138\\n678. J. Ye. On measuring and correcting the eﬀects of data mining and model selec-\\ntion.J Am Stat Assoc , 93:120–131, 1998. ⋄10\\n679. T. W. Yee and C. J. Wild. Vector generalized additive models. J Roy Stat Soc\\nB, 58:481–493, 1996. ⋄324\\n680. F. W. Young, Y. Takane, and J. de Leeuw. The principal components of mixed\\nmeasurement level multivariate data: An alternating least squares method with\\noptimal scaling features. Psychometrika , 43:279–281, 1978. ⋄81\\n681. R. M. Yucel and A. M. Zaslavsky. U sing calibration to improve rounding in\\nimputation. Am Statistician , 62(2):125–129, 2008. ⋄56\\n682. H. Zhang. Classiﬁcation trees for multiple binary responses. J Am Stat Assoc ,\\n93:180–193, 1998. ⋄41\\n683. H. Zhang, T. Holford, and M. B. Bracken. A tree-based method of analysis for\\nprospective studies. Stat Med , 15:37–49, 1996. ⋄41\\n684. B. Zheng and A. Agresti. Summarizing the predictive power of a generalized\\nlinear model. Stat Med , 19:1771–1781, 2000. ⋄215, 273\\n685. X. Zheng and W. Loh. Consistent variable selection in linear models. JA m\\nStat Assoc , 90:151–156, 1995. ⋄214\\n686. H. Zhou, T. Hastie, and R. Tibshirani. Sparse principal component analysis. J\\nComp Graph Stat , 15:265–286, 2006. ⋄101\\n687. X. Zhou. Eﬀect of veriﬁcation bias on positive and negative predictive values.\\nStat Med , 13:1737–1745, 1994. ⋄328\\n688. X. Zhou, G. J. Eckert, and W. M. Tiern ey. Multiple imput ation in public health\\nresearch. Stat Med , 20:1541–1549, 2001. ⋄59\\n689. H. Zou, T. Hastie, and R. Tibshirani. On the“degrees of freedom”of the lasso.\\nAnn Stat , 35:2173–2192, 2007. ⋄11\\n690. H. Zou and M. Yuan. Composite quantile regression and the oracle model\\nselection theory. Ann Stat , 36(3):1108–1126, 2008. ⋄361\\n691. D. M. Zucker. The eﬃciency of a weighted log-rank test under a percent error\\nmisspeciﬁcation model for the log hazard ratio. Biometrics , 48:893–899, 1992.\\n⋄518', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eaa79bb7-58e8-475c-a93c-74a0e7963382', embedding=None, metadata={'page_label': '571', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index\\nEntries in this font are names of software components. Page numbers in\\nbolddenote the most comprehensive treatment of the topic.\\nSymbols\\nDxy,105,142,257,257–259,269,\\n284,318,461,505,529\\ncensored data, 505,517\\nR2,110,111,206,272,390,391\\nadjusted, 74,77,105\\ngeneralized, 207\\nsigniﬁcant diﬀerence in, 215\\ncindex,93,100,105,142,257,\\n257,259,318,505,517\\ncensored data, 505\\ngeneralized, 318,505\\nHbA1c,365\\n15:1 rule, 72,100\\nA\\nAalen survival function estimator,\\nseesurvival function\\nabs.error.pred ,102\\naccelerated failure time, see\\nmodel\\naccuracy, 104,111,113,114,210,\\n354,446\\ng-index,105\\nabsolute, 93,102apparent, 114,269,529\\napproximation, 119,275,\\n287,348,469\\nbias-corrected, 100,109,\\n114,115,141,391,529\\ncalibration, 72–78,\\n88,92,93,105,111,115,141,\\n236,237,259,260,\\n264,269,271,284,301,322,\\n446,467,506\\ndiscrimination, 72,92,93,\\n105,111,111,257,259,\\n269,284,287,318,331,346,\\n467,505,506,508\\nfuture,211\\nindex,122,123,141\\nACE,82,176,179,390,391,392\\nace,176,392\\nacepackpackage, 176,392\\nactuarial survival, 410\\nadequacy index, 207\\nAIC,28,69,78,88,172,204,204,\\n210,211,214,215,\\n240,241,269,275,277,332,\\n374,375\\n©Springer International Publishing Switzerland 2015\\nF.E. Harrell, Jr., Regression Modeling Strategies , Springer Series\\nin Statistics, DOI 10.1007/978-3-319-19425-7571', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='5e9c792b-803d-43bb-ac3e-cb3ff9b818ac', embedding=None, metadata={'page_label': '572', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='572 Index\\nAIC,134,135,277\\nAkaike information criterion, see\\nAIC\\nanalysis of covariance, see\\nANOCOVA\\nANOCOVA, 16,223,230,447\\nANOVA, 13,32,75,230,235,317,\\n447,480,531\\nanova,65,127,133,134,136,\\n149,155,278,302,306,336,\\n342,346,464,466\\nanova.gls ,149\\nareg.boot ,392–394\\naregImpute ,51,53–56,59,\\n304,305\\nArjas plot, 495\\nasis,132,133\\nassumptions\\naccelerated failure time,\\n436,437,458\\nadditivity, 37,248\\ncontinuation ratio, 320,\\n321,338\\ncorrelation pattern, 148,153\\ndistributional, 39,97,\\n148,317,446,525\\nlinearity, 21–26\\nordinality, 312,319,333,340\\nproportional hazards, 429,\\n494–503\\nproportional odds, 313,\\n315,317,336,362\\nAVAS,390–392\\ncase study, 393–398\\navas,392,394,395\\nB\\nB-spline, seespline function\\nbattery reduction, 87\\nBayesian modeling, 71,209,215\\nBIC,211,214,269\\nbinary response, seeresponse\\nbj,131,135,447,449\\nbootcov,134–136,198–202,319\\nbootkm,419bootstrap, 106–109,114–116\\n.632,115,123\\nadjusting for imputation, 53\\napproximate Bayesian, 50\\nbasic,202,203\\nBCa,202,203\\ncluster,135,197,199,213\\nconditional, 115,122,197\\nconﬁdence intervals, see\\nconﬁdence intervals, 199\\ncovariance matrix, 135,198\\ndensity,107,136\\ndistribution, 201\\nestimating shrinkage, 77,115\\nmodel uncertainty, 11,113,304\\noverﬁtting correction, 112,\\n114,115,257,391\\nranks,117\\nvariable selection, 70,97,\\n113,177,260,275,282,286\\nbplot,134\\nBreslow survival function\\nestimator, seesurvival\\nfunction\\nBrier score, 142,237,\\n257–259,271,318\\nC\\nCABG,484\\ncalibrate ,135,141,269,\\n271,284,300,319,323,355,\\n450,467,517\\ncalibration, seeaccuracy\\ncaliper matching, 372\\ncancor,141\\ncanonical correlation, 141\\ncanonical variate, 82,83,129,\\n141,167,169,393\\nCART,seerecursive partitioning\\ncasewise deletion, seemissing\\ndata\\ncategorical predictor, see\\npredictor\\ncategorization of continuous\\nvariable, 8,18–21', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='21d899d8-fef6-4844-ac22-56b48d946bad', embedding=None, metadata={'page_label': '573', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 573\\ncatg,132,133\\ncausal inference, 103\\ncause removal, 414\\ncensoring, 401–402,406,424\\ninformative, 402,414,415,420\\ninterval, 401,418,420\\nleft,401\\nright,402,418\\ntype I,401\\ntype II,402\\nciapower,513\\nclassiﬁcation, 4,6\\nclassiﬁer, 4,6\\nclustered data, 197,417\\nclustering\\nhierarchical, 129,166,330\\nvariable, 81,101,175,355\\nClustOfVar ,101\\ncoef,134\\ncoeﬃcient of discrimination, see\\naccuracy\\ncollinearity, 78–79\\ncompeting risks, 414,420\\nconcordance probability, seec\\nindex\\nconditional logistic model, see\\nlogistic model\\nconditional probability, 320,404,\\n476,484\\nconﬁdence intervals, 10,30,\\n35,64,66,96,136,185,\\n198,273,282,391\\nbootstrap, 107,109,\\n119,122,135,149,199,\\n201–203,214,217\\ncoverage, 35,198,199,389\\nsimultaneous, 136,199,\\n202,214,420,517\\nconfounding, 31,103,231\\nconfplot,214\\ncontingency table, 195,228,\\n230,235\\ncontrast, seehypothesis test\\ncontrast,134,136,\\n192,193,198,199convergence, 193,264\\ncoronary artery disease, 48,207,\\n240,245,252,492,497\\ncorrelation structures, 147,148\\ncorrespondence analysis, 81,129\\ncost-eﬀectiveness, 4\\nCox model, 362,375,392,\\n475–517\\ncase study, 521–531\\ndata reduction example, 172\\nmultiple imputation, 54\\ncox.zph,499,516,517,526\\ncoxph,131,422,513\\ncph,131,133,135,172,422,\\n448,513,513,514,516,517\\ncpower,513\\ncr.setup,323,340,354\\ncross-validation, seevalidation of\\nmodel\\ncubic spline, seespline function\\ncumcategory ,357\\ncumulative hazard function, see\\nhazard function\\ncumulative probability model,\\n359,361–363,370,371\\ncut2,129,133,334,419\\ncutpoint, 21\\nD\\ndata reduction, 79–88,275\\ncase study 1, 161–177\\ncase study 2, 277\\ncase study 3, 329–333\\ndata-splitting, seevalidation of\\nmodel\\ndata.frame ,309\\ndatadist,130,130,138,292,463\\ndatasets, 535\\ncdystonia, 149\\ncervical dystonia, 149\\ndiabetes, 317\\nmeningitis, 266,267\\nNHANES, 365\\nprostate, 161,275,521\\nSUPPORT, 59,453', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb080242-3854-40d1-9a5b-8adf7d0bc238', embedding=None, metadata={'page_label': '574', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='574 Index\\nTitanic, 291\\ndegrees of freedom, 193\\neﬀective, 30,41,77,96,136,\\n210,269\\ngeneralized, 10\\nphantom, 35,111\\ndelayed entry, 401\\ndelta method, 439\\ndescribe,129,291,453\\ndeviance, 236,449,487,516\\nDFBETA, 91\\nDFBETAS, 91\\nDFFIT, 91\\nDFFITS, 91\\ndiabetes, seedatasets, 365\\ndiﬀerence in predictions, 192,201\\ndimensionality, 88\\ndiscriminant analysis, 220,230,\\n272\\ndiscrimination, seeaccuracy, see\\naccuracy\\ndistribution, 317\\nt,186\\nbinomial, 73,181,194,235\\nCauchy, 362\\nexponential, 142,407,408,\\n425,427,451\\nextreme value, 362,363,427,\\n437\\nGumbel, 362,363\\nlog-logistic, 9,423,\\n427,440,442,503\\nlog-normal, 9,106,\\n391,423,427,442,463,464\\nnormal,187\\nWeibull, 39,408,408,420,426,\\n432–437,444,448\\ndose-response, 523\\ndoubly nonlinear, 131\\ndrop-in, 513\\ndropouts, 143\\ndummy variable, 1,seeindicator\\nvariable, 75,129,130,\\n209,210E\\neconomists, 71\\neffective.df ,134,136,345,346\\nEmax,353\\nepidemiology, 38\\nestimation, 2,98,104\\nestimator\\nBuckley–James, 447,449\\nmaximum likelihood, 181\\nmean,362\\npenalized, seemaximum\\nlikelihood, 175\\nquantile, 362\\nself-consistent, 525\\nsmearing, 392,393\\nexplained variation, 273\\nexponential distribution, see\\ndistribution\\nExProb,135\\nexternal validation, seevalidation\\nof model\\nF\\nfailure time, 399\\nfastbw,133,134,137,280,286,\\n351,469\\nfeature selection, 94\\nﬁnancial data, 3\\nfit.mult.impute ,54,306\\nFleming–Harrington survival\\nfunction estimator, see\\nsurvival function\\nformula,134\\nfractional polynomial, 40\\nFunction,134,135,138,149,310,\\n395\\nfunctions, generating Rcode,395\\nG\\nGAM,seegeneralized additive\\nmodel,seegeneralized\\nadditive model\\ngampackage, 390\\nGDF,seedegrees of freedom\\nGEE,147', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='65fe01d4-686a-4036-a3f9-1ee522b5914c', embedding=None, metadata={'page_label': '575', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 575\\nGehan–Wilcoxon test, see\\nhypothesis test\\ngendata,134,136\\ngeneralized additive model,\\n29,41,138,142,390\\ncase study, 393–398\\ngetHdata,59,178,535\\nggplot,134\\nggplot2package, xi,134,294\\ngIndex,105\\nglht,199\\nGlm,131,135,271\\nglm,131,141,271\\nGls,131,135,149\\ngls,131,149\\ngoodness of ﬁt, 236,269,\\n427,440,458\\nGreenwood’s formula, seesurvival\\nfunction\\ngroupkm,419\\nH\\nhare,450\\nhat matrix, 91\\nHazard,135,448\\nhazard function, 135,362,\\n375,400,402,405,409,427,\\n475,476\\nbathtub, 408\\ncause-speciﬁc, 414,415\\ncumulative, 402–409\\nhazard ratio, 429–431,\\n433,478,479,481\\ninterval-speciﬁc, 495–497,502\\nhazard.ratio.plot ,517\\nhclust,129\\nheft,419\\nheterogeneity, unexplained, 4,231,\\n400\\nhistSpikeg ,294\\nHmiscpackage, xi,129,133,137,\\n167,176,273,277,294,304,\\n319,357,392,418,458,463,\\n513,536\\nhoeffd,129Hoeﬀding D,129,166,458\\nHosmer–Lemeshow test, 236,237\\nHotelling test, seehypothesis test\\nHuber–White estimator, 196\\nhypothesis test, 1,18,32,99\\nadditivity, 37,248\\nassociation, 2,18,32,43,66,\\n129,235,338,486\\ncontrast, 157,192,193,198\\nequal slopes, 315,321,322,\\n338,339,458,460,495\\nexponentiality, 408,426\\nGehan-Wilcoxon, 505\\nglobal,69,97,189,205,\\n230,232,342,526\\nHotelling, 230\\nindependence, 129,166\\nKruskal–Wallis, 2,66,129\\nlinearity, 18,32,35,36,39,42,\\n66,91,238\\nlog-rank, 41,363,422,475,486,\\n513,518\\nMantel–Haenszel, 486\\nnormal scores, 364\\npartial,190\\nPearson χ2,195,235\\nrobust,9,81,311\\nVan der Waerden, 364\\nWilcoxon, 1,73,129,\\n230,257,311,313,325,\\n363,364\\nI\\nignorable nonresponse, see\\nmissing data\\nimbalances, baseline, 400\\nimproveProb ,142\\nimputation, 47–57,83\\nchained equations, 55,304\\nmodel for, 49,50,50–52,\\n59,84,129\\nmultiple, 47,53,54,54–56,\\n95,129,304,382,537\\ncensored data, 54', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='899ac380-8878-4bd7-ae6a-93b36586f0b6', embedding=None, metadata={'page_label': '576', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='576 Index\\npredictive mean matching, 51,\\n52,55\\nsingle,52,56,57,138,\\n171,275,276,334\\nimpute,129,135,138,171,\\n276,277,334,461\\nincidence\\ncrude,416\\ncumulative, 415\\nincomplete principal component\\nregression, 170,275\\nindicator variable, 16,17,38,39\\ninﬁnite regression coeﬃcient, 234\\ninﬂuential observations, 90–92,\\n116,255,256,269,504\\ninformation function, 182,183\\ninformation matrix, 79,188,189,\\n191,196,208,211,232,346\\ninformative missing, seemissing\\ndata\\ninteraction, 16,36,375\\ninterquartile-range eﬀect, 104,136\\nintracluster correlation, 135,141,\\n197,417\\nisotropic correlation structure, see\\ncorrelation structures\\nJ\\njackknife, 113,504\\nK\\nKalbﬂeisch–Prentice estimator,\\nseesurvival function\\nKaplan–Meier estimator, see\\nsurvival function\\nknots,22\\nKullback–Leibler information, 215\\nL\\nlandmark survival time analysis,\\n447\\nlasso,71,100,121,175,356\\nLATEX,129,536latex,129,134,135,137,138,149,\\n246,282,292,336,342,346,\\n453,466,470,536\\nlatticepackage, 134\\nleast squares\\ncensored, 447\\nleave-out-one, seevalidation of\\nmodel\\nleft truncation, 401,420\\nlife expectancy, 4,408,472\\nlift curve, 5\\nlikelihood function, 182,\\n187,188,190,\\n194,195,424,425,476\\npartial,477\\nlikelihood ratio test, 185–186,\\n189–191,193–195,\\n198,204,205,207,228,240\\nlinear model, 73,74,143,311,359,\\n361,362,364,368,370,372\\ncase study, 143\\nlinear spline, seespline function\\nlink function, 15\\nCauchy, 362\\ncomplementary log-log, 362\\nlog-log,362\\nprobit,362\\nlm,131\\nlme,149\\nlocal regression, see\\nnonparametric\\nloess,seenonparametric\\nloess,29,142,493\\nlog-rank, seehypothesis test\\nLOGISTIC,315\\nlogistic model\\nbinary,219–231\\ncase study 1, 275–288\\ncase study 2, 291–310\\nconditional, 483\\ncontinuation ratio, 319–323\\ncase study, 338–340\\nextended continuation ratio,\\n321–322\\ncase study, 340–355', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='ca2ea212-58d2-4c3d-bf49-b2281b6c16f4', embedding=None, metadata={'page_label': '577', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 577\\nordinal,311\\nproportional odds, 73,311,312,\\n313–319,333,362,364\\ncase study, 333–338\\nlogLik,134,135\\nlongitudinal data, 143\\nlowess,seenonparametric\\nlowess,141,294\\nlrm,65,131,134,135,201,\\n269,269,273,277,278,\\n296,297,302,306,319,323,\\n335,337,339,341,342,448,\\n513\\nlrtest,134,135\\nlsp,133\\nM\\nMallows’ Cp,69\\nMantel–Haenszel test, see\\nhypothesis test\\nmarginal distribution, 26,417,\\n478\\nmarginal estimates, see\\nunconditioning\\nmartingale residual, 487,493,494,\\n515,516\\nmatrix,133\\nmatrx,133\\nmaximal correlation, 390\\nmaximum generalized variance,\\n82,83\\nmaximum likelihood, 147\\nestimation, 181,231,424,425,\\n477\\npenalized, 11,77,78,115,136,\\n209–212,269,327,328,353\\ncase study, 342–355\\nweighted, 208\\nmaximum total variance, 81\\nMean,135,319,448,472,513,514\\nmeningitis, seedatasets\\nmgcvpackage, 390\\nMGV,seemaximum generalized\\nvariance\\nMICE,54,55,59missing data, 143,302\\ncasewise deletion, 47,48,81,\\n296,307,384\\ndescribing patterns, see\\nnaclus, naplot\\nimputation, seeimputation\\ninformative, 46,424\\nrandom, 46\\nMLE,seemaximum likelihood\\nmodel\\naccelerated failure time,\\n436–446,453\\ncase study, 453–473\\nAndersen–Gill, 513\\napproximate, 119–123,\\n275,287,349,352–354,356\\nBuckley–James, 447,449\\ncomparing more than one, 92\\nCox,seeCox model\\ncumulative link, seecumulative\\nprobability model\\ncumulative probability, see\\ncumulative probability\\nmodel\\nextended linear, 146\\ngeneralized additive, see\\ngeneralized additive model,\\n359\\ngeneralized linear, 146,359\\ngrowth curve, 146\\nlinear,seelinear model,\\n117,199,287,317,389\\nlog-logistic, 437\\nlog-normal, 437,453\\nlogistic,seelogistic model\\nlongitudinal, 143\\nols,146\\nordinal,seeordinal model\\nparametric proportional\\nhazards, 427\\nquantile regression, seequantile\\nregression\\nsemiparametric, see\\nsemiparametric model', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='6e51c1e1-8192-4b3a-9b4a-f573ef0acc9f', embedding=None, metadata={'page_label': '578', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='578 Index\\nvalidation, seevalidation of\\nmodel\\nmodel approximation, seemodel\\nmodel uncertainty, 170,304\\nmodel validation, seevalidation\\nof model\\nmodeling strategy, seestrategy\\nmonotone,393\\nmonotonicity, 66,83,84,\\n95,129,166,389,390,393,\\n458\\nMTV,seemaximum total\\nvariance\\nmultcomp package, 199,202\\nmulti-state model, 420\\nmultiple events, 417\\nN\\nna.action ,131\\nna.delete ,131,132\\nna.detail.response ,131\\nna.fail,132\\nna.fun.response ,131\\nna.omit,132\\nnaclus,47,142,302,458,461\\nnaplot,47,302,461\\nnaprint,135\\nnaresid,132,135\\nnatural spline, seerestricted\\ncubic spline\\nnearest neighbor, 51\\nNelson estimator, seesurvival\\nfunction, 422\\nNewlabels ,473\\nNewton–Raphson algorithm, 193,\\n195,196,209,231,426\\nNHANES, 365\\nnlmepackage, 131,148,149\\nnoise,34,68,69,72,209,488,523\\nnomogram, 104,268,\\n310,318,353,514,531\\nnomogram,135,138,149,282,319,\\n353,473,514\\nnon-proportional hazards, 73,450,\\n506noncompliance, 402,513\\nnonignorable nonresponse, see\\nmissing data\\nnonparametric\\ncorrelation, 66\\ncensored data, 517\\ngeneralized Spearman\\ncorrelation, 66,376\\nindependence test, 129,166\\nregression, 29,41,105,142,245,\\n285\\ntest,2,66,129\\nnonproportional hazards, 495\\nnpsurv,418,419\\nns,132,133\\nnuisance parameter, 190,191\\nO\\nobject-oriented program, x,127,\\n133\\nobservational study, 3,58,\\n230,400\\nodds ratio, 222,224,318\\nOLS,seelinear model\\nols,131,135,137,350,351,\\n448,469,470\\noptimism, 109,111,114,391\\nordered,133\\nordinal model, 311,359,361–363,\\n370,371\\ncase study, 327–356,359–387\\nprobit,364\\nordinal response, seeresponse\\nordinality, seeassumptions\\norm,131,135,319,362,363\\noutlier,116,294\\noveradjustment, 2\\noverﬁtting, 72,109–110\\nP\\nparsimony, 87,97,119\\npartial eﬀect plot, 104,318\\npartial residual, seeresidual\\npartial test, seehypothesis test\\nPC,seeprincipal component,\\n170,172,175,275', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='9e738831-066c-46c6-b5e1-24ed1864b3ba', embedding=None, metadata={'page_label': '579', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 579\\npcaPPpackage, 175\\npecpackage, 519\\npenalized maximum likelihood,\\nseemaximum likelihood\\npentrace,134,136,269,323,342,\\n344\\nperson-years, 408,425\\nplclust,129\\nplot.lrm.partial ,339\\nplot.xmean.ordinaly ,319,323,333\\nplsmo,358\\nPoisson model, 271\\npol,133\\npoly,132,133\\npolynomial, 21\\npopower,319\\nposamsize ,319\\npower calculation, seecpower,\\nspower, ciapower, popower\\npphsm,448\\nprcomp,141\\npreconditioning, 118,123\\npredab.resample ,141,269,323\\nPredict,130,134,136,149,\\n198,199,202,278,299,307,\\n319,448,466\\npredict,127,132,136,140,309,\\n319,469,517,526\\npredictor\\ncontinuous, 21,40\\nnominal, 16,210\\nordinal,38\\nprincipal component, 81,87,\\n101,275\\nsparse,101,175\\nprincomp,141,171\\nPRINQUAL,82,83\\nproduct-limit estimator, see\\nsurvival function\\npropensity score, 3,58,231\\nproportional hazards model, see\\nCox model\\nproportional odds model, see\\nlogistic modelprostate, seedatasets\\npsm,131,135,448,448,\\n460,464,513\\nQ\\nQ–R decomposition, 23\\nQ-Q plot, 148\\nqr,192\\nQuantile,135,448,472,513,514\\nquantile regression, 359,360,364,\\n370,379,392\\ncomposite, 361\\nquantreg,131,360\\nR\\nrandom forests, 100\\nrank correlation, see\\nnonparametric\\nRao score test, 186–187,\\n191,193–195,198\\nrcorr,166\\nrcorr.cens ,142,461,517\\nrcorrcens ,461\\nrcorrp.cens ,142\\nrcs,133,296,297\\nrcspline.eval ,129\\nrcspline.plot ,273\\nrcspline.restate ,129\\nreceiver operating characteristic\\ncurve,6,11\\narea,92,93,111,257,346\\narea, generalized, 318,505\\nrecursive partitioning, 10,30,31,\\n41,46,47,51,52,83,87,\\n100,120,142,302,349\\nredun,80,463\\nredundancy analysis, 80,175\\nregression to the mean, 75,530\\nresampling, 105,112\\nresid,134,336,337,460,516\\nresidual\\nlogistic score, 314,336\\nmartingale, 487,493,494,\\n515,516\\npartial,34,272,315,321,337', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='f3b94fcb-ec65-47f3-b7fb-4f640c6f1893', embedding=None, metadata={'page_label': '580', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='580 Index\\nSchoenfeld score, 314,487,\\n498,499,516,517,525,526\\nresiduals ,132,134,269,336,337,\\n460,516\\nresiduals.coxph ,516\\nresponse\\nbinary,219–221\\ncensored or truncated, 401\\ncontinuous, 389–398\\nordinal,311,327,359\\nrestricted cubic spline, seespline\\nfunction\\nridge regression, 77,115,209,210\\nrisk diﬀerence, 224,430\\nrisk ratio, 224,430\\nrmspackage, xi,129,130–141,\\n149,192,193,198,199,211,\\n214,319,362,363,418,\\n422,535\\nrobcov,134,135,198,202\\nrobust covariance estimator, see\\nvariance–covariance matrix\\nrobustgam package, 390\\nROC,seereceiver operating\\ncharacteristic curve, 105\\nrpart,142,302,303\\nRq,131,135,360\\nrq,131\\nrunif,460\\nS\\nsample size, 73,74,148,\\n233,363,486\\nsample survey, 135,197,208,417\\nsas.get,129\\nsascode,138\\nscientiﬁc quantity, 20\\nscore function, 182,183,186\\nscore test, seeRao score test,\\n235,363\\nscore.binary ,86\\nscored,132,133\\nscoring, hierarchical, 86\\nscree plot, 172semiparametric model, 311,359,\\n361–363,370,371,475\\nsensuc,134\\nshrinkage, 75–78,87,88,\\n209–212,342–348\\nsimilarity measure, 81,330,458\\nsmearing estimator, seeestimator\\nsmoother, 390\\nSomers’ rank correlation, seeDxy\\nsomers2,346\\nspcapackage, 175\\nsPCAgrid,175,179\\nSpearman rank correlation, see\\nnonparametric\\nspearman2 ,129,460\\nspecs,134,135\\nspline function, 22,30,\\n167,192,393\\nB-spline, 23,41,132,500\\ncubic,23\\nlinear,22,133\\nnormalization, 26\\nrestricted cubic, 24–28\\ntensor,37,247,374,375\\nspower,513\\nstandardized regression\\ncoeﬃcient, 103\\nstate transition, 416,420\\nstep,134\\nstep halving, 196\\nstrat,133\\nstrata,133\\nstrategy, 63\\ncomparing models, 92\\ndata reduction, 79\\ndescribing model, 103,318\\ndeveloping imputations, 49\\ndeveloping model for eﬀect\\nestimation, 98\\ndeveloping models for\\nhypothesis testing, 99\\ndeveloping predictive model, 95\\nglobal,94\\nin a nutshell, ix,95\\ninﬂuential observations, 90', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='d8f5413c-349a-4dec-8867-41916c4c7ad7', embedding=None, metadata={'page_label': '581', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Index 581\\nmaximum number of\\nparameters, 72\\nmodel approximation, 118,275,\\n287\\nmultiple imputation, 53\\nprespeciﬁcation of complexity,\\n64\\nshrinkage, 77\\nvalidation, 109,110\\nvariable selection, 63,67\\nstratiﬁcation, 225,237,238,254,\\n418,419,481–483,488\\nsubgroup estimates, 34,241,400\\nsummary,127,130,134,136,149,\\n167,198,199,201,278,292,\\n466\\nsummary.formula ,302,319,357\\nsummary.gls ,149\\nsuper smoother, 29\\nSUPPORT study, seedatasets\\nsuppression, 101\\nsupsmu,141,273,390\\nSurv,172,418,422,458,516\\nsurvConcordance ,517\\nsurvdiff,517\\nsurvest,135,448\\nsurvfit,135,418,419\\nSurvival,135,448,513,514\\nsurvival function\\nAalen estimator, 412,413\\nBreslow estimator, 485\\ncrude,416\\nFleming–Harrington estimator,\\n412,413,485\\nKalbﬂeisch–Prentice estimator,\\n484,485\\nKaplan–Meier estimator,\\n409–413,414–416,420\\nmultiple state estimator, 416,\\n420\\nNelson estimator, 412,413,418,\\n485\\nstandard error, 412\\nsurvival package, 131,\\n418,422,499,513,517,536survplot,135,419,448,458,460\\nsurvreg,131,448\\nsurvreg.auxinfo ,449\\nsurvreg.distributions ,449\\nT\\ntest of linearity, seehypothesis\\ntest\\ntest statistic, seehypothesis test\\ntime to event, 399\\nand severity of event, 417\\ntime-dependent covariable,\\n322,418,447,499–503,\\n513,518,526\\nTitanic, seedatasets\\ntraining sample, 111–113,122\\ntransace,176,177\\ntranscan,51,55,80,83,\\n83–85,129,135,138,167,\\n170–172,175–177,\\n276,277,330,334,335,521,\\n525\\ntransform both sides regression,\\n176,389,392\\ntransformation, 389,393,395\\npost,133\\npre,179\\ntree model, seerecursive\\npartitioning\\ntruncation, 401\\nU\\nunconditioning, 119\\nuniqueness analysis, 94\\nunivariable screening, 72\\nunivarLR,134,135\\nunsupervised learning, 79\\nV\\nval.prob,109,135,271\\nval.surv,109,449,517\\nvalidate,135,141,142,\\n260,269,271,282,286,\\n300,301,319,323,354,466,\\n517', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n",
              " Document(id_='eb60950e-c727-4432-a1d9-edd6187b4818', embedding=None, metadata={'page_label': '582', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='582 Index\\nvalidation of model, 109–116,\\n259,299,318,322,353,446,\\n466,506,529\\nbootstrap, 114–116\\ncross,113,115,116,210\\ndata-splitting, 111,112,271\\nexternal, 109,110,237,\\n271,449,517\\nleave-out-one, 113,122,\\n215,255\\nquantities to validate, 110\\nrandomization, 113\\nvarclus,79,129,167,330,458,\\n463\\nvariable selection, 67–72,171\\nstep-down, 70,137,\\n275,280,282,286,377\\nvariance inﬂation factors, 79,135,\\n138,255\\nvariance stabilization, 390variance–covariance matrix,\\n51,54,120,129,189,\\n191,193,196–198,208,\\n211,215\\ncluster sandwich, 197,202\\nHuber–White estimator, 147\\nsandwich, 147,211,217\\nvariogram, 148,153\\nvcov,134,135\\nvif,135,138\\nW\\nwaiting time, 401\\nWald statistic, 186,189,191,192,\\n194,196,198,206,244,278\\nweighted analysis, seemaximum\\nlikelihood\\nwhich.influence ,134,137,269\\nworking independence model, 197', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vi5qvrTp-t6t",
        "outputId": "3460ca96-aa7f-41c1-ff76-f4c63bb88798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-embeddings-huggingface in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: huggingface-hub[inference]>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (0.23.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (0.10.37)\n",
            "Requirement already satisfied: sentence-transformers<3.0.0,>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-huggingface) (2.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.11.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.9.5)\n",
            "Requirement already satisfied: minijinja>=1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.30)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.8)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.1)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.30.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (9.4.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.14.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (4.40.2)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.11.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (4.0.3)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.11)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (6.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.12)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (12.4.127)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (0.4.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (3.5.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers<3.0.0,>=2.6.1->llama-index-embeddings-huggingface) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-huggingface) (1.1.1)\n",
            "Requirement already satisfied: llama-index-embeddings-instructor in /usr/local/lib/python3.10/dist-packages (0.1.3)\n",
            "Requirement already satisfied: instructorembedding<2.0.0,>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-instructor) (1.0.1)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-instructor) (0.10.37)\n",
            "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-instructor) (2.7.0)\n",
            "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index-embeddings-instructor) (2.2.1+cu121)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.6.1)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.30.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.31.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.14.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (4.40.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (0.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (1.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (12.4.127)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (4.0.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (24.0)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.11)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.7.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (6.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.0.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (0.4.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (3.21.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->llama-index-embeddings-instructor) (3.5.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-embeddings-instructor) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (2.18.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (0.16.0)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-embeddings-instructor) (1.1.1)\n"
          ]
        }
      ],
      "source": [
        "%pip install llama-index-embeddings-huggingface\n",
        "%pip install llama-index-embeddings-instructor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding Model Setup:"
      ],
      "metadata": {
        "id": "_-o-7ETiwWOB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rxD7gro_kBO",
        "outputId": "9fb544d5-b2f1-45eb-b555-8c7d346444d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "embed_model=HuggingFaceEmbedding(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prompt Template Setup"
      ],
      "metadata": {
        "id": "CUVtTq1yweP4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28w_4vds__Cw"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import PromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bg517ZhMCtGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3535ae5c-e19d-4c39-954d-b28c5783a177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_id\" has conflict with protected namespace \"model_\".\n",
            "\n",
            "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.llms.huggingface import HuggingFaceLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xttY2wVFAurg"
      },
      "outputs": [],
      "source": [
        "system_prompt=\"\"\"\"<|SYSTEM|># You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5Ef0VGC7uy"
      },
      "outputs": [],
      "source": [
        "# This will wrap the default prompts that are internal to llama-index\n",
        "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qofq-le6UXKc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj2cWYf4DEc-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "3e648c31-b03d-49f8-fddb-eca1af074082"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-17-a3e7acf5a1ac>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-a3e7acf5a1ac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1. '''system & assistant(LLM)\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "1. '''system & assistant(LLM)\n",
        "2. user'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssB66cocIgAC"
      },
      "source": [
        "model_name: str = Field(\n",
        "        default=DEFAULT_HUGGINGFACE_MODEL,\n",
        "        description=(\n",
        "            \"The model name to use from HuggingFace. \"\n",
        "            \"Unused if `model` is passed in directly.\"\n",
        "        ),\n",
        "    )\n",
        "    context_window: int = Field(\n",
        "        default=DEFAULT_CONTEXT_WINDOW,\n",
        "        description=\"The maximum number of tokens available for input.\",\n",
        "        gt=0,\n",
        "    )\n",
        "    max_new_tokens: int = Field(\n",
        "        default=DEFAULT_NUM_OUTPUTS,\n",
        "        description=\"The maximum number of tokens to generate.\",\n",
        "        gt=0,\n",
        "    )\n",
        "    system_prompt: str = Field(\n",
        "        default=\"\",\n",
        "        description=(\n",
        "            \"The system prompt, containing any extra instructions or context. \"\n",
        "            \"The model card on HuggingFace should specify if this is needed.\"\n",
        "        ),\n",
        "    )\n",
        "    query_wrapper_prompt: PromptTemplate = Field(\n",
        "        default=PromptTemplate(\"{query_str}\"),\n",
        "        description=(\n",
        "            \"The query wrapper prompt, containing the query placeholder. \"\n",
        "            \"The model card on HuggingFace should specify if this is needed. \"\n",
        "            \"Should contain a `{query_str}` placeholder.\"\n",
        "        ),\n",
        "    )\n",
        "    tokenizer_name: str = Field(\n",
        "        default=DEFAULT_HUGGINGFACE_MODEL,\n",
        "        description=(\n",
        "            \"The name of the tokenizer to use from HuggingFace. \"\n",
        "            \"Unused if `tokenizer` is passed in directly.\"\n",
        "        ),\n",
        "    )\n",
        "    device_map: str = Field(\n",
        "        default=\"auto\", description=\"The device_map to use. Defaults to 'auto'.\"\n",
        "    )\n",
        "    stopping_ids: List[int] = Field(\n",
        "        default_factory=list,\n",
        "        description=(\n",
        "            \"The stopping ids to use. \"\n",
        "            \"Generation stops when these token IDs are predicted.\"\n",
        "        ),\n",
        "    )\n",
        "    tokenizer_outputs_to_remove: list = Field(\n",
        "        default_factory=list,\n",
        "        description=(\n",
        "            \"The outputs to remove from the tokenizer. \"\n",
        "            \"Sometimes huggingface tokenizers return extra inputs that cause errors.\"\n",
        "        ),\n",
        "    )\n",
        "    tokenizer_kwargs: dict = Field(\n",
        "        default_factory=dict, description=\"The kwargs to pass to the tokenizer.\"\n",
        "    )\n",
        "    model_kwargs: dict = Field(\n",
        "        default_factory=dict,\n",
        "        description=\"The kwargs to pass to the model during initialization.\",\n",
        "    )\n",
        "    generate_kwargs: dict = Field(\n",
        "        default_factory=dict,\n",
        "        description=\"The kwargs to pass to the model during generation.\",\n",
        "    )\n",
        "    is_chat_model: bool = Field(\n",
        "        default=False,\n",
        "        description=(\n",
        "            LLMMetadata.__fields__[\"is_chat_model\"].field_info.description\n",
        "            + \" Be sure to verify that you either pass an appropriate tokenizer \"\n",
        "            \"that can convert prompts to properly formatted chat messages or a \"\n",
        "            \"`messages_to_prompt` that does so.\"\n",
        "        ),\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Add your Hugging face credentials here"
      ],
      "metadata": {
        "id": "stgJ2N-CwuTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "f9e8b85d3861478b8d1dc6643aac5971",
            "38454d2dd56344cf9cda53c660d84814",
            "2a31cbea702e4dc995f1d067117c1c81",
            "488dbeb2076f41e5a2c4dca1bdfbc266",
            "70efe59f8b594174a4c381445281e5d9",
            "a0ed164a02374125abd61d7cf262c398",
            "72d6085d366f47f0a49ba90c2a66da93",
            "1e782e815b824c7a809cac3fa8e7610a",
            "65063994d67b4ca6aadc892e02956b56",
            "2d064a529ebd4663a80b8a1d3d4886ab",
            "bc9589c76dc746c8ada02dbe888373e6",
            "7614f73049034173bdc116adb3f65062",
            "d0b2210be94b43f3bd627324cb46701e",
            "dfbacc2739e143c4988bad79a6fd6eba",
            "2d6eacb99afe4c70b5fc8572bc762bd8",
            "a340a1e8a2244de8a6fae6cde9773efd",
            "ff84982425cd4637bda5dab9cfb191ad",
            "0774cab667c144a98c4ad57584d5261a",
            "27a5617835534ce18493e260ef56eecf",
            "35c2b5b1a87b48678cdbf259b1b176a8",
            "743b0e6bafc846a8a18cd977cf04498d",
            "dac5d70c811e452e9d7d395a1a4b1597",
            "c2e334a8a3e54f41aa14ccd94e447bae",
            "f79f846a999a44c5bf0f3ad2f8a191e1",
            "a64d9892733b425f86976f0018bbe2d9",
            "32bee8d5fbf441668692111c95b55771",
            "594c94b722674da8b926d26074ea3bd6",
            "1c67a6847dae4fd7bfa3684492da8a08",
            "c3db7dce4df84924b9832b4bfa7c134f",
            "5ba52e312be44477b2e62122f079c34a",
            "77d6e9b19aab4f448efe4053548d548e",
            "962ed3418acc4adc88072563732277ac"
          ]
        },
        "id": "n6ij9i-tAn6I",
        "outputId": "50237c08-0a11-4cff-f0f2-906854a684e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9e8b85d3861478b8d1dc6643aac5971"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating and Querying the LLM Setup with HuggingFace Models\n",
        "## Setting Up HuggingFace LLM:"
      ],
      "metadata": {
        "id": "5NciWPKcw3tH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "36e2f0a035e5475fb4ea463d90375101",
            "472d5aa51969481380b1842f30b18aa9",
            "5fcb0c7b3dbd49b0a1bb951de6b4a73d",
            "2d46d2df9ac64a00b329911178cdea6a",
            "5ab13aa7cf1e4dc4ac7d3a150512e658",
            "2e6db3ff3059480e9bf1420849f77fd6",
            "58b6d295f3c74d1d90abf0e954109057",
            "9259a0f74de24d3eb0cbcc407b622352",
            "82de1ec5476f40b899278478391b0c87",
            "6d2d51ad3e87477f833e9f14221f52f7",
            "5676410ac99c48729b0b647f30d0ad5f"
          ]
        },
        "id": "IxrxKMsiCltf",
        "outputId": "5b56f295-ffe4-45d8-dadb-50e9864a71d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36e2f0a035e5475fb4ea463d90375101"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "llm=HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    query_wrapper_prompt=query_wrapper_prompt,\n",
        "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    model_name=\"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "    device_map=\"auto\",\n",
        "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
        "    tokenizer_kwargs={\"max_length\": 4096},\n",
        "    # uncomment this if using CUDA to reduce memory usage\n",
        "    model_kwargs={\"torch_dtype\": torch.float16}\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creates a service context with the specified LLM and embedding model, then creates an index from the loaded documents.**"
      ],
      "metadata": {
        "id": "8Q18dZRpw9Rj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFSjT22yEMqC",
        "outputId": "378508e9-d2c1-4b95-c6c1-c67be2a1a386"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-f0b118d025f9>:3: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  Service_Context=ServiceContext.from_defaults(\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import ServiceContext\n",
        "\n",
        "Service_Context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIQlOscsKnhx"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import VectorStoreIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQAwKWzELKsw"
      },
      "outputs": [],
      "source": [
        "index=VectorStoreIndex.from_documents(documents,service_context=Service_Context)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sets up a query engine and queries the index with a specific question, printing the response.**"
      ],
      "metadata": {
        "id": "ABADZdmCxFfv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d0MGJ5pLb6h"
      },
      "outputs": [],
      "source": [
        "query_engine=index.as_query_engine(k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcPnjVGxLkyw",
        "outputId": "eb94aae6-0cef-4ceb-f09f-869e6e19846f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "result=query_engine.query(\"If we have highly Imbalance data and my goal is not to loose any of the potential customer what would my best Modelling stratedy and how do I proceed with it? \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "INmas8eIMpZ1",
        "outputId": "16c8f877-709f-460b-c8c0-86b508155541"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n\\nIf you have highly imbalanced data and your goal is not to lose any potential customers, your best modeling strategy would be to use a cost-sensitive learning algorithm. This type of algorithm takes into account the cost of misclassifying a particular class, which in this case would be the potential customers.\\n\\nTo proceed with this strategy, you would first need to determine the cost of misclassifying each class. This can be done by assigning a cost to each class based on its importance or value to your business. Once you have determined the costs, you can then use a cost-sensitive learning algorithm to train your model. This algorithm will take into account the costs and adjust the model's predictions accordingly.\\n\\nIt is also important to note that you should also consider using techniques such as oversampling, undersampling, or a combination of both to balance your data. This will help to ensure that your model is not biased towards the majority class and is able to accurately predict the potential customers.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "result.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8OXBNLlL0gw",
        "outputId": "a1dc36c1-2b29-4592-bbe0-6e0497d7f00a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Response(response=\"\\n\\nIf you have highly imbalanced data and your goal is not to lose any potential customers, your best modeling strategy would be to use a cost-sensitive learning algorithm. This type of algorithm takes into account the cost of misclassifying a particular class, which in this case would be the potential customers.\\n\\nTo proceed with this strategy, you would first need to determine the cost of misclassifying each class. This can be done by assigning a cost to each class based on its importance or value to your business. Once you have determined the costs, you can then use a cost-sensitive learning algorithm to train your model. This algorithm will take into account the costs and adjust the model's predictions accordingly.\\n\\nIt is also important to note that you should also consider using techniques such as oversampling, undersampling, or a combination of both to balance your data. This will help to ensure that your model is not biased towards the majority class and is able to accurately predict the potential customers.\", source_nodes=[NodeWithScore(node=TextNode(id_='b1838d1d-89ae-4a8c-9e6d-41ff88a08fe4', embedding=None, metadata={'page_label': 'C1', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='35c48751-b987-44ea-97e1-1d9ef77358ca', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': 'C1', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, hash='88c8e2efa96593dff1023ad0e63886e356b018884040cdf9ea14fd439e46f73f')}, text='Regression \\nModeling \\nStrategiesFrank E. Harrell, Jr.\\nWith Applications to Linear Models, \\nLogistic and Ordinal Regression, \\nand Survival Analysis\\nSecond EditionSpringer Series in Statistics', start_char_idx=0, end_char_idx=190, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.3902429864573474), NodeWithScore(node=TextNode(id_='df2ef5b3-d110-4dc4-8a72-30e1a2abeb59', embedding=None, metadata={'page_label': 'iii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='846c8095-5d7d-48e2-97fb-b2d8f146483e', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': 'iii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, hash='255480babe444d9d50c0cce448ead376993db3ff4451b2b112203b6efdee3dde')}, text='Frank E. Harrell, Jr.\\nRegression Modeling\\nStrategies\\nWith Applications to Linear Models,\\nLogistic and Ordinal Regression,\\nand Survival Analysis\\nSecond Edition\\n123', start_char_idx=0, end_char_idx=162, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), score=0.38759005803234375)], metadata={'b1838d1d-89ae-4a8c-9e6d-41ff88a08fe4': {'page_label': 'C1', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}, 'df2ef5b3-d110-4dc4-8a72-30e1a2abeb59': {'page_label': 'iii', 'file_name': '2015_Book_RegressionModelingStrategies.pdf', 'file_path': '/content/Data/2015_Book_RegressionModelingStrategies.pdf', 'file_type': 'application/pdf', 'file_size': 7590830, 'creation_date': '2024-05-17', 'last_modified_date': '2024-05-17'}})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWLOLzulM_J5",
        "outputId": "993f6be1-b680-4d07-f329-e0faafa24b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "If you have highly imbalanced data and your goal is not to lose any potential customers, your best modeling strategy would be to use a cost-sensitive learning algorithm. This type of algorithm takes into account the cost of misclassifying a particular class, which in this case would be the potential customers.\n",
            "\n",
            "To proceed with this strategy, you would first need to determine the cost of misclassifying each class. This can be done by assigning a cost to each class based on its importance or value to your business. Once you have determined the costs, you can then use a cost-sensitive learning algorithm to train your model. This algorithm will take into account the costs and adjust the model's predictions accordingly.\n",
            "\n",
            "It is also important to note that you should also consider using techniques such as oversampling, undersampling, or a combination of both to balance your data. This will help to ensure that your model is not biased towards the majority class and is able to accurately predict the potential customers.\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19ZNwPrHNfaz",
        "outputId": "9eab0bcd-0d2e-4877-eee3-6e6c08f4f85b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention mechanism is not mentioned in the provided context information.\n"
          ]
        }
      ],
      "source": [
        "result2=query_engine.query(\"what is attention mechnisam?\")\n",
        "print(result2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YhZspl5wOJKk",
        "outputId": "e68903f2-e393-4f7d-e252-e1c407e4284c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Attention mechanism is not mentioned in the provided context information.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "result2.response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTlbTuGDN4hx"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "3fV_AH7YN08a",
        "outputId": "c24b8d30-5f76-4c24-acc5-58ef41a8385a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<b>\n\nIf you have highly imbalanced data and your goal is not to lose any potential customers, your best modeling strategy would be to use a cost-sensitive learning algorithm. This type of algorithm takes into account the cost of misclassifying a particular class, which in this case would be the potential customers.\n\nTo proceed with this strategy, you would first need to determine the cost of misclassifying each class. This can be done by assigning a cost to each class based on its importance or value to your business. Once you have determined the costs, you can then use a cost-sensitive learning algorithm to train your model. This algorithm will take into account the costs and adjust the model's predictions accordingly.\n\nIt is also important to note that you should also consider using techniques such as oversampling, undersampling, or a combination of both to balance your data. This will help to ensure that your model is not biased towards the majority class and is able to accurately predict the potential customers.</b>"
          },
          "metadata": {}
        }
      ],
      "source": [
        "display(Markdown(f\"<b>{result}</b>\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4qJGVHxO3ah"
      },
      "outputs": [],
      "source": [
        "# fixing unicode error in google colab\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BK47n3GOQ4y"
      },
      "outputs": [],
      "source": [
        "# WEAVIATE_CLUSTER=\"https://aamir-iqbal-wr2uvybi.weaviate.network\"\n",
        "# WEAVIATE_API_KEY=\"CLbmSREHHVRDdFbaXV9FVYxzFi8fIWyFO6gb\"\n",
        "\n",
        "# import weaviate\n",
        "# client=weaviate.Client(url=WEAVIATE_CLUSTER,auth_client_secret=weaviate.AuthApiKey(WEAVIATE_API_KEY))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sBhh01z7OjdQ",
        "outputId": "14ebfaf4-ed61-4c68-ac40-f1b86aff96e5",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index-vector-stores-weaviate in /usr/local/lib/python3.10/dist-packages (1.0.0)\n",
            "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-vector-stores-weaviate) (0.10.37)\n",
            "Collecting weaviate-client<5.0.0,>=4.5.7 (from llama-index-vector-stores-weaviate)\n",
            "  Using cached weaviate_client-4.6.1-py3-none-any.whl (324 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.6.6)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.27.0)\n",
            "Requirement already satisfied: jsonpath-ng<2.0.0,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.6.1)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.30.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.31.0)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.7.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (4.11.0)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.14.1)\n",
            "Collecting validators==0.28.1 (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate)\n",
            "  Using cached validators-0.28.1-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: authlib<2.0.0,>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.3.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (2.7.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.63.0)\n",
            "Requirement already satisfied: grpcio-tools<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.63.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.57.0 in /usr/local/lib/python3.10/dist-packages (from weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.63.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (4.0.3)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.10/dist-packages (from authlib<2.0.0,>=1.2.1->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (42.0.7)\n",
            "Requirement already satisfied: protobuf<6.0dev,>=5.26.1 in /usr/local/lib/python3.10/dist-packages (from grpcio-health-checking<2.0.0,>=1.57.0->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (5.26.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from grpcio-tools<2.0.0,>=1.57.0->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (67.7.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.14.0)\n",
            "Requirement already satisfied: ply in /usr/local/lib/python3.10/dist-packages (from jsonpath-ng<2.0.0,>=1.6.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2023.12.25)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.0->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.0.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (6.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.1.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.4.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (3.21.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.2.1)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.16.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.1.4)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (0.16.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography->authlib<2.0.0,>=1.2.1->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (2.1.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography->authlib<2.0.0,>=1.2.1->weaviate-client<5.0.0,>=4.5.7->llama-index-vector-stores-weaviate) (2.22)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.7.1->llama-index-core<0.11.0,>=0.10.1->llama-index-vector-stores-weaviate) (1.1.1)\n",
            "Installing collected packages: validators, weaviate-client\n",
            "  Attempting uninstall: validators\n",
            "    Found existing installation: validators 0.22.0\n",
            "    Uninstalling validators-0.22.0:\n",
            "      Successfully uninstalled validators-0.22.0\n",
            "  Attempting uninstall: weaviate-client\n",
            "    Found existing installation: weaviate-client 4.5.5\n",
            "    Uninstalling weaviate-client-4.5.5:\n",
            "      Successfully uninstalled weaviate-client-4.5.5\n",
            "Successfully installed validators-0.28.1 weaviate-client-4.6.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "validators",
                  "weaviate"
                ]
              },
              "id": "780904e68b6a4ac2bae2ef4f24e378a5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# !pip install llama-index==0.10.28\n",
        "# !pip install weaviate-client==4.5.5\n",
        "!pip install llama-index-vector-stores-weaviate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The above embeddings were done in locale, here we are setting up weaviate vector database and querying results from that.\n",
        "The first part sets up a basic pipeline to load data, embed it, and query it locally.\n",
        "The latter part will enhances this setup by integrating with Weaviate, a vector database, to store and retrieve embeddings efficiently, especially useful for larger datasets and more complex queries.\n",
        "# Weaviate Vector Store Integration:\n",
        "**Sets up and integrates Weaviate as a vector store with LlamaIndex, creating an index from documents stored in Weaviate.**"
      ],
      "metadata": {
        "id": "Olj5HsyRxVef"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vidnyVPGOocY"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores.weaviate import WeaviateVectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dYDyyf_RBbl"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import StorageContext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_url = \"https://aamir-iqbal-wr2uvybi.weaviate.network\"\n",
        "api_key = \"CLbmSREHHVRDdFbaXV9FVYxsdfzsdFi8fIWyFO6gb\"\n",
        "\n",
        "client = weaviate.connect_to_wcs(\n",
        "    cluster_url=cluster_url,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(api_key),\n",
        ")\n",
        "\n",
        "# local\n",
        "# client = weaviate.connect_to_local()"
      ],
      "metadata": {
        "id": "ZORJt1unOiDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERdVVojZPRCg"
      },
      "outputs": [],
      "source": [
        "vector_store=WeaviateVectorStore(weaviate_client=client,index_name=\"Llamaindex\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_y5UXNyuQeq7"
      },
      "outputs": [],
      "source": [
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_a9j7MMxRvyI"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context,embed_model=embed_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Queries the Weaviate-integrated index with the same question, printing the response.**"
      ],
      "metadata": {
        "id": "EYTbWhCyx1DQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EbCc0ElvRzph"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine(llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result3=query_engine.query(\"If we have highly Imbalance data and my goal is not to loose any of the potential customer what would my best Modelling stratedy and how do I proceed with it? \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnmIVs24QTGn",
        "outputId": "e8f72d26-8283-4575-a60a-a2204b1be286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRXjqWALSZ71",
        "outputId": "dc24a29d-67a2-4900-f3d4-76286b45295a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "If you have highly imbalanced data and your goal is not to lose any potential customers, your best modeling strategy would be to use a cost-sensitive learning algorithm. This type of algorithm takes into account the cost of misclassifying a particular class, which in this case would be the potential customers.\n",
            "\n",
            "To proceed with this strategy, you would first need to determine the cost of misclassifying each class. This can be done by assigning a cost to each class based on its importance or value to your business. Once you have determined the costs, you can then use a cost-sensitive learning algorithm to train your model. This algorithm will take into account the costs and adjust the model's predictions accordingly.\n",
            "\n",
            "It is also important to note that you should also consider using techniques such as oversampling, undersampling, or a combination of both to balance your data. This will help to ensure that your model is not biased towards the majority class and is able to accurately predict the potential customers.\n"
          ]
        }
      ],
      "source": [
        "print(result3.response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result3=query_engine.query('How to evaluate highly imbalanced data?')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh4nHwg0S1si",
        "outputId": "49fa1d24-9904-42ce-e35f-5def78ffb7e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result3.response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwH-w_wgTP7E",
        "outputId": "00285c52-411d-4ff1-da2b-68afd736a62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "When evaluating highly imbalanced data, it is important to consider both discrimination and calibration measures. Discrimination measures such as the Brier score, Spearman’s ρ, Kendall’s τ, and Somers’ Dxy can be used to assess the model’s ability to distinguish between the two classes. Calibration measures such as the calibration slope and intercept, mean absolute calibration error, and 0.9 quantile of calibration error can be used to assess the model’s ability to predict the correct class probabilities. Additionally, it may be useful to use partial g-indexes, which are robust and efficient measures of variation. When comparing two models, rank measures such as ROC area (cindex) may not be very sensitive, while measures such as R2 and the model χ2 statistic can be more sensitive. It is also important to consider the characteristics of the subjects where the differences are greatest, as large differences may be caused by an omitted, underweighted, or improperly transformed predictor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result3=query_engine.query('How to measue the model performance?')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_yUsBYjTSNd",
        "outputId": "9bb6ee3f-77e2-4cac-f1a5-9149b650fca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result3.response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8LpS1_uWUWg",
        "outputId": "8ffc319e-edbf-421b-a9b1-6a245debfd4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3p8wJHBTEc6"
      },
      "outputs": [],
      "source": [
        "multimodelrag evaltion of rag system"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f9e8b85d3861478b8d1dc6643aac5971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_743b0e6bafc846a8a18cd977cf04498d",
              "IPY_MODEL_dac5d70c811e452e9d7d395a1a4b1597",
              "IPY_MODEL_c2e334a8a3e54f41aa14ccd94e447bae",
              "IPY_MODEL_f79f846a999a44c5bf0f3ad2f8a191e1"
            ],
            "layout": "IPY_MODEL_72d6085d366f47f0a49ba90c2a66da93"
          }
        },
        "38454d2dd56344cf9cda53c660d84814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e782e815b824c7a809cac3fa8e7610a",
            "placeholder": "​",
            "style": "IPY_MODEL_65063994d67b4ca6aadc892e02956b56",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2a31cbea702e4dc995f1d067117c1c81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2d064a529ebd4663a80b8a1d3d4886ab",
            "placeholder": "​",
            "style": "IPY_MODEL_bc9589c76dc746c8ada02dbe888373e6",
            "value": ""
          }
        },
        "488dbeb2076f41e5a2c4dca1bdfbc266": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_7614f73049034173bdc116adb3f65062",
            "style": "IPY_MODEL_d0b2210be94b43f3bd627324cb46701e",
            "value": true
          }
        },
        "70efe59f8b594174a4c381445281e5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_dfbacc2739e143c4988bad79a6fd6eba",
            "style": "IPY_MODEL_2d6eacb99afe4c70b5fc8572bc762bd8",
            "tooltip": ""
          }
        },
        "a0ed164a02374125abd61d7cf262c398": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a340a1e8a2244de8a6fae6cde9773efd",
            "placeholder": "​",
            "style": "IPY_MODEL_ff84982425cd4637bda5dab9cfb191ad",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "72d6085d366f47f0a49ba90c2a66da93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "1e782e815b824c7a809cac3fa8e7610a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65063994d67b4ca6aadc892e02956b56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2d064a529ebd4663a80b8a1d3d4886ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc9589c76dc746c8ada02dbe888373e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7614f73049034173bdc116adb3f65062": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0b2210be94b43f3bd627324cb46701e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfbacc2739e143c4988bad79a6fd6eba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d6eacb99afe4c70b5fc8572bc762bd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "a340a1e8a2244de8a6fae6cde9773efd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff84982425cd4637bda5dab9cfb191ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0774cab667c144a98c4ad57584d5261a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27a5617835534ce18493e260ef56eecf",
            "placeholder": "​",
            "style": "IPY_MODEL_35c2b5b1a87b48678cdbf259b1b176a8",
            "value": "Connecting..."
          }
        },
        "27a5617835534ce18493e260ef56eecf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35c2b5b1a87b48678cdbf259b1b176a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "743b0e6bafc846a8a18cd977cf04498d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a64d9892733b425f86976f0018bbe2d9",
            "placeholder": "​",
            "style": "IPY_MODEL_32bee8d5fbf441668692111c95b55771",
            "value": "Token is valid (permission: read)."
          }
        },
        "dac5d70c811e452e9d7d395a1a4b1597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_594c94b722674da8b926d26074ea3bd6",
            "placeholder": "​",
            "style": "IPY_MODEL_1c67a6847dae4fd7bfa3684492da8a08",
            "value": "Your token has been saved in your configured git credential helpers (store)."
          }
        },
        "c2e334a8a3e54f41aa14ccd94e447bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3db7dce4df84924b9832b4bfa7c134f",
            "placeholder": "​",
            "style": "IPY_MODEL_5ba52e312be44477b2e62122f079c34a",
            "value": "Your token has been saved to /root/.cache/huggingface/token"
          }
        },
        "f79f846a999a44c5bf0f3ad2f8a191e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77d6e9b19aab4f448efe4053548d548e",
            "placeholder": "​",
            "style": "IPY_MODEL_962ed3418acc4adc88072563732277ac",
            "value": "Login successful"
          }
        },
        "a64d9892733b425f86976f0018bbe2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32bee8d5fbf441668692111c95b55771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "594c94b722674da8b926d26074ea3bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c67a6847dae4fd7bfa3684492da8a08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3db7dce4df84924b9832b4bfa7c134f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ba52e312be44477b2e62122f079c34a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "77d6e9b19aab4f448efe4053548d548e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962ed3418acc4adc88072563732277ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36e2f0a035e5475fb4ea463d90375101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_472d5aa51969481380b1842f30b18aa9",
              "IPY_MODEL_5fcb0c7b3dbd49b0a1bb951de6b4a73d",
              "IPY_MODEL_2d46d2df9ac64a00b329911178cdea6a"
            ],
            "layout": "IPY_MODEL_5ab13aa7cf1e4dc4ac7d3a150512e658"
          }
        },
        "472d5aa51969481380b1842f30b18aa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e6db3ff3059480e9bf1420849f77fd6",
            "placeholder": "​",
            "style": "IPY_MODEL_58b6d295f3c74d1d90abf0e954109057",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5fcb0c7b3dbd49b0a1bb951de6b4a73d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9259a0f74de24d3eb0cbcc407b622352",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82de1ec5476f40b899278478391b0c87",
            "value": 2
          }
        },
        "2d46d2df9ac64a00b329911178cdea6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d2d51ad3e87477f833e9f14221f52f7",
            "placeholder": "​",
            "style": "IPY_MODEL_5676410ac99c48729b0b647f30d0ad5f",
            "value": " 2/2 [01:11&lt;00:00, 33.28s/it]"
          }
        },
        "5ab13aa7cf1e4dc4ac7d3a150512e658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e6db3ff3059480e9bf1420849f77fd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58b6d295f3c74d1d90abf0e954109057": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9259a0f74de24d3eb0cbcc407b622352": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82de1ec5476f40b899278478391b0c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6d2d51ad3e87477f833e9f14221f52f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5676410ac99c48729b0b647f30d0ad5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}